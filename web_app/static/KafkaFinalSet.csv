,File Name,Commit msg,Complexity,Nol Added,Nol Removed,No of lines in file,Token Count,No of Changed Methods,No of Methods,Code Churn Files Count,Code Churn Files Max,Code Churn Files Avg,Commits Count,Hunks Count,Added Lines Count,Added Lines Max,Added Lines avg,Removed Lines Count,Removed Lines max,Removed Lines avg,BugLabelLevel,CommitBug,AddRemoved,CommitAddRemoved
streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java,streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java,"MINOR: Apply try-with-resource to KafkaStreamsTest (#10668)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Matthias J. Sax <matthias@confluent.io>",71,199,181,989,8763,24,51,1150,153,11,107,4,2980,418,28,1830,460,17,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignor.java,"KAFKA-12675: improve the sticky general assignor scalability and performance (#10552)

I did code refactor/optimization, keep the same algorithm in this PR.

Originally, With this setting:
topicCount = 50;
partitionCount = 800;
consumerCount = 800;
We complete in 10 seconds, after my code refactor, the time down to 100~200 ms

With the 1 million partitions setting:
topicCount = 500;
partitionCount = 2000;
consumerCount = 2000;
No OutOfMemory will be thrown anymore. The time will take 4~5 seconds.

Reviewers: Vahid Hashemian <vahid.hashemian@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",194,314,197,809,6922,30,44,1205,864,120,10,5.5,1639,864,164,434,197,43,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java,"KAFKA-12675: improve the sticky general assignor scalability and performance (#10552)

I did code refactor/optimization, keep the same algorithm in this PR.

Originally, With this setting:
topicCount = 50;
partitionCount = 800;
consumerCount = 800;
We complete in 10 seconds, after my code refactor, the time down to 100~200 ms

With the 1 million partitions setting:
topicCount = 500;
partitionCount = 2000;
consumerCount = 2000;
No OutOfMemory will be thrown anymore. The time will take 4~5 seconds.

Reviewers: Vahid Hashemian <vahid.hashemian@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",28,72,59,202,2385,7,8,252,689,19,13,4,1333,689,103,1081,818,83,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractStickyAssignorTest.java,"KAFKA-12675: improve the sticky general assignor scalability and performance (#10552)

I did code refactor/optimization, keep the same algorithm in this PR.

Originally, With this setting:
topicCount = 50;
partitionCount = 800;
consumerCount = 800;
We complete in 10 seconds, after my code refactor, the time down to 100~200 ms

With the 1 million partitions setting:
topicCount = 500;
partitionCount = 2000;
consumerCount = 2000;
No OutOfMemory will be thrown anymore. The time will take 4~5 seconds.

Reviewers: Vahid Hashemian <vahid.hashemian@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",95,73,79,677,7557,16,39,919,799,131,7,7,1096,799,157,177,79,25,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Rate.java,"KAFKA-12880: Remove deprecated `Count` and `SampledTotal` in 3.0 (#10808)

They were both deprecated in Apache Kafka 2.4 and it's a straightforward change
to use the non deprecated variants.

Reviewers: David Jacot <djacot@confluent.io>",10,0,7,51,377,0,9,104,85,6,17,1,227,85,13,123,22,7,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java,"KAFKA-12867: Fix ConsumeBenchWorker exit behavior for maxMessages config (#10797)

The trogdor ConsumeBenchWorker allows several consumption tasks to be run in parallel, the number is configurable using the threadsPerWorker config. If one of the consumption tasks completes executing successfully due to maxMessages being consumed, then, the consumption task prematurely notifies the doneFuture causing the entire ConsumeBenchWorker to halt. This becomes a problem when more than 1 consumption task is running in parallel, because the successful completion of 1 of the tasks shuts down the entire worker while the other tasks are still running. When the worker is shut down, it kills all the active consumption tasks, though they have not consumed maxMessages yet. This commit defers notification of the doneFuture to the CloseStatusUpdater thread, which is already responsible for tracking the status of the tasks and updating their status when all of the tasks complete.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",68,1,1,466,3315,2,41,566,298,51,11,2,672,298,61,106,50,10,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerInterceptor.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerInterceptor.java,"MINOR: remove unneccessary public keyword from ProducerInterceptor/ConsumerInterceptor interface (#10801)

Co-authored-by: “KahnCheny” <“kahn.cheny@gmail.com”>

Reviewers: Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>",0,3,3,9,88,0,0,83,72,12,7,1,99,72,14,16,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/ProducerInterceptor.java,clients/src/main/java/org/apache/kafka/clients/producer/ProducerInterceptor.java,"MINOR: remove unneccessary public keyword from ProducerInterceptor/ConsumerInterceptor interface (#10801)

Co-authored-by: “KahnCheny” <“kahn.cheny@gmail.com”>

Reviewers: Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>",0,3,3,7,66,0,0,95,88,19,5,1,102,88,20,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionOptions.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",1,8,0,11,56,1,1,31,23,16,2,1.0,31,23,16,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/Admin.java,clients/src/main/java/org/apache/kafka/clients/admin/Admin.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",48,22,0,238,1902,1,48,1562,937,56,28,2.0,1768,937,63,206,103,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeProducersResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeProducersResult.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",9,7,0,54,383,1,6,83,76,42,2,1.0,83,76,42,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeTransactionsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeTransactionsOptions.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",1,7,0,12,63,1,1,38,31,19,2,1.0,38,31,19,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java,clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",538,31,18,3859,31951,6,141,4848,1065,28,174,4.0,7283,1065,42,2435,203,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListTransactionsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListTransactionsOptions.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",5,91,0,33,198,5,5,91,91,91,1,1,91,91,91,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/ListTransactionsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListTransactionsResult.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",10,125,0,66,534,4,4,125,125,125,1,1,125,125,125,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/TransactionListing.java,clients/src/main/java/org/apache/kafka/clients/admin/TransactionListing.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",12,74,0,48,237,7,7,74,74,74,1,1,74,74,74,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/AbortTransactionHandler.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AbortTransactionHandler.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",23,8,6,141,1135,3,8,186,184,93,2,2.0,192,184,96,6,6,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiDriver.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiDriver.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",48,82,78,306,2002,18,30,469,465,234,2,12.5,547,465,274,78,78,39,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiFuture.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiFuture.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",12,125,0,58,418,11,11,125,125,125,1,1,125,125,125,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiHandler.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiHandler.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",5,15,55,55,413,4,5,124,139,31,4,2.0,181,139,45,57,55,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiLookupStrategy.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiLookupStrategy.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",5,24,0,43,360,2,5,128,88,43,3,2,128,88,43,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",30,1,1,151,893,0,20,258,247,29,9,1,300,247,33,42,24,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategy.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategy.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",31,212,0,153,1071,19,19,212,212,212,1,1,212,212,212,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/ApiRequestScope.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/ApiRequestScope.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",1,5,1,7,39,0,1,46,42,23,2,1.0,47,42,24,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeProducersHandler.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeProducersHandler.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",18,17,17,163,1169,5,7,205,205,68,3,2,224,205,75,19,17,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandler.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandler.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",21,10,7,151,1006,5,9,192,189,96,2,3.5,199,189,100,7,7,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/ListTransactionsHandler.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/ListTransactionsHandler.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",13,130,0,99,749,7,7,130,130,130,1,1,130,130,130,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/StaticBrokerStrategy.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/StaticBrokerStrategy.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",6,65,0,33,196,6,6,65,65,65,1,1,65,65,65,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java,clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",253,55,0,4772,49523,1,202,5918,500,44,133,5,7725,508,58,1807,279,14,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/ListTransactionsResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/ListTransactionsResultTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",3,119,0,79,816,3,3,119,119,119,1,1,119,119,119,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java,clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",175,5,0,844,6342,1,75,983,293,22,45,2,1192,387,26,209,94,5,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/AdminApiDriverTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/AdminApiDriverTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",62,120,87,601,5159,37,53,792,759,264,3,2,881,759,294,89,87,30,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategyIntegrationTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategyIntegrationTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",12,247,0,187,2009,11,11,247,247,247,1,1,247,247,247,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategyTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/AllBrokersStrategyTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",5,124,0,87,863,5,5,124,124,124,1,1,124,124,124,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/admin/internals/DescribeProducersHandlerTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/DescribeProducersHandlerTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",22,12,19,267,2109,6,18,328,335,164,2,6.5,347,335,174,19,19,10,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandlerTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandlerTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",15,3,4,182,1401,3,13,228,229,114,2,2.0,232,229,116,4,4,2,1,0,1,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/ListTransactionsHandlerTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/ListTransactionsHandlerTest.java,"KAFKA-12709; Add Admin API for `ListTransactions` (#10616)

This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. 

Reviewers: David Jacot <djacot@confluent.io>",12,185,0,150,1289,11,11,185,185,185,1,1,185,185,185,0,0,0,0,0,0,0
core/src/main/scala/kafka/admin/TopicCommand.scala,core/src/main/scala/kafka/admin/TopicCommand.scala,"KAFKA-12596: remove --zookeeper option from topic command (#10457)

Also:
* Remove `ZookeeperTopicService`
* Remove `TopicCommandWithZKClientTest`
* Fix a topic create validation bug
* Adjust existing tests

Reviewers: Ismael Juma <ismael@juma.me.uk>",106,37,216,499,4472,15,33,581,232,5,110,3.0,1834,448,17,1253,216,11,2,1,0,1
core/src/test/scala/unit/kafka/admin/TopicCommandIntegrationTest.scala,core/src/test/scala/unit/kafka/admin/TopicCommandIntegrationTest.scala,"KAFKA-12596: remove --zookeeper option from topic command (#10457)

Also:
* Remove `ZookeeperTopicService`
* Remove `TopicCommandWithZKClientTest`
* Fix a topic create validation bug
* Adjust existing tests

Reviewers: Ismael Juma <ismael@juma.me.uk>",53,23,18,622,5439,11,47,814,574,22,37,2,1099,574,30,285,93,8,2,1,0,1
core/src/test/scala/unit/kafka/admin/TopicCommandTest.scala,core/src/test/scala/unit/kafka/admin/TopicCommandTest.scala,"KAFKA-12596: remove --zookeeper option from topic command (#10457)

Also:
* Remove `ZookeeperTopicService`
* Remove `TopicCommandWithZKClientTest`
* Fix a topic create validation bug
* Adjust existing tests

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,22,2,118,752,5,11,151,82,30,5,3,167,84,33,16,10,3,2,1,0,1
core/src/main/scala/kafka/zk/KafkaZkClient.scala,core/src/main/scala/kafka/zk/KafkaZkClient.scala,"KAFKA-12866: Avoid root access to Zookeeper (#10795)

The broker shouldn't assume create access to the chroot. There are
deployement scenarios where the chroot is already created is the only
znode which the broker can access.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",313,3,1,1233,9816,1,130,2032,684,30,68,4.0,3140,684,46,1108,218,16,2,1,0,1
core/src/test/scala/unit/kafka/zk/ZkClientAclTest.scala,core/src/test/scala/unit/kafka/zk/ZkClientAclTest.scala,"KAFKA-12866: Avoid root access to Zookeeper (#10795)

The broker shouldn't assume create access to the chroot. There are
deployement scenarios where the chroot is already created is the only
znode which the broker can access.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",3,63,0,35,280,3,3,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java,streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",71,0,6,892,6090,0,30,1496,201,9,164,2.0,3027,355,18,1531,293,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggregate.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggregate.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",12,2,2,103,791,1,8,142,171,4,33,2,330,171,10,188,89,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoin.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",16,2,2,144,1308,1,5,215,97,8,28,3.5,342,105,12,127,32,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoinProcessor.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoinProcessor.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",8,2,2,56,532,1,4,88,62,5,17,3,132,62,8,44,8,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamReduce.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamReduce.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",12,2,2,98,726,1,8,139,167,5,27,3,316,167,12,177,89,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregate.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregate.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",16,3,6,170,1333,1,10,214,165,8,26,3.5,350,165,13,136,25,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregate.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",68,3,5,428,3037,1,20,535,307,107,5,8,647,307,129,112,65,22,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregate.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregate.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",14,3,5,163,1187,1,9,209,171,6,36,3.0,428,171,12,219,43,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableInnerJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableInnerJoin.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",20,2,2,125,998,1,13,172,115,6,28,3.0,325,115,12,153,34,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoin.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",22,2,2,130,1020,1,13,178,112,7,25,4,283,112,11,105,18,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableOuterJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableOuterJoin.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",26,2,2,138,1068,1,13,186,112,7,25,4,292,112,12,106,18,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableRightJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableRightJoin.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",18,2,2,121,954,1,13,169,113,7,25,3,276,113,11,107,18,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",13,2,2,91,682,1,8,129,78,5,25,4,255,78,10,126,31,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionProcessorSupplier.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",12,1,1,80,766,1,5,118,114,15,8,1.5,134,114,17,16,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionSendProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignJoinSubscriptionSendProcessorSupplier.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",16,1,1,125,1074,1,4,167,116,21,8,5.5,200,116,25,33,8,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionStoreReceiveProcessorSupplier.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",10,1,1,77,719,1,2,115,112,12,10,1.5,136,112,14,21,9,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",17,2,2,113,806,1,6,154,110,5,29,2,233,110,8,79,24,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNode.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",28,9,41,140,1004,5,16,196,99,5,41,3,676,105,16,480,140,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",39,1,1,230,1688,1,12,311,123,6,51,4,1025,219,20,714,207,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordQueue.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",24,1,1,121,803,1,13,214,140,6,37,3,432,140,12,218,75,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNode.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNode.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",8,1,1,63,515,1,8,101,64,3,33,2,251,64,8,150,23,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",187,1,7,886,5745,1,61,1257,352,6,202,4.0,4119,422,20,2862,465,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",140,1,6,858,5794,1,56,1255,477,4,294,4.0,7464,562,25,6209,633,21,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",9,0,235,197,1047,9,9,231,420,33,7,3,499,420,71,268,235,38,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImpl.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",95,4,21,725,4788,4,66,839,210,16,53,4,1716,210,32,877,100,17,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",12,20,67,239,1200,6,12,278,268,28,10,3.0,382,268,38,104,67,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetrics.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",18,11,22,320,1747,11,18,359,179,24,15,5,684,179,46,325,91,22,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",36,1,3,266,1873,1,25,334,299,20,17,1,415,299,24,81,41,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",63,1,3,404,2769,1,30,524,378,35,15,3,596,378,40,72,14,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",68,0,4,428,3165,1,34,564,347,28,20,4.0,905,347,45,341,106,17,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",86,1,3,477,3315,1,40,597,393,27,22,2.5,994,393,45,397,119,18,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",47,10,10,310,2593,3,30,383,273,7,56,4.0,1474,273,26,1091,197,19,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",36,6,8,341,2384,4,22,396,169,14,29,5,732,169,25,336,119,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowStore.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",35,5,5,281,2134,3,22,512,200,11,45,3,929,200,21,417,105,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/NamedCacheMetrics.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/NamedCacheMetrics.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",2,8,38,37,213,1,2,60,86,12,5,3,106,86,21,46,38,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",19,20,161,425,2144,32,19,481,569,80,6,3.0,658,569,110,177,161,30,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java,streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",128,0,7,976,7581,1,123,1149,131,15,75,3,1628,148,22,479,64,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",31,69,200,655,5988,26,25,735,532,28,26,6.0,1792,532,69,1057,251,41,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImplTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",55,2,49,627,6543,8,55,769,223,17,46,7.0,1998,226,43,1229,196,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",81,27,48,907,8686,3,22,1756,578,37,47,6,3708,778,79,1952,407,42,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",17,4,45,276,2154,8,14,344,146,11,31,4,642,146,21,298,45,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregateProcessorTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregateProcessorTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",18,51,130,447,3626,15,16,540,286,13,43,3,1065,286,25,525,130,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregateTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregateTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",6,42,128,392,5054,14,6,459,155,9,51,5,2329,243,46,1870,255,37,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableInnerJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableInnerJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",57,1,20,326,4064,4,9,480,180,12,41,6,1596,212,39,1116,205,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",62,1,20,383,4853,4,6,545,189,13,42,5.0,1506,189,36,961,233,23,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableOuterJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableOuterJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",50,1,20,301,4005,4,5,437,196,12,35,5,1390,205,40,953,240,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableRightJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableRightJoinTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",1,1,19,37,387,3,1,63,53,9,7,1,92,53,13,29,19,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",6,0,53,256,2574,4,6,322,117,9,37,5,832,117,22,510,97,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",5,7,118,160,1293,3,2,199,203,17,12,6.0,427,203,36,228,118,19,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorNodeTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorNodeTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",15,16,44,136,1265,3,13,182,65,5,36,3.0,429,65,12,247,47,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/SourceNodeTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/SourceNodeTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",5,20,42,80,841,4,5,109,65,10,11,3,176,65,16,67,42,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",121,62,112,2053,17117,47,109,2571,201,16,161,5,6897,548,43,4326,1131,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",86,21,120,2321,19615,14,64,2830,389,14,197,4,8060,458,41,5230,1062,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",9,1,189,180,1084,11,9,218,383,36,6,1.5,447,383,74,229,189,38,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImplTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsImplTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",77,13,70,1016,8266,21,65,1212,206,35,35,7,1764,312,50,552,158,16,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",9,34,128,211,1421,7,9,260,263,29,9,3,412,263,46,152,128,17,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ThreadMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",16,18,45,365,2319,17,16,436,244,29,15,13,882,244,59,446,128,30,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",31,20,54,409,4151,4,21,511,487,32,16,3.5,656,487,41,145,54,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractSessionBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractSessionBytesStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",43,20,57,573,7023,4,36,724,553,43,17,5,958,553,56,234,66,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractWindowBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractWindowBytesStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",47,20,59,1035,10009,4,40,1223,1104,68,18,5.0,1703,1104,95,480,195,27,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemorySessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemorySessionStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",3,0,5,52,581,1,3,86,502,17,5,1,540,502,108,454,432,91,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryWindowStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",4,0,11,104,1122,2,4,162,476,15,11,2,719,476,65,557,518,51,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",37,21,45,402,3285,7,36,496,202,18,28,4.0,739,202,26,243,51,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",46,30,45,482,4027,16,45,607,236,22,27,4,869,236,32,262,62,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",30,7,45,395,3238,5,26,479,257,28,17,5,660,257,39,181,51,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",32,106,131,387,3280,17,31,489,189,14,35,3,920,189,26,431,131,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",2,0,5,39,383,1,2,66,156,2,34,2.0,569,156,17,503,304,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",12,0,11,560,4334,2,7,646,672,9,69,4,4695,688,68,4049,1159,59,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/NamedCacheMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/NamedCacheMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",2,6,40,59,540,3,2,85,117,17,5,1,134,117,27,49,40,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",20,31,147,349,1878,20,20,403,457,81,5,3,563,457,113,160,147,32,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java,streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",149,1,1,869,6430,1,78,1349,624,15,93,2,2140,624,23,791,114,9,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/processor/MockProcessorContext.java,streams/test-utils/src/main/java/org/apache/kafka/streams/processor/MockProcessorContext.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",64,1,1,346,2046,1,51,566,478,18,32,2.0,687,478,21,121,21,4,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/processor/api/MockProcessorContext.java,streams/test-utils/src/main/java/org/apache/kafka/streams/processor/api/MockProcessorContext.java,"KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4 (#10765)

As specified in KIP-743, this PR removes the built-in metrics
in Streams that are superseded by the refactoring proposed in KIP-444.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Luke Chen <showuon@gmail.com>",59,1,1,314,2067,1,44,498,494,124,4,1.0,506,494,126,8,6,2,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java,raft/src/test/java/org/apache/kafka/raft/RaftEventSimulationTest.java,"MINOR: replace by org.junit.jupiter.api.Tag by net.jqwik.api.Tag for raft module (#10791)

The command `./gradlew raft:integrationTest`  can't run any integration test since `org.junit.jupiter.api.Tag` does not work for jqwik engine (see https://github.com/jlink/jqwik/issues/36#issuecomment-436535760). 

Reviewers: Ismael Juma <ismael@juma.me.uk>",141,1,1,925,6943,0,83,1184,1122,51,23,5,1842,1122,80,658,348,29,2,1,0,1
core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataListenerTest.scala,core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataListenerTest.scala,"HOTFIX: fix build error (#10796)

Fix compile error in scala tests.

The compile error is:

```
[Error] /home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-9229/core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataListenerTest.scala:97: polymorphic expression cannot be instantiated to expected type;

[2021-05-29T02:34:50.308Z]  found   : [T]()T

[2021-05-29T02:34:50.308Z]  required: kafka.server.RequestLocal
```

This error happens only in scala 2.12

Reviewers: Bruno Cadonna <cadonna@apache.org>",7,1,1,163,1078,1,6,204,187,51,4,1.0,223,187,56,19,17,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/compress/ZstdFactory.java,clients/src/main/java/org/apache/kafka/common/compress/ZstdFactory.java,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,16,1,42,298,1,3,73,58,36,2,2.0,74,58,37,1,1,0,2,1,0,1
core/src/main/scala/kafka/cluster/Partition.scala,core/src/main/scala/kafka/cluster/Partition.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",169,3,3,950,5989,3,54,1426,208,7,202,3.0,4185,284,21,2759,248,14,2,1,0,1
core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala,core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",256,48,32,1280,7857,29,64,1627,632,16,99,5,3317,632,34,1690,162,17,2,1,0,1
core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala,core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",191,16,12,930,6946,10,50,1313,952,10,133,4,3799,952,29,2486,372,19,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionCoordinator.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",121,21,12,535,3203,11,22,694,434,15,47,5,1532,434,33,838,260,18,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",49,3,4,333,2106,1,18,432,159,10,42,2.0,810,197,19,378,64,9,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",98,7,5,538,3531,3,22,740,429,13,59,3,1435,429,24,695,184,12,2,1,0,1
core/src/main/scala/kafka/log/Log.scala,core/src/main/scala/kafka/log/Log.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",266,11,5,1606,10440,6,101,2776,361,9,300,3.0,7897,436,26,5121,634,17,2,1,0,1
core/src/main/scala/kafka/log/LogValidator.scala,core/src/main/scala/kafka/log/LogValidator.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",77,18,9,417,2785,7,12,591,239,14,42,4.0,1377,239,33,786,106,19,2,1,0,1
core/src/main/scala/kafka/raft/KafkaMetadataLog.scala,core/src/main/scala/kafka/raft/KafkaMetadataLog.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",68,3,3,346,2073,1,25,456,169,25,18,2.5,638,195,35,182,71,10,2,1,0,1
core/src/main/scala/kafka/server/ControllerApis.scala,core/src/main/scala/kafka/server/ControllerApis.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",126,4,4,683,5511,4,35,770,453,55,14,4.5,1106,453,79,336,114,24,2,1,0,1
core/src/main/scala/kafka/server/KafkaApis.scala,core/src/main/scala/kafka/server/KafkaApis.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",580,52,42,2778,21589,40,144,3381,219,7,512,4.0,13304,326,26,9923,380,19,2,1,0,1
core/src/main/scala/kafka/server/KafkaRequestHandler.scala,core/src/main/scala/kafka/server/KafkaRequestHandler.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",46,12,6,281,2077,4,23,375,79,7,52,3.0,758,88,15,383,59,7,2,1,0,1
core/src/main/scala/kafka/server/ReplicaManager.scala,core/src/main/scala/kafka/server/ReplicaManager.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",270,6,5,1496,10247,4,70,2036,185,7,301,3,6341,254,21,4305,244,14,2,1,0,1
core/src/main/scala/kafka/server/RequestLocal.scala,core/src/main/scala/kafka/server/RequestLocal.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,37,0,9,66,1,1,37,37,37,1,1,37,37,37,0,0,0,2,1,0,1
core/src/main/scala/kafka/server/metadata/BrokerMetadataListener.scala,core/src/main/scala/kafka/server/metadata/BrokerMetadataListener.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",51,2,2,220,1562,1,22,291,269,58,5,2,362,269,72,71,37,14,2,1,0,1
core/src/main/scala/kafka/tools/TestRaftRequestHandler.scala,core/src/main/scala/kafka/tools/TestRaftRequestHandler.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,2,2,78,606,2,8,112,179,9,13,3,280,179,22,168,77,13,2,1,0,1
core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala,core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",30,5,4,298,2181,3,19,398,291,22,18,3.0,499,291,28,101,39,6,2,1,0,1
core/src/test/scala/unit/kafka/cluster/PartitionTest.scala,core/src/test/scala/unit/kafka/cluster/PartitionTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",125,9,6,1632,13420,3,75,2060,321,32,64,5.0,3287,382,51,1227,153,19,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala,core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",31,9,3,171,1371,5,24,230,226,19,12,1.5,261,226,22,31,7,3,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala,core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",226,25,14,3046,27210,8,183,4110,907,40,104,3.5,5712,907,55,1602,306,15,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala,core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataManagerTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",134,23,21,1879,16653,11,96,2462,407,31,80,5.0,3139,407,39,677,50,8,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorConcurrencyTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",75,7,5,493,4191,1,37,626,388,28,22,2.0,716,388,33,90,19,4,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",74,16,0,966,9263,15,73,1249,787,43,29,5,2108,787,73,859,246,30,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelManagerTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelManagerTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,4,0,396,3831,4,12,495,283,17,29,4,919,283,32,424,184,15,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionStateManagerTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",55,21,16,655,6255,10,44,877,354,18,48,2.0,1267,354,26,390,72,8,2,1,0,1
core/src/test/scala/unit/kafka/log/LogValidatorTest.scala,core/src/test/scala/unit/kafka/log/LogValidatorTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",101,81,43,1405,9401,40,78,1560,395,42,37,7,2340,395,63,780,118,21,2,1,0,1
core/src/test/scala/unit/kafka/server/ControllerApisTest.scala,core/src/test/scala/unit/kafka/server/ControllerApisTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",27,2,1,606,5864,1,27,669,264,56,12,4.0,778,284,65,109,39,9,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaApisTest.scala,core/src/test/scala/unit/kafka/server/KafkaApisTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",257,91,58,3165,25149,41,188,3894,647,30,131,3,5470,756,42,1576,270,12,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala,core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",107,1,1,2055,18183,1,92,2521,221,21,119,5,3808,225,32,1287,120,11,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/BaseRecordBatchBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/BaseRecordBatchBenchmark.java,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,4,4,101,799,1,3,149,149,50,3,1,154,149,51,5,4,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/CompressedRecordBatchValidationBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/CompressedRecordBatchValidationBenchmark.java,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,2,1,44,372,1,2,65,64,32,2,1.0,66,64,33,1,1,0,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/RecordBatchIterationBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/RecordBatchIterationBenchmark.java,"MINOR: Reduce allocations in requests via buffer caching (#9229)

Use a caching `BufferSupplier` per request handler thread so that
decompression buffers are cached if supported by the underlying
`CompressionType`. This achieves a similar outcome as #9220, but
with less contention.

We introduce a `RequestLocal` class to make it easier to introduce
new request scoped stateful instances (one example we discussed
previously was an `ActionQueue` that could be used to avoid
some of the complex group coordinator locking).

This is a small win for zstd (no synchronization or soft references) and
a more significant win for lz4. In particular, it reduces allocations
significantly when the number of partitions is high. The decompression
buffer size is typically 64 KB, so a produce request with 1000 partitions
results in 64 MB of allocations even if each produce batch is small (likely,
when there are so many partitions).

I did a quick producer perf local test with 5000 partitions, 1 KB record
size,
1 broker, lz4 and ~0.5 for the producer compression rate metric:

Before this change:
> 20000000 records sent, 346314.349535 records/sec (330.27 MB/sec),
148.33 ms avg latency, 2267.00 ms max latency, 115 ms 50th, 383 ms 95th, 777 ms 99th, 1514 ms 99.9th.

After this change:
> 20000000 records sent, 431956.113259 records/sec (411.95 MB/sec),
117.79 ms avg latency, 1219.00 ms max latency, 99 ms 50th, 295 ms 95th, 440 ms 99th, 662 ms 99.9th.

That's a 25% throughput improvement and p999 latency was reduced to
under half (in this test).

Default arguments will be removed in a subsequent PR.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",12,6,8,63,544,3,4,86,145,12,7,6,224,145,32,138,116,20,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableFilter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableFilter.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",27,41,35,133,972,22,13,186,87,9,21,4,296,87,14,110,35,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",150,27,3,965,8780,4,60,1200,250,11,111,5,3300,284,30,2100,258,19,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableNewProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableNewProcessorSupplier.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",0,40,0,6,76,0,0,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionCacheFlushListener.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionCacheFlushListener.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",3,22,7,41,380,5,3,63,38,6,11,3,108,38,10,45,8,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedCacheFlushListener.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedCacheFlushListener.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",5,35,8,56,465,6,4,80,53,20,4,3.0,98,53,24,18,8,4,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedTupleForwarder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedTupleForwarder.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",11,27,5,54,461,4,5,85,53,17,5,2,94,53,19,9,5,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/To.java,streams/src/main/java/org/apache/kafka/streams/processor/To.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",13,7,0,51,267,1,9,99,68,20,5,1,100,68,20,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContext.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",33,1,1,175,931,0,27,243,195,8,32,2.0,354,195,11,111,30,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImpl.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",27,1,1,100,749,0,15,143,81,5,30,3.0,294,81,10,151,25,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalProcessorContext.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",1,2,2,41,410,0,1,121,49,5,24,1.0,154,49,6,33,6,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",456,27,24,1635,12090,13,156,2053,1491,26,79,4,3333,1491,42,1280,112,16,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",63,1,1,259,1702,0,25,333,214,4,77,3,1583,214,21,1250,437,16,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorTopology.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",76,16,16,191,1590,7,22,257,65,11,24,5.5,487,84,20,230,60,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordDeserializer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordDeserializer.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",6,3,3,76,497,3,3,106,66,8,14,3.0,146,68,10,40,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",6,4,4,67,561,4,6,110,64,3,35,2,255,64,7,145,19,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CacheFlushListener.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CacheFlushListener.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",0,8,0,7,89,0,0,42,35,8,5,2,54,35,11,12,6,2,1,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",338,4,4,2635,22288,2,227,3050,997,40,77,6,4591,1460,60,1541,463,20,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableReduceTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableReduceTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",3,1,1,55,554,1,3,84,81,17,5,2,100,81,20,16,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionCacheFlushListenerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionCacheFlushListenerTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",1,1,1,31,268,1,1,52,52,13,4,1.0,55,52,14,3,1,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimestampedCacheFlushListenerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimestampedCacheFlushListenerTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",2,5,4,51,416,2,2,76,75,19,4,2.0,84,75,21,8,4,2,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimestampedTupleForwarderTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimestampedTupleForwarderTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",6,21,6,78,691,3,5,111,89,22,5,2,127,89,25,16,6,3,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractProcessorContextTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",35,1,1,190,1368,0,33,248,173,7,36,2.0,383,173,11,135,26,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateTaskTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateTaskTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",19,3,3,199,1608,1,14,243,144,11,22,3.5,385,144,18,142,53,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyFactories.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyFactories.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",3,1,1,30,204,2,3,53,53,13,4,1.0,58,53,14,5,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordDeserializerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordDeserializerTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",6,1,1,78,566,0,4,105,109,6,18,2.5,175,109,10,70,28,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordQueueTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",14,4,2,316,3470,0,13,414,116,12,35,3,623,116,18,209,27,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",3,4,4,35,354,0,2,61,145,3,22,2.0,357,145,16,296,102,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java,streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",21,1,0,183,1501,0,14,422,441,7,61,2,885,441,15,463,86,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractKeyValueStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",37,2,0,430,3702,0,32,562,191,16,36,2.5,870,191,24,308,167,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CacheFlushListenerStub.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CacheFlushListenerStub.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",3,12,0,39,302,1,3,61,49,30,2,1.5,61,49,30,0,0,0,1,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",52,2,2,499,4534,2,45,582,151,15,40,3.0,862,151,22,280,55,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemorySessionStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",52,16,2,650,7064,3,47,770,204,18,43,3,1211,204,28,441,90,10,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentSessionStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",50,16,2,663,6863,3,45,781,766,260,3,1,783,766,261,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CachingPersistentWindowStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",68,2,2,806,8116,2,51,933,198,18,51,3,1343,216,26,410,71,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",20,2,1,184,1753,1,19,229,165,11,21,2,327,165,16,98,37,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedKeyValueBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedKeyValueBytesStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",19,2,1,176,1813,1,19,224,195,45,5,2,271,195,54,47,41,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",32,9,2,313,2589,1,32,392,199,18,22,3.0,517,199,24,125,46,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",13,1,0,181,1721,0,9,234,164,13,18,2.0,284,165,16,50,16,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryLRUCacheStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryLRUCacheStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",7,1,0,114,1068,0,5,164,148,8,20,2.0,381,148,19,217,123,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueSegmentsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueSegmentsTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",28,1,1,288,2559,1,23,353,193,15,23,3,550,193,24,197,42,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",14,1,1,211,1676,1,10,251,92,17,15,2,350,92,23,99,76,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",63,2,1,737,6311,1,48,894,130,18,50,5.5,1576,130,32,682,71,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStoreTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",11,1,1,174,1696,1,11,239,169,80,3,1,258,187,86,19,18,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentIteratorTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",11,2,1,172,1651,1,10,223,148,10,23,2,349,148,15,126,36,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedSegmentsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedSegmentsTest.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",28,1,1,289,2559,1,23,354,315,51,7,2,366,315,52,12,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java,streams/src/test/java/org/apache/kafka/test/InternalMockProcessorContext.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",78,4,4,391,2421,4,42,461,143,6,80,3.0,1039,143,13,578,100,7,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockInternalProcessorContext.java,streams/src/test/java/org/apache/kafka/test/MockInternalProcessorContext.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",25,1,1,127,877,0,25,172,104,13,13,3,266,104,20,94,66,7,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockProcessorNode.java,streams/src/test/java/org/apache/kafka/test/MockProcessorNode.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",7,1,1,41,325,2,7,71,49,5,13,3,112,49,9,41,13,3,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockSourceNode.java,streams/src/test/java/org/apache/kafka/test/MockSourceNode.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,35,289,2,4,60,46,5,13,2,89,46,7,29,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java,streams/src/test/java/org/apache/kafka/test/NoOpProcessorContext.java,"KAFKA-8410: KTableProcessor migration groundwork (#10744)

* Lay the groundwork for migrating KTable Processors to the new PAPI.
* Migrate the KTableFilter processor to prove that the groundwork works.

This is an effort to help break up #10507 into multiple PRs.

Reviewers: Boyang Chen <boyang@apache.org>",20,1,1,112,840,0,20,152,84,6,26,1.0,215,84,8,63,12,2,2,1,0,1
release.py,release.py,"MINOR: Add a release script that helps generate release candidates.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Gwen Shapira

Closes #2795 from ewencp/release-script and squashes the following commits:

f1d0590 [Ewen Cheslack-Postava] Don't expose promotion to the user since it is not implemented yet.
1a6947a [Ewen Cheslack-Postava] Handle cleanup if there's a failure during generation of release notes.
fa58401 [Ewen Cheslack-Postava] Fix hard-coded uses of trunk
639bcca [Ewen Cheslack-Postava] Try to cleanup after most failures.
a3a7245 [Ewen Cheslack-Postava] Fix SCRIPT_DIR to be an absolute path so git clones against the REPO work when it is also your cwd
de54c97 [Ewen Cheslack-Postava] Load/save preferences in a .release-settings.json file so you don't have to keep entering the same info repeatedly
b559a61 [Ewen Cheslack-Postava] Check that the user doesn't have any oustanding diffs before starting the rest of the script
ff0b330 [Ewen Cheslack-Postava] Store original starting branch to switch back to instead of using a default.
b793562 [Ewen Cheslack-Postava] Use 2.12 instead of specific Scala version so we use the default 2.12 version.
382b7f9 [Ewen Cheslack-Postava] MINOR: Add a release script that helps generate release candidates.##MINOR: Fix sftp_mkdir in release.py

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3789 from ijuma/ftp-release-py-fixes##MINOR: Use sha512 instead of sha2 suffix in release artifacts

As per Apache guidelines:

http://www.apache.org/dev/release-distribution#sigs-and-sums

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3844 from ijuma/fix-sha512-naming##MINOR: update release script for streams quickstart

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>

Closes #3846 from dguy/minor-release-script##HOTFIX: Updates on release.py before 1.0.0

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4054 from guozhangwang/KMinor-pre-1.0-release##MINOR: Update release script with new remote, better error handling, correct mvn deploy profile

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Damian Guy <damian.guy@gmail.com>, Ismael Juma <github@juma.me.uk>

Closes #4528 from ewencp/update-release-script##MINOR: Extend release.py with a subcommand for staging docs into the kafka-site repo

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Damian Guy <damian.guy@gmail.com>

Closes #3917 from ewencp/stage-docs##KAFKA-4423: Drop support for Java 7 (KIP-118) and update deps (#5046)

* Set --source, --target and --release to 1.8.
* Build Scala 2.12 by default.
* Remove some conditionals in the build file now that Java 8
is the minimum version.
* Bump the version of Jetty, Jersey and Checkstyle (the newer
versions require Java 8).
* Fixed issues uncovered by the new version if Checkstyle.
* A couple of minor updates to handle an incompatible source
change in the new version of Jetty.
* Add dependency to jersey-hk2 to fix failing tests caused
by Jersey upgrade.
* Update release script to use Java 8 and to take into account
that Scala 2.12 is now built by default.
* While we're at it, bump the version of Gradle, Gradle plugins,
ScalaLogging, JMH and apache directory api.
* Minor documentation updates including the readme and upgrade
notes. A number of Streams Java 7 examples can be removed
subsequently.##MINOR: update release.py (#5374)

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>, Dong Lin <dolin@linkedin.com>##MINOR: Update build.gradle and release.py to upload streams-scala_2.12 (#5368)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>##MINOR: Fix some typos

Just a doc change

Author: John Eismeier <john.eismeier@gmail.com>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4573 from jeis2497052/trunk##KAFKA-7131: Update release script to generate announcement email text

Author: Bibin Sebastian <bisebastian@DELC02QP51SG8WN.sea.corp.expecn.com>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Matthias J. Sax <mjsax@apache.org>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5572 from bibinss/release_mail##KAFKA-7524: Recommend Scala 2.12 and use it for development (#5530)

Scala 2.12 has better support for newer Java versions and includes additional
compiler warnings that are helpful during development. In addition, Scala 2.11
hasn't been supported by the Scala community for a long time, the soon to be
released Spark 2.4.0 will finally support Scala 2.12 (this was the main reason
preventing many from upgrading to Scala 2.12) and Scala 2.13 is at the RC stage.
It's time to start recommending the Scala 2.12 build as we prepare support for
Scala 2.13 and start thinking about removing support for Scala 2.11.

In the meantime, Jenkins will continue to build all supported Scala versions (including
Scala 2.11) so the PR and trunk jobs will fail if people accidentally use methods
introduced in Scala 2.12.

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>##MINOR: Improve maven artifactory url in release.py (#5931)##MINOR: release.py: fix some compatibility problems.

Rather than using sed, use built-in Python regular expressions to strip
the SNAPSHOT expression from the pom.xml files.  Sed has different flags
on different platforms, such as Linux.  Using Python directly here is
more compatible, as well as being more efficient, and not requiring an
rm command afterwards.

When running release_notes.py, use the current Python interpreter.
This is needed to prevent attempting to run release_notes.py with
Python 3 on some systems.  release_notes.py will not (yet) work with
Python 3.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Magnus Edenhill <magnus@edenhill.se>, David Arthur <mumrah@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #6198 from cmccabe/release_py##MINOR: fix release.py script (#6317)

Reviewer: Ewen Cheslack-Postava <ewen@confluent.io>##HOTFIX: Change header back to http instead of https to path license header test (#6347)

Reviewers: Jason Gustafson <jason@confluent.io>##Add retries to release.py script (#8021)##MINOR: Remove unwanted regexReplace on tests/kafkatest/__init__.py

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>

Closes #8072 from omkreddy/release-script##Add log message in release.py (#8461)

When building a release candidate with release.py, if it's not the first RC, we need to drop the previous RC's artifacts from the staging repository before closing the new ones. This adds a log message to remind the release manager of this##MINOR: Update to Gradle 6.3 (#7677)

* Introduce `gradlewAll` script to replace `*All` tasks since the approach
used by the latter doesn't work since Gradle 6.0 and it's unclear when,
if ever, it will work again ( see https://github.com/gradle/gradle/issues/11301).
* Update release script and README given the above.
* Update zinc to 1.3.5.
* Update gradle-versions-plugin to 0.28.0.

The major improvements in Gradle 6.0 to 6.3 are:
- Improved incremental compilation for Scala
- Support for Java 14 (although some Gradle plugins
like spotBugs may need to be updated or disabled,
will do that separately)
- Improved scalac reporting, warnings are clearly
marked as such, which is very helpful.

Tested `gradlewAll` manually for the commands listed in the README
and release script. For `uploadArchive`, I tested it with a local Maven
repository.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>##KAFKA-9907: Switch default build to Scala 2.13 (#8537)

Scala 2.13.2 introduces support for suppressing warnings,
which makes it possible to enable fatal warnings. This is
useful enough from a development perspective to justify
this change.

In addition, Scala 2.13.2 also has a Vector implementation
with significant performance improvements and encoding
of String matches to switches.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>##MINOR: speed up release script (#9070)

Fixes slow release due to establishing a separate SSH connection per file to copy.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>##MINOR: Adjust 'release.py' script to use shell when using gradlewAll and PGP signing, which were required to build the 2.6.0 RCs (#9045)##MINOR: Add releaseTarGz to args for building docs (#9528)

Reviewers: Ismael Juma <ismael@confluent.io>##KAFKA-12415 Prepare for Gradle 7.0 and restrict transitive scope for non api dependencies (#10203)

Gradle 7.0 is required for Java 16 compatibility and it removes a number of
deprecated APIs. Fix most issues preventing the upgrade to Gradle 7.0.
The remaining ones are more complicated and should be handled
in a separate PR. Details of the changes:

* Release tarball no longer includes includes test, sources, javadoc and test sources jars (these
are still published to the Maven Central repository).
* Replace `compile` with `api` or `implementation` - note that `implementation`
dependencies appear with `runtime` scope in the pom file so this is a (positive)
change in behavior
* Add missing dependencies that were uncovered by the usage of `implementation`
* Replace `testCompile` with `testImplementation`
* Replace `runtime` with `runtimeOnly` and `testRuntime` with `testRuntimeOnly`
* Replace `configurations.runtime` with `configurations.runtimeClasspath`
* Replace `configurations.testRuntime` with `configurations.testRuntimeClasspath` (except for
the usage in the `streams` project as that causes a cyclic dependency error)
* Use `java-library` plugin instead of `java`
* Use `maven-publish` plugin instead of deprecated `maven` plugin - this changes the
commands used to publish and to install locally, but task aliases for `install` and
`uploadArchives` were added for backwards compatibility
* Removed `-x signArchives` line from the readme since it was wrong (it was a
no-op before and it fails now, however)
* Replaces `artifacts` block with an approach that works with the `maven-publish` plugin
* Don't publish `jmh-benchmark` module - the shadow jar is pretty large and not
particularly useful (before this PR, we would publish the non shadow jars)
* Replace `version` with `archiveVersion`, `baseName` with `archiveBaseName` and
`classifier` with `archiveClassifier`
* Update Gradle and plugins to the latest stable version (7.0 is not stable yet)
* Use `plugin` DSL to configure plugins
* Updated notable changes for 3.0

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Randall Hauch <rhauch@gmail.com>##MINOR: add missing docs for record-e2e-latency metrics (#10251)

Need to add missing docs for the record-e2e-latency metrics, and new TRACE recording level

Reviewers: Walker Carlson <wcarlson@confluent.io>##MINOR: Use Java 11 for generating aggregated javadoc in release.py (#10399)

Java 11 generates html5 pages with search support, which provides a better user experience.

Fixed `get_jdk` bugs found during testing and updated `release.py` blurb to indicate that
both JDK 8 and JDK 11 are required to perform a release.

Tested by running `python2 release.py stage-docs`, which triggers the `aggregateJavadoc`
path without some of the undesired (for testing) release steps.

Reviewers: John Roesler <vvcephei@apache.org>##KAFKA-12614: Use Jenkinsfile for trunk and release branch builds (#10473)

* Run all JDK/Scala version combinations for trunk/release branch builds.
* Only retry failures in PR builds for now (we can remove this distinction if/when
we report flaky failures as described in KAFKA-12216).
* Disable concurrent builds
* Send email to dev list on build failure
* Use triple double quotes in `doValidation` since we use string interpolation
for `SCALA_VERSION`.
* Update release.py to output new `Unit/integration tests` Jenkins link

Reviewers: Gwen Shapira <cshapi@gmail.com>, David Arthur <mumrah@gmail.com>##KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>##KAFKA-12782: Fix Javadocs generation by upgrading JDK (#10780)

This, upgrades JDK to version 15 for the docs generation, this way we
can circumvent bug https://bugs.openjdk.java.net/browse/JDK-8215291
present in JDK11

Reviewers: Ismael Juma <ismael@juma.me.uk>",69,926,168,604,3044,37,21,758,473,24,31,2,926,473,30,168,29,5,2,1,0,1
clients/src/main/java/org/apache/kafka/server/authorizer/Action.java,clients/src/main/java/org/apache/kafka/server/authorizer/Action.java,"MINOR: make sure all fiedls of o.p.k.s.a.Action are NOT null (#10764)

I'm migrating Ranger's kafka plugin from deprecated Authorizer (this is already removed by 976e78e) to new API (see https://issues.apache.org/jira/browse/RANGER-3231). The kafka plugin needs to take something from field resourcePattern but it does not know whether the field is nullable (or users need to add null check). I check all usages and I don't observe any null case.

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,9,4,68,369,1,9,129,124,64,2,2.5,133,124,66,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java,clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java,"KAFKA-12791: ConcurrentModificationException in AbstractConfig use by KafkaProducer (#10704)

Recently we have noticed multiple instances where KafkaProducers have failed to constructor due to the following exception:

```
org.apache.kafka.common.KafkaException: Failed to construct kafka producer at 
org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:440) at 
org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:291) at 
org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:318) 
java.base/java.lang.Thread.run(Thread.java:832) Caused by: java.util.ConcurrentModificationException at 
java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1584) at 
java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1607) at 
java.base/java.util.AbstractSet.removeAll(AbstractSet.java:171) at 
org.apache.kafka.common.config.AbstractConfig.unused(AbstractConfig.java:221) at 
org.apache.kafka.common.config.AbstractConfig.logUnused(AbstractConfig.java:379) at 
org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:433) ... 9 more 
exception.class:org.apache.kafka.common.KafkaException exception.message:Failed to construct kafka producer
```

This is due to the fact that `used` below is a synchronized set. `used` is being modified while removeAll is being called. This is due to the use of RecordingMap in the Sender thread (see below). Switching to a ConcurrentHashSet avoids this issue as it support concurrent iteration.

```
	at org.apache.kafka.clients.producer.ProducerConfig.ignore(ProducerConfig.java:569)
	at org.apache.kafka.common.config.AbstractConfig$RecordingMap.get(AbstractConfig.java:638)
	at org.apache.kafka.common.network.ChannelBuilders.createPrincipalBuilder(ChannelBuilders.java:242)
	at org.apache.kafka.common.network.PlaintextChannelBuilder$PlaintextAuthenticator.<init>(PlaintextChannelBuilder.java:96)
	at org.apache.kafka.common.network.PlaintextChannelBuilder$PlaintextAuthenticator.<init>(PlaintextChannelBuilder.java:89)
	at org.apache.kafka.common.network.PlaintextChannelBuilder.lambda$buildChannel$0(PlaintextChannelBuilder.java:66)
	at org.apache.kafka.common.network.KafkaChannel.<init>(KafkaChannel.java:174)
	at org.apache.kafka.common.network.KafkaChannel.<init>(KafkaChannel.java:164)
	at org.apache.kafka.common.network.PlaintextChannelBuilder.buildChannel(PlaintextChannelBuilder.java:79)
	at org.apache.kafka.common.network.PlaintextChannelBuilder.buildChannel(PlaintextChannelBuilder.java:67)
	at org.apache.kafka.common.network.Selector.buildAndAttachKafkaChannel(Selector.java:356)
	at org.apache.kafka.common.network.Selector.registerChannel(Selector.java:347)
	at org.apache.kafka.common.network.Selector.connect(Selector.java:274)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1097)
	at org.apache.kafka.clients.NetworkClient.access$700(NetworkClient.java:87)
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1276)
	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1164)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:637)
	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:327)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:242)
```

Reviewers: Ismael Juma <ismael@juma.me.uk>",116,7,3,378,3375,1,48,674,156,12,55,2,839,159,15,165,24,3,2,1,0,1
core/src/main/scala/kafka/server/PartitionMetadataFile.scala,core/src/main/scala/kafka/server/PartitionMetadataFile.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",20,4,2,107,677,2,8,146,144,49,3,2,155,144,52,9,7,3,2,1,0,1
core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala,core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",51,1,1,580,5452,1,42,831,147,20,42,3.0,1180,170,28,349,86,8,2,1,0,1
core/src/test/scala/unit/kafka/log/LogCleanerTest.scala,core/src/test/scala/unit/kafka/log/LogCleanerTest.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",121,1,1,1280,14267,1,79,1816,227,21,87,3,2688,248,31,872,154,10,2,1,0,1
core/src/test/scala/unit/kafka/log/LogLoaderTest.scala,core/src/test/scala/unit/kafka/log/LogLoaderTest.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",84,5,5,1137,10387,6,48,1497,1501,250,6,5.5,1525,1501,254,28,9,5,2,1,0,1
core/src/test/scala/unit/kafka/log/LogTest.scala,core/src/test/scala/unit/kafka/log/LogTest.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",228,4,5,2606,24539,1,165,3628,429,18,197,4,9299,596,47,5671,1494,29,2,1,0,1
core/src/test/scala/unit/kafka/utils/SchedulerTest.scala,core/src/test/scala/unit/kafka/utils/SchedulerTest.scala,"MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups (#10761)

Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.

Also remove an unnecessary parameter in `PartitionTest`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Justine Olshan <jolshan@confluent.io>",11,1,1,140,1104,1,11,185,82,11,17,1,250,82,15,65,22,4,2,1,0,1
core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala,core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala,"KAFKA-12856: Upgrade Jackson to 2.12.3 (#10778)

2.10.x is no longer supported, so we should move to 2.12 for the 3.0
release.

ScalaObjectMapper has been deprecated and it looks like we don't
actually need it, so remove its usage.

Reviewers: David Jacot <djacot@confluent.io>",219,1,2,1008,8521,0,46,1160,310,14,84,3.0,3095,406,37,1935,333,23,2,1,0,1
core/src/main/scala/kafka/log/LogSegments.scala,core/src/main/scala/kafka/log/LogSegments.scala,"MINOR: Improve Log layer segment iteration logic and few other areas (#10684)

In Log.collectAbortedTransactions() I've restored a previously used logic, such that it would handle the case where the starting segment could be null. This was the case previously, but the PR #10401 accidentally changed the behavior causing the code to assume that the starting segment won't be null.

In Log.rebuildProducerState() I've removed usage of the allSegments local variable. The logic looks a bit simpler after I removed it.

I've introduced a new LogSegments.higherSegments() API. This is now used to make the logic a bit more readable in Log. collectAbortedTransactions() and Log.deletableSegments() APIs.

I've removed the unnecessary use of java.lang.Long in LogSegments class' segments map definition.

I've converted a few LogSegments API from public to private, as they need not be public.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Cong Ding <cong@ccding.com>, Jun Rao <junrao@gmail.com>",20,18,7,78,752,4,17,231,220,116,2,4.5,238,220,119,7,7,4,2,1,0,1
core/src/test/scala/unit/kafka/log/LogSegmentsTest.scala,core/src/test/scala/unit/kafka/log/LogSegmentsTest.scala,"MINOR: Improve Log layer segment iteration logic and few other areas (#10684)

In Log.collectAbortedTransactions() I've restored a previously used logic, such that it would handle the case where the starting segment could be null. This was the case previously, but the PR #10401 accidentally changed the behavior causing the code to assume that the starting segment won't be null.

In Log.rebuildProducerState() I've removed usage of the allSegments local variable. The logic looks a bit simpler after I removed it.

I've introduced a new LogSegments.higherSegments() API. This is now used to make the logic a bit more readable in Log. collectAbortedTransactions() and Log.deletableSegments() APIs.

I've removed the unnecessary use of java.lang.Long in LogSegments class' segments map definition.

I've converted a few LogSegments API from public to private, as they need not be public.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Cong Ding <cong@ccding.com>, Jun Rao <junrao@gmail.com>",8,52,7,156,1242,5,8,226,163,56,4,4.5,236,163,59,10,7,2,2,1,0,1
clients/src/test/java/org/apache/kafka/test/TestUtils.java,clients/src/test/java/org/apache/kafka/test/TestUtils.java,"MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout` (#10759)

New parameters in overloaded methods should appear later apart from
lambdas that should always be last.",71,8,8,369,3054,7,43,579,73,9,66,3.0,1012,73,15,433,145,7,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTestEnv.java,metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTestEnv.java,"MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout` (#10759)

New parameters in overloaded methods should appear later apart from
lambdas that should always be last.",12,1,1,63,411,1,4,88,88,29,3,1,90,88,30,2,1,1,2,1,0,1
metadata/src/test/java/org/apache/kafka/metalog/LocalLogManagerTest.java,metadata/src/test/java/org/apache/kafka/metalog/LocalLogManagerTest.java,"MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout` (#10759)

New parameters in overloaded methods should appear later apart from
lambdas that should always be last.",15,1,1,121,1003,1,5,162,153,23,7,1,182,153,26,20,17,3,2,1,0,1
metadata/src/test/java/org/apache/kafka/metalog/LocalLogManagerTestEnv.java,metadata/src/test/java/org/apache/kafka/metalog/LocalLogManagerTestEnv.java,"MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout` (#10759)

New parameters in overloaded methods should appear later apart from
lambdas that should always be last.",20,1,1,106,672,1,7,150,143,38,4,1.0,163,143,41,13,12,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java,"MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout` (#10759)

New parameters in overloaded methods should appear later apart from
lambdas that should always be last.",12,2,2,156,1505,1,8,207,335,11,19,4,443,335,23,236,72,12,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KTable.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KTable.scala,"KAFKA-12796: Removal of deprecated classes under streams-scala (#10710)

Removes previously deprecated methods in older KIPs

Reviewers: Bruno Cadonna <cadonna@apache.org>",10,1,1,143,2442,0,10,693,325,41,17,2,914,366,54,221,71,13,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/CheckpointTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/CheckpointTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",1,8,4,23,225,1,1,44,40,15,3,1,50,40,17,6,4,2,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/HeartbeatTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/HeartbeatTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",1,6,3,20,187,1,1,41,38,14,3,1,46,38,15,5,3,2,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorCheckpointConnectorTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorCheckpointConnectorTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",6,18,12,87,787,6,6,135,67,22,6,2.0,153,67,26,18,12,3,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorCheckpointTaskTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorCheckpointTaskTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",3,34,17,97,939,3,3,141,67,35,4,3.0,162,67,40,21,17,5,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorConfigTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorConfigTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",18,40,24,219,1724,16,18,265,113,38,7,3,311,113,44,46,24,7,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorHeartBeatConnectorTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorHeartBeatConnectorTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",2,2,2,24,197,2,2,52,52,17,3,2,56,52,19,4,2,1,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorHeartbeatTaskTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorHeartbeatTaskTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",2,5,3,21,194,1,1,42,40,21,2,2.0,45,40,22,3,3,2,1,0,1,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorMakerConfigTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorMakerConfigTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",19,10,10,290,2056,4,18,353,234,59,6,6.5,465,234,78,112,83,19,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorSourceConnectorTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorSourceConnectorTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",9,4,4,186,2142,2,9,254,115,28,9,3,321,115,36,67,29,7,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorSourceTaskTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorSourceTaskTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",7,37,25,138,1623,4,5,174,99,35,5,4,228,99,46,54,25,11,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/OffsetSyncStoreTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/OffsetSyncStoreTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",3,10,5,38,320,1,3,72,67,24,3,2,79,67,26,7,5,2,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/OffsetSyncTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/OffsetSyncTest.java,"KAFKA-12819: Add assert messages to MirrorMaker tests plus other quality of life improvements (#10762)

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",1,6,3,21,204,1,1,42,39,14,3,1,47,39,16,5,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TaskId.java,streams/src/main/java/org/apache/kafka/streams/processor/TaskId.java,"MINOR: deprecate TaskMetadata constructor and add KIP-740 notes to upgrade guide (#10755)

Quick followup to KIP-740 to actually deprecate this constructor, and update the upgrade guide with what we changed in KIP-740. I also noticed the TaskId#parse method had been modified previously, and should be re-added to the public TaskId class. It had no tests, so now it does

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Luke Chen <showuon@gmail.com>",35,32,0,119,861,1,14,189,101,12,16,3.0,339,118,21,150,86,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TaskMetadata.java,streams/src/main/java/org/apache/kafka/streams/processor/TaskMetadata.java,"MINOR: deprecate TaskMetadata constructor and add KIP-740 notes to upgrade guide (#10755)

Quick followup to KIP-740 to actually deprecate this constructor, and update the upgrade guide with what we changed in KIP-740. I also noticed the TaskId#parse method had been modified previously, and should be re-added to the public TaskId class. It had no tests, so now it does

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Luke Chen <showuon@gmail.com>",15,13,0,79,472,1,11,137,74,23,6,2.5,143,74,24,6,3,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java,"MINOR: deprecate TaskMetadata constructor and add KIP-740 notes to upgrade guide (#10755)

Quick followup to KIP-740 to actually deprecate this constructor, and update the upgrade guide with what we changed in KIP-740. I also noticed the TaskId#parse method had been modified previously, and should be re-added to the public TaskId class. It had no tests, so now it does

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Luke Chen <showuon@gmail.com>",49,12,0,561,4484,2,44,687,169,20,34,4.0,1065,169,31,378,74,11,2,1,0,1
core/src/main/scala/kafka/log/LogLoader.scala,core/src/main/scala/kafka/log/LogLoader.scala,"MINOR: Log more information when producer snapshot is written (#10757)

This patch logs more information when a producer snapshot is written to the disk.

Reviewers: Ismael Juma <mlists@juma.me.uk>, Lucas Bradstreet <lucas@confluent.io>",41,5,1,309,1954,1,9,534,525,178,3,1,553,525,184,19,18,6,2,1,0,1
core/src/main/scala/kafka/log/ProducerStateManager.scala,core/src/main/scala/kafka/log/ProducerStateManager.scala,"MINOR: Log more information when producer snapshot is written (#10757)

This patch logs more information when a producer snapshot is written to the disk.

Reviewers: Ismael Juma <mlists@juma.me.uk>, Lucas Bradstreet <lucas@confluent.io>",106,7,4,582,4124,2,44,856,590,19,44,3.5,1567,590,36,711,103,16,2,1,0,1
core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala,core/src/test/scala/unit/kafka/log/ProducerStateManagerTest.scala,"MINOR: Log more information when producer snapshot is written (#10757)

This patch logs more information when a producer snapshot is written to the disk.

Reviewers: Ismael Juma <mlists@juma.me.uk>, Lucas Bradstreet <lucas@confluent.io>",50,9,9,739,6554,9,50,978,562,29,34,3.0,1500,562,44,522,200,15,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,"KAFKA-12260: Avoid hitting NPE for partitionsFor (#10017)

Remove null pointer from the public partitionsFor API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",141,3,3,887,6201,1,68,2484,949,9,274,2.0,6259,1407,23,3775,794,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java,clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java,"KAFKA-12260: Avoid hitting NPE for partitionsFor (#10017)

Remove null pointer from the public partitionsFor API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",104,1,1,455,3207,1,65,576,192,9,64,3.0,1033,192,16,457,122,7,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java,"KAFKA-12260: Avoid hitting NPE for partitionsFor (#10017)

Remove null pointer from the public partitionsFor API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",167,22,0,2006,21140,1,119,2712,424,19,141,3,4590,476,33,1878,281,13,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java,"KAFKA-12260: Avoid hitting NPE for partitionsFor (#10017)

Remove null pointer from the public partitionsFor API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,2,2,262,1902,1,17,436,331,21,21,4,540,331,26,104,27,5,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorTopicsIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorTopicsIntegrationTest.java,"KAFKA-12260: Avoid hitting NPE for partitionsFor (#10017)

Remove null pointer from the public partitionsFor API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,6,5,217,2168,1,8,327,321,109,3,2,332,321,111,5,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImplTest.java,"MINOR: add window verification to sliding-window co-group test (#10745)

Reviewers: Luke Chen <showuon@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",13,35,34,196,2540,2,13,264,249,53,5,5,326,249,65,62,34,12,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MessageGenerator.java,generator/src/main/java/org/apache/kafka/message/MessageGenerator.java,"KAFKA-12800: Configure generator to fail on trailing JSON tokens (#10717)

Reviewers: David Jacot <djacot@confluent.io>",34,1,0,270,1842,0,10,367,202,19,19,3,480,202,25,113,34,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java,clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java,"MINOR: clarify message ordering with max in-flight requests and idempotent producer (#10690)

The docs for the max.in.flight.requests.per.connection and enable.idempotence configs currently imply that setting the max in-flight request greater than 1 will break the message ordering guarantee, but that is only true if enable.idempotence is false. When using an idempotent producer, the max in-flight request can be up to 5 without re-ordering messages.

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Ismael Juma <mlists@juma.me.uk>, Luke Chen <showuon@gmail.com>",31,6,6,392,2626,0,13,519,148,5,108,2.0,1158,156,11,639,129,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Produced.java,streams/src/main/java/org/apache/kafka/streams/kstream/Produced.java,"MINOR: update java doc for deprecated methods (#10722)

Reviewers: Matthias J. Sax <matthias@confluent.io>",23,2,6,83,741,0,14,197,163,33,6,2.5,226,163,38,29,15,5,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java,tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java,"MINOR: update java doc for deprecated methods (#10722)

Reviewers: Matthias J. Sax <matthias@confluent.io>",78,1,1,532,3336,0,49,682,611,32,21,2,722,611,34,40,6,2,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",13,1,1,157,1284,1,7,248,127,7,35,3,497,127,14,249,65,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",36,1,1,340,3039,1,17,446,473,15,29,4,1371,473,47,925,302,32,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/EmitOnChangeIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/EmitOnChangeIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",5,1,1,112,849,1,4,143,132,48,3,1,147,132,49,4,3,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",64,8,3,738,6082,3,26,1001,782,25,40,3.0,1707,782,43,706,228,18,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/EosV2UpgradeIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/EosV2UpgradeIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",69,3,2,842,6497,1,26,1209,1105,71,17,2,1528,1105,90,319,172,19,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/integration/FineGrainedAutoResetIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/FineGrainedAutoResetIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,2,2,254,2209,2,12,326,223,13,25,3,519,223,21,193,46,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",17,1,1,383,2785,1,15,458,390,31,15,5,683,390,46,225,76,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",18,1,1,339,3029,1,13,405,280,12,33,4,687,280,21,282,65,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/GlobalThreadShutDownOrderTest.java,streams/src/test/java/org/apache/kafka/streams/integration/GlobalThreadShutDownOrderTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",13,1,1,167,1264,1,11,233,217,15,16,2.5,305,217,19,72,42,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",18,1,1,258,1969,1,11,317,280,63,5,1,355,280,71,38,34,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/InternalTopicIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/InternalTopicIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",13,1,1,184,1856,1,10,262,169,6,46,2.5,594,169,13,332,89,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",40,1,1,953,10134,1,25,1127,472,18,61,3,1917,472,31,790,152,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",35,1,1,641,4791,1,29,817,826,74,11,4,904,826,82,87,47,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",14,1,1,225,2170,1,8,283,254,31,9,5,338,254,38,55,19,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableSourceTopicRestartIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",17,1,1,199,1542,1,15,265,260,19,14,2.0,368,260,26,103,39,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",19,1,1,297,2818,1,9,368,311,46,8,3.0,420,311,52,52,21,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",105,1,1,1034,8796,1,31,1232,380,17,74,5.5,2502,395,34,1270,207,17,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",26,1,1,388,3568,1,15,498,365,10,52,3.0,1198,365,23,700,161,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/ResetPartitionTimeIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/ResetPartitionTimeIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",7,1,1,165,1398,1,7,204,190,23,9,4,237,190,26,33,14,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",34,1,1,384,3602,1,20,502,191,19,27,2,682,191,25,180,55,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",16,1,1,328,2210,1,10,416,216,24,17,4,623,223,37,207,129,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",25,1,1,361,3563,1,11,468,344,31,15,4,681,344,45,213,101,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StoreUpgradeIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StoreUpgradeIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",80,1,1,873,8297,1,34,1075,995,108,10,6.5,1258,995,126,183,123,18,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinTopologyOptimizationIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinTopologyOptimizationIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",17,1,1,207,1639,1,12,268,256,67,4,2.5,274,256,68,6,3,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/SuppressionDurabilityIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/SuppressionDurabilityIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",13,4,3,283,2431,1,11,356,260,36,10,3.0,431,260,43,75,41,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/SuppressionIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/SuppressionIntegrationTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",17,1,1,445,3975,1,15,543,280,30,18,2.5,1089,282,60,546,338,30,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",16,1,1,382,2667,1,10,454,440,32,14,2.0,1033,440,74,579,261,41,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",14,1,1,241,1771,1,8,303,302,25,12,2.5,582,302,48,279,156,23,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java,streams/src/test/java/org/apache/kafka/streams/tests/BrokerCompatibilityTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,129,1199,1,2,166,110,6,30,2.0,264,110,9,98,24,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java,streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",16,1,1,126,1031,1,4,173,150,10,17,3,308,150,18,135,70,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/RelationalSmokeTest.java,streams/src/test/java/org/apache/kafka/streams/tests/RelationalSmokeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",103,13,11,828,5885,1,54,1004,998,201,5,5,1099,998,220,95,49,19,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StaticMemberTestClient.java,streams/src/test/java/org/apache/kafka/streams/tests/StaticMemberTestClient.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,49,476,1,1,82,84,27,3,1,92,84,31,10,9,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsBrokerDownResilienceTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsBrokerDownResilienceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",12,1,1,99,927,1,3,150,139,9,16,3.0,206,139,13,56,10,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsStandByReplicaTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsStandByReplicaTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",16,1,1,121,1176,1,3,169,162,13,13,3,241,162,19,72,37,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",34,1,1,316,2333,1,12,390,268,14,27,3,577,276,21,187,35,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,102,925,1,3,137,136,46,3,1,140,136,47,3,2,1,2,1,0,1
streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",5,1,1,72,582,1,2,103,107,26,4,1.0,114,107,28,11,9,3,2,1,0,1
streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,548,1,1,89,90,30,3,1,96,90,32,7,6,2,1,0,1,1
streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",5,1,1,72,582,1,2,106,110,26,4,1.0,117,110,29,11,9,3,2,1,0,1
streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,548,1,1,88,89,29,3,1,95,89,32,7,6,2,1,0,1,1
streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,66,511,1,2,97,104,16,6,1.0,111,104,18,14,7,2,2,1,0,1
streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-0102/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",3,1,1,52,496,1,1,84,85,28,3,1,89,85,30,5,4,2,1,0,1,1
streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,66,511,1,2,97,104,16,6,1.0,111,104,18,14,7,2,2,1,0,1
streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-0110/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",3,1,1,52,496,1,1,83,87,28,3,1,91,87,30,8,7,3,1,0,1,1
streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,66,513,1,2,97,104,16,6,1.0,111,104,18,14,7,2,2,1,0,1
streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-10/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,101,925,1,3,135,136,45,3,1,140,136,47,5,4,2,1,0,1,1
streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,66,513,1,2,97,104,16,6,1.0,111,104,18,14,7,2,1,0,1,1
streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-11/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,101,925,1,3,135,136,45,3,1,140,136,47,5,4,2,1,0,1,1
streams/upgrade-system-tests-20/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-20/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,98,12,7,1,128,98,18,42,30,6,2,1,0,1
streams/upgrade-system-tests-20/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-20/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,101,925,1,3,135,136,45,3,1,140,136,47,5,4,2,1,0,1,1
streams/upgrade-system-tests-21/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-21/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,98,14,6,1.5,127,98,21,41,30,7,2,1,0,1
streams/upgrade-system-tests-21/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-21/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,101,925,1,3,135,136,45,3,1,140,136,47,5,4,2,1,0,1,1
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,90,22,4,1.0,95,90,24,9,7,2,1,0,1,1
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,1,1,101,925,1,3,135,136,45,3,1,140,136,47,5,4,2,1,0,1,1
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,90,22,4,1.0,95,90,24,9,7,2,1,0,1,1
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeToCooperativeRebalanceTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",11,1,1,90,821,1,2,124,125,41,3,1,129,125,43,5,4,2,1,0,1,1
streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,86,29,3,1,88,86,29,2,1,1,1,0,1,1
streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,86,43,2,1.0,87,86,44,1,1,0,1,0,1,1
streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,86,43,2,1.0,87,86,44,1,1,0,1,0,1,1
streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java,"KAFKA-12499: add transaction timeout verification (#10482)

This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to be larger or equal to the given transaction timeout, or vise versa.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,1,1,56,460,1,2,86,86,43,2,1.0,87,86,44,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java,clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",33,2,1,221,1807,0,21,281,133,3,108,3.0,1061,191,10,780,319,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AbstractRequest.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",92,2,0,238,1591,1,20,311,62,4,72,1.0,579,94,8,268,79,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AbstractResponse.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",85,2,0,217,1555,1,14,274,86,3,97,1,492,94,5,218,81,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AllocateProducerIdsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AllocateProducerIdsRequest.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",7,72,0,43,283,7,7,72,72,72,1,1,72,72,72,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/AllocateProducerIdsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AllocateProducerIdsResponse.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",5,63,0,31,208,5,5,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/protocol/ApiKeysTest.java,clients/src/test/java/org/apache/kafka/common/protocol/ApiKeysTest.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",11,1,1,52,427,1,5,89,34,6,15,2,134,34,9,45,7,3,2,1,0,1
core/src/main/scala/kafka/api/ApiVersion.scala,core/src/main/scala/kafka/api/ApiVersion.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",16,12,1,333,1660,0,7,472,78,9,55,2,736,122,13,264,96,5,2,1,0,1
core/src/main/scala/kafka/controller/KafkaController.scala,core/src/main/scala/kafka/controller/KafkaController.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",433,60,4,1976,13793,6,147,2901,416,13,227,3,8948,687,39,6047,829,27,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/ProducerIdManager.scala,core/src/main/scala/kafka/coordinator/transaction/ProducerIdManager.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",35,168,87,171,1123,16,13,242,153,17,14,2.0,444,168,32,202,87,14,2,1,0,1
core/src/main/scala/kafka/network/RequestConvertToJson.scala,core/src/main/scala/kafka/network/RequestConvertToJson.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",147,2,0,201,2813,2,6,225,207,19,12,2.0,230,207,19,5,2,0,2,1,0,1
core/src/main/scala/kafka/server/BrokerServer.scala,core/src/main/scala/kafka/server/BrokerServer.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",49,3,3,363,2645,2,8,504,450,28,18,2.5,575,459,32,71,14,4,2,1,0,1
core/src/main/scala/kafka/server/KafkaServer.scala,core/src/main/scala/kafka/server/KafkaServer.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",91,29,15,560,4130,2,14,810,132,3,286,2.0,2904,158,10,2094,85,7,2,1,0,1
core/src/main/scala/kafka/zk/ZkData.scala,core/src/main/scala/kafka/zk/ZkData.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",127,27,1,699,5485,2,71,1014,248,23,44,3.0,1401,248,32,387,39,9,2,1,0,1
core/src/test/java/kafka/test/ClusterConfig.java,core/src/test/java/kafka/test/ClusterConfig.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",32,23,3,168,1014,8,32,227,207,76,3,1,232,207,77,5,3,2,1,0,1,1
core/src/test/java/kafka/test/ClusterInstance.java,core/src/test/java/kafka/test/ClusterInstance.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",2,2,0,32,187,0,2,99,97,33,3,1,100,97,33,1,1,0,1,0,1,1
core/src/test/java/kafka/test/annotation/ClusterTest.java,core/src/test/java/kafka/test/annotation/ClusterTest.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",1,1,0,23,178,1,1,45,44,22,2,1.0,45,44,22,0,0,0,1,0,0,0
core/src/test/java/kafka/test/junit/ClusterTestExtensions.java,core/src/test/java/kafka/test/junit/ClusterTestExtensions.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",25,5,1,132,1008,1,6,215,220,54,4,2.0,234,220,58,19,15,5,1,0,1,1
core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java,core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",21,15,6,150,1085,3,17,205,196,68,3,2,213,196,71,8,6,3,1,0,1,1
core/src/test/java/kafka/test/junit/ZkClusterInvocationContext.java,core/src/test/java/kafka/test/junit/ZkClusterInvocationContext.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",26,25,1,204,1355,2,17,276,252,92,3,1,278,252,93,2,1,1,1,0,1,1
core/src/test/java/kafka/testkit/BrokerNode.java,core/src/test/java/kafka/testkit/BrokerNode.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",14,8,0,76,397,2,10,107,99,54,2,3.0,107,99,54,0,0,0,1,0,0,0
core/src/test/java/kafka/testkit/KafkaClusterTestKit.java,core/src/test/java/kafka/testkit/KafkaClusterTestKit.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",56,1,0,419,3444,1,21,490,494,98,5,1,501,494,100,11,8,2,2,1,0,1
core/src/test/scala/integration/kafka/coordinator/transaction/ProducerIdsIntegrationTest.scala,core/src/test/scala/integration/kafka/coordinator/transaction/ProducerIdsIntegrationTest.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",5,85,0,57,551,5,5,85,85,85,1,1,85,85,85,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/coordinator/transaction/ProducerIdManagerTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/ProducerIdManagerTest.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",9,76,18,101,971,9,6,144,105,11,13,2,270,105,21,126,37,10,2,1,0,1
core/src/test/scala/unit/kafka/integration/KafkaServerTestHarness.scala,core/src/test/scala/unit/kafka/integration/KafkaServerTestHarness.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",24,12,1,114,876,3,16,197,49,4,49,2,309,49,6,112,14,2,2,1,0,1
core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala,core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",108,5,2,656,5070,3,29,795,420,6,140,2.0,1302,420,9,507,75,4,2,1,0,1
server-common/src/main/java/org/apache/kafka/server/common/ProducerIdsBlock.java,server-common/src/main/java/org/apache/kafka/server/common/ProducerIdsBlock.java,"KAFKA-12620 Allocate producer ids on the controller (#10504)

Introduce new AllocateProducerIds RPC and IBP 3.0-IV0 as part of KIP-730.

This change adds a new AllocateProducerIds RPC which is used by the broker to request a block of 
producer IDs from the controller. The new IBP added will determine if the broker should talk directly to 
ZooKeeper (IBP < 3.0) or it if should use the new RPC to talk to the controller (IBP >= 3.0).

Per-broker property overrides for ClusterTests were also added (in order to test mixed IBPs in a cluster)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",13,80,0,45,252,8,8,80,80,80,1,1,80,80,80,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/StreamsMetrics.java,streams/src/main/java/org/apache/kafka/streams/StreamsMetrics.java,"KAFKA-12808: Remove Deprecated Methods under StreamsMetrics (#10724)

Removal of methods already deprecated since 2.5.
Adapt test to use the new alternative method.

Reviewers: Bruno Cadonna <cadonna@apache.org>",0,0,80,24,163,0,0,144,68,10,15,2,301,75,20,157,80,10,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java,"KAFKA-12522: Cast SMT should allow null value records to pass through (#10375)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Lee Dongjin <dongjin@apache.org>, Chris Egerton  <fearthecellos@gmail.com>",107,4,0,396,3256,1,26,477,417,48,10,2.0,514,417,51,37,21,4,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/CastTest.java,"KAFKA-12522: Cast SMT should allow null value records to pass through (#10375)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Lee Dongjin <dongjin@apache.org>, Chris Egerton  <fearthecellos@gmail.com>",39,36,0,461,5446,4,39,576,384,58,10,5.5,683,384,68,107,82,11,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java,HOTFIX: fix checkstyle issue in KAFKA-12697,34,2,2,700,6957,1,27,802,204,62,13,3,915,211,70,113,40,9,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java,metadata/src/main/java/org/apache/kafka/controller/BrokersToIsrs.java,"KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller (#10572)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",63,17,0,267,1788,4,21,346,314,69,5,2,356,314,71,10,9,2,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ControllerMetrics.java,metadata/src/main/java/org/apache/kafka/controller/ControllerMetrics.java,"KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller (#10572)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",0,8,0,15,88,0,0,45,29,15,3,1,45,29,15,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/QuorumControllerMetrics.java,metadata/src/main/java/org/apache/kafka/controller/QuorumControllerMetrics.java,"KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller (#10572)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",14,43,1,120,686,5,13,154,70,51,3,6,155,70,52,1,1,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java,metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java,"KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller (#10572)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",198,26,0,1040,8880,5,48,1249,908,83,15,6,1491,908,99,242,142,16,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/MockControllerMetrics.java,metadata/src/test/java/org/apache/kafka/controller/MockControllerMetrics.java,"KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller (#10572)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",13,25,0,61,255,5,13,95,47,32,3,3,96,47,32,1,1,0,2,1,0,1
core/src/main/scala/kafka/raft/RaftManager.scala,core/src/main/scala/kafka/raft/RaftManager.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",23,17,10,263,1439,7,16,317,280,16,20,3.5,428,280,21,111,20,6,2,1,0,1
core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala,core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",33,7,13,310,1583,3,18,388,187,19,20,4.5,607,187,30,219,85,11,2,1,0,1
core/src/main/scala/kafka/server/ControllerServer.scala,core/src/main/scala/kafka/server/ControllerServer.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",24,11,12,181,1289,1,4,226,180,23,10,3.0,262,189,26,36,12,4,2,1,0,1
core/src/main/scala/kafka/server/KafkaRaftServer.scala,core/src/main/scala/kafka/server/KafkaRaftServer.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",7,4,7,122,731,0,4,179,103,18,10,3.0,210,103,21,31,7,3,2,1,0,1
core/src/main/scala/kafka/tools/DumpLogSegments.scala,core/src/main/scala/kafka/tools/DumpLogSegments.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",74,1,1,401,3228,0,17,484,118,6,82,2.0,1326,171,16,842,160,10,2,1,0,1
core/src/main/scala/kafka/tools/TestRaftServer.scala,core/src/main/scala/kafka/tools/TestRaftServer.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",47,8,7,369,2454,3,20,468,404,22,21,3,919,404,44,451,217,21,2,1,0,1
core/src/test/scala/unit/kafka/tools/DumpLogSegmentsTest.scala,core/src/test/scala/unit/kafka/tools/DumpLogSegmentsTest.scala,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",19,1,1,199,1765,0,12,254,96,17,15,2,334,96,22,80,26,5,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/QuorumController.java,metadata/src/main/java/org/apache/kafka/controller/QuorumController.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",156,172,115,989,6607,16,81,1269,941,85,15,2,1460,941,97,191,115,13,2,1,0,1
metadata/src/main/java/org/apache/kafka/metadata/MetadataRecordSerde.java,metadata/src/main/java/org/apache/kafka/metadata/MetadataRecordSerde.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",1,1,1,10,80,0,1,29,66,7,4,1.0,71,66,18,42,41,10,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/MockSnapshotWriter.java,metadata/src/test/java/org/apache/kafka/controller/MockSnapshotWriter.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",15,27,11,79,514,1,10,110,94,37,3,1,122,94,41,12,11,4,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java,metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",15,13,12,385,3870,7,12,439,180,49,9,2,459,180,51,20,12,2,2,1,0,1
metadata/src/test/java/org/apache/kafka/metadata/MetadataRecordSerdeTest.java,metadata/src/test/java/org/apache/kafka/metadata/MetadataRecordSerdeTest.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",3,2,2,42,369,0,2,72,72,24,3,1,75,72,25,3,2,1,2,1,0,1
metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java,metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",71,138,43,402,2629,30,34,529,378,88,6,1.0,576,378,96,47,43,8,2,1,0,1
metadata/src/test/java/org/apache/kafka/metalog/MockMetaLogManagerListener.java,metadata/src/test/java/org/apache/kafka/metalog/MockMetaLogManagerListener.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",12,63,19,91,736,7,6,121,77,40,3,1,140,77,47,19,19,6,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java,raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",354,48,30,1873,12635,19,121,2471,1784,63,39,6,3375,1784,87,904,161,23,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/LeaderAndEpoch.java,raft/src/main/java/org/apache/kafka/raft/LeaderAndEpoch.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",12,22,2,39,214,4,7,64,44,32,2,2.0,66,44,33,2,2,1,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/QuorumState.java,raft/src/main/java/org/apache/kafka/raft/QuorumState.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",88,4,0,396,2262,1,37,552,443,46,12,5.0,615,443,51,63,18,5,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/RaftClient.java,raft/src/main/java/org/apache/kafka/raft/RaftClient.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",2,38,14,23,185,4,2,193,79,18,11,4,281,79,26,88,33,8,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java,raft/src/main/java/org/apache/kafka/raft/ReplicatedCounter.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",16,11,12,123,708,3,6,155,132,26,6,6.5,295,132,49,140,77,23,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java,raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientSnapshotTest.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",49,1,1,1353,10252,1,44,1695,1153,154,11,6,1872,1153,170,177,114,16,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java,raft/src/test/java/org/apache/kafka/raft/KafkaRaftClientTest.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",87,3,3,1840,17972,3,82,2535,2456,98,26,11.5,4773,2456,184,2238,1368,86,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockLog.java,raft/src/test/java/org/apache/kafka/raft/MockLog.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",108,5,99,491,3314,14,54,619,445,39,16,5.0,890,445,56,271,99,17,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java,raft/src/test/java/org/apache/kafka/raft/RaftClientTestContext.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",119,17,13,1005,7062,7,94,1181,718,47,25,7,1525,718,61,344,88,14,2,1,0,1
raft/src/test/java/org/apache/kafka/snapshot/MockRawSnapshotReader.java,raft/src/test/java/org/apache/kafka/snapshot/MockRawSnapshotReader.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",5,57,0,34,236,5,5,57,57,57,1,1,57,57,57,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/snapshot/MockRawSnapshotWriter.java,raft/src/test/java/org/apache/kafka/snapshot/MockRawSnapshotWriter.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",10,88,0,58,305,9,9,88,88,88,1,1,88,88,88,0,0,0,2,1,0,1
shell/src/main/java/org/apache/kafka/shell/MetadataNodeManager.java,shell/src/main/java/org/apache/kafka/shell/MetadataNodeManager.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",35,8,32,258,1991,7,15,300,302,50,6,3.0,343,302,57,43,32,7,2,1,0,1
shell/src/main/java/org/apache/kafka/shell/SnapshotFileReader.java,shell/src/main/java/org/apache/kafka/shell/SnapshotFileReader.java,"KAFKA-12342: Remove MetaLogShim and use RaftClient directly (#10705)

This patch removes the temporary shim layer we added to bridge the interface
differences between MetaLogManager and RaftClient. Instead, we now use the
RaftClient directly from the metadata module.  This also means that the
metadata gradle module now depends on raft, rather than the other way around.
Finally, this PR also consolidates the handleResign and handleNewLeader APIs
into a single handleLeaderChange API.

Co-authored-by: Jason Gustafson <jason@confluent.io>",18,29,14,165,1092,5,9,202,194,50,4,3.5,229,194,57,27,14,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",24,2,2,249,1497,2,12,302,225,20,15,4,473,225,32,171,40,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogTopics.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogTopics.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",18,3,3,99,867,1,6,134,132,45,3,2,145,132,48,11,8,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",111,1,1,488,3269,1,41,676,232,6,104,3.5,1993,309,19,1317,304,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTaskCreator.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",9,2,2,126,773,2,7,164,113,20,8,2.0,191,113,24,27,13,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",83,3,2,350,2372,2,22,497,184,13,37,4,988,184,27,491,86,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",22,31,0,115,878,2,7,186,114,8,22,3.0,421,114,19,235,74,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",150,1,2,1034,7242,1,49,1431,270,9,155,5,5131,441,33,3700,341,24,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",218,2,1,993,7251,1,54,1348,524,10,129,3,3577,524,28,2229,422,17,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfo.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",79,6,4,413,3122,3,34,497,125,19,26,5.5,946,150,36,449,86,17,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConsumerProtocolUtils.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConsumerProtocolUtils.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",16,109,0,72,567,5,5,109,109,109,1,1,109,109,109,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",50,6,6,256,1914,2,22,317,128,13,24,3.5,871,161,36,554,268,23,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",22,1,1,89,741,1,3,114,53,6,20,2.0,274,53,14,160,42,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",58,1,1,848,6178,1,49,1053,449,14,74,5.0,3165,449,43,2112,621,29,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",100,2,2,1693,17074,2,81,2185,363,19,113,6,6156,422,54,3971,319,35,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentTestUtils.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",51,1,1,405,3343,1,25,478,318,40,12,1.5,520,324,43,42,19,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/LegacySubscriptionInfoSerde.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/LegacySubscriptionInfoSerde.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",49,4,2,219,1579,2,19,280,276,56,5,3,303,276,61,23,16,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",68,1,1,601,6488,1,43,759,515,38,20,3.5,1428,515,71,669,335,33,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java,"KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId() (#10735)

As described in KIP-740, we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata

Reviewers: Guozhang Wang <guozhang@confluent.io>",33,1,1,429,3201,1,23,485,211,7,74,2.0,883,211,12,398,38,5,2,1,0,1
core/src/test/scala/unit/kafka/log/LogTestUtils.scala,core/src/test/scala/unit/kafka/log/LogTestUtils.scala,"MINOR: Eliminate redundant functions in LogTest suite (#10732)

Reviewers: Satish Duggana <satishd@apache.org>, Jun Rao <junrao@gmail.com>",34,7,7,213,1822,4,24,278,238,40,7,1,294,240,42,16,7,2,1,0,1,1
tests/kafkatest/version.py,tests/kafkatest/version.py,"MINOR: Updating files with release 2.7.1 (#10660)


Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>,  Matthias J. Sax <mjsax@apache.org>, Chia-Ping Tsai <chia7712@gmail.com>",24,2,1,123,603,0,14,216,61,4,52,1.0,276,61,5,60,10,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/Stores.java,streams/src/main/java/org/apache/kafka/streams/state/Stores.java,"KAFKA-12809: Remove deprecated methods of Stores factory (#10729)

Removes methods deprecated via KIP-319 and KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",26,4,60,192,1437,3,16,466,257,8,55,3,1288,257,23,822,414,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/StoresTest.java,streams/src/test/java/org/apache/kafka/streams/state/StoresTest.java,"KAFKA-12809: Remove deprecated methods of Stores factory (#10729)

Removes methods deprecated via KIP-319 and KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",32,0,7,223,2010,1,32,274,84,16,17,4,440,111,26,166,68,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/ProcessorContext.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,0,37,36,283,0,0,262,106,7,40,2.0,454,106,11,192,37,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ForwardingDisabledProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ForwardingDisabledProcessorContext.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",22,0,8,114,659,1,22,159,149,14,11,2,203,149,18,44,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreToProcessorContextAdapter.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreToProcessorContextAdapter.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",24,0,6,113,659,1,23,155,163,39,4,1.0,173,163,43,18,12,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImplTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalProcessorContextImplTest.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",27,0,6,180,1381,1,17,224,137,12,18,2.0,307,137,17,83,15,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorContextImplTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorContextImplTest.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",39,0,10,606,5119,1,37,758,260,38,20,4.5,910,260,46,152,43,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreToProcessorContextAdapterTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreToProcessorContextAdapterTest.java,"KAFKA-12813: Remove deprecated schedule method in ProcessorContext (#10730)

Removes methods deprecated via KIP-358.

Reviewers: Matthias J. Sax <matthias@confluent.io>",13,0,6,73,471,1,13,105,123,35,3,1,123,123,41,18,12,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMapValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMapValues.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",22,2,3,122,856,1,13,171,85,7,23,2,267,85,12,96,21,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMaterializedValueGetterSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMaterializedValueGetterSupplier.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",5,1,2,28,191,1,5,52,54,9,6,1.5,69,54,12,17,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTablePassThrough.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTablePassThrough.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",8,1,2,55,373,1,7,88,89,44,2,1.5,90,89,45,2,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSourceValueGetterSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSourceValueGetterSupplier.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",5,1,2,26,190,1,5,51,50,4,14,2.0,88,50,6,37,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableTransformValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableTransformValues.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",21,22,3,136,1097,3,12,181,149,20,9,1,206,149,23,25,13,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorSupplier.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",19,2,3,170,1362,2,12,216,214,27,8,2.0,237,214,30,21,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableTransformValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableTransformValuesTest.java,"KAFKA-12815: Preserve context for KTable.transformValues when getting value from upstream state store (#10720)

Reviewers: Victoria Xia <victoria.xia@confluent.io>, John Roesler <john@confluent.io>",48,25,5,469,4034,3,41,597,547,28,21,3,730,547,35,133,27,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/TopologyDescription.java,streams/src/main/java/org/apache/kafka/streams/TopologyDescription.java,"KAFKA-12810: Remove deprecated TopologyDescription.Source#topics (#10727)

Removes methods that were deprecated via KIP-321.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,0,7,35,199,0,0,173,152,14,12,1.5,200,152,17,27,7,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java,clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java,"MINOR: add ConfigUtils method for printing configurations (#10714)

Reviewers: Luke Chen <showuon@gmail.com>, David Arthur <mumrah@gmail.com>",282,5,1,990,7748,1,97,1588,526,23,69,2,2178,589,32,590,63,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ConfigUtils.java,clients/src/main/java/org/apache/kafka/common/utils/ConfigUtils.java,"MINOR: add ConfigUtils method for printing configurations (#10714)

Reviewers: Luke Chen <showuon@gmail.com>, David Arthur <mumrah@gmail.com>",12,31,0,86,833,1,3,147,116,74,2,2.5,147,116,74,0,0,0,1,0,0,0
clients/src/test/java/org/apache/kafka/common/utils/ConfigUtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/ConfigUtilsTest.java,"MINOR: add ConfigUtils method for printing configurations (#10714)

Reviewers: Luke Chen <showuon@gmail.com>, David Arthur <mumrah@gmail.com>",9,28,0,139,1225,2,9,171,143,43,4,2.5,182,143,46,11,7,3,2,1,0,1
core/src/main/scala/kafka/server/DynamicBrokerConfig.scala,core/src/main/scala/kafka/server/DynamicBrokerConfig.scala,"MINOR: add ConfigUtils method for printing configurations (#10714)

Reviewers: Luke Chen <showuon@gmail.com>, David Arthur <mumrah@gmail.com>",176,3,2,723,5607,1,61,958,360,22,43,3,1240,360,29,282,45,7,2,1,0,1
core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala,core/src/main/scala/kafka/security/authorizer/AclAuthorizer.scala,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",131,3,1,577,4722,1,38,754,476,31,24,5.5,1004,476,42,250,54,10,2,1,0,1
core/src/main/scala/kafka/server/AclApis.scala,core/src/main/scala/kafka/server/AclApis.scala,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",16,155,0,122,914,5,6,155,155,155,1,1,155,155,155,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala,core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",85,26,2,979,8886,2,72,1350,528,33,41,5,1810,552,44,460,65,11,2,1,0,1
core/src/test/scala/unit/kafka/zookeeper/ZooKeeperClientTest.scala,core/src/test/scala/unit/kafka/zookeeper/ZooKeeperClientTest.scala,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",73,1,1,591,4854,1,59,717,329,20,35,4,1175,329,34,458,93,13,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/metadata/MetadataRequestBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/metadata/MetadataRequestBenchmark.java,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",8,2,1,188,1639,1,8,223,206,11,20,2.0,259,206,13,36,5,2,2,1,0,1
tests/kafkatest/sanity_checks/test_bounce.py,tests/kafkatest/sanity_checks/test_bounce.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",11,4,4,44,408,2,4,72,72,36,2,2.5,76,72,38,4,4,2,2,1,0,1
tests/kafkatest/sanity_checks/test_console_consumer.py,tests/kafkatest/sanity_checks/test_console_consumer.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",6,4,4,63,645,0,4,100,80,6,18,2.0,178,80,10,78,19,4,2,1,0,1
tests/kafkatest/sanity_checks/test_verifiable_producer.py,tests/kafkatest/sanity_checks/test_verifiable_producer.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",7,9,9,109,998,4,5,172,73,13,13,2,210,73,16,38,9,3,2,1,0,1
tests/kafkatest/services/kafka/kafka.py,tests/kafkatest/services/kafka/kafka.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",307,80,59,1215,8342,16,73,1590,318,16,98,3.0,2372,377,24,782,72,8,2,1,0,1
tests/kafkatest/services/kafka/quorum.py,tests/kafkatest/services/kafka/quorum.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",13,26,26,91,258,1,3,144,144,36,4,2.5,178,144,44,34,26,8,2,1,0,1
tests/kafkatest/services/security/kafka_acls.py,tests/kafkatest/services/security/kafka_acls.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",12,30,2,53,474,3,8,153,78,22,7,4,257,114,37,104,55,15,2,1,0,1
tests/kafkatest/services/security/security_config.py,tests/kafkatest/services/security/security_config.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",74,21,19,302,2183,4,31,432,133,14,30,4.0,665,133,22,233,35,8,2,1,0,1
tests/kafkatest/tests/core/compatibility_test_new_broker_test.py,tests/kafkatest/tests/core/compatibility_test_new_broker_test.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",9,1,1,70,1409,1,3,94,78,4,21,2,170,78,8,76,29,4,2,1,0,1
tests/kafkatest/tests/core/replication_test.py,tests/kafkatest/tests/core/replication_test.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",14,3,3,118,847,1,8,184,165,8,24,3.0,449,165,19,265,124,11,2,1,0,1
tests/kafkatest/tests/core/round_trip_fault_test.py,tests/kafkatest/tests/core/round_trip_fault_test.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",17,2,2,107,1007,2,9,134,89,17,8,2.5,159,89,20,25,13,3,2,1,0,1
tests/kafkatest/tests/core/security_test.py,tests/kafkatest/tests/core/security_test.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",21,8,8,106,769,2,7,176,106,14,13,2,273,106,21,97,39,7,2,1,0,1
tests/kafkatest/tests/core/zookeeper_authorizer_test.py,tests/kafkatest/tests/core/zookeeper_authorizer_test.py,"MINOR: Support using the ZK authorizer with KRaft (#10550)

This patch adds support for running the ZooKeeper-based
kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the
authorizer.class.name config as well as the zookeeper.connect config while also
setting the typical KRaft configs (node.id, process.roles, etc.), and the
cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system
test that exercises the authorizer is included.

This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also
fixes a bug where system test admin clients were unable to connect to a cluster
with broker credentials via the SSL security protocol when the broker was using
that for inter-broker communication and SASL for client communication.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",5,101,0,66,453,3,3,101,101,101,1,1,101,101,101,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,"KAFKA-12789: Remove Stale comments for meta response handling logic (#10700)

Correct empty meta response comment, since it is no longer related only to brokers associating with the query topic.

Reviewers: Boyang Chen <boyang@confluent.io>",157,2,2,880,5791,1,67,1278,383,10,128,3.0,2548,383,20,1270,132,10,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotReader.java,raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotReader.java,"Fix compile errors from KAFKA-12543 (#10719)

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Jun Rao <junrao@gmail.com>, José Armando García Sancio <jsancio@users.noreply.github.com>",8,1,1,57,290,1,7,92,81,18,5,3,110,81,22,18,7,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java,"HOTFIX: undo renaming of public part of Subtopology API (#10713)

In #10676 we renamed the internal Subtopology class that implemented the TopologyDescription.Subtopology interface. By mistake, we also renamed the interface itself, which is a public API. This wasn't really the intended point of that PR, so rather than do a retroactive KIP, let's just reverse the renaming.

Reviewers: Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>",36,2,2,484,4898,1,31,584,220,12,48,4.0,1600,226,33,1016,225,21,2,1,0,1
core/src/test/scala/kafka/raft/KafkaMetadataLogTest.scala,core/src/test/scala/kafka/raft/KafkaMetadataLogTest.scala,"KAFKA-12543: Change RawSnapshotReader ownership model (#10431)

Kafka networking layer doesn't close FileRecords and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for FileRawSnapshotReader so that they are owned by KafkaMetadataLog.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jun Rao <junrao@gmail.com>",37,1,3,566,4086,1,35,729,335,73,10,2.0,827,335,83,98,69,10,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotReader.java,raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotReader.java,"KAFKA-12543: Change RawSnapshotReader ownership model (#10431)

Kafka networking layer doesn't close FileRecords and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for FileRawSnapshotReader so that they are owned by KafkaMetadataLog.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jun Rao <junrao@gmail.com>",0,1,3,10,76,0,0,52,52,13,4,5.0,76,52,19,24,13,6,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/Snapshots.java,raft/src/main/java/org/apache/kafka/snapshot/Snapshots.java,"KAFKA-12543: Change RawSnapshotReader ownership model (#10431)

Kafka networking layer doesn't close FileRecords and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for FileRawSnapshotReader so that they are owned by KafkaMetadataLog.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jun Rao <junrao@gmail.com>",15,32,12,95,723,5,9,141,66,35,4,8.5,165,66,41,24,12,6,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockLogTest.java,raft/src/test/java/org/apache/kafka/raft/MockLogTest.java,"KAFKA-12543: Change RawSnapshotReader ownership model (#10431)

Kafka networking layer doesn't close FileRecords and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for FileRawSnapshotReader so that they are owned by KafkaMetadataLog.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jun Rao <junrao@gmail.com>",66,2,3,672,5822,1,48,867,421,62,14,2.5,989,421,71,122,69,9,2,1,0,1
raft/src/test/java/org/apache/kafka/snapshot/SnapshotsTest.java,raft/src/test/java/org/apache/kafka/snapshot/SnapshotsTest.java,"KAFKA-12543: Change RawSnapshotReader ownership model (#10431)

Kafka networking layer doesn't close FileRecords and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for FileRawSnapshotReader so that they are owned by KafkaMetadataLog.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jun Rao <junrao@gmail.com>",6,1,1,84,808,1,5,126,78,32,4,1.0,134,78,34,8,6,2,2,1,0,1
core/src/main/scala/kafka/server/AlterIsrManager.scala,core/src/main/scala/kafka/server/AlterIsrManager.scala,"KAFKA-12686 AlterIsr and LeaderAndIsr race condition (#10561)

Remove the clearPending method from AlterIsrManager

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",29,0,6,192,1260,2,15,272,180,25,11,2,347,180,32,75,27,7,2,1,0,1
core/src/main/scala/kafka/server/ZkIsrManager.scala,core/src/main/scala/kafka/server/ZkIsrManager.scala,"KAFKA-12686 AlterIsr and LeaderAndIsr race condition (#10561)

Remove the clearPending method from AlterIsrManager

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",7,0,6,55,412,2,3,103,109,52,2,1.0,109,109,54,6,6,3,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala,core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala,"KAFKA-12686 AlterIsr and LeaderAndIsr race condition (#10561)

Remove the clearPending method from AlterIsrManager

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",131,62,2,1027,9698,4,71,1237,317,29,43,4,1502,317,35,265,46,6,2,1,0,1
core/src/test/scala/unit/kafka/server/AlterIsrManagerTest.scala,core/src/test/scala/unit/kafka/server/AlterIsrManagerTest.scala,"KAFKA-12686 AlterIsr and LeaderAndIsr race condition (#10561)

Remove the clearPending method from AlterIsrManager

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",24,14,2,266,2811,1,21,373,302,41,9,8,536,302,60,163,68,18,2,1,0,1
core/src/test/scala/unit/kafka/utils/TestUtils.scala,core/src/test/scala/unit/kafka/utils/TestUtils.scala,"KAFKA-12686 AlterIsr and LeaderAndIsr race condition (#10561)

Remove the clearPending method from AlterIsrManager

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",245,0,4,1402,12235,2,135,1879,294,6,319,2,4151,294,13,2272,141,7,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java,metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",77,11,7,371,2160,4,33,604,597,201,3,3,613,597,204,9,7,3,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java,metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",58,5,5,309,2499,3,20,407,346,68,6,1.5,420,346,70,13,5,2,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacer.java,metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacer.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",0,5,5,14,95,0,0,50,47,17,3,2,56,47,19,6,5,2,0,0,0,0
metadata/src/main/java/org/apache/kafka/controller/StripedReplicaPlacer.java,metadata/src/main/java/org/apache/kafka/controller/StripedReplicaPlacer.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",43,429,0,199,1273,19,19,429,429,429,1,1,429,429,429,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/OptionalStringComparator.java,metadata/src/main/java/org/apache/kafka/metadata/OptionalStringComparator.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",4,40,0,19,130,1,1,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/UsableBroker.java,metadata/src/main/java/org/apache/kafka/metadata/UsableBroker.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",10,11,4,36,212,6,7,68,61,34,2,3.5,72,61,36,4,4,2,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/BrokerHeartbeatManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/BrokerHeartbeatManagerTest.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",11,8,6,262,2563,2,9,298,296,149,2,2.0,304,296,152,6,6,3,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",10,4,4,180,1705,4,4,207,150,41,5,1,216,150,43,9,4,2,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/StripedReplicaPlacerTest.java,metadata/src/test/java/org/apache/kafka/controller/StripedReplicaPlacerTest.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",10,213,0,172,2169,9,9,213,213,213,1,1,213,213,213,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/metadata/OptionalStringComparatorTest.java,metadata/src/test/java/org/apache/kafka/metadata/OptionalStringComparatorTest.java,"KAFKA-12788: improve KRaft replica placement (#10494)

Implement a striped replica placement algorithm for KRaft. This also
means implementing rack awareness.  Previously, KRraft just chose
replicas randomly in a non-rack-aware fashion.  Also, allow replicas to
be placed on fenced brokers if there are no other choices.  This was
specified in KIP-631 but previously not implemented.

Reviewers: Jun Rao <junrao@gmail.com>",1,40,0,18,229,1,1,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
tests/kafkatest/tests/streams/streams_named_repartition_topic_test.py,tests/kafkatest/tests/streams/streams_named_repartition_topic_test.py,"MINOR: Add missing @cluster annotation to StreamsNamedRepartitionTopicTest (#10697)

The StreamsNamedRepartitionTopicTest system tests did not have the @cluster annotation and was therefore taking up the entire cluster. For example, we see this in the log output:

kafkatest.tests.streams.streams_named_repartition_topic_test.StreamsNamedRepartitionTopicTest.test_upgrade_topology_with_named_repartition_topic is using entire cluster. It's possible this test has no associated cluster metadata.

This PR adds the missing annotation.

Reviewers: Bill Bejeck <bbejeck@apache.org>",7,2,0,64,439,0,4,98,119,24,4,3.0,131,119,33,33,29,8,2,1,0,1
tests/kafkatest/tests/core/security_rolling_upgrade_test.py,tests/kafkatest/tests/core/security_rolling_upgrade_test.py,"MINOR: fix system test TestSecurityRollingUpgrade (#10694)

Ensure security protocol and sasl mechanism are updated in the cached SecurityConfig during rolling system tests. Also explicitly indicate which SASL mechanisms we wish to expose during the tests.

Reviewers: David Arthur <mumrah@gmail.com>",17,5,2,143,1241,2,17,256,124,12,21,2,337,124,16,81,21,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java,clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",121,2,8,689,5415,1,44,1372,240,7,200,3.0,2881,240,14,1509,82,8,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java,clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",72,1,1,388,2589,0,41,549,201,11,49,2,795,201,16,246,42,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",365,2,6,1338,8786,3,146,1756,866,23,75,4,2979,866,40,1223,341,16,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java,clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitRequest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",28,5,26,180,1262,5,16,223,195,12,18,4.0,539,195,30,316,173,18,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",87,6,22,1084,10886,3,64,1348,145,20,68,3.0,2125,167,31,777,148,11,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",67,3,0,639,4790,0,60,771,430,26,30,2.5,1062,449,35,291,105,10,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",98,1,1,870,9500,1,39,1110,195,16,71,4,1879,199,26,769,89,11,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",159,13,13,2343,23638,12,101,3134,662,24,129,4,5090,672,39,1956,208,15,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionManagerTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",177,18,44,2696,23903,19,156,3582,588,44,81,4,5807,641,72,2225,495,27,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",243,3,6,2594,23835,2,176,2918,173,14,215,4,5485,231,26,2567,232,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitRequestTest.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",4,4,32,100,661,4,2,130,109,19,7,3,225,109,32,95,35,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java,"KAFKA-12574: remove internal Producer config and auto downgrade logic  (#10675)

Minor followup to #10573. Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request in Kafka Streams

Reviewers: Ismael Juma <ismael@juma.me.uk>, Matthias J. Sax <mjsax@confluent.io>, Guozhang Wang <guozhang@confluent.io>",41,5,2,238,1470,1,15,329,227,21,16,3.5,494,227,31,165,62,10,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java,metadata/src/main/java/org/apache/kafka/timeline/SnapshotRegistry.java,"KAFKA-12792: Fix metrics bug and introduce TimelineInteger (#10707)

Introduce a TimelineInteger class which represents a single integer
value which can be changed while maintaining snapshot consistency. Fix a
case where a metric value would be corrupted after a snapshot restore.

Reviewers: David Arthur <mumrah@gmail.com>",29,2,0,139,866,0,21,257,251,86,3,1,257,251,86,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/TimelineInteger.java,metadata/src/main/java/org/apache/kafka/timeline/TimelineInteger.java,"KAFKA-12792: Fix metrics bug and introduce TimelineInteger (#10707)

Introduce a TimelineInteger class which represents a single integer
value which can be changed while maintaining snapshot consistency. Fix a
case where a metric value would be corrupted after a snapshot restore.

Reviewers: David Arthur <mumrah@gmail.com>",19,114,0,74,417,13,13,114,114,114,1,1,114,114,114,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/TimelineLong.java,metadata/src/main/java/org/apache/kafka/timeline/TimelineLong.java,"KAFKA-12792: Fix metrics bug and introduce TimelineInteger (#10707)

Introduce a TimelineInteger class which represents a single integer
value which can be changed while maintaining snapshot consistency. Fix a
case where a metric value would be corrupted after a snapshot restore.

Reviewers: David Arthur <mumrah@gmail.com>",19,114,0,74,435,13,13,114,114,114,1,1,114,114,114,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/timeline/TimelineIntegerTest.java,metadata/src/test/java/org/apache/kafka/timeline/TimelineIntegerTest.java,"KAFKA-12792: Fix metrics bug and introduce TimelineInteger (#10707)

Introduce a TimelineInteger class which represents a single integer
value which can be changed while maintaining snapshot consistency. Fix a
case where a metric value would be corrupted after a snapshot restore.

Reviewers: David Arthur <mumrah@gmail.com>",3,73,0,50,401,3,3,73,73,73,1,1,73,73,73,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/timeline/TimelineLongTest.java,metadata/src/test/java/org/apache/kafka/timeline/TimelineLongTest.java,"KAFKA-12792: Fix metrics bug and introduce TimelineInteger (#10707)

Introduce a TimelineInteger class which represents a single integer
value which can be changed while maintaining snapshot consistency. Fix a
case where a metric value would be corrupted after a snapshot restore.

Reviewers: David Arthur <mumrah@gmail.com>",3,73,0,50,407,3,3,73,73,73,1,1,73,73,73,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedDeserializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedDeserializer.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",17,21,12,58,458,1,6,92,79,31,3,2,105,79,35,13,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedSerializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedSerializer.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",17,19,12,64,508,1,7,97,85,19,5,1,114,85,23,17,12,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedDeserializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedDeserializer.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",27,22,12,91,673,1,9,133,92,27,5,4,154,92,31,21,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedSerializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedSerializer.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",17,19,12,65,513,1,7,98,83,24,4,3.5,113,83,28,15,12,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowedDeserializerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowedDeserializerTest.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",5,28,14,46,408,8,5,72,58,24,3,2,88,58,29,16,14,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowedSerializerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowedSerializerTest.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",5,27,14,46,413,8,5,71,58,24,3,2,87,58,29,16,14,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowedDeserializerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowedDeserializerTest.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",7,26,19,64,587,10,7,91,59,23,4,3.0,112,59,28,21,19,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowedSerializerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowedSerializerTest.java,"KAFKA-12313: KIP-725: Streamlining configs for Windowed Deserialisers (#10542)

This PR aims to streamline the configurations for WindowedDeserialisers as described in KIP-725. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds windowed.inner.class.serde. 

Reviewers: Anna Sophie Blee-Goldman<ableegoldman@apache.org>",5,28,14,46,413,8,5,72,58,24,3,2,88,58,29,16,14,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/IncompleteBatches.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/IncompleteBatches.java,"KAFKA-12736: KafkaProducer.flush holds onto completed ProducerBatch(s) until flush completes (#10620)

When flush is called a copy of incomplete batches is made. This
means that the full ProducerBatch(s) are held in memory until the flush
has completed. Note that the `Sender` removes producer batches
from the original incomplete collection when they're no longer
needed.

For batches where the existing memory pool is used this
is not as wasteful as the memory will be returned to the pool,
but for non pool memory it can only be GC'd after the flush has
completed. Rather than use copyAll we can make a new array with only the
produceFuture(s) and await on those.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",7,7,0,38,223,1,6,66,59,33,2,1.5,66,59,33,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java,"KAFKA-12736: KafkaProducer.flush holds onto completed ProducerBatch(s) until flush completes (#10620)

When flush is called a copy of incomplete batches is made. This
means that the full ProducerBatch(s) are held in memory until the flush
has completed. Note that the `Sender` removes producer batches
from the original incomplete collection when they're no longer
needed.

For batches where the existing memory pool is used this
is not as wasteful as the memory will be returned to the pool,
but for non pool memory it can only be GC'd after the flush has
completed. Rather than use copyAll we can make a new array with only the
produceFuture(s) and await on those.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",120,6,2,514,3820,1,38,840,236,8,99,4,1852,236,19,1012,149,10,2,1,0,1
core/src/main/scala/kafka/tools/ClusterTool.scala,core/src/main/scala/kafka/tools/ClusterTool.scala,"MINOR: Fix typo in `ClusterTool` (#10706)

Reviewers: David Jacot <djacot@confluent.io>",15,1,1,99,620,1,3,124,125,41,3,1,137,125,46,13,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/Partitioner.java,clients/src/main/java/org/apache/kafka/clients/producer/Partitioner.java,"MINOR: remove unneccessary `public` keyword from `Partitioner` interface (#10708)

Reviewers: David Jacot <djacot@confluent.io>",1,3,5,10,95,1,1,55,46,11,5,2,65,46,13,10,5,2,2,1,0,1
core/src/test/java/kafka/test/MockController.java,core/src/test/java/kafka/test/MockController.java,"KAFKA-12778: Fix QuorumController request timeouts and electLeaders (#10688)

The QuorumController should honor the timeout for RPC requests
which feature a timeout. For electLeaders, attempt to trigger a leader
election for all partitions when the request specifies null for the topics
argument.

Reviewers: David Arthur <mumrah@gmail.com>",52,4,4,290,2355,8,28,350,222,58,6,4.0,358,222,60,8,4,1,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/Controller.java,metadata/src/main/java/org/apache/kafka/controller/Controller.java,"KAFKA-12778: Fix QuorumController request timeouts and electLeaders (#10688)

The QuorumController should honor the timeout for RPC requests
which feature a timeout. For electLeaders, attempt to trigger a leader
election for all partitions when the request specifies null for the topics
argument.

Reviewers: David Arthur <mumrah@gmail.com>",1,17,6,70,684,0,1,268,180,38,7,3,279,180,40,11,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java,"KAFKA-12754: Improve endOffsets for TaskMetadata (#10634)

Improve endOffsets for TaskMetadata by updating immediately after polling a new batch

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",42,0,5,224,1362,1,21,340,88,4,87,3,1144,138,13,804,147,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java,"KAFKA-12754: Improve endOffsets for TaskMetadata (#10634)

Improve endOffsets for TaskMetadata by updating immediately after polling a new batch

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",13,0,7,95,671,0,12,251,122,6,42,2.0,543,144,13,292,72,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/TaskMetadataIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/TaskMetadataIntegrationTest.java,"KAFKA-12754: Improve endOffsets for TaskMetadata (#10634)

Improve endOffsets for TaskMetadata by updating immediately after polling a new batch

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",15,200,0,157,1379,9,9,200,200,200,1,1,200,200,200,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java,"KAFKA-12754: Improve endOffsets for TaskMetadata (#10634)

Improve endOffsets for TaskMetadata by updating immediately after polling a new batch

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",160,0,7,2762,25986,1,136,3497,603,42,84,6.5,5391,638,64,1894,430,23,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/FileRecords.java,clients/src/main/java/org/apache/kafka/common/record/FileRecords.java,"Rework on KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10680)

(reverted #10405). #10405 has several issues, for example:

It fails to create a topic with 9000 partitions.
It flushes in several unnecessary places.
If multiple segments of the same partition are flushed at roughly the same time, we may end up doing multiple unnecessary flushes: the logic of handling the flush in LogSegments.scala is weird.
Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Did performance test which shows this PR has a minimal performance impact on Kafka clusters.

Reviewers: Jun Rao <junrao@gmail.com>",80,0,7,328,2203,1,41,556,361,16,35,3,852,413,24,296,66,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Utils.java,clients/src/main/java/org/apache/kafka/common/utils/Utils.java,"Rework on KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10680)

(reverted #10405). #10405 has several issues, for example:

It fails to create a topic with 9000 partitions.
It flushes in several unnecessary places.
If multiple segments of the same partition are flushed at roughly the same time, we may end up doing multiple unnecessary flushes: the logic of handling the flush in LogSegments.scala is weird.
Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Did performance test which shows this PR has a minimal performance impact on Kafka clusters.

Reviewers: Jun Rao <junrao@gmail.com>",188,11,12,772,6230,3,86,1367,230,12,110,2.0,1903,230,17,536,92,5,2,1,0,1
core/src/main/scala/kafka/log/LogManager.scala,core/src/main/scala/kafka/log/LogManager.scala,"Rework on KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10680)

(reverted #10405). #10405 has several issues, for example:

It fails to create a topic with 9000 partitions.
It flushes in several unnecessary places.
If multiple segments of the same partition are flushed at roughly the same time, we may end up doing multiple unnecessary flushes: the logic of handling the flush in LogSegments.scala is weird.
Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Did performance test which shows this PR has a minimal performance impact on Kafka clusters.

Reviewers: Jun Rao <junrao@gmail.com>",180,3,2,873,5610,3,47,1271,291,8,155,3,3086,324,20,1815,167,12,2,1,0,1
core/src/main/scala/kafka/log/LogSegment.scala,core/src/main/scala/kafka/log/LogSegment.scala,"Rework on KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10680)

(reverted #10405). #10405 has several issues, for example:

It fails to create a topic with 9000 partitions.
It flushes in several unnecessary places.
If multiple segments of the same partition are flushed at roughly the same time, we may end up doing multiple unnecessary flushes: the logic of handling the flush in LogSegments.scala is weird.
Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Did performance test which shows this PR has a minimal performance impact on Kafka clusters.

Reviewers: Jun Rao <junrao@gmail.com>",85,4,18,400,2991,6,32,675,151,7,92,2.0,1369,151,15,694,60,8,2,1,0,1
tests/kafkatest/services/streams.py,tests/kafkatest/services/streams.py,"MINOR: set replication.factor to 1 to make StreamsBrokerCompatibilityService work with old broker (#10673)

Reviewers: Matthias J. Sax <mjsax@conflunet.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",87,9,0,616,4154,1,59,763,178,14,56,2.0,919,181,16,156,20,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java,streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java,"MINOR: prevent cleanup() from being called while Streams is still shutting down (#10666)

Currently KafkaStreams#cleanUp only throw an IllegalStateException if the state is RUNNING or REBALANCING, however the application could be in the process of shutting down in which case StreamThreads may still be running. We should also throw if the state is PENDING_ERROR or PENDING_SHUTDOWN

Reviewers: Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>",198,1,1,1014,7168,1,62,1624,140,9,175,4,4044,255,23,2420,284,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/CommonClientConfigs.java,clients/src/main/java/org/apache/kafka/clients/CommonClientConfigs.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",3,20,0,137,854,0,1,205,58,5,41,1,319,58,8,114,26,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ListDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ListDeserializer.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",40,191,0,157,1267,11,11,191,191,191,1,1,191,191,191,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/ListSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ListSerializer.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",27,142,0,111,824,7,7,142,142,142,1,1,142,142,142,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/Serdes.java,clients/src/main/java/org/apache/kafka/common/serialization/Serdes.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",44,30,0,183,1250,3,32,298,193,23,13,3,391,193,30,93,61,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/serialization/ListDeserializerTest.java,clients/src/test/java/org/apache/kafka/common/serialization/ListDeserializerTest.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",18,251,0,212,1579,18,18,251,251,251,1,1,251,251,251,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/serialization/ListSerializerTest.java,clients/src/test/java/org/apache/kafka/common/serialization/ListSerializerTest.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",11,153,0,119,913,11,11,153,153,153,1,1,153,153,153,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/serialization/SerializationTest.java,clients/src/test/java/org/apache/kafka/common/serialization/SerializationTest.java,"KAFKA-8326: Introduce List Serde (#6592)

Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to KIP-466

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Matthias J. Sax <mjsax@conflunet.io>, John Roesler <roesler@confluent.io>, Michael Noll <michael@confluent.io>",37,188,0,310,3028,19,33,371,188,22,17,2,624,188,37,253,180,15,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGrouper.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGrouper.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",9,6,4,52,489,2,2,101,97,5,22,2.0,202,97,9,101,35,5,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopics.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopics.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",29,5,4,159,1131,7,8,213,212,106,2,3.0,217,212,108,4,4,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/TopologyMetadata.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/TopologyMetadata.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",7,51,0,28,154,3,3,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTaskId.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTaskId.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",5,40,0,20,123,3,3,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",72,6,3,882,7648,3,65,1044,235,20,52,4.0,1633,240,31,589,107,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/TopologyTest.java,streams/src/test/java/org/apache/kafka/streams/TopologyTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",122,26,25,1446,9294,13,96,1680,405,44,38,4.0,1997,405,53,317,42,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ChangelogTopicsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ChangelogTopicsTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",5,14,10,172,1814,5,5,212,208,106,2,6.0,222,208,111,10,10,5,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",100,30,24,972,9039,9,84,1176,709,24,48,3.0,1733,709,36,557,70,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGrouperTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGrouperTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",3,20,21,72,908,3,3,105,76,8,13,4,203,76,16,98,24,8,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicsTest.java,"KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata (#10676)

Introduce a Subtopology class to wrap the topicGroupId and namedTopology metadata.

Reviewers: Walker Carlson <wcarlson@confluent.io>",16,14,11,385,2859,7,13,432,429,216,2,4.5,443,429,222,11,11,6,1,0,1,1
core/src/main/scala/kafka/coordinator/transaction/TransactionMetadata.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionMetadata.scala,"KAFKA-12772: Move all transaction state transition rules into their states (#10667)

Co-authored-by: dengziming <dengziming@growingio.com>",87,11,15,348,2274,2,23,541,175,22,25,3,798,175,32,257,67,10,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java,metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",52,1,1,255,2148,0,12,329,275,66,5,1,351,275,70,22,13,4,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java,metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",69,1,1,337,2607,0,18,415,367,69,6,1.5,419,367,70,4,2,1,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ControllerResult.java,metadata/src/main/java/org/apache/kafka/controller/ControllerResult.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",14,1,1,58,398,0,10,89,75,30,3,1,102,75,34,13,12,4,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ControllerResultAndOffset.java,metadata/src/main/java/org/apache/kafka/controller/ControllerResultAndOffset.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",11,1,1,42,316,0,6,69,69,23,3,1,86,69,29,17,16,6,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java,metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",22,1,1,115,954,0,9,154,136,38,4,1.0,173,136,43,19,17,5,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/NoOpSnapshotWriter.java,metadata/src/main/java/org/apache/kafka/controller/NoOpSnapshotWriter.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",5,1,1,24,114,0,5,55,49,18,3,1,56,49,19,1,1,0,1,0,1,1
metadata/src/main/java/org/apache/kafka/controller/SnapshotGenerator.java,metadata/src/main/java/org/apache/kafka/controller/SnapshotGenerator.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",16,1,1,99,597,0,8,146,146,73,2,1.0,147,146,74,1,1,0,1,0,1,1
metadata/src/main/java/org/apache/kafka/controller/SnapshotWriter.java,metadata/src/main/java/org/apache/kafka/controller/SnapshotWriter.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,1,1,10,71,0,0,55,55,28,2,1.0,56,55,28,1,1,0,1,0,1,1
metadata/src/main/java/org/apache/kafka/metalog/MetaLogManager.java,metadata/src/main/java/org/apache/kafka/metalog/MetaLogManager.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,1,1,12,93,0,0,96,79,24,4,1.0,100,79,25,4,3,1,1,0,1,1
metadata/src/test/java/org/apache/kafka/controller/ClientQuotaControlManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/ClientQuotaControlManagerTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",15,1,1,231,2869,0,15,302,238,76,4,1.5,306,238,76,4,2,1,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/ConfigurationControlManagerTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",10,1,1,215,1886,0,8,248,203,62,4,1.5,273,203,68,25,23,6,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ControllerTestUtils.java,metadata/src/test/java/org/apache/kafka/controller/ControllerTestUtils.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",19,1,1,76,594,0,4,102,51,34,3,1,103,51,34,1,1,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java,metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",6,1,1,147,1281,0,5,176,132,44,4,2.5,197,132,49,21,18,5,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/SnapshotGeneratorTest.java,metadata/src/test/java/org/apache/kafka/controller/SnapshotGeneratorTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",2,1,1,85,999,0,2,110,110,55,2,1.0,111,110,56,1,1,0,1,0,1,1
raft/src/main/java/org/apache/kafka/raft/BatchReader.java,raft/src/main/java/org/apache/kafka/raft/BatchReader.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,1,1,9,58,0,0,60,115,20,3,1,117,115,39,57,56,19,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java,raft/src/main/java/org/apache/kafka/raft/internals/BatchAccumulator.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",48,1,1,303,1613,0,26,448,346,64,7,4,509,346,73,61,39,9,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java,raft/src/main/java/org/apache/kafka/raft/internals/BatchBuilder.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",24,1,1,223,1245,0,16,354,306,59,6,2.0,395,306,66,41,29,7,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java,raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",16,1,1,94,538,0,9,131,210,33,4,1.0,267,210,67,136,134,34,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/RecordsIterator.java,raft/src/main/java/org/apache/kafka/raft/internals/RecordsIterator.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",30,1,1,174,1234,0,11,239,239,120,2,1.0,240,239,120,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/StringSerde.java,raft/src/main/java/org/apache/kafka/raft/internals/StringSerde.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",4,1,1,25,194,0,4,48,39,12,4,1.5,51,39,13,3,2,1,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/metadata/MetaLogRaftShim.java,raft/src/main/java/org/apache/kafka/raft/metadata/MetaLogRaftShim.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",18,1,1,106,631,0,15,152,119,38,4,1.5,155,119,39,3,1,1,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/SnapshotReader.java,raft/src/main/java/org/apache/kafka/snapshot/SnapshotReader.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",6,1,1,43,254,0,6,81,81,40,2,1.0,82,81,41,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/SnapshotWriter.java,raft/src/main/java/org/apache/kafka/snapshot/SnapshotWriter.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",10,1,1,72,420,0,7,150,156,25,6,1.0,172,156,29,22,13,4,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/RecordsIteratorTest.java,raft/src/test/java/org/apache/kafka/raft/internals/RecordsIteratorTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",14,1,1,144,1105,0,9,188,188,94,2,1.0,189,188,94,1,1,0,2,1,0,1
server-common/src/main/java/org/apache/kafka/server/common/ApiMessageAndVersion.java,server-common/src/main/java/org/apache/kafka/server/common/ApiMessageAndVersion.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",10,1,1,33,193,0,6,62,62,31,2,1.0,63,62,32,1,1,0,0,0,0,0
server-common/src/main/java/org/apache/kafka/server/common/serialization/AbstractApiMessageSerde.java,server-common/src/main/java/org/apache/kafka/server/common/serialization/AbstractApiMessageSerde.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",4,2,3,45,384,0,3,92,93,46,2,1.5,95,93,48,3,3,2,0,0,0,0
server-common/src/main/java/org/apache/kafka/server/common/serialization/BytesApiMessageSerde.java,server-common/src/main/java/org/apache/kafka/server/common/serialization/BytesApiMessageSerde.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,7,7,27,237,3,3,68,68,34,2,3.5,75,68,38,7,7,4,0,0,0,0
server-common/src/main/java/org/apache/kafka/server/common/serialization/RecordSerde.java,server-common/src/main/java/org/apache/kafka/server/common/serialization/RecordSerde.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,1,1,9,93,0,0,58,52,12,5,1,79,52,16,21,18,4,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/MetadataShell.java,shell/src/main/java/org/apache/kafka/shell/MetadataShell.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",21,1,1,143,962,0,7,180,174,60,3,1,182,174,61,2,1,1,2,1,0,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogMetadataSerde.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogMetadataSerde.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",9,2,1,70,666,0,7,105,104,52,2,1.0,106,104,53,1,1,0,1,0,1,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogMetadataTransform.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogMetadataTransform.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,1,1,7,73,0,0,52,52,26,2,1.0,53,52,26,1,1,0,1,0,1,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataTransform.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataTransform.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",6,1,1,69,699,0,5,98,98,49,2,1.0,99,98,50,1,1,0,1,0,1,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataUpdateTransform.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemoteLogSegmentMetadataUpdateTransform.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,1,1,35,419,0,3,59,59,30,2,1.0,60,59,30,1,1,0,1,0,1,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemotePartitionDeleteMetadataTransform.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serialization/RemotePartitionDeleteMetadataTransform.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,1,1,32,341,0,3,54,54,27,2,1.0,55,54,28,1,1,0,1,0,1,1
storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataTransformTest.java,storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataTransformTest.java,"KAFKA-12758 Added `server-common` module to have server side common classes.  (#10638)

Added server-common module to have server side common classes. Moved ApiMessageAndVersion, RecordSerde, AbstractApiMessageSerde, and BytesApiMessageSerde to server-common module.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",4,1,1,56,575,0,4,85,85,42,2,1.0,86,85,43,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java,"KAFKA-5876: KIP-216 Part 4, Apply InvalidStateStorePartitionException for Interactive Queries (#10657)

KIP-216, part 4 - apply InvalidStateStorePartitionException

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",8,2,1,43,290,1,3,70,55,5,13,2,121,55,9,51,14,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java,"KAFKA-5876: KIP-216 Part 4, Apply InvalidStateStorePartitionException for Interactive Queries (#10657)

KIP-216, part 4 - apply InvalidStateStorePartitionException

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",8,8,0,77,744,1,7,108,71,7,15,3,153,71,10,45,15,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,"MINOR: remove unnecessary placeholder from WorkerSourceTask#recordSent (#10659)

Reviewers: Tom Bentley <tbentley@redhat.com>",91,1,1,563,4058,1,33,721,310,11,67,3,1194,310,18,473,92,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadOnlyDecorator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadOnlyDecorator.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",53,5,5,228,1550,4,38,291,252,48,6,2.5,308,252,51,17,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadWriteDecorator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractReadWriteDecorator.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",52,6,6,223,1549,4,37,284,248,41,7,2,304,248,43,20,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/ReadOnlySessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/ReadOnlySessionStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",12,193,57,79,461,14,12,316,136,32,10,2.0,388,193,39,72,57,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/SessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/SessionStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",5,71,6,64,439,5,5,117,65,9,13,2,179,71,14,62,40,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",60,15,14,408,3004,6,33,505,242,13,40,3.0,934,242,23,429,82,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingSessionBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingSessionBytesStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",14,6,6,79,732,6,14,117,92,11,11,4,169,92,15,52,15,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlySessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlySessionStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",30,12,10,214,1423,6,10,248,169,35,7,3,317,169,45,69,24,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSessionStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSessionStore.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",12,12,6,94,612,6,12,125,143,7,18,2.0,322,143,18,197,80,11,2,1,0,1
streams/src/test/java/org/apache/kafka/test/ReadOnlySessionStoreStub.java,streams/src/test/java/org/apache/kafka/test/ReadOnlySessionStoreStub.java,"KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore (#10390)

Implements: KIP-666 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-666%3A+Add+Instant-based+methods+to+ReadOnlySessionStore)

Reviewers: John Roesler <vvcephei@apache.org>",32,8,7,149,1148,6,17,196,89,20,10,1.5,221,89,22,25,7,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java,"MINOR: replace deprecated Class.newInstance() to new one (#10610)

* replace deprecated Class.newInstance() to class.getDeclaredConstructor().newInstance()
* throw ReflectiveOperationException to cover all other exceptions

Reviewers: Tom Bentley <tbentley@redhat.com>",72,8,7,403,2851,6,33,486,299,20,24,4.0,631,299,26,145,36,6,2,1,0,1
core/src/main/scala/kafka/tools/ConsoleConsumer.scala,core/src/main/scala/kafka/tools/ConsoleConsumer.scala,"MINOR: replace deprecated Class.newInstance() to new one (#10610)

* replace deprecated Class.newInstance() to class.getDeclaredConstructor().newInstance()
* throw ReflectiveOperationException to cover all other exceptions

Reviewers: Tom Bentley <tbentley@redhat.com>",85,1,1,494,3824,1,28,631,211,5,124,3.0,1857,211,15,1226,179,10,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConstantThroughputGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/ConstantThroughputGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",12,19,20,54,280,7,5,111,112,37,3,1,131,112,44,20,20,7,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianFlushGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianFlushGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",7,3,3,43,258,3,5,100,100,33,3,1,103,100,34,3,3,1,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianThroughputGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianThroughputGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",15,23,26,74,410,9,7,149,152,50,3,1,175,152,58,26,26,9,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianTimestampConstantPayloadGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianTimestampConstantPayloadGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",8,125,0,60,385,6,6,125,125,125,1,1,125,125,125,0,0,0,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianTimestampRandomPayloadGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/GaussianTimestampRandomPayloadGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",8,8,3,61,392,4,6,127,122,42,3,1,130,122,43,3,3,1,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/PayloadGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/PayloadGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",0,3,1,20,221,0,0,53,146,8,7,1,181,146,26,128,124,18,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/TimestampConstantPayloadGenerator.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/TimestampConstantPayloadGenerator.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",4,78,0,32,227,3,3,78,78,78,1,1,78,78,78,0,0,0,2,1,0,1
trogdor/src/main/java/org/apache/kafka/trogdor/workload/TimestampRecordProcessor.java,trogdor/src/main/java/org/apache/kafka/trogdor/workload/TimestampRecordProcessor.java,"MINOR: Improvements and fixes for Trogdor payload generators. (#10621)

* Changes the new Throughput Generators to track messages per window
instead of making per-second calculations which can have rounding errors.
Also, one of these had a calculation error which prompted this change in
the first place.

* Fixes a couple typos.

* Fixes an error where certain JSON fields were not exposed, causing the
workloads to not behave as intended.

* Fixes a bug where we use wait not in a loop, which exits too quickly.

* Adds additional constant payload generators.

* Fixes problems with an example spec.

* Fixes several off-by-one comparisons.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",14,1,1,104,712,0,12,162,162,54,3,1,163,162,54,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",113,2,2,410,3524,2,27,549,203,13,42,3.0,1087,204,26,538,80,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GlobalStoreNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GlobalStoreNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",3,3,1,47,319,2,3,73,76,18,4,4.0,92,76,23,19,12,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GraphNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GraphNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",25,2,1,100,558,0,20,143,97,18,8,4.0,200,97,25,57,19,7,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GroupedTableOperationRepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GroupedTableOperationRepartitionNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",10,3,1,90,537,2,9,125,150,16,8,2.0,245,150,31,120,61,15,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableJoinNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableJoinNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",24,2,1,175,1210,2,23,234,155,20,12,6.0,402,155,34,168,67,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/OptimizableRepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/OptimizableRepartitionNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",8,3,1,85,430,2,8,121,153,12,10,1.5,251,153,25,130,85,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorGraphNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorGraphNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,3,1,28,209,2,5,64,79,7,9,2,130,79,14,66,30,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StateStoreNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StateStoreNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",3,3,1,24,175,2,3,49,47,8,6,1.0,57,47,10,8,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StatefulProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StatefulProcessorNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",15,2,1,67,586,2,5,113,115,10,11,3,228,115,21,115,58,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSinkNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSinkNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",10,3,1,47,459,2,3,77,72,9,9,2,130,72,14,53,30,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamSourceNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",11,2,1,63,485,2,5,93,92,9,10,2.5,203,92,20,110,53,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamStreamJoinNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",19,6,3,150,1386,2,17,206,213,23,9,6,342,213,38,136,54,15,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamTableJoinNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamTableJoinNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,2,1,36,266,2,3,71,59,10,7,2,109,59,16,38,17,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamToTableNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StreamToTableNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,3,1,40,377,2,3,69,67,14,5,1,75,67,15,6,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",10,2,1,51,418,2,4,79,47,6,13,2,127,47,10,48,23,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",17,2,1,133,960,2,14,186,141,13,14,2.0,275,141,20,89,29,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/UnoptimizableRepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/UnoptimizableRepartitionNode.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,3,1,76,396,2,5,107,105,36,3,1,109,105,36,2,1,1,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamLeftJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamLeftJoinTest.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",38,9,0,609,6659,3,16,1025,381,26,39,4,1925,405,49,900,194,23,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamOuterJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamOuterJoinTest.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",33,6,0,579,6349,2,15,978,873,244,4,2.5,983,873,246,5,3,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNodeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNodeTest.java,"KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology (#10640)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",3,3,1,49,430,1,3,78,76,39,2,1.5,79,76,40,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",56,16,1,360,2578,1,18,458,246,10,45,3,856,246,19,398,70,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",47,15,6,317,1959,2,23,454,199,13,36,5.0,754,199,21,300,53,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStore.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",19,2,2,105,765,2,16,145,93,8,19,3,220,93,12,75,15,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",291,321,31,1370,5987,60,290,1692,1362,282,6,3.5,1728,1362,288,36,31,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",59,1,1,533,3641,1,35,708,265,6,110,3.0,1817,265,17,1109,210,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",70,1,1,390,2605,1,26,471,394,34,14,1.0,564,394,40,93,28,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStreamThreadTest.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",15,19,13,254,1853,6,12,307,116,10,30,3.0,490,116,16,183,35,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueSegmentTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueSegmentTest.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",5,10,0,95,983,4,5,131,99,19,7,1,147,99,21,16,13,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",52,24,0,302,1913,2,10,352,263,44,8,4.5,416,263,52,64,44,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedSegmentTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedSegmentTest.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",5,14,0,101,1007,4,5,137,99,20,7,1,152,99,22,15,12,2,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorTest.java,"KAFKA-8897: Upgrade RocksDB to 6.19.3 (#10568)

This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Guozhang Wang <wangguoz@gmail.com>",10,4,0,95,991,2,2,149,185,25,6,2.5,233,185,39,84,77,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/WindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/WindowStore.java,"KAFKA-12451: Remove deprecation annotation on long-based read operations in WindowStore (#10296)

Complete https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore by removing deprecation annotation on long-based read operations in WindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",9,6,6,79,641,0,9,191,64,7,28,2.0,325,81,12,134,34,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java,"KAFKA-12451: Remove deprecation annotation on long-based read operations in WindowStore (#10296)

Complete https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore by removing deprecation annotation on long-based read operations in WindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",65,0,3,467,3376,0,32,593,170,11,54,3.0,1055,192,20,462,51,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStore.java,"KAFKA-12451: Remove deprecation annotation on long-based read operations in WindowStore (#10296)

Complete https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore by removing deprecation annotation on long-based read operations in WindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",16,0,3,109,800,0,15,150,82,6,24,3.0,269,82,11,119,26,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java,"KAFKA-12451: Remove deprecation annotation on long-based read operations in WindowStore (#10296)

Complete https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore by removing deprecation annotation on long-based read operations in WindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",15,0,3,82,763,0,12,115,283,2,47,3,939,283,20,824,331,18,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java,"KAFKA-8531: Change default replication factor config (#10532)

Implements KIP-733

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",106,26,4,664,4835,1,22,803,262,16,50,4.0,1555,264,31,752,195,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicManagerTest.java,"KAFKA-8531: Change default replication factor config (#10532)

Implements KIP-733

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",82,57,0,1542,13253,1,79,1750,675,55,32,4.5,2399,735,75,649,156,20,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java,"KAFKA-12252 and KAFKA-12262: Fix session key rotation when leadership changes (#10014)

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",252,5,4,1418,9481,2,77,1860,323,23,80,3.0,3295,528,41,1435,254,18,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java,"KAFKA-12252 and KAFKA-12262: Fix session key rotation when leadership changes (#10014)

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",85,102,3,1943,19277,5,61,2518,301,47,54,5.0,3584,411,66,1066,215,20,2,1,0,1
tests/kafkatest/tests/streams/streams_broker_compatibility_test.py,tests/kafkatest/tests/streams/streams_broker_compatibility_test.py,"MINOR: fix streams_broker_compatibility_test.py (#10632)

The log message was changed and so the system test can't capture expected message.

Reviewers: Anna Sophie Blee-Goldman ableegoldman@apache.org>",6,2,2,119,965,1,6,178,90,7,24,2.0,283,90,12,105,20,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImplJoin.java,"KAFKA-10847: Set shared outer store to an in-memory store when in-memory stores are supplied (#10613)

When users supply in-memory stores for left/outer joins, then the internal shared outer store must be switch to in-memory store too. This will allow users who want to keep all stores in memory to continue doing so.

Added unit tests to validate topology and left/outer joins work fine with an in-memory shared store.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",53,51,25,285,2539,4,11,361,214,36,10,3.5,416,214,42,55,25,6,2,1,0,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadata.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadata.java,"KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. (#10271)

KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format.
Added tests for all the event types for that topic.

This is part of the tiered storaqe implementation KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,56,0,17,91,3,3,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadata.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadata.java,"KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. (#10271)

KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format.
Added tests for all the event types for that topic.

This is part of the tiered storaqe implementation KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",27,13,36,120,660,6,13,259,282,65,4,6.5,326,282,82,67,36,17,1,0,1,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadataUpdate.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadataUpdate.java,"KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. (#10271)

KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format.
Added tests for all the event types for that topic.

This is part of the tiered storaqe implementation KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",12,9,34,47,276,6,6,97,120,24,4,5.5,143,120,36,46,34,12,1,0,1,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteMetadata.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteMetadata.java,"KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. (#10271)

KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format.
Added tests for all the event types for that topic.

This is part of the tiered storaqe implementation KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",12,17,33,50,283,7,6,94,110,31,3,1,127,110,42,33,33,11,1,0,1,1
storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdeTest.java,storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdeTest.java,"KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. (#10271)

KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format.
Added tests for all the event types for that topic.

This is part of the tiered storaqe implementation KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",9,111,0,73,643,9,9,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java,"KAFKA-10767: Adding test cases for all, reverseAll and reverseRange for ThreadCache (#9779)

The test cases for ThreaCache didn't have the corresponding unit tests for all, reverseAll and reverseRange methods. This PR aims to add the same.

Reviewers: Bruno Cadonna <cadonna@apache.org>",67,168,57,516,6094,26,47,626,434,26,24,3.5,1048,434,44,422,74,18,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyWindowStore.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",4,0,67,25,250,0,4,208,81,11,19,2,361,127,19,153,67,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",19,8,46,156,1117,6,10,190,74,11,18,3.0,395,87,22,205,46,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreFacade.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreFacade.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",15,0,24,88,679,3,15,124,123,31,4,3.0,168,123,42,44,24,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/NoOpWindowStore.java,streams/src/test/java/org/apache/kafka/streams/state/NoOpWindowStore.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",19,0,21,93,423,3,19,136,58,12,11,2,180,58,16,44,21,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreFacadeTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreFacadeTest.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",9,6,6,161,1835,3,9,207,207,104,2,3.5,213,207,106,6,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java,"KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore (#10294)

Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",61,6,36,330,2699,6,23,409,151,24,17,4,561,179,33,152,36,9,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/RequestTestUtils.java,clients/src/test/java/org/apache/kafka/common/requests/RequestTestUtils.java,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,0,4,154,1392,1,13,193,193,64,3,1,197,193,66,4,4,1,2,1,0,1
core/src/test/scala/integration/kafka/server/IntegrationTestUtils.scala,core/src/test/scala/integration/kafka/server/IntegrationTestUtils.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,4,4,82,699,2,5,115,115,38,3,1,120,115,40,5,4,2,2,1,0,1
core/src/test/scala/unit/kafka/network/RequestChannelTest.scala,core/src/test/scala/unit/kafka/network/RequestChannelTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,2,3,167,1753,1,12,223,195,22,10,2.0,243,195,24,20,4,2,2,1,0,1
core/src/test/scala/unit/kafka/network/RequestConvertToJsonTest.scala,core/src/test/scala/unit/kafka/network/RequestConvertToJsonTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,7,16,145,1305,3,8,188,197,63,3,4,208,197,69,20,16,7,2,1,0,1
core/src/test/scala/unit/kafka/network/SocketServerTest.scala,core/src/test/scala/unit/kafka/network/SocketServerTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",202,7,7,1681,13342,7,134,2245,506,16,137,3,3450,511,25,1205,100,9,2,1,0,1
core/src/test/scala/unit/kafka/server/AbstractApiVersionsRequestTest.scala,core/src/test/scala/unit/kafka/server/AbstractApiVersionsRequestTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,6,4,47,482,1,4,73,47,6,12,1.5,110,47,9,37,8,3,2,1,0,1
core/src/test/scala/unit/kafka/server/BaseClientQuotaManagerTest.scala,core/src/test/scala/unit/kafka/server/BaseClientQuotaManagerTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,6,9,54,556,0,6,84,92,8,10,2.0,118,92,12,34,9,3,2,1,0,1
core/src/test/scala/unit/kafka/server/BaseRequestTest.scala,core/src/test/scala/unit/kafka/server/BaseRequestTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,6,7,109,891,1,7,153,106,5,33,2,379,106,11,226,94,7,2,1,0,1
core/src/test/scala/unit/kafka/server/ForwardingManagerTest.scala,core/src/test/scala/unit/kafka/server/ForwardingManagerTest.scala,"MINOR: Remove duplicate method in test classes (#10535)

1. Remove duplicate serializing auto-generated data in RequestConvertToJsonTest, this is inspired by #9964
2. Remove RequestTestUtils.serializeRequestWithHeader since we added a AbstractRequest.serializeWithHeader in #10142

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,1,1,191,1656,1,8,254,157,21,12,2.5,357,157,30,103,80,9,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ConfigEntry.java,clients/src/main/java/org/apache/kafka/clients/admin/ConfigEntry.java,"KAFKA-12661 ConfigEntry#equal does not compare other fields when value is NOT null (#10446)

Reviewers: Ismael Juma <ismael@juma.me.uk>",38,12,6,161,881,3,21,286,124,26,11,1,325,131,30,39,23,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/ConfigTest.java,clients/src/test/java/org/apache/kafka/clients/admin/ConfigTest.java,"KAFKA-12661 ConfigEntry#equal does not compare other fields when value is NOT null (#10446)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,15,0,69,652,2,10,100,91,20,5,2,138,91,28,38,37,8,2,1,0,1
core/src/test/scala/unit/kafka/admin/ConfigCommandTest.scala,core/src/test/scala/unit/kafka/admin/ConfigCommandTest.scala,"KAFKA-12661 ConfigEntry#equal does not compare other fields when value is NOT null (#10446)

Reviewers: Ismael Juma <ismael@juma.me.uk>",167,8,1,1358,11877,2,139,1632,182,35,47,3,2121,245,45,489,150,10,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/Batch.java,raft/src/main/java/org/apache/kafka/raft/Batch.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",17,133,0,70,419,11,11,133,133,133,1,1,133,133,133,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/FollowerState.java,raft/src/main/java/org/apache/kafka/raft/FollowerState.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",25,2,0,130,722,1,16,173,127,29,6,6.0,187,127,31,14,9,2,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/LeaderState.java,raft/src/main/java/org/apache/kafka/raft/LeaderState.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",52,15,11,272,1718,2,33,369,301,31,12,2.5,480,301,40,111,45,9,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ReplicatedLog.java,raft/src/main/java/org/apache/kafka/raft/ReplicatedLog.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",14,5,6,74,520,1,3,278,133,25,11,2,325,133,30,47,20,4,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/MemoryBatchReader.java,raft/src/main/java/org/apache/kafka/raft/internals/MemoryBatchReader.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",7,1,0,45,273,0,6,71,70,36,2,1.0,71,70,36,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java,raft/src/main/java/org/apache/kafka/snapshot/FileRawSnapshotWriter.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",19,70,39,127,628,8,11,170,117,42,4,6.5,215,117,54,45,39,11,2,1,0,1
raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotWriter.java,raft/src/main/java/org/apache/kafka/snapshot/RawSnapshotWriter.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",0,6,17,13,92,0,0,77,74,19,4,6.0,106,74,26,29,17,7,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/MemoryBatchReaderTest.java,raft/src/test/java/org/apache/kafka/raft/internals/MemoryBatchReaderTest.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",1,13,7,41,360,1,1,70,64,35,2,2.0,77,64,38,7,7,4,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/RecordsBatchReaderTest.java,raft/src/test/java/org/apache/kafka/raft/internals/RecordsBatchReaderTest.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",4,9,48,82,683,5,3,119,158,40,3,1,168,158,56,49,48,16,2,1,0,1
raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java,raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",27,35,27,266,2281,8,13,358,289,72,5,20,429,289,86,71,27,14,2,1,0,1
raft/src/test/java/org/apache/kafka/snapshot/SnapshotWriterReaderTest.java,raft/src/test/java/org/apache/kafka/snapshot/SnapshotWriterReaderTest.java,"KAFKA-12154; Raft Snapshot Loading API (#10085)

Implement Raft Snapshot loading API.

1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.

2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s

3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. 

4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.

5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.

6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.

Reviewers: dengziming <swzmdeng@163.com>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",11,33,11,103,887,8,7,138,116,34,4,1.0,151,116,38,13,11,3,2,1,0,1
tests/kafkatest/tests/connect/connect_distributed_test.py,tests/kafkatest/tests/connect/connect_distributed_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",79,1,1,403,4275,1,24,600,136,17,35,4,848,141,24,248,37,7,2,1,0,1
tests/kafkatest/tests/core/delegation_token_test.py,tests/kafkatest/tests/core/delegation_token_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",10,1,1,93,772,1,10,132,130,33,4,1.0,134,130,34,2,1,0,2,1,0,1
tests/kafkatest/tests/core/fetch_from_follower_test.py,tests/kafkatest/tests/core/fetch_from_follower_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",11,1,1,85,709,1,4,134,144,34,4,2.0,155,144,39,21,15,5,2,1,0,1
tests/kafkatest/tests/core/log_dir_failure_test.py,tests/kafkatest/tests/core/log_dir_failure_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",16,1,1,116,977,1,5,185,177,62,3,1,215,177,72,30,29,10,2,1,0,1
tests/kafkatest/tests/core/reassign_partitions_test.py,tests/kafkatest/tests/core/reassign_partitions_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",11,1,1,92,688,1,7,160,109,12,13,1,190,109,15,30,22,2,2,1,0,1
tests/kafkatest/tests/tools/log_compaction_test.py,tests/kafkatest/tests/tools/log_compaction_test.py,"MINOR: system test spelling/pydoc/dead code fixes (#10604)

Reviewers: Kamal Chandraprakash <kamal@nmsworks.co.in>, Chia-Ping Tsai <chia7712@gmail.com>",7,1,1,41,320,1,5,70,66,23,3,1,76,66,25,6,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java,streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",21,13,3,184,1870,2,21,614,1211,15,41,4,1766,1211,43,1152,589,28,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/Topology.java,streams/src/main/java/org/apache/kafka/streams/Topology.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",33,1,1,312,2076,0,31,941,640,50,19,3,1042,640,55,101,34,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StreamsAssignmentProtocolVersions.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StreamsAssignmentProtocolVersions.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",1,4,2,8,60,0,1,29,30,4,7,1,42,30,6,13,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/KafkaStreamsNamedTopologyWrapper.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",4,42,0,18,125,4,4,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTopology.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTopology.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",4,48,0,22,140,3,3,48,48,48,1,1,48,48,48,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTopologyStreamsBuilder.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/namedtopology/NamedTopologyStreamsBuilder.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",3,44,0,21,128,3,3,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java,"KAFKA-12648: basic skeleton API for NamedTopology (#10615)

Just the API for NamedTopology.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Walker Carlson <wcarlson@confluent.io>",15,7,2,264,2698,2,12,340,332,38,9,1,363,332,40,23,13,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java,"KAFKA-12396: added null check for state stores key (#10548)

Reviewers: Bruno Cadonna <bruno@confluent.io>, Matthias J. Sax <matthias@confluent.io>",33,0,3,160,1102,1,24,213,187,10,22,2.5,396,187,18,183,45,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java,"MINOR: clean up some remaining locking stuff in StateDirectory (#10608)

Minor followup to #10342 that I noticed while working on the NamedTopology stuff. Cleans up a few things:

We no longer need locking for the global state directory either, since it's contained within the top-level state directory lock. Definitely less critical than the task directory locking, since it's less vulnerable to IOExceptions given that it's just locked and unlocked once during the application lifetime, but nice to have nonetheless
Clears out misc. usages of the LOCK_FILE_NAME that no longer apply. This has the awesome side effect of finally being able to actually delete obsolete task directories, whereas previously we had to leave behind the empty directory due to a ridiculous Windows bug (though I'm sure they would claim ""it's not a bug it's a feature"" 😉 )
Lazily delete old-and-now-unused lock files in the StateDirectory#taskDirIsEmpty method to clean up the state directory for applications that upgraded from an older version that still used task locking

Reviewers: Walker Carlson <wcarlson@confluent.io>",62,0,79,1009,7433,6,51,1198,542,29,41,5,1874,601,46,676,102,16,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/BrokersToIsrsTest.java,metadata/src/test/java/org/apache/kafka/controller/BrokersToIsrsTest.java,"MINOR: clean up some replication code (#10564)

Centralize leader and ISR changes in generateLeaderAndIsrUpdates.
Consolidate handleNodeDeactivated and handleNodeActivated into this
function.

Rename BrokersToIsrs#noLeaderIterator to BrokersToIsrs#partitionsWithNoLeader.
Create BrokersToIsrs#partitionsLedByBroker, BrokersToIsrs#partitionsWithBrokerInIsr

In ReplicationControlManagerTest, createTestTopic should be a member
function of ReplicationControlTestContext.  It should invoke
ReplicationControlTestContext#replay so that records are applied to all
parts of the test context.

Reviewers: Jun Rao <junrao@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",7,2,2,82,1013,1,5,109,109,36,3,2,129,109,43,20,18,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java,clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",93,1,4,495,3568,2,58,667,109,10,66,3.0,1710,276,26,1043,172,16,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java,clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsTest.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",80,2,0,838,8530,1,24,1025,136,24,42,3.0,1745,270,42,720,275,17,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java,raft/src/test/java/org/apache/kafka/raft/LeaderStateTest.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",24,41,22,248,2355,18,21,310,228,39,8,10.0,417,228,52,107,30,13,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java,raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",60,21,18,888,8176,16,60,1073,942,134,8,2.0,1116,942,140,43,18,5,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/BatchAccumulatorTest.java,raft/src/test/java/org/apache/kafka/raft/internals/BatchAccumulatorTest.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",17,129,1,403,3283,4,16,518,296,86,6,2.5,562,296,94,44,39,7,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/KafkaRaftMetricsTest.java,raft/src/test/java/org/apache/kafka/raft/internals/KafkaRaftMetricsTest.java,"KAFKA-12265; Move the BatchAccumulator in KafkaRaftClient to LeaderState (#10480)

The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",11,4,1,198,2374,1,10,267,263,53,5,3,277,263,55,10,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosLogin.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosLogin.java,"KAFKA-12730; Avoid duplicate logout if Kerberos login fails (#10611)

From Java 9 onwards, LoginContext#logout() throws an NPE if invoked multiple times due to https://bugs.openjdk.java.net/browse/JDK-8173069. KerberosLogin currently attempts logout followed by login in a background refresh thread. If login fails we retry the same sequence. As a result, a single login failure prevents subsequent re-login. And clients will never be able to authenticate successfully after the first failure, until the process is restarted. The commit checks if logout is necessary before invoking LoginContext#logout(). Also adds a test for this case.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",56,11,3,299,1987,2,14,396,389,16,24,2.5,674,389,28,278,107,12,2,1,0,1
core/src/test/scala/integration/kafka/server/GssapiAuthenticationTest.scala,core/src/test/scala/integration/kafka/server/GssapiAuthenticationTest.scala,"KAFKA-12730; Avoid duplicate logout if Kerberos login fails (#10611)

From Java 9 onwards, LoginContext#logout() throws an NPE if invoked multiple times due to https://bugs.openjdk.java.net/browse/JDK-8173069. KerberosLogin currently attempts logout followed by login in a background refresh thread. If login fails we retry the same sequence. As a result, a single login failure prevents subsequent re-login. And clients will never be able to authenticate successfully after the first failure, until the process is restarted. The commit checks if logout is necessary before invoking LoginContext#logout(). Also adds a test for this case.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",24,52,12,227,1704,7,19,314,155,22,14,1.5,368,155,26,54,15,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",217,2,0,1213,10790,0,100,1494,219,11,131,5,4950,512,38,3456,370,26,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSide.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSide.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",12,81,0,41,255,7,7,81,81,81,1,1,81,81,81,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideDeserializer.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,66,0,38,324,6,6,66,66,66,1,1,66,66,66,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerde.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerde.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,29,0,11,112,1,1,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializer.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",7,69,0,38,304,5,5,69,69,69,1,1,69,69,69,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java,streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValue.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",23,113,0,55,367,9,9,113,113,113,1,1,113,113,113,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueDeserializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueDeserializer.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",10,74,0,46,410,5,5,74,74,74,1,1,74,74,74,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerde.java,streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerde.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,33,0,15,147,1,1,33,33,33,1,1,33,33,33,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerializer.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,88,0,54,432,5,5,88,88,88,1,1,88,88,88,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreBuilder.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,0,4,55,396,1,4,80,84,40,2,1.0,84,84,42,4,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",9,4,4,351,3463,4,9,407,262,37,11,7,765,262,70,358,192,33,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplValueJoinerWithKeyTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplValueJoinerWithKeyTest.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",10,2,4,177,1377,2,9,227,229,114,2,1.5,231,229,116,4,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializerTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/KeyAndJoinSideSerializerTest.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,72,0,39,330,3,3,72,72,72,1,1,72,72,72,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerializerTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/LeftOrRightValueSerializerTest.java,"KAFKA-10847: Fix spurious results on left/outer stream-stream joins (#10462)

Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.

To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.

A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The KStreamStreamJoin has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.

Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The KStreamStreamJoin checks for expired records and emit them every time a new record is processed in the join processor.

The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of <joinSide-recordKey>. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, KeyAndJoinSideSerde which serializes a boolean value that specifies the side where the key is found, and ValueOrOtherValueSerde that serializes either V1 or V2 based on where the key was found.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,78,0,44,377,4,4,78,78,78,1,1,78,78,78,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/Producer.java,clients/src/main/java/org/apache/kafka/clients/producer/Producer.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",0,1,0,31,277,0,0,107,38,6,17,3,160,38,9,53,19,3,2,1,0,1
core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala,core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",153,3,3,1818,16490,3,115,2194,388,13,175,4,4671,419,27,2477,365,14,2,1,0,1
core/src/test/scala/integration/kafka/api/TransactionsBounceTest.scala,core/src/test/scala/integration/kafka/api/TransactionsBounceTest.scala,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",12,2,0,152,1207,0,8,210,185,10,21,2,305,185,15,95,25,5,2,1,0,1
core/src/test/scala/integration/kafka/api/TransactionsTest.scala,core/src/test/scala/integration/kafka/api/TransactionsTest.scala,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",53,6,4,592,4885,4,28,785,400,17,46,2.0,1212,400,26,427,194,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",5,2,1,125,1222,1,4,174,147,17,10,2.0,203,147,20,29,11,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",15,2,1,290,2368,1,13,339,295,31,11,3,518,295,47,179,108,16,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",26,27,19,353,2886,17,26,490,235,45,11,3,822,375,75,332,158,30,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",46,13,10,802,5615,6,43,942,328,20,48,8.0,2692,658,56,1750,521,36,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",32,3,2,479,4175,2,29,633,190,6,99,3,2131,324,22,1498,643,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",72,9,7,871,5465,7,72,1124,638,86,13,9,1750,638,135,626,169,48,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsEosTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsEosTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",15,5,2,62,465,1,1,95,57,11,9,3,120,57,13,25,8,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",12,8,3,70,545,1,1,110,75,6,19,3,180,75,9,70,8,4,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverEosTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverEosTest.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",1,1,1,7,44,1,1,26,26,13,2,1.0,27,26,14,1,1,0,1,0,1,1
tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java,tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java,"KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2 (#10573)

Deprecates the following 

1. StreamsConfig.EXACTLY_ONCE
2. StreamsConfig.EXACTLY_ONCE_BETA
3. Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)

And introduces a new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""

Reviewers: Matthias J. Sax <mjsax@confluent.io>",26,3,5,310,2497,1,11,385,287,26,15,2,495,287,33,110,43,7,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationBaseTest.java,"KAFKA-12284: increase request timeout to make tests reliable (#10547)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",32,23,7,452,4471,2,16,643,601,80,8,4.0,773,601,97,130,65,16,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java,"KAFKA-12284: increase request timeout to make tests reliable (#10547)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",66,22,9,344,3193,5,31,481,339,40,12,5.0,540,339,45,59,18,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionResult.java,"KAFKA-12716; Add `Admin` API to abort transactions (#10599)

This patch adds the Admin API to abort transactions from KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. The `WriteTxnMarker` API needs to be sent to partition leaders, so we are able to reuse `PartitionLeaderStrategy`, which was introduced when support for `DescribeProducers` was added.

Reviewers: David Jacot <djacot@confluent.io>",2,50,0,16,140,2,2,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionSpec.java,clients/src/main/java/org/apache/kafka/clients/admin/AbortTransactionSpec.java,"KAFKA-12716; Add `Admin` API to abort transactions (#10599)

This patch adds the Admin API to abort transactions from KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. The `WriteTxnMarker` API needs to be sent to partition leaders, so we are able to reuse `PartitionLeaderStrategy`, which was introduced when support for `DescribeProducers` was added.

Reviewers: David Jacot <djacot@confluent.io>",14,85,0,57,282,8,8,85,85,85,1,1,85,85,85,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java,clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersRequest.java,"KAFKA-12716; Add `Admin` API to abort transactions (#10599)

This patch adds the Admin API to abort transactions from KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. The `WriteTxnMarker` API needs to be sent to partition leaders, so we are able to reuse `PartitionLeaderStrategy`, which was introduced when support for `DescribeProducers` was added.

Reviewers: David Jacot <djacot@confluent.io>",37,5,0,163,1135,1,19,208,186,16,13,3,385,186,30,177,120,14,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/AbortTransactionHandlerTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/AbortTransactionHandlerTest.java,"KAFKA-12716; Add `Admin` API to abort transactions (#10599)

This patch adds the Admin API to abort transactions from KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. The `WriteTxnMarker` API needs to be sent to partition leaders, so we are able to reuse `PartitionLeaderStrategy`, which was introduced when support for `DescribeProducers` was added.

Reviewers: David Jacot <djacot@confluent.io>",11,218,0,172,1760,11,11,218,218,218,1,1,218,218,218,0,0,0,0,0,0,0
core/src/main/scala/kafka/tools/StreamsResetter.java,core/src/main/scala/kafka/tools/StreamsResetter.java,"KAFKA-6435: KIP-623 Add internal topics option to streamResetter (#8923)

Allow user to specify subset of internal topics to clean up with application reset tool

Reviewers: Boyang Chen <boyang@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Walker Carlson <wcarlson@confluent.io>",93,39,15,557,4431,6,23,696,260,14,50,4.0,1270,264,25,574,83,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java,"KAFKA-6435: KIP-623 Add internal topics option to streamResetter (#8923)

Allow user to specify subset of internal topics to clean up with application reset tool

Reviewers: Boyang Chen <boyang@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Walker Carlson <wcarlson@confluent.io>",14,32,0,239,2076,2,14,348,255,7,47,3,1093,255,23,745,344,16,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tools/StreamsResetterTest.java,streams/src/test/java/org/apache/kafka/streams/tools/StreamsResetterTest.java,"KAFKA-6435: KIP-623 Add internal topics option to streamResetter (#8923)

Allow user to specify subset of internal topics to clean up with application reset tool

Reviewers: Boyang Chen <boyang@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Walker Carlson <wcarlson@confluent.io>",19,4,4,238,2500,1,18,321,293,29,11,4,414,293,38,93,38,8,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/CogroupedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/CogroupedKStream.scala,"KAFKA-12344 Support SlidingWindows in the Scala API (#10519)

Support SlidingWindows in the Scala API

Reviewers: Leah Thomas <lthomas@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",4,17,1,27,311,1,4,111,73,37,3,2,118,73,39,7,6,2,1,0,1,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KGroupedStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KGroupedStream.scala,"KAFKA-12344 Support SlidingWindows in the Scala API (#10519)

Support SlidingWindows in the Scala API

Reviewers: Leah Thomas <lthomas@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",6,11,0,63,753,1,6,189,145,11,17,2,297,145,17,108,46,6,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KTableTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KTableTest.scala,"KAFKA-12344 Support SlidingWindows in the Scala API (#10519)

Support SlidingWindows in the Scala API

Reviewers: Leah Thomas <lthomas@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",14,48,2,386,2917,2,12,498,224,36,14,4.0,696,224,50,198,80,14,2,1,0,1
core/src/main/scala/kafka/admin/LeaderElectionCommand.scala,core/src/main/scala/kafka/admin/LeaderElectionCommand.scala,"KAFKA-12684: Fix noop set is incorrectly replaced with succeeded set from LeaderElectionCommand (#10558)

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",33,1,1,233,1369,1,5,289,240,32,9,1,307,240,34,18,7,2,2,1,0,1
core/src/test/scala/unit/kafka/admin/LeaderElectionCommandTest.scala,core/src/test/scala/unit/kafka/admin/LeaderElectionCommandTest.scala,"KAFKA-12684: Fix noop set is incorrectly replaced with succeeded set from LeaderElectionCommand (#10558)

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",16,42,0,270,1801,2,16,343,373,26,13,1,520,373,40,177,86,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,"KAFKA-10746: Change to Warn logs when necessary to notify users (#9627)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",213,11,6,1072,7300,1,77,1547,638,12,133,4,3030,638,23,1483,186,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/JoinStoreIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/JoinStoreIntegrationTest.java,"KAFKA-5876: Apply UnknownStateStoreException for Interactive Queries (#9821)

KIP-216: IQ should throw different exceptions for different errors, Part 2

Reviewers: Matthias J. Sax <mjsax@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Bruno Cadonna <cadonna@confluent.io>",6,3,3,98,865,1,5,133,114,27,5,3,154,114,31,21,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java,"KAFKA-10283; Consolidate client-level and consumer-level assignment within ClientState (#9640)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",81,73,65,321,2607,38,53,426,80,18,24,7.0,857,127,36,431,118,18,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTask.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTask.java,"KAFKA-10283; Consolidate client-level and consumer-level assignment within ClientState (#9640)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,45,0,22,146,4,4,45,45,45,1,1,45,45,45,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ClientStateTest.java,"KAFKA-10283; Consolidate client-level and consumer-level assignment within ClientState (#9640)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",48,11,0,408,3638,2,45,493,151,31,16,5.5,715,151,45,222,63,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeTransactionsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeTransactionsResult.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",6,81,0,39,327,3,3,81,81,81,1,1,81,81,81,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/TransactionDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/TransactionDescription.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",20,113,0,83,418,11,11,113,113,113,1,1,113,113,113,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/TransactionState.java,clients/src/main/java/org/apache/kafka/clients/admin/TransactionState.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",4,56,0,31,204,3,3,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/CoordinatorKey.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/CoordinatorKey.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",10,62,0,36,229,6,6,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/CoordinatorStrategy.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/CoordinatorStrategy.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",17,118,0,83,560,8,8,118,118,118,1,1,118,118,118,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/PartitionLeaderStrategy.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/PartitionLeaderStrategy.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",24,0,1,139,941,1,8,183,184,92,2,1.0,184,184,92,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/ConsumerGroupState.java,clients/src/main/java/org/apache/kafka/common/ConsumerGroupState.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",4,6,9,27,176,1,3,57,61,19,3,1,67,61,22,10,9,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/CoordinatorStrategyTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/CoordinatorStrategyTest.java,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",9,140,0,101,914,9,9,140,140,140,1,1,140,140,140,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMetadataTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMetadataTest.scala,"KAFKA-12586; Add `DescribeTransactions` Admin API (#10483)

This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",34,6,0,442,2998,1,24,531,188,53,10,1.5,593,193,59,62,22,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java,"KAFKA-12704: Improve cache access during connector class instantiation in config validations (#10580)

Concurrent requests to validate endpoint for the same connector type calls AbstractHerder::getConnector to get the cached connector instances and if the connector hasn't been cached yet then there is a race condition in the AbstractHerder::getConnector method that potentially fails to detect that an instance of the connector has already been created and, as a result, can create another instance

Existing tests are present with enough coverage so no new tests are added.

Reviewers: Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",93,3,8,551,4426,1,45,688,156,15,47,3,935,156,20,247,50,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java,"KAFKA-12700: override toString method to show correct value in doc (#10574)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",39,5,0,447,2779,1,15,552,141,13,44,2.0,693,141,16,141,24,3,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/SourceAndTarget.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/SourceAndTarget.java,"MINOR: Remove unthrown exceptions, fix typo, etc. (#10402)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",7,1,1,27,130,0,6,52,52,26,2,1.0,53,52,26,1,1,0,2,1,0,1
connect/mirror-client/src/test/java/org/apache/kafka/connect/mirror/MirrorClientTest.java,connect/mirror-client/src/test/java/org/apache/kafka/connect/mirror/MirrorClientTest.java,"MINOR: Remove unthrown exceptions, fix typo, etc. (#10402)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",12,6,7,126,1092,6,12,162,163,54,3,2,173,163,58,11,7,4,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorHeartbeatTask.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorHeartbeatTask.java,"MINOR: Remove unthrown exceptions, fix typo, etc. (#10402)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",7,1,1,56,363,1,6,85,84,28,3,1,87,84,29,2,1,1,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java,"MINOR: Remove unthrown exceptions, fix typo, etc. (#10402)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",65,2,4,397,3720,2,40,507,390,56,9,4,597,390,66,90,37,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/BufferFullStrategy.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/BufferFullStrategy.java,"HOTFIX: remove unimplemented SPILL_TO_DISK buffer full strategy (#10571)

Remove enum for the spill-to-disk strategy since this hasn't been implemented.

Reviewers: Walker Carlson <wcarlson@confluent.io>",0,0,1,5,23,0,0,22,23,11,2,1.0,23,23,12,1,1,0,2,1,0,1
core/src/main/scala/kafka/log/LogCleaner.scala,core/src/main/scala/kafka/log/LogCleaner.scala,"KAFKA-12553: Refactor recovery logic to introduce LogLoader (#10478)

In this PR, I have refactored the recovery logic code introducing a new class kafka.log.LogLoader responsible for all activities related with recovery of log segments from disk. With this change, the recovery logic has been moved out of the Log class and into the new LogLoader class.

Advantages:
This refactor has the following advantages over the existing code:

As such, the recovery logic is invoked once only during Log instantiation. Some parts of the recovery logic are fairly independent from the rest of the Log class. By moving the independent private logic to a separate LogLoader class, the existing Log class has become more modular, and the constructor behavior is a lot simpler now. Therefore, this makes the code more maintainable.
This PR takes us a step closer towards the Log layer reactor work (KAFKA-12554). The Log recovery logic reads and writes to LeaderEpochFileCache and ProducerStateManager instances, so as such the logic does not fit very well into the definition of a ""local log"". By extracting it out of the Log class, in the future this will make it much easier to clearly define the separation of concerns between LocalLog and UnifiedLog.

Reviewers: Satish Duggana <satishd@apache.org>, Jun Rao <junrao@gmail.com>",150,5,6,717,5202,4,53,1170,557,11,106,3.5,2411,557,23,1241,128,12,2,1,0,1
core/src/main/scala/kafka/log/LogConfig.scala,core/src/main/scala/kafka/log/LogConfig.scala,"KAFKA-12553: Refactor recovery logic to introduce LogLoader (#10478)

In this PR, I have refactored the recovery logic code introducing a new class kafka.log.LogLoader responsible for all activities related with recovery of log segments from disk. With this change, the recovery logic has been moved out of the Log class and into the new LogLoader class.

Advantages:
This refactor has the following advantages over the existing code:

As such, the recovery logic is invoked once only during Log instantiation. Some parts of the recovery logic are fairly independent from the rest of the Log class. By moving the independent private logic to a separate LogLoader class, the existing Log class has become more modular, and the constructor behavior is a lot simpler now. Therefore, this makes the code more maintainable.
This PR takes us a step closer towards the Log layer reactor work (KAFKA-12554). The Log recovery logic reads and writes to LeaderEpochFileCache and ProducerStateManager instances, so as such the logic does not fit very well into the definition of a ""local log"". By extracting it out of the Log class, in the future this will make it much easier to clearly define the separation of concerns between LocalLog and UnifiedLog.

Reviewers: Satish Duggana <satishd@apache.org>, Jun Rao <junrao@gmail.com>",20,7,0,334,3042,0,15,415,96,6,67,2,941,142,14,526,107,8,2,1,0,1
core/src/test/scala/unit/kafka/admin/TopicCommandWithZKClientTest.scala,core/src/test/scala/unit/kafka/admin/TopicCommandWithZKClientTest.scala,"MINOR: Fix nonsense test line from TopicCommandTest (#10551)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,1,1,423,3641,1,37,590,242,14,43,2,925,370,22,335,128,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java,"MINOR: Modify unnecessary access specifiers (#9861)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,4,4,70,553,1,5,104,56,10,10,3.0,160,56,16,56,26,6,2,1,0,1
core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala,core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala,"MINOR: Remove redundant code from BrokerApiVersionsCommand (#10556)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",27,7,17,270,1754,8,14,330,243,15,22,2.0,400,245,18,70,17,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java,clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java,"MINOR: remove `checksumOrNull` and `isValid` from Record (#10498)

1. rewrite the checksum of DumpLogSegments
2. remove checksumOrNull and isValid from Record

Reviewers: Ismael Juma <ismael@juma.me.uk>",106,0,5,454,2559,1,71,608,456,32,19,1,680,456,36,72,25,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java,clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java,"MINOR: remove `checksumOrNull` and `isValid` from Record (#10498)

1. rewrite the checksum of DumpLogSegments
2. remove checksumOrNull and isValid from Record

Reviewers: Ismael Juma <ismael@juma.me.uk>",107,0,12,468,3164,2,38,620,457,30,21,2,820,457,39,200,42,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/Record.java,clients/src/main/java/org/apache/kafka/common/record/Record.java,"MINOR: remove `checksumOrNull` and `isValid` from Record (#10498)

1. rewrite the checksum of DumpLogSegments
2. remove checksumOrNull and isValid from Record

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,0,13,21,128,0,0,129,286,5,26,1.5,993,370,38,864,566,33,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/DefaultRecordBatchTest.java,clients/src/test/java/org/apache/kafka/common/record/DefaultRecordBatchTest.java,"MINOR: remove `checksumOrNull` and `isValid` from Record (#10498)

1. rewrite the checksum of DumpLogSegments
2. remove checksumOrNull and isValid from Record

Reviewers: Ismael Juma <ismael@juma.me.uk>",30,6,10,340,3611,6,21,423,210,24,18,2.0,480,210,27,57,22,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegment.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegment.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",10,6,0,50,323,1,8,78,68,16,5,2,85,68,17,7,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowStore.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",28,12,17,148,1033,9,25,207,236,69,3,3,248,236,83,41,24,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java,streams/src/main/java/org/apache/kafka/streams/state/internals/Segment.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",0,1,0,8,79,0,0,29,42,2,17,2,104,42,6,75,42,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentedBytesStore.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",1,20,0,32,405,1,1,227,122,21,11,2,257,122,23,30,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchema.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchema.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",25,41,65,128,1035,13,20,191,215,96,2,10.0,256,215,128,65,65,32,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegment.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegment.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",10,6,0,50,319,1,8,78,70,20,4,2.0,84,70,21,6,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchemaTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeySchemaTest.java,"KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally (#10537)

This PR changes the TimeOrderedKeySchema composite key from time-seq-key -> time-key-seq to allow deletion of duplicated time-key records using the RocksDB deleteRange API. It also removes all duplicates when put(key, null) is called. Currently, the put(key, null) was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.

The RocksDB deleteRange(keyFrom, keyTo) deletes a range of keys from keyFrom (inclusive) to keyTo (exclusive). To make keyTo inclusive, I incremented the end key by one when calling the RocksDBAccessor.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",7,0,93,57,589,6,7,84,177,42,2,3.0,177,177,88,93,93,46,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java,"KAFKA-12633: Remove deprecated APIs in TopologyTestDriver (#10508)

As well as related test classes.

Reviewers: John Roesler <vvcephei@apache.org>",116,16,173,1407,11299,10,80,1706,692,38,45,5,2510,692,56,804,173,18,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/TransformationDoc.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",5,7,1,69,784,0,4,100,87,12,8,2.5,116,87,14,16,8,2,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/DropHeaders.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",6,74,0,46,390,4,4,74,74,74,1,1,74,74,74,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HeaderFrom.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",29,238,0,188,1521,16,16,238,238,238,1,1,238,238,238,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertHeader.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",4,77,0,46,386,4,4,77,77,77,1,1,77,77,77,0,0,0,0,0,0,0
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/DropHeadersTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/DropHeadersTest.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",9,117,0,85,751,7,7,117,117,117,1,1,117,117,117,0,0,0,0,0,0,0
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HeaderFromTest.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",35,359,0,303,2617,17,17,359,359,359,1,1,359,359,359,0,0,0,0,0,0,0
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertHeaderTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertHeaderTest.java,"KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader (#9549)

These SMTs were originally specified in KIP-145 but never implemented
at the time.

HeaderTo is not included since its original specification doesn't deal with
the fact that there can be >1 header with the same name, but a field can only
have a single value (which could be an array, but not if the headers for
the given name had different schemas).

Reviewers: Chris Egerton <chrise@confluent.io>, Mickael Maison <mickael.maison@gmail.com>",10,121,0,87,801,8,8,121,121,121,1,1,121,121,121,0,0,0,0,0,0,0
core/src/test/scala/integration/kafka/api/TransactionsWithMaxInFlightOneTest.scala,core/src/test/scala/integration/kafka/api/TransactionsWithMaxInFlightOneTest.scala,"MINOR: fix package name in integration test (#10400)

Change package name of IntegrationTestUtils ,TransactionsWithMaxInFlightOneTest, ControllerContextTest and DefaultMessageFormatterTest to kafka.server since we set the package name to kafka.xxx in all other classes.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,3,3,89,714,1,6,131,131,44,3,3,138,131,46,7,4,2,2,1,0,1
core/src/test/scala/kafka/tools/DefaultMessageFormatterTest.scala,core/src/test/scala/kafka/tools/DefaultMessageFormatterTest.scala,"MINOR: fix package name in integration test (#10400)

Change package name of IntegrationTestUtils ,TransactionsWithMaxInFlightOneTest, ControllerContextTest and DefaultMessageFormatterTest to kafka.server since we set the package name to kafka.xxx in all other classes.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,5,6,204,1154,0,7,234,237,58,4,4.5,274,237,68,40,32,10,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerContextTest.scala,core/src/test/scala/unit/kafka/controller/ControllerContextTest.scala,"MINOR: fix package name in integration test (#10400)

Change package name of IntegrationTestUtils ,TransactionsWithMaxInFlightOneTest, ControllerContextTest and DefaultMessageFormatterTest to kafka.server since we set the package name to kafka.xxx in all other classes.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,2,6,153,1741,0,10,206,189,21,10,2.0,279,189,28,73,41,7,2,1,0,1
core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala,core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala,"MINOR: fix package name in integration test (#10400)

Change package name of IntegrationTestUtils ,TransactionsWithMaxInFlightOneTest, ControllerContextTest and DefaultMessageFormatterTest to kafka.server since we set the package name to kafka.xxx in all other classes.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",50,8,10,457,4317,1,26,609,443,47,13,4,799,443,61,190,68,15,2,1,0,1
core/src/test/scala/unit/kafka/server/SaslApiVersionsRequestTest.scala,core/src/test/scala/unit/kafka/server/SaslApiVersionsRequestTest.scala,"MINOR: fix package name in integration test (#10400)

Change package name of IntegrationTestUtils ,TransactionsWithMaxInFlightOneTest, ControllerContextTest and DefaultMessageFormatterTest to kafka.server since we set the package name to kafka.xxx in all other classes.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,5,6,80,704,0,6,109,78,5,23,2,248,78,11,139,33,6,2,1,0,1
core/src/main/scala/kafka/network/RequestChannel.scala,core/src/main/scala/kafka/network/RequestChannel.scala,"MINOR: fix some bugs in ControllerApis.scala (#10505)

Fix some cases where ControllerApis was blocking on the controller
thread.  This should not be necessary, since the controller thread can
just interface directly with the network threads.

Fix some cases where ControllerApis wasn't doing authorization
correctly.  Since the previous release of KRaft did not support
authorizers, this bug is not as severe as it could have been, but it
still needs to be fixed.  Add authorization unit tests for each API.

Add support for the deprecated ALTER_CONFIGS API in ControllerApis.  It
was already supported in QuorumController, but wasn't exposed
previously.

Fix how we validate duplicate config resources and unknown config
resource types in ControllerApis.  Duplicates should yield an
INVALID_REQUEST error, and unknown resource types should give an error
with the corresponding numerical resource type and UNSUPPORTED_VERSION.
Fix some redaction code in RequestChannel that was throwing an exception
when duplicate config resources were present in the request.

Fix a comment in ControllerApis#deleteTopics that no longer reflects
what the code is doing when we don't have ""describe"" permission.

Add function stubs for the KIP-455 reassignment APIs in ControllerApis
and QuorumController.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Arthur <mumrah@gmail.com>",58,16,26,435,3093,0,30,579,83,5,127,2,1400,90,11,821,65,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsRequest.java,"MINOR: Update default for field in DeleteTopicsRequest

When using DeleteTopicsRequest with topic IDs, the topic name field should be null. Before, the code was programmatically assigning null, but it will be easier and less error prone to simply set that as the default. 

Tested using the previously created `DeleteTopicsRequestTest.java` file.

Reviewers: David Jacot <djacot@confluent.io>",21,0,6,87,705,1,12,121,90,7,18,3.0,276,90,15,155,56,9,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecord.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",19,69,46,149,748,6,17,312,127,18,17,4,465,127,27,153,66,9,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",282,1,1,1440,11564,1,71,1907,459,10,186,4.0,4785,459,26,2878,186,15,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/RecordMetadata.java,clients/src/main/java/org/apache/kafka/clients/producer/RecordMetadata.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",14,25,28,58,361,5,12,142,39,9,15,2,211,39,14,69,28,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadata.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",19,3,9,80,536,4,10,120,63,8,15,4,214,63,14,94,27,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerBatch.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",72,3,3,343,2551,2,45,520,124,9,60,3.0,936,133,16,416,54,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerInterceptors.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerInterceptors.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",18,1,1,68,522,1,5,139,106,23,6,2.0,149,106,25,10,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java,clients/src/main/java/org/apache/kafka/common/record/MemoryRecordsBuilder.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",133,31,48,502,3572,15,61,838,461,19,44,3.5,1394,461,32,556,132,13,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,58,10,86,868,4,2,113,48,14,8,2.0,133,58,17,20,10,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordsTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerRecordsTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,8,3,37,432,1,1,62,58,16,4,3.0,73,58,18,11,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/MockConsumerTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/MockConsumerTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",5,9,4,110,1376,2,5,142,32,6,23,3,211,32,9,69,18,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerInterceptorsTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerInterceptorsTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",13,8,6,126,1238,1,9,180,174,30,6,3.0,199,174,33,19,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/RecordMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/producer/RecordMetadataTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",3,28,19,63,591,6,3,90,79,22,4,2.0,112,79,28,22,19,6,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java,clients/src/test/java/org/apache/kafka/clients/producer/RecordSendTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",7,3,3,67,584,3,4,106,76,6,17,3,175,76,10,69,15,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/FutureRecordMetadataTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",4,0,1,50,388,1,4,81,82,27,3,1,83,82,28,2,1,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerBatchTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",33,0,26,238,2092,2,12,307,72,19,16,4.0,406,72,25,99,26,6,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerInterceptorsTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerInterceptorsTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",16,1,1,139,1163,1,10,208,147,23,9,1,225,147,25,17,9,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java,clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",78,1,2,654,6629,1,30,788,253,24,33,4,1649,285,50,861,330,26,2,1,0,1
clients/src/test/java/org/apache/kafka/test/MockConsumerInterceptor.java,clients/src/test/java/org/apache/kafka/test/MockConsumerInterceptor.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",10,6,4,74,670,1,7,105,80,13,8,2.5,120,80,15,15,4,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",19,3,2,111,867,1,9,167,167,56,3,1,171,167,57,4,2,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",67,9,5,1368,12061,3,49,1783,367,27,66,3.0,2765,367,42,982,442,15,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",31,7,8,532,4454,3,20,705,563,21,33,3,1040,563,32,335,116,10,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",107,1,1,1185,10865,1,67,1618,389,26,63,4,2051,424,33,433,60,7,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ProcessingContextTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ProcessingContextTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",3,1,1,32,304,1,3,55,55,28,2,1.0,56,55,28,1,1,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",37,93,45,780,9836,9,27,1064,508,34,31,5,1321,508,43,257,45,8,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStoreTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStoreTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",14,28,10,325,3592,3,11,420,458,20,21,4,837,458,40,417,250,20,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaStatusBackingStoreTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaStatusBackingStoreTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",15,3,1,327,3272,1,15,444,373,49,9,3,542,373,60,98,45,11,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/KafkaBasedLogTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/KafkaBasedLogTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",17,22,10,442,4406,4,17,580,463,29,20,4.0,912,463,46,332,166,17,2,1,0,1
core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala,core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",44,0,3,343,3053,3,18,488,306,7,66,4.0,1274,306,19,786,85,12,2,1,0,1
core/src/test/scala/unit/kafka/tools/ConsoleConsumerTest.scala,core/src/test/scala/unit/kafka/tools/ConsoleConsumerTest.scala,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",21,4,21,381,2912,3,21,519,110,17,30,3.5,907,111,30,388,156,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",3,0,1,36,217,0,3,60,60,20,3,1,66,60,22,6,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestampTest.java,streams/src/test/java/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestampTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,6,2,35,193,1,2,60,56,20,3,3,68,56,23,8,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/TimestampExtractorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/TimestampExtractorTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",1,6,2,28,156,1,1,52,48,13,4,2.0,61,48,15,9,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/testutil/ConsumerRecordUtil.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/testutil/ConsumerRecordUtil.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,6,2,27,150,1,2,50,46,25,2,2.5,52,46,26,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,4,1,29,315,1,2,52,49,26,2,2.0,53,49,26,1,1,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",35,62,52,836,6342,6,27,1053,604,105,10,23.0,1398,604,140,345,118,34,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockRestoreConsumer.java,streams/src/test/java/org/apache/kafka/test/MockRestoreConsumer.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",23,3,2,107,872,1,9,159,155,20,8,3.0,193,155,24,34,12,4,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/test/ConsumerRecordFactory.java,streams/test-utils/src/main/java/org/apache/kafka/streams/test/ConsumerRecordFactory.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",42,3,2,262,2035,1,29,583,415,73,8,2.5,608,415,76,25,17,3,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/TestRecordTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/TestRecordTest.java,"KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)

The methods have been deprecated since 0.11 without replacement since
message format 2 moved the checksum to the record batch (instead of the
record).

Unfortunately, we did not deprecate the constructors that take a checksum
(even though we intended to) so we cannot remove them. I have deprecated
them for removal in 4.0 and added a single non deprecated constructor to
`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.
`ConsumerRecord` could do with one additional convenience constructor, but
that requires a KIP and hence should be done separately.

Also:
* Removed `ChecksumMessageFormatter`, which is technically not public
API, but may have been used with the console consumer.
* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors
to use the non deprecated ones.
* Added tests for deprecated `ConsumerRecord/`RecordMetadata`
constructors.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",8,3,2,128,1408,1,8,172,168,25,7,2,195,168,28,23,12,3,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java,metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java,"KAFKA-12471: Implement createPartitions in KIP-500 mode (#10343)

Implement the createPartitions RPC which adds more partitions to a topic
in the KIP-500 controller.  Factor out some of the logic for validating
manual partition assignments, so that it can be shared between
createTopics and createPartitions.  Add a startPartition argument to the
replica placer.

Reviewers: Jason Gustafson <jason@confluent.io>",8,2,1,48,390,2,2,78,77,39,2,1.0,79,77,40,1,1,0,2,1,0,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataManager.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataManager.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",0,7,7,30,211,0,0,200,200,50,4,1.0,208,200,52,8,7,2,1,0,1,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",10,28,3,43,303,1,4,115,90,29,4,1.0,119,90,30,4,3,1,1,0,1,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",8,29,6,41,277,1,4,109,86,36,3,1,115,86,38,6,6,2,1,0,1,1
storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageException.java,storage/api/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageException.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,5,4,13,77,2,3,37,36,12,3,1,41,36,14,4,4,1,1,0,1,1
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogLeaderEpochState.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogLeaderEpochState.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",16,172,0,81,568,8,8,172,172,172,1,1,172,172,172,0,0,0,0,0,0,0
storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCache.java,storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCache.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",30,316,0,149,1159,12,12,316,316,316,1,1,316,316,316,0,0,0,0,0,0,0
storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCacheTest.java,storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataCacheTest.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",24,387,0,252,2432,13,13,387,387,387,1,1,387,387,387,0,0,0,0,0,0,0
storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java,storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",16,161,0,104,692,12,12,161,161,161,1,1,161,161,161,0,0,0,0,0,0,0
storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java,storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",3,130,0,74,739,3,3,130,130,130,1,1,130,130,130,0,0,0,0,0,0,0
storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java,storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",19,177,0,124,909,10,10,177,177,177,1,1,177,177,177,0,0,0,0,0,0,0
storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java,storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java,"KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager. (#10218)

KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.

Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage.
Added tests for both the implementations and their supported classes.
This is part of tiered storage implementation, KIP-405.

Reivewers:  Kowshik Prakasam <kprakasam@confluent.io>, Jun Rao <junrao@gmail.com>",14,246,0,161,1575,11,11,246,246,246,1,1,246,246,246,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java,tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java,"KAFKA-12611: Fix using random payload in ProducerPerformance incorrectly (#10469)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,11,7,346,2514,1,11,433,132,11,38,2.0,685,176,18,252,44,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerPartitionAssignor.java,"KAFKA-12637: Remove deprecated PartitionAssignor interface (#10512)

Remove PartitionAssignor and related classes, update docs and move unit test

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",39,41,0,171,997,1,28,298,206,50,6,2.0,305,206,51,7,4,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerPartitionAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerPartitionAssignorTest.java,"KAFKA-12637: Remove deprecated PartitionAssignor interface (#10512)

Remove PartitionAssignor and related classes, update docs and move unit test

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",8,109,0,77,610,8,8,109,109,109,1,1,109,109,109,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java,"MINOR: Improve description of `max.poll.records` config (#10506)

Reviewers: Jason Gustafson <jason@confluent.io>",19,3,1,433,2531,0,10,631,187,7,89,2,1020,193,11,389,103,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java,"KAFKA-9988: Suppress uncaught exceptions in log messages during Connect task shutdown (#10503)

Uncaught exceptions logged during task stop were misleading because the task is already on its way of being shutdown.

The suppression of exception causes a change in behavior as the caller method now calls `statusListener.onShutdown` instead of `statusListener.onFailure` which is the right behavior. A new test was added to test the right behavior for uncaught exception during shutdown and existing test was modified to test uncaught exception during normal execution.

Reviewers: Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",59,8,2,309,1947,1,39,458,210,15,30,3.0,679,215,23,221,39,7,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/CandidateState.java,raft/src/main/java/org/apache/kafka/raft/CandidateState.java,"KAFKA-12607; Test case for resigned state vote granting (#10510)

This patch adds unit tests to verify vote behavior when in the ""resigned"" state.

Reviewers: Jason Gustafson <jason@confluent.io>",33,1,1,167,989,1,27,275,252,46,6,1.0,281,252,47,6,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ForeachProcessor.java,streams/src/main/java/org/apache/kafka/streams/kstream/ForeachProcessor.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",2,34,0,13,126,2,2,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamBranch.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamBranch.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",6,14,14,34,334,4,3,60,52,9,7,5,99,52,14,39,14,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFilter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFilter.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",4,10,9,26,240,3,3,48,48,7,7,4,73,48,10,25,9,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMap.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMap.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",9,14,11,27,328,6,3,49,47,5,9,3,83,47,9,34,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapValues.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",5,13,12,25,270,6,3,47,47,5,9,3,85,47,9,38,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamMap.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamMap.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",5,14,11,25,295,6,3,48,46,5,9,3,82,46,9,34,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamMapValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamMapValues.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",3,12,11,23,234,6,3,45,45,6,8,4.0,80,45,10,35,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamPeek.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamPeek.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",3,11,14,23,219,5,3,46,45,12,4,4.5,66,45,16,20,14,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamPrint.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamPrint.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",5,9,9,27,229,3,4,52,89,13,4,3.5,102,89,26,50,40,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/PassThrough.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/PassThrough.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",2,9,8,17,168,3,2,37,37,5,8,2.5,61,37,8,24,8,3,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/PrintedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/PrintedInternal.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",4,2,2,14,131,1,3,34,36,7,5,1,43,36,9,9,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/api/ContextualProcessor.java,streams/src/main/java/org/apache/kafka/streams/processor/api/ContextualProcessor.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",3,47,0,12,93,3,3,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/PrintedTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/PrintedTest.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",11,15,19,96,875,4,11,131,126,19,7,5,167,126,24,36,19,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamBranchTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamBranchTest.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",4,2,2,58,680,1,2,91,90,4,24,2.5,228,90,10,137,32,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamPrintTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamPrintTest.java,"KAFKA-8410: Migrate KStream Stateless operators to new Processor API (#10381)

Migrate KStream stateless operators to new Processor API.
Following PRs will complete migration of KStream stateful operators and KTable.
No expected functionality changes.

Reviewers: John Roesler <vvcephei@apache.org>",4,7,6,51,441,2,2,81,91,7,12,4.5,228,91,19,147,53,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilder.java,"KAFKA-12449: Remove deprecated WindowStore#put (#10293)

Removes `WindowStore#put(K,V)` that was deprecated via KIP-474.

Reviewers: Matthias J. Sax <matthias@confluent.io>",31,0,7,170,1156,1,23,218,101,24,9,3,237,104,26,19,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WindowToTimestampedWindowByteStoreAdapter.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WindowToTimestampedWindowByteStoreAdapter.java,"KAFKA-12449: Remove deprecated WindowStore#put (#10293)

Removes `WindowStore#put(K,V)` that was deprecated via KIP-474.

Reviewers: Matthias J. Sax <matthias@confluent.io>",27,0,7,151,1097,1,25,199,152,40,5,1,220,152,44,21,14,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedWindowBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedWindowBytesStoreTest.java,"KAFKA-12449: Remove deprecated WindowStore#put (#10293)

Removes `WindowStore#put(K,V)` that was deprecated via KIP-474.

Reviewers: Matthias J. Sax <matthias@confluent.io>",8,3,3,106,895,2,8,155,156,22,7,2,222,156,32,67,41,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStoreTest.java,"KAFKA-12449: Remove deprecated WindowStore#put (#10293)

Removes `WindowStore#put(K,V)` that was deprecated via KIP-474.

Reviewers: Matthias J. Sax <matthias@confluent.io>",10,3,5,123,1026,2,10,178,132,12,15,3,266,132,18,88,26,6,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/WindowStoreFacadeTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/WindowStoreFacadeTest.java,"KAFKA-12449: Remove deprecated WindowStore#put (#10293)

Removes `WindowStore#put(K,V)` that was deprecated via KIP-474.

Reviewers: Matthias J. Sax <matthias@confluent.io>",9,0,11,98,685,1,9,136,135,19,7,1,159,135,23,23,11,3,2,1,0,1
core/src/main/scala/kafka/server/ClientQuotaManager.scala,core/src/main/scala/kafka/server/ClientQuotaManager.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",89,2,12,415,3199,1,38,686,250,15,46,3.0,1493,346,32,807,225,18,2,1,0,1
core/src/main/scala/kafka/server/KafkaConfig.scala,core/src/main/scala/kafka/server/KafkaConfig.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",97,0,12,1634,12708,0,33,2012,527,7,280,2.0,3972,848,14,1960,426,7,2,1,0,1
core/src/main/scala/kafka/server/QuotaFactory.scala,core/src/main/scala/kafka/server/QuotaFactory.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",14,4,24,89,567,5,9,119,74,9,13,2,172,74,13,53,24,4,2,1,0,1
core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala,core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",5,8,3,54,329,1,5,77,55,6,12,3.0,143,55,12,66,22,6,2,1,0,1
core/src/test/scala/integration/kafka/api/CustomQuotaCallbackTest.scala,core/src/test/scala/integration/kafka/api/CustomQuotaCallbackTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",56,0,2,353,3124,1,37,469,453,23,20,3.0,545,453,27,76,19,4,2,1,0,1
core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala,core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",5,0,2,59,426,1,5,84,66,6,13,3,160,66,12,76,23,6,2,1,0,1
core/src/test/scala/integration/kafka/api/UserQuotaTest.scala,core/src/test/scala/integration/kafka/api/UserQuotaTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",6,1,3,58,414,1,6,83,61,6,15,3,148,61,10,65,20,4,2,1,0,1
core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala,core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",184,6,2,1495,14300,2,110,1874,471,24,77,3,2508,471,33,634,158,8,2,1,0,1
core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala,core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",20,26,12,291,3310,12,16,427,159,13,32,5.5,1065,187,33,638,241,20,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala,core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala,"KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations (#10427)

`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. Dynamic default quotas must be used instead. This patch removes them for AK 3.0. 

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",291,0,2,1005,8763,1,69,1242,152,13,99,1,1674,153,17,432,116,4,2,1,0,1
core/src/main/scala/kafka/server/MetadataSupport.scala,core/src/main/scala/kafka/server/MetadataSupport.scala,KAFKA-12406 Integrate client quotas with KRaft broker (#10254),16,3,2,62,464,0,9,125,108,42,3,2,129,108,43,4,2,1,2,1,0,1
core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala,core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala,KAFKA-12406 Integrate client quotas with KRaft broker (#10254),69,11,1,197,1519,2,5,307,297,154,2,2.0,308,297,154,1,1,0,1,0,1,1
core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala,core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala,KAFKA-12406 Integrate client quotas with KRaft broker (#10254),43,17,9,136,1025,2,3,182,174,91,2,2.0,191,174,96,9,9,4,1,0,1,1
core/src/test/java/kafka/testkit/TestKitNodes.java,core/src/test/java/kafka/testkit/TestKitNodes.java,KAFKA-12406 Integrate client quotas with KRaft broker (#10254),33,1,1,142,1001,2,17,181,181,90,2,1.0,182,181,91,1,1,0,1,0,1,1
core/src/test/scala/integration/kafka/server/RaftClusterTest.scala,core/src/test/scala/integration/kafka/server/RaftClusterTest.scala,KAFKA-12406 Integrate client quotas with KRaft broker (#10254),16,106,5,262,2206,8,8,316,216,105,3,8,330,216,110,14,9,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/Uuid.java,clients/src/main/java/org/apache/kafka/common/Uuid.java,"KAFKA-12457; Add sentinel ID to metadata topic (#10492)

KIP-516 introduces topic IDs to topics, but there is a small issue with how the KIP-500 metadata topic will interact with topic IDs. 

For example, https://github.com/apache/kafka/pull/9944 aims to replace topic names in the Fetch request with topic IDs. In order to get these IDs, brokers must fetch from the metadata topic. This leads to a sort of ""chicken and the egg"" problem concerning how we find out the metadata topic's topic ID. 

This PR adds the a special sentinel topic ID for the metadata topic, which gets around this problem.
More information can be found in the [JIRA](https://issues.apache.org/jira/browse/KAFKA-12457) and in [KIP-516](https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers).

Reviewers: Jason Gustafson <jason@confluent.io>",21,6,2,78,585,1,10,149,120,25,6,2.0,179,120,30,30,26,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/UuidTest.java,clients/src/test/java/org/apache/kafka/common/UuidTest.java,"KAFKA-12457; Add sentinel ID to metadata topic (#10492)

KIP-516 introduces topic IDs to topics, but there is a small issue with how the KIP-500 metadata topic will interact with topic IDs. 

For example, https://github.com/apache/kafka/pull/9944 aims to replace topic names in the Fetch request with topic IDs. In order to get these IDs, brokers must fetch from the metadata topic. This leads to a sort of ""chicken and the egg"" problem concerning how we find out the metadata topic's topic ID. 

This PR adds the a special sentinel topic ID for the metadata topic, which gets around this problem.
More information can be found in the [JIRA](https://issues.apache.org/jira/browse/KAFKA-12457) and in [KIP-516](https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers).

Reviewers: Jason Gustafson <jason@confluent.io>",7,1,3,71,648,1,7,108,70,18,6,2.0,131,70,22,23,17,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/KafkaClientSupplier.java,streams/src/main/java/org/apache/kafka/streams/KafkaClientSupplier.java,"KAFKA-12630: Remove deprecated KafkaClientSupplier#getAdminClient in Streams (#10502)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",1,1,17,16,196,2,1,78,47,6,13,3,136,47,10,58,17,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultKafkaClientSupplier.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultKafkaClientSupplier.java,"KAFKA-12630: Remove deprecated KafkaClientSupplier#getAdminClient in Streams (#10502)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",5,0,7,32,334,1,5,56,45,6,9,3,80,45,9,24,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/Crc32Test.java,clients/src/test/java/org/apache/kafka/common/utils/Crc32Test.java,"MINOR: Update Crc32Test#testUpdate method with correct Crc32 (#10406)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,26,235,1,2,52,61,13,4,2.0,89,61,22,37,30,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Joined.java,streams/src/main/java/org/apache/kafka/streams/kstream/Joined.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",15,0,27,65,631,2,15,206,146,41,5,2,257,146,51,51,27,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/KStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/KStream.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",0,0,318,207,3336,0,0,4715,1064,40,117,5,11361,2196,97,6646,2195,57,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/KTable.java,streams/src/main/java/org/apache/kafka/streams/kstream/KTable.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",0,0,34,137,2180,0,0,2258,979,34,67,6,5107,1078,76,2849,812,43,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/StreamJoined.java,streams/src/main/java/org/apache/kafka/streams/kstream/StreamJoined.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",15,4,0,221,934,0,15,363,287,73,5,1,378,287,76,15,12,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/JoinedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/JoinedInternal.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",5,0,2,20,129,0,5,43,45,22,2,1.0,45,45,22,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java,"KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined (#10421)

This is related to KIP-307 / KIP-372 / KIP-479.

Reviewers: John Roesler <vvcephei@apache.org>",42,6,6,549,4712,2,28,701,610,58,12,3.0,959,610,80,258,175,22,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java,"KAFKA-7785: move internal DefaultPartitionGrouper (#10302)

Reviewers: Guozhang Wang <wangguoz@gmail.com>",49,0,8,218,1348,1,15,270,188,12,22,4.0,551,188,25,281,114,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedWindowBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedWindowBytesStore.java,"KAFKA-10847: Add new RocksDBTimeOrderedWindowStore that persists (time-key)-value records (#10331)

This new store is more efficient when calling range queries with only time parameters, like `fetch(from, to)`. For range queries using key ranges, then the current RocksDBWindowStore should be used.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",3,1,1,20,179,1,2,41,41,14,3,1,44,41,15,3,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbWindowBytesStoreSupplier.java,"KAFKA-10847: Add new RocksDBTimeOrderedWindowStore that persists (time-key)-value records (#10331)

This new store is more efficient when calling range queries with only time parameters, like `fetch(from, to)`. For range queries using key ranges, then the current RocksDBWindowStore should be used.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,56,23,115,454,4,10,144,90,10,15,2,224,90,15,80,23,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreBuilder.java,"KAFKA-10847: Add new RocksDBTimeOrderedWindowStore that persists (time-key)-value records (#10331)

This new store is more efficient when calling range queries with only time parameters, like `fetch(from, to)`. For range queries using key ranges, then the current RocksDBWindowStore should be used.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",9,5,1,58,410,1,5,84,68,12,7,2,100,68,14,16,11,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/WindowKeySchemaTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/WindowKeySchemaTest.java,"KAFKA-10847: Add new RocksDBTimeOrderedWindowStore that persists (time-key)-value records (#10331)

This new store is more efficient when calling range queries with only time parameters, like `fetch(from, to)`. For range queries using key ranges, then the current RocksDBWindowStore should be used.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",24,1,1,219,2406,2,22,280,131,25,11,4,377,131,34,97,32,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/internals/Topic.java,clients/src/main/java/org/apache/kafka/common/internals/Topic.java,"KAFKA-10769 Remove JoinGroupRequest#containsValidPattern as it is dup… (#9851)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",22,16,9,50,435,2,6,96,60,16,6,1.5,132,60,22,36,22,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java,clients/src/main/java/org/apache/kafka/common/requests/JoinGroupRequest.java,"KAFKA-10769 Remove JoinGroupRequest#containsValidPattern as it is dup… (#9851)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,4,27,74,502,2,9,114,87,3,34,3.0,576,87,17,462,211,14,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/JoinGroupRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/JoinGroupRequestTest.java,"KAFKA-10769 Remove JoinGroupRequest#containsValidPattern as it is dup… (#9851)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,0,12,58,445,1,4,86,65,11,8,2.0,121,65,15,35,12,4,2,1,0,1
core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala,core/src/test/scala/unit/kafka/server/ListOffsetsRequestTest.scala,"KAFKA-12384: stabilize ListOffsetsRequestTest#testResponseIncludesLeaderEpoch (#10389)

Reviewers: Luke Chen <showuon@gmail.com>, dengziming <swzmdeng@163.com>, Ismael Juma <ismael@juma.me.uk>",18,16,8,164,1707,4,9,227,88,16,14,3.0,356,88,25,129,50,9,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/EmptySnapshotReader.java,metadata/src/main/java/org/apache/kafka/controller/EmptySnapshotReader.java,"KAFKA-12467: Implement QuorumController snapshot generation (#10366)

Implement controller-side snapshot generation.Implement QuorumController snapshot
generation.  Note that this PR does not handle KRaft integration, just the internal 
snapshot record generation and consumption logic.

Reading a snapshot is relatively straightforward.  When the  QuorumController
starts up, it loads the most recent snapshot.  This is just a series of records
that we replay, plus a log offset (""snapshot epoch"") that we advance to.

Writing a snapshot is more complex.  There are several components:
the SnapshotWriter which persists the snapshot, the SnapshotGenerator
which manages writing each batch of records, and the SnapshotGeneratorManager
which interfaces the preceding two classes with the event queue.

Controller snapshots are done incrementally.  In order to avoid blocking the
controller thread for a long time, we pull a few record batches at a time from
our record batch iterators.  These iterators are implemented by controller
manager classes such as ReplicationControlManager, ClusterControlManager, etc.

Finally, this PR adds ControllerTestUtils#deepSortRecords and
ControllerTestUtils#assertBatchIteratorContains, which make it easier to write
unit tests.  Since records are often constructed from unsorted data structures,
it is often useful to sort them before comparing them.

Reviewers: David Arthur <mumrah@gmail.com>",5,52,0,25,113,5,5,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/controller/SnapshotReader.java,metadata/src/main/java/org/apache/kafka/controller/SnapshotReader.java,"KAFKA-12467: Implement QuorumController snapshot generation (#10366)

Implement controller-side snapshot generation.Implement QuorumController snapshot
generation.  Note that this PR does not handle KRaft integration, just the internal 
snapshot record generation and consumption logic.

Reading a snapshot is relatively straightforward.  When the  QuorumController
starts up, it loads the most recent snapshot.  This is just a series of records
that we replay, plus a log offset (""snapshot epoch"") that we advance to.

Writing a snapshot is more complex.  There are several components:
the SnapshotWriter which persists the snapshot, the SnapshotGenerator
which manages writing each batch of records, and the SnapshotGeneratorManager
which interfaces the preceding two classes with the event queue.

Controller snapshots are done incrementally.  In order to avoid blocking the
controller thread for a long time, we pull a few record batches at a time from
our record batch iterators.  These iterators are implemented by controller
manager classes such as ReplicationControlManager, ClusterControlManager, etc.

Finally, this PR adds ControllerTestUtils#deepSortRecords and
ControllerTestUtils#assertBatchIteratorContains, which make it easier to write
unit tests.  Since records are often constructed from unsorted data structures,
it is often useful to sort them before comparing them.

Reviewers: David Arthur <mumrah@gmail.com>",0,37,0,8,60,0,0,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/controller/MockSnapshotReader.java,metadata/src/test/java/org/apache/kafka/controller/MockSnapshotReader.java,"KAFKA-12467: Implement QuorumController snapshot generation (#10366)

Implement controller-side snapshot generation.Implement QuorumController snapshot
generation.  Note that this PR does not handle KRaft integration, just the internal 
snapshot record generation and consumption logic.

Reading a snapshot is relatively straightforward.  When the  QuorumController
starts up, it loads the most recent snapshot.  This is just a series of records
that we replay, plus a log offset (""snapshot epoch"") that we advance to.

Writing a snapshot is more complex.  There are several components:
the SnapshotWriter which persists the snapshot, the SnapshotGenerator
which manages writing each batch of records, and the SnapshotGeneratorManager
which interfaces the preceding two classes with the event queue.

Controller snapshots are done incrementally.  In order to avoid blocking the
controller thread for a long time, we pull a few record batches at a time from
our record batch iterators.  These iterators are implemented by controller
manager classes such as ReplicationControlManager, ClusterControlManager, etc.

Finally, this PR adds ControllerTestUtils#deepSortRecords and
ControllerTestUtils#assertBatchIteratorContains, which make it easier to write
unit tests.  Since records are often constructed from unsorted data structures,
it is often useful to sort them before comparing them.

Reviewers: David Arthur <mumrah@gmail.com>",5,55,0,28,144,5,5,55,55,55,1,1,55,55,55,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/utils/ExponentialBackoff.java,clients/src/main/java/org/apache/kafka/common/utils/ExponentialBackoff.java,"MINOR: Support ExponentialBackoff without jitter (#10455)

It is useful to allow ExponentialBackoff to be configured to work
without jitter, in order to make unit tests more repeatable.

Reviewers: David Arthur <mumrah@gmail.com>",5,2,1,25,204,1,2,54,53,27,2,1.0,55,53,28,1,1,0,1,0,1,1
clients/src/test/java/org/apache/kafka/common/utils/ExponentialBackoffTest.java,clients/src/test/java/org/apache/kafka/common/utils/ExponentialBackoffTest.java,"MINOR: Support ExponentialBackoff without jitter (#10455)

It is useful to allow ExponentialBackoff to be configured to work
without jitter, in order to make unit tests more repeatable.

Reviewers: David Arthur <mumrah@gmail.com>",5,9,0,35,270,1,2,57,48,19,3,1,60,48,20,3,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/Consumer.java,clients/src/main/java/org/apache/kafka/clients/consumer/Consumer.java,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",0,0,7,65,632,0,0,276,125,8,33,2,443,125,13,167,62,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/NoOffsetForPartitionException.java,clients/src/main/java/org/apache/kafka/clients/consumer/NoOffsetForPartitionException.java,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",3,0,10,21,147,1,3,54,29,11,5,2,85,29,17,31,12,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/MessageFormatter.java,clients/src/main/java/org/apache/kafka/common/MessageFormatter.java,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",2,3,16,10,92,3,2,53,66,26,2,3.0,69,66,34,16,16,8,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerConfigTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerConfigTest.java,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",5,0,28,75,710,1,4,105,92,13,8,2.5,152,92,19,47,28,6,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/ProducerConfigTest.java,clients/src/test/java/org/apache/kafka/clients/producer/ProducerConfigTest.java,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",1,0,31,37,400,1,1,62,92,16,4,2.5,100,92,25,38,31,10,2,1,0,1
core/src/test/scala/integration/kafka/api/PlaintextConsumerTest.scala,core/src/test/scala/integration/kafka/api/PlaintextConsumerTest.scala,"KAFKA-12579: Remove various deprecated clients classes/methods for 3.0 (#10438)

* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1.
The extra functionality was also made available in `Serializer` and `Deserializer`.
* Remove `close(long, TimeUnit)` from the producer, consumer and admin client,
deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`.
* Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`,
deprecated since 2.7 with no replacement. These methods were not intended to be public API
and are likely not used much (if at all).
* Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()`
should be used instead.
* Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)`
method should be used instead.
* Remove `kafka.common.MessageFormatter`, deprecated since 2.7.
`org.apache.kafka.common.MessageFormatter` should be used instead.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",107,0,10,1329,12232,2,86,1865,300,24,78,3.0,3393,524,44,1528,499,20,2,1,0,1
core/src/main/scala/kafka/server/AutoTopicCreationManager.scala,core/src/main/scala/kafka/server/AutoTopicCreationManager.scala,"KAFKA-12294; forward auto topic request within envelope on behalf of clients (#10142)

When auto-creating topics in KIP-500, the broker will send a `CreateTopics` request to the controller. It is useful in this case to preserve the original principal from the corresponding `Metadata` request by wrapping the `CreateTopics` request in an envelope so that the controller may repeat the authorization and to improve auditability. This follows a similar pattern to how standard `CreateTopics` requests are forwarded to the controller.

Reviewers: Jason Gustafson <jason@confluent.io>",33,50,10,228,1379,7,12,301,321,60,5,6,382,321,76,81,33,16,2,1,0,1
core/src/main/scala/kafka/server/ForwardingManager.scala,core/src/main/scala/kafka/server/ForwardingManager.scala,"KAFKA-12294; forward auto topic request within envelope on behalf of clients (#10142)

When auto-creating topics in KIP-500, the broker will send a `CreateTopics` request to the controller. It is useful in this case to preserve the original principal from the corresponding `Metadata` request by wrapping the `CreateTopics` request in an envelope so that the controller may repeat the authorization and to improve auditability. This follows a similar pattern to how standard `CreateTopics` requests are forwarded to the controller.

Reviewers: Jason Gustafson <jason@confluent.io>",13,18,13,97,593,3,7,147,85,11,13,4,285,85,22,138,31,11,2,1,0,1
core/src/test/scala/unit/kafka/server/AutoTopicCreationManagerTest.scala,core/src/test/scala/unit/kafka/server/AutoTopicCreationManagerTest.scala,"KAFKA-12294; forward auto topic request within envelope on behalf of clients (#10142)

When auto-creating topics in KIP-500, the broker will send a `CreateTopics` request to the controller. It is useful in this case to preserve the original principal from the corresponding `Metadata` request by wrapping the `CreateTopics` request in an envelope so that the controller may repeat the authorization and to improve auditability. This follows a similar pattern to how standard `CreateTopics` requests are forwarded to the controller.

Reviewers: Jason Gustafson <jason@confluent.io>",33,127,7,315,2593,9,29,402,194,100,4,6.0,430,194,108,28,15,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ProduceRequestResult.java,"KAFKA-12548; Propagate record error messages to application (#10445)

KIP-467 added a field in the produce response to allow the broker to indicate which specific records failed validation. This patch adds the logic to propagate this message up to the application.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",13,11,7,55,336,4,11,134,83,12,11,3,182,83,17,48,18,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java,"KAFKA-12548; Propagate record error messages to application (#10445)

KIP-467 added a field in the produce response to allow the broker to indicate which specific records failed validation. This patch adds the logic to propagate this message up to the application.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",155,72,21,736,5934,8,38,1031,503,7,142,4.0,3178,503,22,2147,570,15,2,1,0,1
core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala,core/src/test/scala/integration/kafka/api/PlaintextProducerSendTest.scala,"KAFKA-12548; Propagate record error messages to application (#10445)

KIP-467 added a field in the produce response to allow the broker to indicate which specific records failed validation. This patch adds the logic to propagate this message up to the application.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",16,0,2,137,1192,1,13,196,72,8,24,3.0,315,73,13,119,46,5,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/EpochState.java,raft/src/main/java/org/apache/kafka/raft/EpochState.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",1,11,0,12,72,0,1,54,42,18,3,1,55,42,18,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ResignedState.java,raft/src/main/java/org/apache/kafka/raft/ResignedState.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",13,13,1,89,470,3,12,159,144,53,3,1,160,144,53,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/UnattachedState.java,raft/src/main/java/org/apache/kafka/raft/UnattachedState.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",12,15,1,81,389,3,11,116,91,29,4,3.0,117,91,29,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/VotedState.java,raft/src/main/java/org/apache/kafka/raft/VotedState.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",13,17,1,90,448,3,12,128,99,26,5,1,131,99,26,3,2,1,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/CandidateStateTest.java,raft/src/test/java/org/apache/kafka/raft/CandidateStateTest.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",13,54,22,163,1297,13,13,198,165,50,4,8.0,237,165,59,39,22,10,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/FollowerStateTest.java,raft/src/test/java/org/apache/kafka/raft/FollowerStateTest.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",4,36,22,68,579,4,4,98,82,24,4,3.0,126,82,32,28,22,7,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/ResignedStateTest.java,raft/src/test/java/org/apache/kafka/raft/ResignedStateTest.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",3,35,10,64,525,3,3,93,68,46,2,4.5,103,68,52,10,10,5,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/UnattachedStateTest.java,raft/src/test/java/org/apache/kafka/raft/UnattachedStateTest.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",3,34,9,57,479,3,3,87,62,44,2,3.0,96,62,48,9,9,4,1,0,1,1
raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java,raft/src/test/java/org/apache/kafka/raft/VotedStateTest.java,"KAFKA-12539; Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity (#10393)

1. Add `canGrantVote` to `EpochState`
2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState`
3. Add unit tests for `canGrantVote`

Reviewers: Jason Gustafson <jason@confluent.io>",3,34,10,60,490,3,3,117,63,29,4,1.5,127,63,32,10,10,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/Selector.java,clients/src/main/java/org/apache/kafka/common/network/Selector.java,"KAFKA-12615: Fix `Selector.clear()` javadoc typo (#10477)

The second `clearCompletedSends()` reference should be `clearCompletedReceives()`.

Reviewers: Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Zhao Haiyuan <zhaohaiyuan@mobike.com>",227,1,1,1027,7765,0,82,1474,349,16,94,4.0,2705,349,29,1231,141,13,2,1,0,1
core/src/main/scala/kafka/security/authorizer/AuthorizerUtils.scala,core/src/main/scala/kafka/security/authorizer/AuthorizerUtils.scala,"KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0 (#10450)

These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced
by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.

As part of KIP-500, we will implement a new `Authorizer` implementation that relies
on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance
to remove related tech debt in 3.0.

Details on the issues affecting the old Authorizer interface can be found in the KIP:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",11,1,18,23,216,2,11,47,48,4,13,3,143,49,11,96,46,7,2,1,0,1
core/src/test/scala/integration/kafka/api/GroupAuthorizerIntegrationTest.scala,core/src/test/scala/integration/kafka/api/GroupAuthorizerIntegrationTest.scala,"KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0 (#10450)

These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced
by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.

As part of KIP-500, we will implement a new `Authorizer` implementation that relies
on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance
to remove related tech debt in 3.0.

Details on the issues affecting the old Authorizer interface can be found in the KIP:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",9,2,2,99,901,1,6,135,98,19,7,3,161,107,23,26,9,4,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslGssapiSslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SaslGssapiSslEndToEndAuthorizationTest.scala,"KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0 (#10450)

These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced
by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.

As part of KIP-500, we will implement a new `Authorizer` implementation that relies
on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance
to remove related tech debt in 3.0.

Details on the issues affecting the old Authorizer interface can be found in the KIP:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",0,2,4,21,173,0,0,46,25,5,9,2,59,25,7,13,5,1,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslSslAdminIntegrationTest.scala,core/src/test/scala/integration/kafka/api/SaslSslAdminIntegrationTest.scala,"KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0 (#10450)

These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced
by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.

As part of KIP-500, we will implement a new `Authorizer` implementation that relies
on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance
to remove related tech debt in 3.0.

Details on the issues affecting the old Authorizer interface can be found in the KIP:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",47,42,49,424,4845,6,24,527,137,15,36,2.5,874,181,24,347,56,10,2,1,0,1
core/src/test/scala/integration/kafka/api/SslAdminIntegrationTest.scala,core/src/test/scala/integration/kafka/api/SslAdminIntegrationTest.scala,"KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0 (#10450)

These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced
by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.

As part of KIP-500, we will implement a new `Authorizer` implementation that relies
on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance
to remove related tech debt in 3.0.

Details on the issues affecting the old Authorizer interface can be found in the KIP:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",21,3,60,202,1835,5,13,262,136,16,16,3.5,446,152,28,184,60,12,2,1,0,1
core/src/main/scala/kafka/log/AbstractIndex.scala,core/src/main/scala/kafka/log/AbstractIndex.scala,"KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10405)

Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Reviewers: Jun Rao <junrao@gmail.com>",38,1,1,200,1370,1,20,440,287,12,37,3,614,287,17,174,25,5,2,1,0,1
core/src/main/scala/kafka/log/LazyIndex.scala,core/src/main/scala/kafka/log/LazyIndex.scala,"KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10405)

Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Reviewers: Jun Rao <junrao@gmail.com>",21,1,1,89,659,1,18,166,90,42,4,1.0,184,92,46,18,16,4,2,1,0,1
core/src/main/scala/kafka/log/TransactionIndex.scala,core/src/main/scala/kafka/log/TransactionIndex.scala,"KAFKA-3968: fsync the parent directory of a segment file when the file is created (#10405)

Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.

The problem is that following sequence of calls doesn't guarantee file durability:

fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log""
write(fd);
fsync(fd);

If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.

This PR is to flush the directory when flush() is called for the first time.

Reviewers: Jun Rao <junrao@gmail.com>",35,1,1,170,1191,1,18,263,243,26,10,3.0,299,243,30,36,10,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/RebalanceSourceConnectorsIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/RebalanceSourceConnectorsIntegrationTest.java,"KAFKA-12283: disable flaky testMultipleWorkersRejoining to stabilize build (#10408)

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",15,3,0,249,2417,0,11,382,324,38,10,7.0,547,324,55,165,100,16,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashCollection.java,clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashCollection.java,"MINOR: support ImplicitLinkedHashCollection#sort (#10456)

Support sorting the elements in ImplicitLinkedHashCollection.
This is useful sometimes in unit tests for comparing collections.

Reviewers: Ismael Juma <ismael@juma.me.uk>",94,16,0,415,2478,1,54,695,354,70,10,1.5,811,354,81,116,62,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashCollectionTest.java,clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashCollectionTest.java,"MINOR: support ImplicitLinkedHashCollection#sort (#10456)

Support sorting the elements in ImplicitLinkedHashCollection.
This is useful sometimes in unit tests for comparing collections.

Reviewers: Ismael Juma <ismael@juma.me.uk>",61,54,0,551,5113,3,37,670,536,74,9,2,731,536,81,61,28,7,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ImplicitLinkedHashCollectionBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ImplicitLinkedHashCollectionBenchmark.java,"MINOR: support ImplicitLinkedHashCollection#sort (#10456)

Support sorting the elements in ImplicitLinkedHashCollection.
This is useful sometimes in unit tests for comparing collections.

Reviewers: Ismael Juma <ismael@juma.me.uk>",13,121,0,84,558,11,11,121,121,121,1,1,121,121,121,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipal.java,clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipal.java,"KAFKA-12587 Remove KafkaPrincipal#fromString for 3.0 (#10447)

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,0,15,49,319,1,9,100,58,10,10,3.5,184,58,18,84,28,8,2,1,0,1
core/src/main/scala/kafka/server/RaftReplicaChangeDelegate.scala,core/src/main/scala/kafka/server/RaftReplicaChangeDelegate.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",32,8,6,208,1259,4,4,252,246,84,3,2,258,246,86,6,6,2,2,1,0,1
core/src/main/scala/kafka/server/RaftReplicaManager.scala,core/src/main/scala/kafka/server/RaftReplicaManager.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",48,36,9,307,2236,4,15,404,377,101,4,2.0,423,377,106,19,10,5,2,1,0,1
core/src/main/scala/kafka/server/metadata/MetadataImage.scala,core/src/main/scala/kafka/server/metadata/MetadataImage.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",25,8,0,97,629,2,17,137,122,34,4,1.0,139,122,35,2,2,0,2,1,0,1
core/src/test/scala/other/kafka/StressTestLog.scala,core/src/test/scala/other/kafka/StressTestLog.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",20,3,1,113,719,1,7,148,109,5,27,1,237,109,9,89,16,3,2,1,0,1
core/src/test/scala/other/kafka/TestLinearWriteSpeed.scala,core/src/test/scala/other/kafka/TestLinearWriteSpeed.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",16,1,1,188,1607,0,8,223,77,8,28,1.0,331,90,12,108,26,4,2,1,0,1
core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala,core/src/test/scala/unit/kafka/cluster/AssignmentStateTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",7,1,1,94,1011,1,1,122,126,17,7,1,161,126,23,39,32,6,2,1,0,1
core/src/test/scala/unit/kafka/cluster/ReplicaTest.scala,core/src/test/scala/unit/kafka/cluster/ReplicaTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",9,3,1,94,682,1,5,129,138,12,11,3,182,138,17,53,25,5,2,1,0,1
core/src/test/scala/unit/kafka/log/AbstractLogCleanerIntegrationTest.scala,core/src/test/scala/unit/kafka/log/AbstractLogCleanerIntegrationTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",10,3,1,126,986,1,7,157,113,11,14,2.0,181,113,13,24,8,2,2,1,0,1
core/src/test/scala/unit/kafka/log/BrokerCompressionTest.scala,core/src/test/scala/unit/kafka/log/BrokerCompressionTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",4,1,1,55,496,1,3,88,84,4,25,2,162,84,6,74,19,3,2,1,0,1
core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala,core/src/test/scala/unit/kafka/log/LogConcurrencyTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",18,3,1,138,855,1,9,185,183,62,3,1,194,183,65,9,8,3,2,1,0,1
core/src/test/scala/unit/kafka/log/LogManagerTest.scala,core/src/test/scala/unit/kafka/log/LogManagerTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",48,18,18,478,4248,15,33,735,142,8,96,3.0,1485,142,15,750,90,8,2,1,0,1
core/src/test/scala/unit/kafka/server/BrokerEpochIntegrationTest.scala,core/src/test/scala/unit/kafka/server/BrokerEpochIntegrationTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",20,2,2,219,1656,1,11,283,242,22,13,3,345,242,27,62,18,5,2,1,0,1
core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala,core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",6,3,3,135,1093,2,5,183,146,3,62,3.0,581,146,9,398,70,6,2,1,0,1
core/src/test/scala/unit/kafka/server/LogOffsetTest.scala,core/src/test/scala/unit/kafka/server/LogOffsetTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",20,2,2,201,2011,2,14,267,210,3,83,3,786,210,9,519,124,6,2,1,0,1
core/src/test/scala/unit/kafka/server/RaftReplicaChangeDelegateTest.scala,core/src/test/scala/unit/kafka/server/RaftReplicaChangeDelegateTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",6,10,5,114,849,1,4,151,146,76,2,4.0,156,146,78,5,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/RaftReplicaManagerTest.scala,core/src/test/scala/unit/kafka/server/RaftReplicaManagerTest.scala,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",18,86,7,233,2406,9,14,317,238,158,2,6.0,324,238,162,7,7,4,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",20,3,1,273,2229,1,16,329,312,19,17,3,367,312,22,38,8,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/PartitionMakeFollowerBenchmark.java,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",6,7,2,152,1405,2,4,181,175,18,10,3.0,201,175,20,20,5,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/partition/UpdateFollowerFetchStateBenchmark.java,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",7,5,1,152,1316,1,7,185,175,14,13,3,215,175,17,30,5,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/CheckpointBench.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/server/CheckpointBench.java,"KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager (#10282)

KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.

However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation.
Updated the ReplicaManager path to do the same on newly created topics.

There are also some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).

Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent.
Added tests for creating the log with the new topic ID parameter.

Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Jason Gustafson <jason@confluent.io>",8,2,1,140,1136,1,4,178,174,18,10,2.0,203,174,20,25,10,2,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/FunctionConversions.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/FunctionConversions.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",0,0,3,52,778,0,0,83,108,10,8,1.0,143,108,18,60,44,8,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/ImplicitConversions.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/ImplicitConversions.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",0,0,3,60,799,0,0,102,76,8,12,1.0,145,76,12,43,12,4,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",12,0,3,52,692,0,12,75,71,11,7,1,113,71,16,38,20,5,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/StreamsBuilder.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/StreamsBuilder.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",3,0,3,48,654,0,3,214,179,14,15,2,256,179,17,42,15,3,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KGroupedTable.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KGroupedTable.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",4,0,3,43,530,0,4,142,138,12,12,1.0,246,138,20,104,51,9,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KStream.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",24,0,3,212,3142,0,24,1121,581,39,29,4,1526,581,53,405,91,14,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/SessionWindowedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/SessionWindowedKStream.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",4,0,3,58,697,0,4,148,125,13,11,3,251,125,23,103,45,9,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/TimeWindowedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/TimeWindowedKStream.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",4,0,3,53,661,0,4,142,125,11,13,3,242,125,19,100,42,8,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/package.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/package.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",0,0,3,8,79,0,0,26,27,6,4,1.0,30,27,8,4,3,1,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/serialization/Serdes.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/serialization/Serdes.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",12,0,3,63,777,0,12,87,90,44,2,1.0,90,90,45,3,3,2,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",6,6,6,85,921,0,3,174,237,10,18,3.5,398,237,22,224,93,12,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/TopologyTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/TopologyTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",17,0,3,313,3026,0,16,465,199,18,26,4.0,785,199,30,320,120,12,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/WordCountTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/WordCountTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",9,0,3,168,1496,0,9,249,223,15,17,3,328,223,19,79,17,5,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/ConsumedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/ConsumedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",4,0,2,47,445,0,4,72,73,12,6,3.0,116,73,19,44,23,7,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/JoinedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/JoinedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",2,0,2,22,222,0,2,44,37,6,8,1.0,72,37,9,28,13,4,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamSplitTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamSplitTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",3,0,2,86,853,0,3,125,126,42,3,1,143,126,48,18,16,6,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",33,0,2,334,2777,0,33,466,146,25,19,2,589,147,31,123,63,6,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/MaterializedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/MaterializedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",5,0,2,59,551,0,5,88,84,13,7,3,142,84,20,54,28,8,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/ProducedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/ProducedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",3,0,2,27,270,0,3,49,51,7,7,2,80,51,11,31,13,4,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/RepartitionedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/RepartitionedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",7,0,2,60,571,0,7,90,89,22,4,3.5,131,89,33,41,27,10,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/SuppressedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/SuppressedTest.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",6,0,2,64,516,0,6,93,96,19,5,1,132,96,26,39,36,8,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/StreamToTableJoinScalaIntegrationTestBase.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/StreamToTableJoinScalaIntegrationTestBase.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",8,6,6,94,685,0,7,137,137,14,10,2.0,177,137,18,40,16,4,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/StreamToTableJoinTestData.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/StreamToTableJoinTestData.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",0,6,6,35,249,0,0,60,61,15,4,1.0,68,61,17,8,6,2,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/TestDriver.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/utils/TestDriver.scala,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",1,0,2,20,248,0,1,41,52,7,6,1.5,66,52,11,25,15,4,2,1,0,1
tests/kafkatest/tests/client/message_format_change_test.py,tests/kafkatest/tests/client/message_format_change_test.py,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",8,0,2,60,627,0,4,104,92,9,12,1.0,140,92,12,36,11,3,2,1,0,1
tests/kafkatest/tests/streams/utils/util.py,tests/kafkatest/tests/streams/utils/util.py,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",6,0,2,23,190,0,5,43,36,11,4,1.0,46,36,12,3,2,1,2,1,0,1
tests/kafkatest/utils/util.py,tests/kafkatest/utils/util.py,"KAFKA-12593: Fix Apache License headers (#10452)

* Standardize license headers in scala, python, and gradle files.
* Relocate copyright attribution to the NOTICE.
* Add a license header check to `spotless` for scala files.

Reviewers: Ewen Cheslack-Postava <ewencp@apache.org>, Matthias J. Sax <mjsax@apache.org>, A. Sophie Blee-Goldman <ableegoldman@apache.org",27,0,2,91,703,0,9,180,56,16,11,1,205,56,19,25,10,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ClientDnsLookup.java,clients/src/main/java/org/apache/kafka/clients/ClientDnsLookup.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",3,0,2,17,86,0,3,39,40,10,4,1.0,44,40,11,5,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ClientUtils.java,clients/src/main/java/org/apache/kafka/clients/ClientUtils.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",22,2,13,89,760,2,6,133,44,4,32,3.0,280,44,9,147,18,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java,clients/src/main/java/org/apache/kafka/clients/ClusterConnectionStates.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",72,4,9,281,1888,5,39,538,113,17,32,3.0,753,113,24,215,39,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java,clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",6,1,3,171,1256,1,6,246,163,11,23,2,278,163,12,32,6,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/ClientUtilsTest.java,clients/src/test/java/org/apache/kafka/clients/ClientUtilsTest.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",10,2,14,86,720,4,10,123,42,6,20,3.0,203,42,10,80,14,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/ClusterConnectionStatesTest.java,clients/src/test/java/org/apache/kafka/clients/ClusterConnectionStatesTest.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",20,31,51,298,3029,18,17,425,181,24,18,3.0,555,181,31,130,51,7,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java,clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",77,8,8,828,8038,8,60,1124,216,16,72,4.0,1730,224,24,606,48,8,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",281,1,1,3635,35766,1,176,4713,303,26,179,3,7364,413,41,2651,285,15,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",19,0,2,179,1374,1,14,247,184,5,46,2.0,364,184,8,117,12,3,2,1,0,1
core/src/main/scala/kafka/controller/ControllerChannelManager.scala,core/src/main/scala/kafka/controller/ControllerChannelManager.scala,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",96,0,1,580,3976,1,27,683,177,5,149,2,2009,204,13,1326,123,9,2,1,0,1
core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala,core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",9,0,1,98,549,1,4,127,105,8,15,1,143,105,10,16,4,1,2,1,0,1
core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala,core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala,"KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup` (#10458)

The config has been deprecated since Kafka 2.6 (released ~1 year before
3.0), but it was the default before it got deprecated. As such, it's
reasonably unlikely that people would have set it explicitly.

Given the confusing `default` name even though it's _not_ the default, I
think we should remove it in 3.0.

Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks
a number of code simplications.

Reviewers: David Jacot <djacot@confluent.io>",61,0,1,405,3063,0,20,508,387,11,48,2.0,854,387,18,346,146,7,2,1,0,1
core/src/test/scala/unit/kafka/log/LogSegmentTest.scala,core/src/test/scala/unit/kafka/log/LogSegmentTest.scala,"Initial commit (#10454)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

I've renamed the file: core/src/test/scala/unit/kafka/log/LogUtils.scala to core/src/test/scala/unit/kafka/log/LogTestUtils.scala. Also I've renamed the underlying lass from LogUtils to LogTestUtils. This is going to help avoid a naming conflict with a new file called LogUtils.scala that I plan to introduce in core/src/main/scala/kafka/log/ as part of the recovery logic refactor. The new file will also contain a bunch of static functions.

Tests:
Relying on existing tests to catch regressions (if any) since this is a simple change.

Reviewers: Satish Duggana <satishd@apache.org>, Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>",40,1,1,413,3990,1,30,589,105,10,57,3,1030,105,18,441,69,8,2,1,0,1
core/src/main/scala/kafka/server/LogDirFailureChannel.scala,core/src/main/scala/kafka/server/LogDirFailureChannel.scala,"KAFKA-12575: Eliminate Log.isLogDirOffline boolean attribute (#10430)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

I have made a change to eliminate Log.isLogDirOffline attribute. This boolean also comes in the way of refactoring the recovery logic. This attribute was added in #9676. But it is redundant and can be eliminated in favor of looking up LogDirFailureChannel to check if the logDir is offline. The performance/latency implication of such a ConcurrentHashMap lookup inside LogDirFailureChannel should be very low given that ConcurrentHashMap reads are usually lock free.

Tests:
Relying on existing unit/integration tests.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>",4,4,0,17,138,1,3,62,55,16,4,1.5,76,55,19,14,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/acl/AclBinding.java,clients/src/main/java/org/apache/kafka/common/acl/AclBinding.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,0,14,41,274,1,8,92,74,12,8,1.5,130,74,16,38,19,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java,clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,0,14,51,352,1,10,115,89,13,9,2,160,89,18,45,21,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/SaslConfigs.java,clients/src/main/java/org/apache/kafka/common/config/SaslConfigs.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,0,35,82,819,0,1,118,54,8,15,2,200,54,13,82,35,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/SslConfigs.java,clients/src/main/java/org/apache/kafka/common/config/SslConfigs.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,0,30,132,1148,0,2,178,102,7,25,3,294,102,12,116,30,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/internals/BrokerSecurityConfigs.java,clients/src/main/java/org/apache/kafka/common/config/internals/BrokerSecurityConfigs.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,3,4,59,287,0,0,92,70,12,8,1.5,102,70,13,10,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ChannelBuilders.java,clients/src/main/java/org/apache/kafka/common/network/ChannelBuilders.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",32,1,23,170,1339,3,7,244,52,9,27,4,388,52,14,144,23,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/PlaintextChannelBuilder.java,clients/src/main/java/org/apache/kafka/common/network/PlaintextChannelBuilder.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",17,1,1,82,626,1,12,121,58,6,19,3,182,58,10,61,18,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/SslChannelBuilder.java,clients/src/main/java/org/apache/kafka/common/network/SslChannelBuilder.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",24,1,1,130,937,1,15,184,68,7,28,3.0,334,68,12,150,45,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/resource/Resource.java,clients/src/main/java/org/apache/kafka/common/resource/Resource.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,0,7,40,246,1,7,95,74,12,8,1.0,162,74,20,67,44,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultKafkaPrincipalBuilder.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultKafkaPrincipalBuilder.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",17,4,66,93,805,6,6,135,149,17,8,6.0,235,149,29,100,66,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",123,1,1,582,4265,1,36,726,203,12,61,4,1201,246,20,475,52,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/ChannelBuildersTest.java,clients/src/test/java/org/apache/kafka/common/network/ChannelBuildersTest.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,1,51,77,773,5,4,120,107,13,9,3,185,107,21,65,51,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/auth/DefaultKafkaPrincipalBuilderTest.java,clients/src/test/java/org/apache/kafka/common/security/auth/DefaultKafkaPrincipalBuilderTest.java,"KAFKA-12578: Remove deprecated security classes/methods for 3.0 (#10435)

More specifically, remove deprecated:
- Constants in SslConfigs
- Constants in SaslConfigs
- AclBinding constructor
- AclBindingFilter constructor
- PrincipalBuilder and DefaultPrincipalBuilder classes
- ResourceFilter

Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.

These removals seem non controversial. There is a straightforward alternative. The
deprecations happened in 1.0.0 and 2.0.0.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,3,64,132,1299,9,9,194,170,15,13,2,318,170,24,124,64,10,2,1,0,1
core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala,core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala,"KAFKA-12571: Eliminate LeaderEpochFileCache constructor dependency on logEndOffset (#10426)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

Problems:
For refactoring the recovery logic (KAFKA-12553), we would like to move the logic to initialize LeaderEpochFileCache out of the Log class and into a separate static function. In the future, once we successfully initialize LeaderEpochFileCache outside Log, we will be able pass it as a dependency into both the Log recovery module and Log class constructor. However, currently the LeaderEpochFileCache constructor takes a dependency on logEndOffset (via a callback), which poses the following problems:

Blocks the instantiation of LeaderEpochFileCache outside Log class. Because, outside Log the logEndOffset is unavailable to be passed into LeaderEpochFileCache constructor. As a result, this situation blocks the recovery logic (KAFKA-12553) refactor work.
It turns out the logEndOffset dependency is used only in 1 of the LeaderEpochFileCache methods: LeaderEpochFileCache.endOffsetFor, and just for 1 particular case. Therefore, it is overkill to pass it in the constructor as a dependency. Also a callback is generally not a neat way to access dependencies and it poses code readability problems too.

Solution:
This PR modifies the code such that we only pass the logEndOffset as a parameter into LeaderEpochFileCache.endOffsetFor whenever the method is called, thus eliminating the constructor dependency. This will also unblock the recovery logic refactor work (KAFKA-12553).

Tests:
I have modified the existing tests to suit the above refactor.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>",30,4,5,172,1029,2,12,301,224,13,24,1.5,563,224,23,262,81,11,2,1,0,1
core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala,core/src/test/scala/unit/kafka/server/epoch/LeaderEpochFileCacheTest.scala,"KAFKA-12571: Eliminate LeaderEpochFileCache constructor dependency on logEndOffset (#10426)

This PR is a precursor to the recovery logic refactor work (KAFKA-12553).

Problems:
For refactoring the recovery logic (KAFKA-12553), we would like to move the logic to initialize LeaderEpochFileCache out of the Log class and into a separate static function. In the future, once we successfully initialize LeaderEpochFileCache outside Log, we will be able pass it as a dependency into both the Log recovery module and Log class constructor. However, currently the LeaderEpochFileCache constructor takes a dependency on logEndOffset (via a callback), which poses the following problems:

Blocks the instantiation of LeaderEpochFileCache outside Log class. Because, outside Log the logEndOffset is unavailable to be passed into LeaderEpochFileCache constructor. As a result, this situation blocks the recovery logic (KAFKA-12553) refactor work.
It turns out the logEndOffset dependency is used only in 1 of the LeaderEpochFileCache methods: LeaderEpochFileCache.endOffsetFor, and just for 1 particular case. Therefore, it is overkill to pass it in the constructor as a dependency. Also a callback is generally not a neat way to access dependencies and it poses code readability problems too.

Solution:
This PR modifies the code such that we only pass the logEndOffset as a parameter into LeaderEpochFileCache.endOffsetFor whenever the method is called, thus eliminating the constructor dependency. This will also unblock the recovery logic refactor work (KAFKA-12553).

Tests:
I have modified the existing tests to suit the above refactor.

Reviewers: Dhruvil Shah <dhruvil@confluent.io>, Jun Rao <junrao@gmail.com>",40,63,55,338,2990,17,40,582,721,32,18,3.0,1090,721,61,508,299,28,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerUtilTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerUtilTest.java,"KAFKA-12288: remove task-level filesystem locks (#10342)

The filesystem locks don't protect access between StreamThreads, only across different instances of the same Streams application. Running multiple processes in the same physical state directory is not supported, and as of PR #9978 it's explicitly guarded against), so there's no reason to continue locking the task directories with anything heavier than an in-memory map.

Reviewers: Rohan Desai <rodesai@confluent.io>, Walker Carlson <wcarlson@confluent.io>, Guozhang Wang <guozhang@confluent.io>",12,9,61,222,1881,9,12,337,331,42,8,6.5,456,331,57,119,61,15,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/MockClient.java,clients/src/test/java/org/apache/kafka/clients/MockClient.java,"MINOR: Fix newly added client side timeout tests in `KafkaAdminClientTest` (#10398)

This patch fixes a race condition between the background thread calling `ready` and the call to `MockTime.sleep` in the test. If the call to `sleep` happens first, then the test hangs. I fixed it by giving `MockClient` a way to listen to `ready` calls. This combined with a latch fixes the race. 

This patch also fixes a similar race condition in `testClientSideTimeoutAfterFailureToReceiveResponse`. After the disconnect, there is a race between the background thread sending the retry request and the foreground sleeping for the needed backoff delay.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, David Arthur <mumrah@gmail.com>",142,9,0,608,4157,2,95,799,96,10,80,3.0,1212,117,15,413,77,5,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/AdminClientUnitTestEnv.java,clients/src/test/java/org/apache/kafka/clients/admin/AdminClientUnitTestEnv.java,"KAFKA-12557: Fix hanging KafkaAdminClientTest (#10404)

Fix a hanging test in KafkaAdminClientTest by forcing the admin client to shut down
whether or not there are pending requests once the test harness enters shutdown.

Reviewers: Ismael Juma <ijuma@apache.org>, Guozhang Wang <guozhang@apache.org>",14,5,1,87,638,1,12,139,87,9,15,2,199,87,13,60,23,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/Metric.java,clients/src/main/java/org/apache/kafka/common/Metric.java,"KAFKA-12573; Remove deprecated `Metric#value` (#10425)

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,0,8,5,24,0,0,34,23,4,8,1.5,60,23,8,26,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/KafkaMetric.java,clients/src/main/java/org/apache/kafka/common/metrics/KafkaMetric.java,"KAFKA-12573; Remove deprecated `Metric#value` (#10425)

Reviewers: Ismael Juma <ismael@juma.me.uk>",13,0,9,59,375,1,7,86,55,6,14,1.0,134,55,10,48,12,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java,clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java,"KAFKA-12573; Remove deprecated `Metric#value` (#10425)

Reviewers: Ismael Juma <ismael@juma.me.uk>",76,0,10,758,7893,1,37,955,176,22,43,2,1332,176,31,377,69,9,2,1,0,1
core/src/main/scala/kafka/server/ApiVersionManager.scala,core/src/main/scala/kafka/server/ApiVersionManager.scala,"MINOR: Self-managed -> KRaft (Kafka Raft) (#10414)

`Self-managed` is also used in the context of Cloud vs on-prem and it can
be confusing.

`KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft`
(as in `craftsmanship`).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",11,1,1,92,518,1,6,126,126,42,3,1,128,126,43,2,1,1,2,1,0,1
core/src/main/scala/kafka/tools/StorageTool.scala,core/src/main/scala/kafka/tools/StorageTool.scala,"MINOR: Self-managed -> KRaft (Kafka Raft) (#10414)

`Self-managed` is also used in the context of Cloud vs on-prem and it can
be confusing.

`KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft`
(as in `craftsmanship`).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",37,3,3,200,1247,2,6,238,238,60,4,2.0,251,238,63,13,9,3,2,1,0,1
core/src/test/scala/unit/kafka/api/ApiVersionTest.scala,core/src/test/scala/unit/kafka/api/ApiVersionTest.scala,"MINOR: Self-managed -> KRaft (Kafka Raft) (#10414)

`Self-managed` is also used in the context of Cloud vs on-prem and it can
be confusing.

`KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft`
(as in `craftsmanship`).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",14,1,2,212,1658,1,11,272,108,13,21,2,311,110,15,39,10,2,2,1,0,1
core/src/test/scala/unit/kafka/tools/StorageToolTest.scala,core/src/test/scala/unit/kafka/tools/StorageToolTest.scala,"MINOR: Self-managed -> KRaft (Kafka Raft) (#10414)

`Self-managed` is also used in the context of Cloud vs on-prem and it can
be confusing.

`KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft`
(as in `craftsmanship`).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",12,2,2,154,931,2,10,186,187,46,4,2.0,197,187,49,11,7,3,2,1,0,1
tests/kafkatest/tests/client/client_compatibility_features_test.py,tests/kafkatest/tests/client/client_compatibility_features_test.py,"MINOR: Self-managed -> KRaft (Kafka Raft) (#10414)

`Self-managed` is also used in the context of Cloud vs on-prem and it can
be confusing.

`KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft`
(as in `craftsmanship`).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",17,1,1,106,817,1,5,139,100,6,22,2.0,178,100,8,39,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/RocksDBConfigSetter.java,streams/src/main/java/org/apache/kafka/streams/state/RocksDBConfigSetter.java,"KAFKA-8784: remove default close for RocksDBConfigSetter (#10416)

Remove the default close implementation for RocksDBConfigSetter to avoid accidental memory leaks via C++ backed objects which are constructed but not closed by the user

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",0,1,3,10,89,1,0,61,37,9,7,1,75,37,11,14,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockRocksDbConfigSetter.java,streams/src/test/java/org/apache/kafka/test/MockRocksDbConfigSetter.java,"KAFKA-8784: remove default close for RocksDBConfigSetter (#10416)

Remove the default close implementation for RocksDBConfigSetter to avoid accidental memory leaks via C++ backed objects which are constructed but not closed by the user

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",2,4,0,17,124,1,2,39,35,20,2,1.0,39,35,20,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java,"KAFKA-12523: handle TaskCorruption and TimeoutException during handleCorruption  and handleRevocation (#10407)

Need to handle TaskCorruptedException and TimeoutException that can be thrown from offset commit during handleRevocation or handleCorruption

Reviewers: Matthias J. Sax <mjsax@confluent.org>, Guozhang Wang <guozhang@confluent.io>",21,1,0,145,874,1,13,202,93,3,79,3,756,93,10,554,199,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/Tasks.java,"KAFKA-12523: handle TaskCorruption and TimeoutException during handleCorruption  and handleRevocation (#10407)

Need to handle TaskCorruptedException and TimeoutException that can be thrown from offset commit during handleRevocation or handleCorruption

Reviewers: Matthias J. Sax <mjsax@confluent.org>, Guozhang Wang <guozhang@confluent.io>",50,10,0,226,1787,1,29,307,295,102,3,2,309,295,103,2,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/JoinWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/JoinWindows.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",18,10,116,82,520,16,11,203,91,6,34,4.5,587,91,17,384,116,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Materialized.java,streams/src/main/java/org/apache/kafka/streams/kstream/Materialized.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",16,2,1,98,865,0,15,262,184,22,12,2.0,279,184,23,17,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindows.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",14,10,71,60,369,11,8,160,105,8,21,3,334,105,16,174,71,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindows.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",20,16,118,90,600,14,10,188,125,8,25,4,502,125,20,314,118,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/UnlimitedWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/UnlimitedWindows.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",14,7,49,65,391,7,9,131,63,5,24,3.0,301,63,13,170,49,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Windows.java,streams/src/main/java/org/apache/kafka/streams/kstream/Windows.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",1,3,61,10,91,4,1,69,80,2,28,3.0,344,80,12,275,65,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindowedCogroupedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindowedCogroupedKStreamImpl.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",12,2,6,115,1022,1,6,147,150,24,6,2.5,185,150,31,38,21,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImpl.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",32,2,4,220,2067,1,17,270,199,17,16,3.5,445,199,28,175,59,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedCogroupedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedCogroupedKStreamImpl.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",12,8,37,115,982,1,6,149,180,21,7,4,238,180,34,89,37,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImpl.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",31,14,39,193,1952,1,16,256,143,9,27,4,550,143,20,294,49,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/JoinWindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/JoinWindowsTest.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",11,0,14,98,1215,1,8,145,99,10,15,5,412,99,27,267,55,18,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowsTest.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",8,0,20,52,657,2,7,88,68,7,12,2.0,201,68,17,113,22,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowsTest.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",17,0,36,120,1262,4,13,171,123,10,17,6,427,123,25,256,39,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/UnlimitedWindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/UnlimitedWindowsTest.java,"KAFKA-7106: remove deprecated Windows APIs (#10378)

1. Remove all deprecated APIs in KIP-328.
2. Remove deprecated APIs in Windows in KIP-358.

Reviewers: John Roesler <vvcephei@apache.org>",7,7,21,52,494,6,7,82,80,7,12,3.0,177,80,15,95,21,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java,"KAFKA-12562: Remove deprecated APIs in KafkaStreams and returned state classes (#10412)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",47,0,114,223,1845,3,17,341,237,15,23,3,549,237,24,208,114,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/HostInfo.java,streams/src/main/java/org/apache/kafka/streams/state/HostInfo.java,"KAFKA-12562: Remove deprecated APIs in KafkaStreams and returned state classes (#10412)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",15,0,4,62,373,0,8,112,81,12,9,2,136,81,15,24,9,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ForwardingDisabledProcessorContextTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ForwardingDisabledProcessorContextTest.java,"KAFKA-12452: Remove deprecated overloads of ProcessorContext#forward (#10300)

ProcessorContext#forward was changed via KIP-251 in 2.0.0 release. This PR removes the old and deprecated overloads.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",3,0,12,29,212,2,3,112,61,22,5,1,132,61,26,20,12,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorTopologyTest.java,"KAFKA-12452: Remove deprecated overloads of ProcessorContext#forward (#10300)

ProcessorContext#forward was changed via KIP-251 in 2.0.0 release. This PR removes the old and deprecated overloads.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",74,10,123,798,7673,15,71,976,326,15,65,3,1874,326,29,898,157,14,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/MockProcessorContextTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/MockProcessorContextTest.java,"KAFKA-12452: Remove deprecated overloads of ProcessorContext#forward (#10300)

ProcessorContext#forward was changed via KIP-251 in 2.0.0 release. This PR removes the old and deprecated overloads.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",14,0,42,277,3003,2,8,367,406,23,16,2.0,449,406,28,82,42,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinIntegrationTest.java,"KAFKA-12508: Disable KIP-557 (#10397)

A major issue has been raised that this implementation of
emit-on-change is vulnerable to a number of data-loss bugs
in the presence of recovery with dirty state under at-least-once
semantics. This should be fixed in the future when we implement
a way to avoid or clean up the dirty state under at-least-once,
at which point it will be safe to re-introduce KIP-557 and
complete it.

Reviewers: A. Sophie Blee-Goldman <ableegoldman@apache.org>",73,3,15,587,3958,2,13,702,699,58,12,5.0,1549,699,129,847,623,71,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeProducersOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeProducersOptions.java,"KAFKA-12434; Admin support for `DescribeProducers` API (#10275)

This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

The three new APIs added by KIP-664 require different lookup and request patterns:

- DescribeProducers: send to partition leaders
- DescribeTransactions: send to coordinators
- ListTransactions: send to all brokers

Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail. 

This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. In addition to allowing for easier reuse of lookup strategies, this approach provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`. Follow-up PRs for the rest of KIP-664 will flesh out additional lookup strategies such as for coordinator APIs.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",9,64,0,35,213,5,5,64,64,64,1,1,64,64,64,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ProducerState.java,clients/src/main/java/org/apache/kafka/clients/admin/ProducerState.java,"KAFKA-12434; Admin support for `DescribeProducers` API (#10275)

This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

The three new APIs added by KIP-664 require different lookup and request patterns:

- DescribeProducers: send to partition leaders
- DescribeTransactions: send to coordinators
- ListTransactions: send to all brokers

Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail. 

This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. In addition to allowing for easier reuse of lookup strategies, this approach provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`. Follow-up PRs for the rest of KIP-664 will flesh out additional lookup strategies such as for coordinator APIs.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",18,101,0,73,345,10,10,101,101,101,1,1,101,101,101,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidTopicException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidTopicException.java,"KAFKA-12434; Admin support for `DescribeProducers` API (#10275)

This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

The three new APIs added by KIP-664 require different lookup and request patterns:

- DescribeProducers: send to partition leaders
- DescribeTransactions: send to coordinators
- ListTransactions: send to all brokers

Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail. 

This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. In addition to allowing for easier reuse of lookup strategies, this approach provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`. Follow-up PRs for the rest of KIP-664 will flesh out additional lookup strategies such as for coordinator APIs.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",7,5,0,34,200,1,7,68,38,14,5,1,80,38,16,12,11,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java,clients/src/main/java/org/apache/kafka/common/utils/CollectionUtils.java,"KAFKA-12434; Admin support for `DescribeProducers` API (#10275)

This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

The three new APIs added by KIP-664 require different lookup and request patterns:

- DescribeProducers: send to partition leaders
- DescribeTransactions: send to coordinators
- ListTransactions: send to all brokers

Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail. 

This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. In addition to allowing for easier reuse of lookup strategies, this approach provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`. Follow-up PRs for the rest of KIP-664 will flesh out additional lookup strategies such as for coordinator APIs.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",8,23,5,48,461,2,5,93,62,12,8,2.5,132,62,16,39,17,5,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/PartitionLeaderStrategyTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/PartitionLeaderStrategyTest.java,"KAFKA-12434; Admin support for `DescribeProducers` API (#10275)

This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

The three new APIs added by KIP-664 require different lookup and request patterns:

- DescribeProducers: send to partition leaders
- DescribeTransactions: send to coordinators
- ListTransactions: send to all brokers

Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail. 

This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. In addition to allowing for easier reuse of lookup strategies, this approach provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`. Follow-up PRs for the rest of KIP-664 will flesh out additional lookup strategies such as for coordinator APIs.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",24,302,0,247,1833,22,22,302,302,302,1,1,302,302,302,0,0,0,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/KeyValueStoreFacadeTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/KeyValueStoreFacadeTest.java,"KAFKA-12435: Fix javadoc errors (#10392)

There were errors while generating javadoc for the streams:test-utils module
because the included TopologyTestDriver imported some excluded classes.

This fixes the errors by inlining the previously excluded packages.

Reviewers: Chia-Ping Tsai <chia7712@apache.org>, Ismael Juma <ijuma@apache.org>",12,2,2,132,947,0,12,175,168,35,5,3,192,168,38,17,13,3,2,1,0,1
core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala,core/src/test/scala/integration/kafka/api/PlaintextAdminIntegrationTest.scala,"KAFKA-12432: AdminClient should time out nodes that are never ready (#10281)

Previously, if we assigned one or more calls to a remote node, but it
never became available, AdminClient would block until the calls hit
their the API timeout.  This was particularly unfortunate in the case
where the calls could have been sent to a different node in the cluster.
This PR fixes this behavior by timing out pending connections to
remote nodes if they take longer than the request timeout.

There are a few other small cleanups in this PR: it removes the
unecessary Call#aborted, sets Call#curNode to null after the call has
failed to avoid confusion when debugging or logging, and adds a
""closing"" boolean rather than setting newCalls to null when the client
is closed.  Also, it increases the log level of the log message that
indicates that we timed out some calls because AdminClient closed,
and simplifies the type of callsInFlight.

Reviewers: Jason Gustafson <jason@confluent.io>",117,1,0,1806,18214,1,69,2388,240,20,120,4.0,4020,293,34,1632,224,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java,streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",33,13,2,371,3102,2,18,452,122,65,7,4,480,126,69,28,20,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/JoinWithIncompleteMetadataIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/JoinWithIncompleteMetadataIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",5,18,11,80,728,3,5,116,109,58,2,3.5,127,109,64,11,11,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",12,13,2,210,1862,2,11,269,264,9,30,3.0,515,264,17,246,120,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinDistributedTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinDistributedTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",18,14,7,186,1581,3,11,242,235,121,2,3.5,249,235,124,7,7,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/PurgeRepartitionTopicIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/PurgeRepartitionTopicIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",19,14,7,168,1323,4,8,219,222,20,11,3,293,222,27,74,41,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationWithSslTest.java,streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationWithSslTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",6,13,2,54,379,2,6,91,96,9,10,3.5,160,96,16,69,35,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",15,13,2,109,787,2,7,159,134,18,9,2,173,134,19,14,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskCreationIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",19,13,7,150,1202,3,10,199,186,40,5,4,212,186,42,13,7,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",4,14,2,93,811,2,4,133,122,44,3,2,137,122,46,4,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StreamsUncaughtExceptionHandlerIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StreamsUncaughtExceptionHandlerIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",18,13,2,222,1924,2,16,290,220,58,5,4,360,220,72,70,53,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StreamsUpgradeTestIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StreamsUpgradeTestIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",8,10,3,98,803,3,5,132,123,33,4,2.5,140,123,35,8,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",7,14,2,108,1080,2,3,160,138,27,6,4.0,178,138,30,18,8,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java,streams/src/test/java/org/apache/kafka/streams/integration/utils/EmbeddedKafkaCluster.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",46,2,13,210,1717,3,28,343,128,10,34,3.0,495,128,15,152,23,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/utils/KafkaEmbedded.java,streams/src/test/java/org/apache/kafka/streams/integration/utils/KafkaEmbedded.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",19,4,7,133,1180,2,12,217,189,11,19,4,378,189,20,161,43,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/HandlingSourceTopicDeletionIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/HandlingSourceTopicDeletionIntegrationTest.java,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",7,13,2,97,836,2,5,132,114,33,4,2.5,137,114,34,5,3,1,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/GroupedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/GroupedTest.scala,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",2,13,14,24,238,2,2,46,38,8,6,3.0,80,38,13,34,14,6,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/StreamJoinedTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/StreamJoinedTest.scala,"KAFKA-12173 Migrate streams:streams-scala module to JUnit 5 (#9858)

1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach
5. remove ExternalResource from all scala modules
6. add explicit AfterClass/BeforeClass to stop/start EmbeddedKafkaCluster

Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable.

Reviewers: John Roesler <vvcephei@apache.org>",3,23,22,41,480,3,3,69,69,17,4,3.0,104,69,26,35,22,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/WindowBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/WindowBytesStoreSupplier.java,"KAFKA-12524: Remove deprecated segments() (#10379)

Reviewers: Boyang Chen <boyang@confluent.io>",0,0,10,8,61,0,0,62,56,12,5,1,76,56,15,14,10,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowBytesStoreSupplier.java,"KAFKA-12524: Remove deprecated segments() (#10379)

Reviewers: Boyang Chen <boyang@confluent.io>",9,0,6,60,261,1,9,89,88,18,5,1,104,88,21,15,8,3,2,1,0,1
core/src/main/scala/kafka/server/KafkaBroker.scala,core/src/main/scala/kafka/server/KafkaBroker.scala,"MINOR: Remove duplicate `createKafkaMetricsContext` (#10376)

This patch removes the duplicated `createKafkaMetricsContext` from `KafkaBroker`. It is already present in `Server`. Also added some small style cleanups.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",5,4,19,60,462,3,3,98,103,24,4,2.5,120,103,30,22,19,6,2,1,0,1
core/src/main/scala/kafka/server/Server.scala,core/src/main/scala/kafka/server/Server.scala,"MINOR: Remove duplicate `createKafkaMetricsContext` (#10376)

This patch removes the duplicated `createKafkaMetricsContext` from `KafkaBroker`. It is already present in `Server`. Also added some small style cleanups.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",11,0,1,74,462,1,6,105,73,26,4,2.0,122,73,30,17,16,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/AdminClientTestUtils.java,clients/src/test/java/org/apache/kafka/clients/admin/AdminClientTestUtils.java,"KAFKA-12479: Batch partition offset requests in ConsumerGroupCommand (#10371)

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",8,13,0,48,471,2,8,105,57,35,3,3,105,57,35,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/admin/ConsumerGroupServiceTest.scala,core/src/test/scala/unit/kafka/admin/ConsumerGroupServiceTest.scala,"KAFKA-12479: Batch partition offset requests in ConsumerGroupCommand (#10371)

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,133,0,97,1061,6,6,133,133,133,1,1,133,133,133,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/requests/DeleteTopicsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/DeleteTopicsRequestTest.java,"MINOR: use new method to get number of topics in DeleteTopicsRequest (#10351)

Reviewers: David Jacot <djacot@confluent.io>",12,136,0,95,963,4,4,136,136,136,1,1,136,136,136,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java,clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalSerde.java,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,1,6,53,0,0,43,43,22,2,1.0,44,43,22,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",72,2,2,560,5166,2,22,766,644,85,9,3,808,644,90,42,19,5,2,1,0,1
core/src/main/scala/kafka/admin/FeatureCommand.scala,core/src/main/scala/kafka/admin/FeatureCommand.scala,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",53,1,1,295,1722,0,14,390,408,130,3,1,416,408,139,26,25,9,2,1,0,1
core/src/main/scala/kafka/server/MetadataCache.scala,core/src/main/scala/kafka/server/MetadataCache.scala,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",56,1,1,331,2881,1,25,471,151,8,60,2.5,1008,151,17,537,96,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/BranchedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/BranchedKStream.java,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,1,9,130,0,0,171,171,86,2,1.0,172,171,86,1,1,0,2,1,0,1
streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java,streams/src/test/java/org/apache/kafka/test/StreamsTestUtils.java,"MINOR: Remove duplicate definition about 'the' from kafka project (#10370)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",38,1,1,192,1634,0,17,237,63,9,25,2,317,64,13,80,33,3,2,1,0,1
core/src/test/java/kafka/test/ClusterTestExtensionsTest.java,core/src/test/java/kafka/test/ClusterTestExtensionsTest.java,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>",10,4,4,77,707,1,8,112,112,56,2,2.5,116,112,58,4,4,2,1,0,1,1
core/src/test/java/kafka/test/annotation/Type.java,core/src/test/java/kafka/test/annotation/Type.java,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>",4,34,4,34,225,1,4,58,30,29,2,1.5,62,34,31,4,4,2,1,0,1,1
core/src/test/java/kafka/testkit/ControllerNode.java,core/src/test/java/kafka/testkit/ControllerNode.java,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>",8,63,0,38,172,6,6,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
core/src/test/java/kafka/testkit/TestKitNode.java,core/src/test/java/kafka/testkit/TestKitNode.java,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>",0,23,0,5,20,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/RaftConfig.java,raft/src/main/java/org/apache/kafka/raft/RaftConfig.java,"KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working (#10220)

Introduce ""testkit"" package which includes KafkaClusterTestKit class for enabling integration tests of self-managed clusters. Also make use of this new integration test harness in the ClusterTestExtentions JUnit extension. 

Adds RaftClusterTest for basic self-managed integration test. 

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. McCabe <cmccabe@apache.org>

Co-authored-by: Colin P. McCabe <cmccabe@apache.org>",32,9,4,202,1259,2,20,278,210,31,9,6,423,210,47,145,105,16,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java,"KAFKA-12514: Fix NPE in SubscriptionState (#10369)

Return null for partitionLag if there is no current position.
This was the desired semantics, the lack of the check was an
oversight.

Patches: KIP-695
Patches: a92b986c855592d630fbabf49d1e9a160ad5b230

Reviewers: Walker Carlson <wcarlson@confluent.io>, A. Sophie Blee-Goldman <ableegoldman@apache.org>",206,5,2,800,5161,1,113,1151,264,16,72,5.0,2101,306,29,950,120,13,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java,"KAFKA-12514: Fix NPE in SubscriptionState (#10369)

Return null for partitionLag if there is no current position.
This was the desired semantics, the lack of the check was an
oversight.

Patches: KIP-695
Patches: a92b986c855592d630fbabf49d1e9a160ad5b230

Reviewers: Walker Carlson <wcarlson@confluent.io>, A. Sophie Blee-Goldman <ableegoldman@apache.org>",39,15,0,620,6420,1,39,812,232,20,41,5,1158,236,28,346,42,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueJoinerWithKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueJoinerWithKey.java,"KAFKA-3745: Add access to read-only key in value joiner (#10150)

This PR implements adding read-only access to the key for KStream.join as described in KIP-149

This PR as it stands does not affect the Streams Scala API. Updating the Streams Scala API will be done in a follow-up PR.
Additionally, the original KIP did not include the KTable API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.

Reviewers: Matthias J. Sax <mjsax@apache.org>",0,58,0,4,41,0,0,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/AbstractStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/AbstractStream.java,"KAFKA-3745: Add access to read-only key in value joiner (#10150)

This PR implements adding read-only access to the key for KStream.join as described in KIP-149

This PR as it stands does not affect the Streams Scala API. Updating the Streams Scala API will be done in a follow-up PR.
Additionally, the original KIP did not include the KTable API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.

Reviewers: Matthias J. Sax <mjsax@apache.org>",15,10,0,111,948,2,11,160,78,4,38,3.0,405,78,11,245,42,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableJoin.java,"KAFKA-3745: Add access to read-only key in value joiner (#10150)

This PR implements adding read-only access to the key for KStream.join as described in KIP-149

This PR as it stands does not affect the Streams Scala API. Updating the Streams Scala API will be done in a follow-up PR.
Additionally, the original KIP did not include the KTable API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.

Reviewers: Matthias J. Sax <mjsax@apache.org>",2,3,3,24,254,2,2,45,46,11,4,2.5,59,46,15,14,7,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableJoin.java,"KAFKA-3745: Add access to read-only key in value joiner (#10150)

This PR implements adding read-only access to the key for KStream.join as described in KIP-149

This PR as it stands does not affect the Streams Scala API. Updating the Streams Scala API will be done in a follow-up PR.
Additionally, the original KIP did not include the KTable API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.

Reviewers: Matthias J. Sax <mjsax@apache.org>",2,3,3,22,227,2,2,44,75,7,6,2.5,100,75,17,56,37,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/BaseJoinProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/BaseJoinProcessorNode.java,"KAFKA-3745: Add access to read-only key in value joiner (#10150)

This PR implements adding read-only access to the key for KStream.join as described in KIP-149

This PR as it stands does not affect the Streams Scala API. Updating the Streams Scala API will be done in a follow-up PR.
Additionally, the original KIP did not include the KTable API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.

Reviewers: Matthias J. Sax <mjsax@apache.org>",8,4,4,54,381,3,8,89,78,15,6,1.0,105,78,18,16,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java,"KAFKA-12500: fix memory leak in thread cache (#10355)

Need to exclude threads in PENDING_SHUTDOWN from the num live threads computation used to compute the new cache size per thread. Also adds some logging to help follow what's happening when a thread is added/removed/replaced.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Walker Carlson <wcarlson@confluent.io>, John Roesler <john@confluent.io>",69,3,0,281,1801,1,39,381,311,11,34,2.0,542,311,16,161,23,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java,streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java,"KAFKA-12500: fix memory leak in thread cache (#10355)

Need to exclude threads in PENDING_SHUTDOWN from the num live threads computation used to compute the new cache size per thread. Also adds some logging to help follow what's happening when a thread is added/removed/replaced.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Walker Carlson <wcarlson@confluent.io>, John Roesler <john@confluent.io>",160,0,9,935,7140,1,67,1364,157,21,66,3.0,2123,169,32,759,88,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java,"KAFKA-12508: Emit records with same value and same timestamp (#10360)

Emit on change introduced in Streams with KIP-557 might lead to
data loss if a record is put into a source KTable and emitted
downstream and then a failure happens before the offset could be
committed. After Streams rereads the record, it would find a record
with the same key, value and timestamp in the KTable (i.e. the same
record that was put into the KTable before the failure) and not
forward it downstreams. Hence, the record would never be processed
downstream of the KTable which breaks at-least-once and exactly-once
processing guarantees.

Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",20,1,1,87,647,1,10,132,73,16,8,1.0,175,73,22,43,36,5,2,1,0,1
core/src/main/scala/kafka/server/metadata/MetadataBrokers.scala,core/src/main/scala/kafka/server/metadata/MetadataBrokers.scala,"KAFKA-9189: Use MetadataCache instead of ZK during controlled shutdown to avoid hang (#10361)

This avoids hanging during shutdown if ZK is unavailable. We could change ZK
calls to get the controller id and the broker information to have a timeout, but I
think this approach is better.

The downside is that the metadata cache may be slightly out of date, but we will
retry as per the controlled shutdown configuration. If this broker is partitioned
away from the Controller and is not receiving metadata updates, then we want
to shutdown asap anyway.

I added a test that timed out without this change and included a couple of clean-ups
in `ServerShutdownTest`:
* Removed `testCleanShutdownWithDeleteTopicEnabled`, which is redundant
since delete topics is enabled by default.
* Removed redundant method arguments

Reviewers: David Jacot <djacot@confluent.io>, Jun Rao <junrao@gmail.com>",25,5,0,113,888,2,17,150,138,50,3,1,153,138,51,3,3,1,2,1,0,1
core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala,core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala,"KAFKA-9189: Use MetadataCache instead of ZK during controlled shutdown to avoid hang (#10361)

This avoids hanging during shutdown if ZK is unavailable. We could change ZK
calls to get the controller id and the broker information to have a timeout, but I
think this approach is better.

The downside is that the metadata cache may be slightly out of date, but we will
retry as per the controlled shutdown configuration. If this broker is partitioned
away from the Controller and is not receiving metadata updates, then we want
to shutdown asap anyway.

I added a test that timed out without this change and included a couple of clean-ups
in `ServerShutdownTest`:
* Removed `testCleanShutdownWithDeleteTopicEnabled`, which is redundant
since delete topics is enabled by default.
* Removed redundant method arguments

Reviewers: David Jacot <djacot@confluent.io>, Jun Rao <junrao@gmail.com>",18,17,18,194,1542,4,13,258,121,3,74,2.0,610,121,8,352,57,5,2,1,0,1
core/src/test/scala/unit/kafka/zk/ZooKeeperTestHarness.scala,core/src/test/scala/unit/kafka/zk/ZooKeeperTestHarness.scala,"KAFKA-9189: Use MetadataCache instead of ZK during controlled shutdown to avoid hang (#10361)

This avoids hanging during shutdown if ZK is unavailable. We could change ZK
calls to get the controller id and the broker information to have a timeout, but I
think this approach is better.

The downside is that the metadata cache may be slightly out of date, but we will
retry as per the controlled shutdown configuration. If this broker is partitioned
away from the Controller and is not receiving metadata updates, then we want
to shutdown asap anyway.

I added a test that timed out without this change and included a couple of clean-ups
in `ServerShutdownTest`:
* Removed `testCleanShutdownWithDeleteTopicEnabled`, which is redundant
since delete topics is enabled by default.
* Removed redundant method arguments

Reviewers: David Jacot <djacot@confluent.io>, Jun Rao <junrao@gmail.com>",10,6,2,78,536,2,8,135,50,3,46,2.0,321,52,7,186,37,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotRequest.java,clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotRequest.java,"KAFKA-12440; ClusterId validation for Vote, BeginQuorum, EndQuorum and FetchSnapshot (#10289)

Previously we implemented ClusterId validation for the Fetch API in the Raft implementation. This patch adds ClusterId validation to the remaining Raft RPCs. 

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",9,2,0,78,495,1,9,124,122,31,4,1.5,132,122,33,8,7,2,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ValidOffsetAndEpoch.java,raft/src/main/java/org/apache/kafka/raft/ValidOffsetAndEpoch.java,"KAFKA-12253: Add tests that cover all of the cases for ReplicatedLog's validateOffsetAndEpoch (#10276)

Improves test coverage of `validateOffsetAndEpoch`. 

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",13,25,9,43,251,9,9,71,55,36,2,5.0,80,55,40,9,9,4,1,0,1,1
core/src/main/scala/kafka/server/BrokerLifecycleManager.scala,core/src/main/scala/kafka/server/BrokerLifecycleManager.scala,"MINOR: The new KIP-500 code should treat cluster ID as a string (#10357)

Cluster ID has traditionally been treated as a string by the Kafka protocol (for example,
AdminClient returns it as a string).  The new KIP-500 code should continue this practice.  If
we don't do this, upgrading existing clusters may be harder to do.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>",52,3,3,304,1856,2,23,481,481,160,3,2,486,481,162,5,3,2,2,1,0,1
core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala,core/src/main/scala/kafka/server/BrokerMetadataCheckpoint.scala,"MINOR: The new KIP-500 code should treat cluster ID as a string (#10357)

Cluster ID has traditionally been treated as a string by the Kafka protocol (for example,
AdminClient returns it as a string).  The new KIP-500 code should continue this practice.  If
we don't do this, upgrading existing clusters may be harder to do.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>",23,3,14,186,1041,2,6,240,157,17,14,2.0,310,187,22,70,30,5,2,1,0,1
core/src/test/scala/kafka/server/BrokerMetadataCheckpointTest.scala,core/src/test/scala/kafka/server/BrokerMetadataCheckpointTest.scala,"MINOR: The new KIP-500 code should treat cluster ID as a string (#10357)

Cluster ID has traditionally been treated as a string by the Kafka protocol (for example,
AdminClient returns it as a string).  The new KIP-500 code should continue this practice.  If
we don't do this, upgrading existing clusters may be harder to do.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>",14,21,23,121,738,9,14,156,133,39,4,2.5,182,134,46,26,23,6,2,1,0,1
core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala,core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala,"MINOR: The new KIP-500 code should treat cluster ID as a string (#10357)

Cluster ID has traditionally been treated as a string by the Kafka protocol (for example,
AdminClient returns it as a string).  The new KIP-500 code should continue this practice.  If
we don't do this, upgrading existing clusters may be harder to do.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>",8,2,2,182,1561,0,8,220,217,55,4,1.5,223,217,56,3,2,1,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaRaftServerTest.scala,core/src/test/scala/unit/kafka/server/KafkaRaftServerTest.scala,"MINOR: The new KIP-500 code should treat cluster ID as a string (#10357)

Cluster ID has traditionally been treated as a string by the Kafka protocol (for example,
AdminClient returns it as a string).  The new KIP-500 code should continue this practice.  If
we don't do this, upgrading existing clusters may be harder to do.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jason Gustafson <jason@confluent.io>",9,8,8,141,1074,7,9,202,202,67,3,1,211,202,70,9,8,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java,"KAFKA-10070: parameterize Connect unit tests to remove code duplication (#10299)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",39,45,11,522,4908,6,30,674,474,48,14,2.5,750,474,54,76,31,5,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java,"KAFKA-10070: parameterize Connect unit tests to remove code duplication (#10299)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",69,19,2,1262,11436,3,59,1613,183,30,54,7.0,2384,195,44,771,136,14,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ParameterizedTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ParameterizedTest.java,"KAFKA-10070: parameterize Connect unit tests to remove code duplication (#10299)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",8,83,0,46,309,7,7,83,83,83,1,1,83,83,83,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java,"KAFKA-10697: Remove ProduceResponse.responses (#10332)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",35,0,26,167,1221,1,21,239,74,6,41,2,817,123,20,578,249,14,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ProduceResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/ProduceResponseTest.java,"KAFKA-10697: Remove ProduceResponse.responses (#10332)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,45,24,94,1081,3,3,127,115,18,7,3,185,115,26,58,24,8,2,1,0,1
core/src/test/scala/integration/kafka/network/DynamicConnectionQuotaTest.scala,core/src/test/scala/integration/kafka/network/DynamicConnectionQuotaTest.scala,"KAFKA-10697: Remove ProduceResponse.responses (#10332)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",29,5,5,305,2534,2,21,415,143,19,22,3.0,570,165,26,155,52,7,2,1,0,1
core/src/test/scala/unit/kafka/server/EdgeCaseRequestTest.scala,core/src/test/scala/unit/kafka/server/EdgeCaseRequestTest.scala,"KAFKA-10697: Remove ProduceResponse.responses (#10332)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,7,7,150,1187,2,11,194,171,8,24,2.0,267,171,11,73,10,3,2,1,0,1
core/src/test/scala/unit/kafka/server/ProduceRequestTest.scala,core/src/test/scala/unit/kafka/server/ProduceRequestTest.scala,"KAFKA-10697: Remove ProduceResponse.responses (#10332)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,49,42,197,1956,7,9,253,86,8,32,2.0,397,86,12,144,42,4,2,1,0,1
core/src/test/scala/kafka/server/BrokerToControllerRequestThreadTest.scala,core/src/test/scala/kafka/server/BrokerToControllerRequestThreadTest.scala,"MINOR: Start the broker-to-controller channel for request forwarding (#10340)

Also use different log prefixes for the different channels",21,32,0,282,2242,8,15,393,193,39,10,7.0,541,193,54,148,60,15,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/BaseHashTable.java,metadata/src/main/java/org/apache/kafka/timeline/BaseHashTable.java,"MINOR: Fix BaseHashTable sizing (#10334)

The array backing BaseHashTable is intended to be sized as a power of
two.  Due to a bug, the initial array size was calculated incorrectly
in some cases.

Also make the maximum array size the largest possible 31-bit power of
two.  Previously it was a smaller size but this was due to a typo.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",44,23,7,187,1365,2,12,262,246,131,2,3.5,269,246,134,7,7,4,2,1,0,1
metadata/src/test/java/org/apache/kafka/timeline/BaseHashTableTest.java,metadata/src/test/java/org/apache/kafka/timeline/BaseHashTableTest.java,"MINOR: Fix BaseHashTable sizing (#10334)

The array backing BaseHashTable is intended to be sized as a power of
two.  Due to a bug, the initial array size was calculated incorrectly
in some cases.

Also make the maximum array size the largest possible 31-bit power of
two.  Previously it was a smaller size but this was due to a typo.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jose Sancio <jsancio@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",12,20,0,114,1051,1,8,144,124,72,2,1.0,144,124,72,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateTopicsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateTopicsResult.java,"KAFKA-10357: Add setup method to internal topics (#10317)

For KIP-698, we need a way to setup internal topics without validating them. This PR adds a setup method to the InternalTopicManager for that purpose.

Reviewers: Rohan Desai <rohan@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",15,2,2,76,521,2,14,159,85,18,9,2,174,88,19,15,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsResult.java,"KAFKA-10357: Add setup method to internal topics (#10317)

For KIP-698, we need a way to setup internal topics without validating them. This PR adds a setup method to the InternalTopicManager for that purpose.

Reviewers: Rohan Desai <rohan@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,1,1,18,141,1,3,53,50,9,6,1.5,61,50,10,8,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/serialization/UUIDDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/UUIDDeserializer.java,"MINOR: remove redundant null check when testing specified type (#10314)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,1,1,31,219,1,2,56,60,14,4,1.0,63,60,16,7,5,2,2,1,0,1
shell/src/main/java/org/apache/kafka/shell/CommandUtils.java,shell/src/main/java/org/apache/kafka/shell/CommandUtils.java,"MINOR: remove redundant null check when testing specified type (#10314)

Reviewers: Ismael Juma <ismael@juma.me.uk>",26,1,1,94,741,1,5,148,148,74,2,1.0,149,148,74,1,1,0,2,1,0,1
shell/src/main/java/org/apache/kafka/shell/MetadataNode.java,shell/src/main/java/org/apache/kafka/shell/MetadataNode.java,"MINOR: remove redundant null check when testing specified type (#10314)

Reviewers: Ismael Juma <ismael@juma.me.uk>",26,4,4,107,700,3,10,140,140,70,2,2.5,144,140,72,4,4,2,2,1,0,1
core/src/main/scala/kafka/admin/LogDirsCommand.scala,core/src/main/scala/kafka/admin/LogDirsCommand.scala,"KAFKA-12454: Add ERROR logging on kafka-log-dirs when given brokerIds do not  exist in current kafka cluster (#10304)

When non-existent brokerIds value are given, the kafka-log-dirs tool will have a timeout error:

Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs
at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
at kafka.admin.LogDirsCommand$.describe(LogDirsCommand.scala:50)
at kafka.admin.LogDirsCommand$.main(LogDirsCommand.scala:36)
at kafka.admin.LogDirsCommand.main(LogDirsCommand.scala)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs

When the brokerId entered by the user does not exist, an error message indicating that the node is not present should be printed.

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",14,23,12,99,821,1,4,133,114,12,11,2,174,114,16,41,12,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/LogDirsCommandTest.scala,core/src/test/scala/unit/kafka/admin/LogDirsCommandTest.scala,"KAFKA-12454: Add ERROR logging on kafka-log-dirs when given brokerIds do not  exist in current kafka cluster (#10304)

When non-existent brokerIds value are given, the kafka-log-dirs tool will have a timeout error:

Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs
at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
at kafka.admin.LogDirsCommand$.describe(LogDirsCommand.scala:50)
at kafka.admin.LogDirsCommand$.main(LogDirsCommand.scala:36)
at kafka.admin.LogDirsCommand.main(LogDirsCommand.scala)
Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs

When the brokerId entered by the user does not exist, an error message indicating that the node is not present should be printed.

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",1,76,0,43,400,1,1,76,76,76,1,1,76,76,76,0,0,0,2,1,0,1
core/src/main/scala/kafka/admin/AclCommand.scala,core/src/main/scala/kafka/admin/AclCommand.scala,"MINOR: remove some specifying types in tool command (#10329)

Reviewers: David Jacot <david.jacot@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",116,6,6,516,4523,4,31,667,350,12,56,3.5,1356,350,24,689,123,12,2,1,0,1
core/src/main/scala/kafka/admin/PreferredReplicaLeaderElectionCommand.scala,core/src/main/scala/kafka/admin/PreferredReplicaLeaderElectionCommand.scala,"MINOR: remove some specifying types in tool command (#10329)

Reviewers: David Jacot <david.jacot@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",46,2,3,244,1685,0,10,303,173,8,39,3,576,195,15,273,61,7,2,1,0,1
core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala,core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala,"MINOR: remove some specifying types in tool command (#10329)

Reviewers: David Jacot <david.jacot@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",229,9,12,1221,9345,4,62,1983,1164,26,76,4.0,3598,1636,47,1615,472,21,2,1,0,1
core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala,core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala,"MINOR: remove some specifying types in tool command (#10329)

Reviewers: David Jacot <david.jacot@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",42,1,1,230,1653,1,13,307,237,13,24,2.5,424,237,18,117,22,5,2,1,0,1
tests/kafkatest/tests/client/consumer_test.py,tests/kafkatest/tests/client/consumer_test.py,"KAFKA-12455: Fix OffsetValidationTest.test_broker_rolling_bounce failure with Raft (#10322)

This test was failing when used with a Raft-based metadata quorum but succeeding with a
ZooKeeper-based quorum. This patch increases the consumers' session timeouts to 30 seconds,
which fixes the Raft case and also eliminates flakiness that has historically existed in the
Zookeeper case.

This patch also fixes a minor logging bug in RaftReplicaManager.endMetadataChangeDeferral() that
was discovered during the debugging of this issue, and it adds an extra logging statement in RaftReplicaManager.handleMetadataRecords() when a single metadata batch is applied to mirror
the same logging statement that occurs when deferred metadata changes are applied.

In the Raft system test case the consumer was sometimes receiving a METADATA response with just
1 alive broker, and then when that broker rolled the consumer wouldn't know about any alive nodes.
It would have to wait until the broker returned before it could reconnect, and by that time the group
coordinator on the second broker would have timed-out the client and initiated a group rebalance. The
test explicitly checks that no rebalances occur, so the test would fail. It turns out that the reason why
the ZooKeeper configuration wasn't seeing rebalances was just plain luck. The brokers' metadata
caches in the ZooKeeper configuration show 1 alive broker even more frequently than the Raft
configuration does. If we tweak the metadata.max.age.ms value on the consumers we can easily
get the ZooKeeper test to fail, and in fact this system test has historically been flaky for the
ZooKeeper configuration. We can get the test to pass by setting session.timeout.ms=30000 (which
is longer than the roll time of any broker), or we can increase the broker count so that the client
never sees a METADATA response with just a single alive broker and therefore never loses contact
with the cluster for an extended period of time. We have plenty of system tests with 3+ brokers, so
we choose to keep this test with 2 brokers and increase the session timeout.

Reviewers: Ismael Juma <ismael@juma.me.uk>",39,8,0,264,2437,1,15,473,162,24,20,1.5,678,193,34,205,123,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java,clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",59,5,0,233,1635,1,30,390,171,11,35,2,617,171,18,227,36,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/CumulativeSum.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/CumulativeSum.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",5,4,0,24,128,1,5,54,50,27,2,1.0,54,50,27,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Frequency.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Frequency.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",4,7,0,23,108,1,4,66,59,33,2,1.0,66,59,33,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Meter.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Meter.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",9,10,0,52,370,1,7,95,82,24,4,1.5,109,82,27,14,13,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/SampledStat.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/SampledStat.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",22,19,0,104,701,2,13,152,106,13,12,1.5,237,106,20,85,30,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/TokenBucket.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/TokenBucket.java,"MINOR: Add toString to various Kafka Metrics classes (#10330)

This was useful while debugging a JDK 16 test failure, I noticed these
were missing.

Reviewers: David Jacot <david.jacot@gmail.com>",9,9,0,52,401,1,7,110,101,37,3,1,111,101,37,1,1,0,2,1,0,1
core/src/main/scala/kafka/server/FetchSession.scala,core/src/main/scala/kafka/server/FetchSession.scala,"KAFKA-12330; FetchSessionCache may cause starvation for partitions when FetchResponse is full (#10318)

The incremental FetchSessionCache sessions deprioritizes partitions where a response is returned. This may happen if log metadata such as log start offset, hwm, etc is returned, or if data for that partition is returned.

When a fetch response fills to maxBytes, data may not be returned for partitions even if the fetch offset is lower than the fetch upper bound. However, the fetch response will still contain updates to metadata such as hwm if that metadata has changed. This can lead to degenerate behavior where a partition's hwm or log start offset is updated resulting in the next fetch being unnecessarily skipped for that partition. At first this appeared to be worse, as hwm updates occur frequently, but starvation should result in hwm movement becoming blocked, allowing a fetch to go through and then becoming unstuck. However, it'll still require one more fetch request than necessary to do so. Consumers may be affected more than replica fetchers, however they often remove partitions with fetched data from the next fetch request and this may be helping prevent starvation.

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",85,1,1,512,3691,0,45,790,720,46,17,4,1015,720,60,225,74,13,2,1,0,1
core/src/test/scala/unit/kafka/server/FetchSessionTest.scala,core/src/test/scala/unit/kafka/server/FetchSessionTest.scala,"KAFKA-12330; FetchSessionCache may cause starvation for partitions when FetchResponse is full (#10318)

The incremental FetchSessionCache sessions deprioritizes partitions where a response is returned. This may happen if log metadata such as log start offset, hwm, etc is returned, or if data for that partition is returned.

When a fetch response fills to maxBytes, data may not be returned for partitions even if the fetch offset is lower than the fetch upper bound. However, the fetch response will still contain updates to metadata such as hwm if that metadata has changed. This can lead to degenerate behavior where a partition's hwm or log start offset is updated resulting in the next fetch being unnecessarily skipped for that partition. At first this appeared to be worse, as hwm updates occur frequently, but starvation should result in hwm movement becoming blocked, allowing a fetch to go through and then becoming unstuck. However, it'll still require one more fetch request than necessary to do so. Consumers may be affected more than replica fetchers, however they often remove partitions with fetched data from the next fetch request and this may be helping prevent starvation.

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",23,97,1,764,7968,4,18,912,312,57,16,3.5,1109,312,69,197,104,12,2,1,0,1
core/src/test/scala/unit/kafka/admin/DeleteTopicTest.scala,core/src/test/scala/unit/kafka/admin/DeleteTopicTest.scala,"MINOR; Various code cleanups (#10319)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",31,1,1,318,2879,1,22,450,377,7,65,3,1670,377,26,1220,368,19,2,1,0,1
core/src/test/scala/unit/kafka/admin/PreferredReplicaLeaderElectionCommandTest.scala,core/src/test/scala/unit/kafka/admin/PreferredReplicaLeaderElectionCommandTest.scala,"MINOR; Various code cleanups (#10319)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",34,3,4,299,2104,1,21,383,337,24,16,2.5,478,337,30,95,25,6,2,1,0,1
core/src/test/scala/unit/kafka/network/ConnectionQuotasTest.scala,core/src/test/scala/unit/kafka/network/ConnectionQuotasTest.scala,"MINOR; Various code cleanups (#10319)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",54,5,4,711,5654,4,41,953,356,106,9,9,1301,468,145,348,178,39,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,"KAFKA-12352: Make sure all rejoin group and reset state has a reason (#10232)

1. Create a reason string to be used for INFO log entry whenever we request re-join or reset generation state.
2. Some minor cleanups.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",209,13,15,1047,7843,3,48,1473,595,10,146,3.0,2972,595,20,1499,108,10,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java,"KAFKA-12352: Make sure all rejoin group and reset state has a reason (#10232)

1. Create a reason string to be used for INFO log entry whenever we request re-join or reset generation state.
2. Some minor cleanups.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",107,1,1,1112,9089,1,65,1455,317,24,60,2.5,1910,320,32,455,110,8,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java,"KAFKA-12352: Make sure all rejoin group and reset state has a reason (#10232)

1. Create a reason string to be used for INFO log entry whenever we request re-join or reset generation state.
2. Some minor cleanups.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",105,2,1,450,2982,2,57,604,288,15,39,3,951,312,24,347,149,9,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinatorTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinatorTest.java,"KAFKA-12352: Make sure all rejoin group and reset state has a reason (#10232)

1. Create a reason string to be used for INFO log entry whenever we request re-join or reset generation state.
2. Some minor cleanups.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",25,1,1,454,4315,1,16,586,436,15,38,4.0,878,436,23,292,33,8,2,1,0,1
core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala,core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala,"KAFKA-12398: Fix flaky test `ConsumerBounceTest.testClose` (#10243)

Reviewers: David Jacot <djacot@confluent.io>",47,11,8,392,3227,3,29,539,210,8,71,2,1056,214,15,517,167,7,2,1,0,1
core/src/main/java/kafka/metrics/FilteringJmxReporter.java,core/src/main/java/kafka/metrics/FilteringJmxReporter.java,"MINOR: Remove redundant inheritance from FilteringJmxReporter #onMetricRemoved (#10303)

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",5,0,5,32,200,1,3,57,62,28,2,1.0,62,62,31,5,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsRebalanceListener.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsRebalanceListener.java,"KAFKA-12462: proceed with task revocation in case of thread in PENDING_SHUTDOWN (#10311)

Always invoke TaskManager#handleRevocation when the thread is in PENDING_SHUTDOWN

Reviewers: Walker Carlson <wcarlson@confluent.io>",12,3,1,88,657,1,4,120,169,9,14,2.0,240,169,17,120,78,9,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectCluster.java,"MINOR: Improve error message in MirrorConnectorsIntegrationBaseTest (#10268)


Reviewers: Mickael Maison <mickael.maison@gmail.com>",83,13,0,461,3506,2,50,799,280,40,20,2.0,947,280,47,148,102,7,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java,"MINOR: Add missing unit tests for Mirror Connect (#10192)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Viktor Somogyi-Vass <viktorsomogyi@gmail.com>",38,5,1,245,1877,2,18,297,293,99,3,1,299,293,100,2,1,1,1,0,1,1
raft/src/main/java/org/apache/kafka/snapshot/SnapshotPath.java,raft/src/main/java/org/apache/kafka/snapshot/SnapshotPath.java,"KAFKA-12205; Delete snapshots less than the snapshot at the log start (#10021)

This patch adds logic to delete old snapshots. There are three cases we handle:

1. Remove old snapshots after a follower completes fetching a snapshot and truncates the log to the latest snapshot
2. Remove old snapshots after a new snapshot is created.
3. Remove old snapshots during recovery after the node is restarted.

Reviewers: Cao Manh Dat<caomanhdat317@gmail.com>, José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",2,3,1,19,119,2,2,39,37,20,2,2.0,40,37,20,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeConfigsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeConfigsResult.java,"KAFKA-10357: Add validation method for internal topics (#10266)

For KIP-698, we need a way to validate internal topics before we create them. This PR adds a validation method to the InternalTopicManager for that purpose.

Reviewers: Rohan Desai <rohan@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,1,1,35,300,1,3,73,59,10,7,1,80,59,11,7,2,1,1,0,1,1
core/src/test/scala/unit/kafka/admin/ResetConsumerGroupOffsetTest.scala,core/src/test/scala/unit/kafka/admin/ResetConsumerGroupOffsetTest.scala,"KAFKA-12287: Add WARN logging on consumer-groups when reset-offsets by timestamp or duration can't find an offset and defaults to latest (#10092)

Reviewers: Matthias J. Sax <matthias@confluent.io>",53,10,0,403,3500,2,35,516,601,18,29,4,1410,601,49,894,351,31,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextUtils.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextUtils.java,"KAFKA-10062: Add a methods to retrieve the current timestamps as known by the Streams app (#9744)

Implements KIP-622.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",22,2,4,67,503,2,14,111,49,22,5,1,115,49,23,4,4,1,2,1,0,1
connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSinkTaskTest.java,connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSinkTaskTest.java,"MINOR: Remove unused variables, methods, parameters, unthrown exceptions, and fix typos (#9457)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com",4,1,1,84,796,1,3,115,67,13,9,3,147,67,16,32,13,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResourceTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResourceTest.java,"MINOR: Remove unused variables, methods, parameters, unthrown exceptions, and fix typos (#9457)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com",31,1,2,417,3819,1,29,513,195,19,27,3,850,205,31,337,85,12,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/VerifiableLog4jAppender.java,tools/src/main/java/org/apache/kafka/tools/VerifiableLog4jAppender.java,"MINOR: Remove unused variables, methods, parameters, unthrown exceptions, and fix typos (#9457)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com",19,1,1,183,1290,1,6,259,162,17,15,1,294,162,20,35,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java,"KAFKA-10192: Increase max time to wait for worker to start in some integration tests (#10118)

Author: Luke Chen <showuon@gmail.com>
Reviewers: Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>",96,1,1,619,3672,0,88,799,431,160,5,3,909,458,182,110,108,22,2,1,0,1
metadata/src/main/java/org/apache/kafka/metalog/LocalLogManager.java,metadata/src/main/java/org/apache/kafka/metalog/LocalLogManager.java,"MINOR: Add missing log argument (#10262)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",60,1,1,325,2081,1,30,391,378,130,3,1,394,378,131,3,2,1,1,0,1,1
core/src/main/scala/kafka/server/metadata/CachedConfigRepository.scala,core/src/main/scala/kafka/server/metadata/CachedConfigRepository.scala,"KAFKA-12403; Ensure local state deleted on `RemoveTopicRecord` received (#10252)

This patch implements additional handling logic for `RemoveTopic` records:

- Update `MetadataPartitions` to ensure addition of deleted partitions to `localRemoved` set
- Ensure topic configs are removed from `ConfigRepository`
- Propagate deleted partitions to `GroupCoordinator` so that corresponding offset commits can be removed

This patch also changes the controller topic id generation logic to use `Uuid.randomUuid` rather than `Random`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",10,8,3,60,456,2,6,111,106,37,3,1,115,106,38,4,3,1,2,1,0,1
core/src/main/scala/kafka/server/metadata/MetadataPartitions.scala,core/src/main/scala/kafka/server/metadata/MetadataPartitions.scala,"KAFKA-12403; Ensure local state deleted on `RemoveTopicRecord` received (#10252)

This patch implements additional handling logic for `RemoveTopic` records:

- Update `MetadataPartitions` to ensure addition of deleted partitions to `localRemoved` set
- Ensure topic configs are removed from `ConfigRepository`
- Propagate deleted partitions to `GroupCoordinator` so that corresponding offset commits can be removed

This patch also changes the controller topic id generation logic to use `Uuid.randomUuid` rather than `Random`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",58,71,24,289,2131,14,32,355,280,71,5,7,391,280,78,36,24,7,2,1,0,1
core/src/test/scala/kafka/server/metadata/MetadataPartitionsTest.scala,core/src/test/scala/kafka/server/metadata/MetadataPartitionsTest.scala,"KAFKA-12403; Ensure local state deleted on `RemoveTopicRecord` received (#10252)

This patch implements additional handling logic for `RemoveTopic` records:

- Update `MetadataPartitions` to ensure addition of deleted partitions to `localRemoved` set
- Ensure topic configs are removed from `ConfigRepository`
- Propagate deleted partitions to `GroupCoordinator` so that corresponding offset commits can be removed

This patch also changes the controller topic id generation logic to use `Uuid.randomUuid` rather than `Random`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",18,145,13,264,2293,12,16,324,152,108,3,6,344,152,115,20,13,7,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java,generator/src/main/java/org/apache/kafka/message/MessageDataGenerator.java,"MINOR: add missing space to errro message when setting uint16 (#10274)

Reviewers: David Arthur <mumrah@gmail.com>",250,1,1,1505,10824,1,51,1614,1266,42,38,4.0,3555,1266,94,1941,684,51,2,1,0,1
core/src/test/scala/unit/kafka/raft/RaftManagerTest.scala,core/src/test/scala/unit/kafka/raft/RaftManagerTest.scala,"MINOR: Raft max batch size needs to propagate to log config (#10256)

This patch ensures that the constant max batch size defined in `KafkaRaftClient` is propagated to the constructed log configuration in `KafkaMetadataLog`. We also ensure that the fetch max size is set consistently with appropriate testing. 

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Arthur <mumrah@gmail.com>",2,1,1,39,289,0,2,68,68,14,5,2,73,68,15,5,2,1,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ResultOrError.java,metadata/src/main/java/org/apache/kafka/controller/ResultOrError.java,"MINOR: Enable topic deletion in the KIP-500 controller (#10184)

This patch enables delete topic support for the new KIP-500 controller. Also fixes the following:
- Fix a bug where feature level records were not correctly replayed.
- Fix a bug in TimelineHashMap#remove where the wrong type was being returned.

Reviewers: Jason Gustafson <jason@confluent.io>, Justine Olshan <jolshan@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>

Co-authored-by: Jason Gustafson <jason@confluent.io>",14,1,1,53,295,0,10,84,84,28,3,1,87,84,29,3,2,1,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/SnapshottableHashTable.java,metadata/src/main/java/org/apache/kafka/timeline/SnapshottableHashTable.java,"MINOR: Enable topic deletion in the KIP-500 controller (#10184)

This patch enables delete topic support for the new KIP-500 controller. Also fixes the following:
- Fix a bug where feature level records were not correctly replayed.
- Fix a bug in TimelineHashMap#remove where the wrong type was being returned.

Reviewers: Jason Gustafson <jason@confluent.io>, Justine Olshan <jolshan@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>

Co-authored-by: Jason Gustafson <jason@confluent.io>",66,1,1,294,1923,1,20,455,455,228,2,1.0,456,455,228,1,1,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/timeline/TimelineHashMap.java,metadata/src/main/java/org/apache/kafka/timeline/TimelineHashMap.java,"MINOR: Enable topic deletion in the KIP-500 controller (#10184)

This patch enables delete topic support for the new KIP-500 controller. Also fixes the following:
- Fix a bug where feature level records were not correctly replayed.
- Fix a bug in TimelineHashMap#remove where the wrong type was being returned.

Reviewers: Jason Gustafson <jason@confluent.io>, Justine Olshan <jolshan@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>

Co-authored-by: Jason Gustafson <jason@confluent.io>",85,4,3,315,1896,2,59,415,414,208,2,2.0,418,414,209,3,3,2,2,1,0,1
metadata/src/test/java/org/apache/kafka/timeline/TimelineHashMapTest.java,metadata/src/test/java/org/apache/kafka/timeline/TimelineHashMapTest.java,"MINOR: Enable topic deletion in the KIP-500 controller (#10184)

This patch enables delete topic support for the new KIP-500 controller. Also fixes the following:
- Fix a bug where feature level records were not correctly replayed.
- Fix a bug in TimelineHashMap#remove where the wrong type was being returned.

Reviewers: Jason Gustafson <jason@confluent.io>, Justine Olshan <jolshan@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>

Co-authored-by: Jason Gustafson <jason@confluent.io>",7,2,0,88,903,1,6,115,113,58,2,1.0,115,113,58,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/message/MessageTest.java,clients/src/test/java/org/apache/kafka/common/message/MessageTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",91,12,12,862,6541,7,48,998,307,23,44,3.0,1451,307,33,453,114,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",2,1,1,32,324,1,1,60,60,15,4,1.0,66,60,16,6,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",4,1,1,64,551,1,3,99,100,20,5,1,112,100,22,13,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ControlledShutdownRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/ControlledShutdownRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",3,1,1,29,260,1,2,51,51,17,3,1,55,51,18,4,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/EndTxnRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/EndTxnRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",2,1,1,29,239,1,1,56,56,19,3,1,59,56,20,3,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/EndTxnResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/EndTxnResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",2,1,1,28,263,1,1,53,50,11,5,1,62,50,12,9,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/EnvelopeRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/EnvelopeRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",3,1,1,43,419,1,2,69,40,14,5,1,74,40,15,5,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/EnvelopeResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/EnvelopeResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",2,1,1,28,282,1,1,52,52,17,3,1,55,52,18,3,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/LeaderAndIsrRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/LeaderAndIsrRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",13,2,3,175,1586,2,5,225,147,25,9,3,254,147,28,29,8,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/LeaderAndIsrResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/LeaderAndIsrResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",14,3,3,146,1293,3,6,175,67,18,10,2.5,241,87,24,66,27,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/LeaveGroupRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/LeaveGroupRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",9,1,1,95,668,1,4,130,68,26,5,3,150,78,30,20,10,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/LeaveGroupResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/LeaveGroupResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",16,5,5,127,963,5,6,168,85,24,7,4,215,85,31,47,29,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/OffsetCommitRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/OffsetCommitRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",7,2,2,110,809,2,4,142,112,24,6,2.0,167,120,28,25,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/OffsetCommitResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/OffsetCommitResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",5,1,1,71,587,1,3,103,99,21,5,2,123,99,25,20,13,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",14,3,3,105,815,3,4,137,115,23,6,3.0,158,115,26,21,10,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/OffsetFetchResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",13,2,2,181,1390,2,6,230,223,23,10,2.0,266,223,27,36,9,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",6,3,3,35,312,2,2,58,60,8,7,2,72,60,10,14,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",16,12,20,261,2426,8,15,314,95,29,11,3,443,130,40,129,59,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/StopReplicaRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/StopReplicaRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",28,4,4,195,1618,4,9,246,181,18,14,4.0,327,189,23,81,17,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/StopReplicaResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/StopReplicaResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",5,1,1,71,749,1,4,97,61,10,10,2.0,128,61,13,31,14,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/TxnOffsetCommitResponseTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",3,1,1,41,325,1,2,66,67,13,5,2,86,67,17,20,15,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/UpdateMetadataRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/UpdateMetadataRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",18,2,2,179,1527,2,5,232,187,26,9,2,255,187,28,23,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/WriteTxnMarkersRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/WriteTxnMarkersRequestTest.java,"MINOR: make sure all generated data tests cover all versions (#10078)

Reviewers: David Jacot <djacot@confluent.io>",5,2,2,53,475,2,3,81,81,16,5,2,90,81,18,9,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/FetchSessionHandler.java,clients/src/main/java/org/apache/kafka/clients/FetchSessionHandler.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",61,4,4,310,2289,8,20,484,443,44,11,2,534,443,49,50,13,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java,clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",48,4,7,279,2028,1,34,354,132,7,50,2.0,1204,182,24,850,304,17,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java,clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",29,114,266,142,1178,41,19,229,110,4,54,4.0,1419,185,26,1190,392,22,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java,clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",25,29,36,307,3332,6,14,378,356,63,6,3.0,465,356,78,87,37,14,2,1,0,1
core/src/main/scala/kafka/server/AbstractFetcherThread.scala,core/src/main/scala/kafka/server/AbstractFetcherThread.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",108,23,28,595,4074,1,32,873,149,7,119,3,2220,169,19,1347,110,11,2,1,0,1
core/src/main/scala/kafka/server/FetchDataInfo.scala,core/src/main/scala/kafka/server/FetchDataInfo.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,2,2,11,78,0,0,31,22,4,7,1,39,22,6,8,3,1,2,1,0,1
core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala,core/src/main/scala/kafka/server/ReplicaAlterLogDirsThread.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",39,17,15,216,1573,4,17,300,266,9,35,3,599,266,17,299,69,9,2,1,0,1
core/src/main/scala/kafka/server/ReplicaFetcherThread.scala,core/src/main/scala/kafka/server/ReplicaFetcherThread.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",54,3,3,288,2121,2,17,386,133,3,141,2,1503,152,11,1117,88,8,2,1,0,1
core/src/test/scala/kafka/tools/ReplicaVerificationToolTest.scala,core/src/test/scala/kafka/tools/ReplicaVerificationToolTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,7,3,38,289,1,1,65,60,7,9,1,103,60,11,38,15,4,2,1,0,1
core/src/test/scala/unit/kafka/log/TransactionIndexTest.scala,core/src/test/scala/unit/kafka/log/TransactionIndexTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,2,2,124,1199,1,10,171,173,21,8,2.0,199,173,25,28,10,4,2,1,0,1
core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala,core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",107,14,8,881,7808,2,70,1178,275,31,38,4.5,1583,349,42,405,105,11,2,1,0,1
core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala,core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,11,12,110,1167,6,11,164,165,18,9,2,191,165,21,27,12,3,2,1,0,1
core/src/test/scala/unit/kafka/server/FetchRequestMaxBytesTest.scala,core/src/test/scala/unit/kafka/server/FetchRequestMaxBytesTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,4,5,92,729,3,9,133,135,22,6,2.5,150,135,25,17,6,3,2,1,0,1
core/src/test/scala/unit/kafka/server/FetchRequestTest.scala,core/src/test/scala/unit/kafka/server/FetchRequestTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",48,56,56,551,5586,23,30,722,233,15,48,2.0,960,233,20,238,56,5,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala,core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",29,21,19,704,7469,4,27,995,402,28,36,6.0,1649,402,46,654,141,18,2,1,0,1
core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala,core/src/test/scala/unit/kafka/server/UpdateFeaturesTest.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",26,1,1,410,3310,1,23,577,581,115,5,2,591,581,118,14,5,3,2,1,0,1
core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala,core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,7,9,79,681,4,7,124,80,10,13,3,164,80,13,40,9,3,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,10,8,77,759,2,3,106,104,35,3,1,115,104,38,9,8,3,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,7,10,84,751,1,2,116,119,58,2,3.0,126,119,63,10,10,5,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/RaftUtil.java,raft/src/main/java/org/apache/kafka/raft/RaftUtil.java,"MINOR: remove FetchResponse.AbortedTransaction and redundant construc… (#9758)

1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK
2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction
3. remove redundant constructors from FetchResponse.PartitionData
4. rename recordSet to records
5. add helpers ""recordsOrFail"" and ""recordsSize"" to FetchResponse to process record casting

Reviewers: Ismael Juma <ismael@juma.me.uk>",43,7,7,119,1405,3,12,157,157,78,2,3.0,164,157,82,7,7,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java,clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrRequest.java,"MINOR; Clean up LeaderAndIsrResponse construction in `ReplicaManager#becomeLeaderOrFollower` (#10234)

This patch refactors the code, which constructs the `LeaderAndIsrResponse` in `ReplicaManager#becomeLeaderOrFollower`, to improve the readability and to remove unnecessary operations.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",25,11,14,158,1286,1,15,205,212,6,32,3.0,872,212,27,667,329,21,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrResponse.java,clients/src/main/java/org/apache/kafka/common/requests/LeaderAndIsrResponse.java,"MINOR; Clean up LeaderAndIsrResponse construction in `ReplicaManager#becomeLeaderOrFollower` (#10234)

This patch refactors the code, which constructs the `LeaderAndIsrResponse` in `ReplicaManager#becomeLeaderOrFollower`, to improve the readability and to remove unnecessary operations.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,3,3,79,662,1,9,116,105,6,21,3,304,105,14,188,72,9,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerChannelManagerTest.scala,core/src/test/scala/unit/kafka/controller/ControllerChannelManagerTest.scala,"MINOR; Clean up LeaderAndIsrResponse construction in `ReplicaManager#becomeLeaderOrFollower` (#10234)

This patch refactors the code, which constructs the `LeaderAndIsrResponse` in `ReplicaManager#becomeLeaderOrFollower`, to improve the readability and to remove unnecessary operations.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",105,6,6,757,7297,1,39,963,708,48,20,5.0,1214,708,61,251,69,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/TopicIdPartition.java,clients/src/main/java/org/apache/kafka/common/TopicIdPartition.java,"KAFKA-9548 Added SPIs and public classes/interfaces introduced in KIP-405 for tiered storage feature in Kafka. (#10173)

KIP-405 introduces tiered storage feature in Kafka. With this feature, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments. The new remote tier uses systems, such as HDFS or S3 or other cloud storages to store the completed log segments. Consumers fetch the records stored in remote storage through the brokers with the existing protocol.

We introduced a few SPIs for plugging in log/index store and remote log metadata store.

This involves two parts
1. Storing the actual data in remote storage like HDFS, S3, or other cloud storages.
2. Storing the metadata about where the remote segments are stored. The default implementation uses an internal Kafka topic.

You can read KIP for more details at https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage

Reviewers: Jun Rao <junrao@gmail.com>",10,74,0,39,203,6,6,74,74,74,1,1,74,74,74,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/ListTransactionsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ListTransactionsRequest.java,"KAFKA-12369; Implement `ListTransactions` API (#10206)

This patch implements the `ListTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Tom Bentley <tbentley@redhat.com>, Chia-Ping Tsai <chia7712@gmail.com>",8,77,0,48,309,8,8,77,77,77,1,1,77,77,77,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/ListTransactionsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ListTransactionsResponse.java,"KAFKA-12369; Implement `ListTransactions` API (#10206)

This patch implements the `ListTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Tom Bentley <tbentley@redhat.com>, Chia-Ping Tsai <chia7712@gmail.com>",6,62,0,36,237,6,6,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionLog.scala,"KAFKA-12369; Implement `ListTransactions` API (#10206)

This patch implements the `ListTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: Tom Bentley <tbentley@redhat.com>, Chia-Ping Tsai <chia7712@gmail.com>",19,2,2,116,927,2,6,191,275,13,15,2,509,275,34,318,162,21,2,1,0,1
core/src/main/scala/kafka/controller/ControllerContext.scala,core/src/main/scala/kafka/controller/ControllerContext.scala,"KAFKA-12394; Return `TOPIC_AUTHORIZATION_FAILED` in delete topic response if no describe permission (#10223)

We now accept topicIds in the `DeleteTopic` request. If the client principal does not have `Describe` permission, then we return `TOPIC_AUTHORIZATION_FAILED`. This is justified because the topicId is not considered sensitive. However, in this case, we should not return the name of the topic in the response since we do consider it sensitive.

Reviewers: David Jacot <djacot@confluent.io>, dengziming <dengziming1993@gmail.com>, Justine Olshan <jolshan@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",94,4,0,390,3053,2,57,521,131,24,22,3.5,756,169,34,235,38,11,2,1,0,1
tests/kafkatest/tests/streams/streams_smoke_test.py,tests/kafkatest/tests/streams/streams_smoke_test.py,MINOR: Disable transactional/idempotent system tests for Raft quorums (#10224),4,2,1,77,708,0,2,112,73,6,18,1.0,177,73,10,65,31,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBRangeIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBRangeIteratorTest.java,"KAFKA-10766: Unit test cases for RocksDBRangeIterator (#9717)

This PR aims to add unit test cases for RocksDBRangeIterator which were missing.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",12,440,0,401,3020,12,12,440,440,440,1,1,440,440,440,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.java,"KAFKA-12289: Adding test cases for prefix scan in InMemoryKeyValueStore (#10052)

Co-authored-by: Bruno Cadonna <bruno@confluent.io>

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",3,5,0,18,158,0,3,133,66,13,10,2.0,163,66,16,30,11,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecords.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRecords.java,"KAFKA-12268: Implement task idling semantics via currentLag API (#10137)

Implements KIP-695

Reverts a previous behavior change to Consumer.poll and replaces
it with a new Consumer.currentLag API, which returns the client's
currently cached lag.

Uses this new API to implement the desired task idling semantics
improvement from KIP-695.

Reverts fdcf8fbf72bee9e672d0790cdbe5539846f7dc8e / KAFKA-10866: Add metadata to ConsumerRecords (#9836)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@apache.org>",19,2,101,75,680,12,10,129,97,8,16,1.5,320,101,20,191,101,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/PartitionGroup.java,"KAFKA-12268: Implement task idling semantics via currentLag API (#10137)

Implements KIP-695

Reverts a previous behavior change to Consumer.poll and replaces
it with a new Consumer.currentLag API, which returns the client's
currently cached lag.

Uses this new API to implement the desired task idling semantics
improvement from KIP-695.

Reverts fdcf8fbf72bee9e672d0790cdbe5539846f7dc8e / KAFKA-10866: Add metadata to ConsumerRecords (#9836)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@apache.org>",49,40,59,253,1620,3,17,373,165,12,32,2.0,602,165,19,229,59,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/PartitionGroupTest.java,"KAFKA-12268: Implement task idling semantics via currentLag API (#10137)

Implements KIP-695

Reverts a previous behavior change to Consumer.poll and replaces
it with a new Consumer.currentLag API, which returns the client's
currently cached lag.

Uses this new API to implement the desired task idling semantics
improvement from KIP-695.

Reverts fdcf8fbf72bee9e672d0790cdbe5539846f7dc8e / KAFKA-10866: Add metadata to ConsumerRecords (#9836)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@apache.org>",27,19,10,613,5764,8,27,779,260,25,31,4,1165,310,38,386,68,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java,clients/src/main/java/org/apache/kafka/common/protocol/types/Type.java,"MINOR: correct the error message of validating uint32 (#10193)

Reviewers: Tom Bentley <tbentley@redhat.com>, David Jacot <djacot@confluent.io>",94,2,2,898,4875,1,27,1130,236,45,25,2,1215,238,49,85,26,3,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorDemo.java,"MINOR: Word count should account for extra whitespaces between words (#10229)

Reviewers: Matthias J. Sax <matthias@confluent.io>",9,1,1,102,901,1,2,157,112,4,39,2,349,112,9,192,23,5,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerDemo.java,"MINOR: Word count should account for extra whitespaces between words (#10229)

Reviewers: Matthias J. Sax <matthias@confluent.io>",10,1,1,110,951,1,3,170,160,57,3,1,178,160,59,8,7,3,1,0,1,1
streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java,streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountProcessorTest.java,"MINOR: Word count should account for extra whitespaces between words (#10229)

Reviewers: Matthias J. Sax <matthias@confluent.io>",1,1,1,35,417,1,1,71,70,9,8,1.0,94,70,12,23,17,3,2,1,0,1
streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java,streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountTransformerTest.java,"MINOR: Word count should account for extra whitespaces between words (#10229)

Reviewers: Matthias J. Sax <matthias@confluent.io>",2,1,1,36,392,1,1,70,66,14,5,1,76,66,15,6,4,1,2,1,0,1
core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala,core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala,"MINOR; Small refactor in `GroupMetadata` (#10236)

Reviewers: Jason Gustafson <jason@confluent.io>",116,13,5,511,3638,6,43,801,209,15,54,4.0,1159,209,21,358,68,7,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",112,1,1,842,6611,1,61,1146,236,15,74,3.5,2193,236,30,1047,192,14,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",8,41,4,211,1856,3,8,342,177,31,11,5,508,177,46,166,97,15,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",33,4,4,135,850,0,21,343,131,69,5,3,362,133,72,19,12,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",14,1,0,131,1004,1,11,170,160,28,6,2.0,174,160,29,4,2,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",36,2,1,507,4802,1,28,658,655,132,5,2,667,655,133,9,6,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",92,4,1,1083,10243,2,63,1469,1437,245,6,5.0,1555,1437,259,86,60,14,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java,"KAFKA-10340: Proactively close producer when cancelling source tasks (#10016)

Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.

Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",64,3,1,1186,10791,1,56,1510,1409,189,8,5.0,1602,1409,200,92,67,12,2,1,0,1
core/src/test/scala/unit/kafka/admin/ReassignPartitionsUnitTest.scala,core/src/test/scala/unit/kafka/admin/ReassignPartitionsUnitTest.scala,"KAFKA-12329; kafka-reassign-partitions command should give a better error message when a topic does not exist (#10141)

This patch updates `kafka-reassign-partitions` to provide a meaningful error message to the user when a topic does not exist.

Reviewers:  Chia-Ping Tsai <chia7712@gmail.com>",35,14,0,603,5697,2,30,670,643,84,8,3.5,861,643,108,191,102,24,2,1,0,1
tests/kafkatest/tests/core/produce_bench_test.py,tests/kafkatest/tests/core/produce_bench_test.py,"MINOR: disable test_produce_bench_transactions for Raft metadata quorum (#10222)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",8,0,1,66,554,0,5,88,57,15,6,1.5,102,57,17,14,7,2,2,1,0,1
core/src/main/scala/kafka/server/ConfigHelper.scala,core/src/main/scala/kafka/server/ConfigHelper.scala,"KAFKA-12235: Fix a bug where describeConfigs does not work on 2+ config keys (#9990)

Fix a bug where if more than one configuration key is supplied, describeConfigs fails to return any results at all.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",56,2,2,177,1714,2,11,216,216,108,2,1.5,218,216,109,2,2,1,2,1,0,1
core/src/test/scala/unit/kafka/server/ZkAdminManagerTest.scala,core/src/test/scala/unit/kafka/server/ZkAdminManagerTest.scala,"KAFKA-12235: Fix a bug where describeConfigs does not work on 2+ config keys (#9990)

Fix a bug where if more than one configuration key is supplied, describeConfigs fails to return any results at all.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",6,21,0,100,1010,2,6,137,65,20,7,2,161,65,23,24,10,3,2,1,0,1
core/src/main/scala/kafka/coordinator/group/MemberMetadata.scala,core/src/main/scala/kafka/coordinator/group/MemberMetadata.scala,"MINOR: Clean up group instance id handling in `GroupCoordinator` (#9958)

This is a continuation of a refactor started in #9952. The logic in `GroupCoordinator` is loose and inconsistent in the handling of the `groupInstanceId`. In some cases, such as in the JoinGroup hander, we verify that the groupInstanceId from the request is mapped to the memberId precisely. In other cases, such as Heartbeat, we check the mapping, but only to validate fencing. The patch consolidates the member validation so that all handlers follow the same logic. 

A second problem is that many of the APIs where a `groupInstanceId` is expected use optional arguments. For example:
```scala
def hasStaticMember(groupInstanceId: Option[String]): Boolean

def addStaticMember(groupInstanceId: Option[String], newMemberId: String): Unit
```
If `groupInstanceId` is `None`, then `hasStaticMember` is guaranteed to return `false` while `addStaticMember` raises an `IllegalStateException`. So the APIs suggest a generality which is not supported and does not make sense.

Finally,  the patch attempts to introduce stronger internal  invariants inside `GroupMetadata`. Currently it is possible for an inconsistent `groupInstanceId` to `memberId` mapping to exist because we expose separate APIs to modify `members` and `staticMembers`. We rely on the caller to ensure this doesn't happen.  Similarly, it is possible for a member to be in the `pendingMembers` set as well as the stable `members` map. The patch fixes this by consolidating the paths to addition and removal from these collections and adding assertions to ensure that invariants are maintained. 

Reviewers: David Jacot <djacot@confluent.io>",15,4,5,81,535,0,6,153,99,7,23,2,207,99,9,54,12,2,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala,core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala,"MINOR: Clean up group instance id handling in `GroupCoordinator` (#9958)

This is a continuation of a refactor started in #9952. The logic in `GroupCoordinator` is loose and inconsistent in the handling of the `groupInstanceId`. In some cases, such as in the JoinGroup hander, we verify that the groupInstanceId from the request is mapped to the memberId precisely. In other cases, such as Heartbeat, we check the mapping, but only to validate fencing. The patch consolidates the member validation so that all handlers follow the same logic. 

A second problem is that many of the APIs where a `groupInstanceId` is expected use optional arguments. For example:
```scala
def hasStaticMember(groupInstanceId: Option[String]): Boolean

def addStaticMember(groupInstanceId: Option[String], newMemberId: String): Unit
```
If `groupInstanceId` is `None`, then `hasStaticMember` is guaranteed to return `false` while `addStaticMember` raises an `IllegalStateException`. So the APIs suggest a generality which is not supported and does not make sense.

Finally,  the patch attempts to introduce stronger internal  invariants inside `GroupMetadata`. Currently it is possible for an inconsistent `groupInstanceId` to `memberId` mapping to exist because we expose separate APIs to modify `members` and `staticMembers`. We rely on the caller to ensure this doesn't happen.  Similarly, it is possible for a member to be in the `pendingMembers` set as well as the stable `members` map. The patch fixes this by consolidating the paths to addition and removal from these collections and adding assertions to ensure that invariants are maintained. 

Reviewers: David Jacot <djacot@confluent.io>",52,77,29,514,4163,20,52,679,172,21,33,4,973,172,29,294,38,9,2,1,0,1
tests/kafkatest/tests/core/group_mode_transactions_test.py,tests/kafkatest/tests/core/group_mode_transactions_test.py,"KAFKA-12365; Disable APIs not supported by KIP-500 broker/controller (#10194)

This patch updates request `listeners` tags to be in line with what the KIP-500 broker/controller support today. We will re-enable these APIs as needed once we have added the support.

I have also updated `ControllerApis` to use `ApiVersionManager` and simplified the envelope handling logic.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Colin P. McCabe <cmccabe@apache.org>",29,1,1,247,1639,0,15,329,312,66,5,1,345,312,69,16,10,3,2,1,0,1
tests/kafkatest/tests/core/transactions_test.py,tests/kafkatest/tests/core/transactions_test.py,"KAFKA-12365; Disable APIs not supported by KIP-500 broker/controller (#10194)

This patch updates request `listeners` tags to be in line with what the KIP-500 broker/controller support today. We will re-enable these APIs as needed once we have added the support.

I have also updated `ControllerApis` to use `ApiVersionManager` and simplified the envelope handling logic.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Colin P. McCabe <cmccabe@apache.org>",24,1,2,220,1465,0,13,293,207,23,13,1,411,207,32,118,50,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/SaslChannelBuilder.java,clients/src/main/java/org/apache/kafka/common/network/SaslChannelBuilder.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",66,1,2,340,2693,1,21,405,113,9,44,3.5,643,113,15,238,28,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/ssl/SslFactory.java,clients/src/main/java/org/apache/kafka/common/security/ssl/SslFactory.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",84,48,0,356,2439,2,29,482,228,11,42,3.0,1065,255,25,583,339,14,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java,clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",41,2,2,317,2804,2,19,427,276,11,38,4.0,850,276,22,423,153,11,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java,clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",119,2,2,1051,9598,2,86,1475,654,23,63,4,2819,654,45,1344,419,21,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailureDelayTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailureDelayTest.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",25,1,1,181,1501,1,15,245,229,35,7,2,284,229,41,39,32,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java,"KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while building SslTransportLayer (#10059)

This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",213,5,5,1827,15236,4,139,2577,441,40,64,5.0,3141,479,49,564,95,9,2,1,0,1
tests/kafkatest/tests/core/upgrade_test.py,tests/kafkatest/tests/core/upgrade_test.py,"MINOR: fix syntax error in upgrade_test.py (#10210)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",9,3,2,138,1675,0,5,198,81,6,36,2.0,315,81,9,117,22,3,2,1,0,1
docs/js/templateData.js,docs/js/templateData.js,"MINOR: bump release version to 3.0.0-SNAPSHOT (#10186)

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",0,3,3,6,21,0,0,24,21,1,22,1.0,73,21,3,49,3,2,2,1,0,1
kafka-merge-pr.py,kafka-merge-pr.py,"KAFKA-2327; broker doesn't start if config defines advertised.host but not advertised.port

Added unit tests as well. These fail without the fix, but pass with the fix.

Author: Geoff Anderson <geoff@confluent.io>

Closes #73 from granders/KAFKA-2327 and squashes the following commits:

52a2085 [Geoff Anderson] Cleaned up unecessary toString calls
23b3340 [Geoff Anderson] Fixes KAFKA-2327##KAFKA-2328; merge-kafka-pr.py script should not leave user in a detached branch

The right command to get the branch name is `git rev-parse --abbrev-ref HEAD` instead of `git rev-parse HEAD`. The latter gives the commit hash causing a detached branch when we checkout to it. Seems like a bug we inherited from the Spark script.

Author: Ismael Juma <ismael@juma.me.uk>

Closes #84 from ijuma/kafka-2328-merge-script-no-detached-branch and squashes the following commits:

ae201dd [Ismael Juma] KAFKA-2328; merge-kafka-pr.py script should not leave user in a detached branch##KAFKA-2344; kafka-merge-pr improvements

The first 4 commits are adapted from changes that have been done to the Spark version and the last one is the feature that gwenshap asked for.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira <cshapi@gmail.com>

Closes #90 from ijuma/kafka-2344-merge-pr-improvements and squashes the following commits:

900c371 [Ismael Juma] Allow reviewers to be entered during merge
ac06347 [Ismael Juma] Allow primary author to be overridden during merge
b309829 [Ismael Juma] Set JIRA resolution to ""Fixed"" instead of relying on default transition
0c69a64 [Ismael Juma] Check return value of doctest.testmod()
061cdce [Ismael Juma] Fix instructions on how to install the `jira-python` library##KAFKA-2384: Override commit message title in kafka-merge-pr.py

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang

Closes #105 from ijuma/kafka-2384-override-commit-message-title and squashes the following commits:

e042242 [Ismael Juma] Support overriding of commit message title in kafka-merge-pr.py##HOTFIX; Encode/decode to utf-8 for commit title IO in kafka-merge-pr.py after KAFKA-2384

This fix should be fine for Linux and OS X. Not sure about Windows though. This is a very specific fix for new functionality added in KAFKA-2384. There are other places where a similar error could occur, but are less likely.

The script doesn't really support Unicode input at the moment.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang

Closes #109 from ijuma/kafka-2384-hotfix and squashes the following commits:

0ab8958 [Ismael Juma] Encode/decode to utf-8 for commit title IO in kafka-merge-pr.py##KAFKA-2430; Listing of PR commits in commit message should be optional

If there is a single commit in the PR, then it's never listed.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang

Closes #136 from ijuma/kafka-2430-optional-listing-commits and squashes the following commits:

64f1aec [Ismael Juma] Listing of PR commits in commit message should be optional##KAFKA-2548; kafka-merge-pr tool fails to update JIRA with fix version 0.9.0.0

Simplified the logic to choose the default fix version. We just hardcode
it for `trunk` and try to compute it based on the branch name for the
rest.

Removed logic that tries to handle forked release branches as it
seems to be specific to how the Spark project handles their JIRA.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang

Closes #238 from ijuma/kafka-2548-merge-pr-tool-4-segment-fix-version##KAFKA-2830; Change default fix version to 0.9.1.0 in kafka-merge-pr.py

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang

Closes #523 from ijuma/kafka-2830-fix-version-0.9.1.0##MINOR: Support GitHub OAuth tokens in kafka-merge-pr.py

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Gwen Shapira

Closes #590 from guozhangwang/KOAuth##KAFKA-3277; Update trunk version to be 0.10.0.0-SNAPSHOT

Also update `kafka-merge-pr.py` and `tests/kafkatest/__init__.py`.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #963 from ijuma/update-trunk-0.10.0.0-SNAPSHOT##MINOR: update new version in additional places

Note: This goes only to trunk. 0.10.0 branch will need a separate PR with different versions.

Author: Gwen Shapira <cshapi@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1109 from gwenshap/minor-fix-version-trunk##MINOR: Bump to version 0.10.2##Bump version to 0.10.3.0-SNAPSHOT##MINOR: Bump version to 0.11.0.0-SNAPSHOT

There won't be a 0.10.3.0.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #2628 from ijuma/bump-version-to-0.11.0.0-SNAPSHOT##MINOR: Bump Kafka version to 0.11.1.0-SNAPSHOT

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3095 from ijuma/bump-kafka-version##MINOR: Next release will be 1.0.0

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3580 from ijuma/bump-to-1.0.0-SNAPSHOT##Bump up version to 1.1.0-SNAPSHOT##MINOR: Merge script improvements

- Remove ""list commits"" since we never use it
- Fix release branch detection to just look
for branches that start with digits
- Make script executable

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #4067 from ijuma/merge-script-improvements##MINOR: Exclude Committer Checklist section from commit message

It seems like it's sufficient to be able to refer to it in the PR.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4202 from ijuma/exclude-committer-checklist-when-merging##MINOR: Use GitHub git repo for push by default (#4352)##Bump trunk versions to 1.2-SNAPSHOT (#4505)##MINOR: Bump version to 2.0.0-SNAPSHOT (#4804)##MINOR: Bump version to 2.1.0-SNAPSHOT (#5153)##MINOR: Bump version to 2.2.0-SNAPSHOT

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #5744 from lindong28/bump-up-version-2.2.0##MINOR: Bump version to 2.3.0-SNAPSHOT (#6226)

* MINOR: Bump version to 2.3.0-SNAPSHOT

* Github comment##MINOR: Bump version to 2.4.0-SNAPSHOT (#6774)

Reviewers: Jason Gustafson <jason@confluent.io>##MINOR: Bump version to 2.5.0-SNAPSHOT (#7455)##Bump trunk to 2.6.0-SNAPSHOT (#8026)##Bump trunk to 2.7.0-SNAPSHOT (#8746)##Updating trunk versions after cutting branch for 2.7##bump to 2.9 development version##MINOR: bump release version to 3.0.0-SNAPSHOT (#10186)

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",62,567,91,306,2277,28,14,476,442,15,32,1.0,567,442,18,91,19,3,2,1,0,1
tests/kafkatest/__init__.py,tests/kafkatest/__init__.py,"MINOR: bump release version to 3.0.0-SNAPSHOT (#10186)

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",0,1,1,1,3,0,0,25,16,1,25,1,49,16,2,24,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TransactionalIdNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/TransactionalIdNotFoundException.java,"KAFKA-12267; Implement `DescribeTransactions` API (#10183)

This patch implements the `DescribeTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This is only the server-side implementation and does not contain the `Admin` API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,24,0,6,31,1,1,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/Errors.java,clients/src/main/java/org/apache/kafka/common/protocol/Errors.java,"KAFKA-12267; Implement `DescribeTransactions` API (#10183)

This patch implements the `DescribeTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This is only the server-side implementation and does not contain the `Admin` API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",25,3,1,429,3351,0,12,508,291,4,114,2.0,1409,399,12,901,456,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeTransactionsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeTransactionsRequest.java,"KAFKA-12267; Implement `DescribeTransactions` API (#10183)

This patch implements the `DescribeTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This is only the server-side implementation and does not contain the `Admin` API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,85,0,55,352,8,8,85,85,85,1,1,85,85,85,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/DescribeTransactionsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeTransactionsResponse.java,"KAFKA-12267; Implement `DescribeTransactions` API (#10183)

This patch implements the `DescribeTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This is only the server-side implementation and does not contain the `Admin` API.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,67,0,41,272,6,6,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/RequestContext.java,clients/src/main/java/org/apache/kafka/common/requests/RequestContext.java,"MINOR: Move `RequestChannel.Response` creation logic into `RequestChannel` (#9912)

This patch moves some common response creation logic from `RequestHandlerHelper` and into `RequestChannel`. This refactor has the following benefits:

- It allows us to get rid of some logic that was previously duplicated in both `RequestHandlerHelper` and `TestRaftRequestHandler`. 
- It ensures that we do not need to rely on the caller to ensure that `updateErrorMetrics` gets called since this is handled internally in `RequestChannel`.
- It provides better encapsulation of the quota workflow which relies on custom `Response` objects. Previously it was quite confusing for `KafkaApis` to handle this directly through the `sendResponse` API.

Reviewers: Ismael Juma <ismael@juma.me.uk>",20,15,0,141,776,1,16,193,92,13,15,2,233,92,16,40,12,3,2,1,0,1
core/src/main/scala/kafka/server/RequestHandlerHelper.scala,core/src/main/scala/kafka/server/RequestHandlerHelper.scala,"MINOR: Move `RequestChannel.Response` creation logic into `RequestChannel` (#9912)

This patch moves some common response creation logic from `RequestHandlerHelper` and into `RequestChannel`. This refactor has the following benefits:

- It allows us to get rid of some logic that was previously duplicated in both `RequestHandlerHelper` and `TestRaftRequestHandler`. 
- It ensures that we do not need to rely on the caller to ensure that `updateErrorMetrics` gets called since this is handled internally in `RequestChannel`.
- It provides better encapsulation of the quota workflow which relies on custom `Response` objects. Previously it was quite confusing for `KafkaApis` to handle this directly through the `sendResponse` API.

Reviewers: Ismael Juma <ismael@juma.me.uk>",27,41,65,121,874,14,14,171,195,86,2,9.0,236,195,118,65,65,32,2,1,0,1
core/src/main/scala/kafka/server/ThrottledChannel.scala,core/src/main/scala/kafka/server/ThrottledChannel.scala,"MINOR: Move `RequestChannel.Response` creation logic into `RequestChannel` (#9912)

This patch moves some common response creation logic from `RequestHandlerHelper` and into `RequestChannel`. This refactor has the following benefits:

- It allows us to get rid of some logic that was previously duplicated in both `RequestHandlerHelper` and `TestRaftRequestHandler`. 
- It ensures that we do not need to rely on the caller to ensure that `updateErrorMetrics` gets called since this is handled internally in `RequestChannel`.
- It provides better encapsulation of the quota workflow which relies on custom `Response` objects. Previously it was quite confusing for `KafkaApis` to handle this directly through the `sendResponse` API.

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,12,10,27,183,2,4,61,56,12,5,5,85,56,17,24,10,5,2,1,0,1
core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala,core/src/test/scala/unit/kafka/server/ThrottledChannelExpirationTest.scala,"MINOR: Move `RequestChannel.Response` creation logic into `RequestChannel` (#9912)

This patch moves some common response creation logic from `RequestHandlerHelper` and into `RequestChannel`. This refactor has the following benefits:

- It allows us to get rid of some logic that was previously duplicated in both `RequestHandlerHelper` and `TestRaftRequestHandler`. 
- It ensures that we do not need to rely on the caller to ensure that `updateErrorMetrics` gets called since this is handled internally in `RequestChannel`.
- It provides better encapsulation of the quota workflow which relies on custom `Response` objects. Previously it was quite confusing for `KafkaApis` to handle this directly through the `sendResponse` API.

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,15,40,72,524,5,5,102,90,6,18,2.0,214,90,12,112,40,6,2,1,0,1
core/src/main/scala/kafka/common/InterBrokerSendThread.scala,core/src/main/scala/kafka/common/InterBrokerSendThread.scala,"KAFKA-12273 InterBrokerSendThread#pollOnce throws FatalExitError even… (#10024)

Reviewers: David Jacot <djacot@confluent.io>",37,3,2,162,1053,1,15,214,98,13,17,2,329,128,19,115,30,7,2,1,0,1
core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala,core/src/test/scala/kafka/common/InterBrokerSendThreadTest.scala,"KAFKA-12273 InterBrokerSendThread#pollOnce throws FatalExitError even… (#10024)

Reviewers: David Jacot <djacot@confluent.io>",16,26,5,166,1311,3,10,233,131,19,12,6.0,316,131,26,83,23,7,2,1,0,1
tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py,tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py,"MINOR: fix some ducktape test issues (#10181)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",6,3,3,234,1951,0,6,323,114,32,10,3.0,497,157,50,174,78,17,2,1,0,1
tests/kafkatest/sanity_checks/test_performance_services.py,tests/kafkatest/sanity_checks/test_performance_services.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",5,10,6,64,562,3,3,94,88,13,7,2,112,88,16,18,6,3,2,1,0,1
tests/kafkatest/services/console_consumer.py,tests/kafkatest/services/console_consumer.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",42,5,4,228,1730,1,14,354,146,7,50,2.0,631,146,13,277,39,6,2,1,0,1
tests/kafkatest/services/performance/consumer_performance.py,tests/kafkatest/services/performance/consumer_performance.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",22,5,5,137,868,3,5,187,147,12,15,3,229,147,15,42,10,3,2,1,0,1
tests/kafkatest/services/performance/end_to_end_latency.py,tests/kafkatest/services/performance/end_to_end_latency.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",12,8,5,89,699,3,4,127,59,10,13,4,215,59,17,88,39,7,2,1,0,1
tests/kafkatest/services/performance/producer_performance.py,tests/kafkatest/services/performance/producer_performance.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",18,2,2,122,1069,0,6,174,77,9,20,2.5,305,118,15,131,41,7,2,1,0,1
tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py,tests/kafkatest/tests/client/client_compatibility_produce_consume_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",6,9,6,64,599,4,4,90,75,7,13,2,114,75,9,24,7,2,2,1,0,1
tests/kafkatest/tests/client/compression_test.py,tests/kafkatest/tests/client/compression_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",6,7,6,48,420,4,4,88,85,10,9,1,105,85,12,17,6,2,2,1,0,1
tests/kafkatest/tests/client/consumer_rolling_upgrade_test.py,tests/kafkatest/tests/client/consumer_rolling_upgrade_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",7,5,3,48,464,2,4,88,94,10,9,1,116,94,13,28,22,3,2,1,0,1
tests/kafkatest/tests/client/pluggable_test.py,tests/kafkatest/tests/client/pluggable_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",3,6,1,30,278,2,2,56,51,28,2,2.0,57,51,28,1,1,0,1,0,1,1
tests/kafkatest/tests/connect/connect_test.py,tests/kafkatest/tests/connect/connect_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",12,9,7,143,1437,3,4,209,72,10,21,2,277,72,13,68,15,3,2,1,0,1
tests/kafkatest/tests/core/consume_bench_test.py,tests/kafkatest/tests/core/consume_bench_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",15,27,13,167,1364,15,10,218,132,54,4,4.5,244,132,61,26,13,6,2,1,0,1
tests/kafkatest/tests/core/consumer_group_command_test.py,tests/kafkatest/tests/core/consumer_group_command_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",11,10,8,67,593,7,7,108,105,11,10,1.0,129,105,13,21,10,2,2,1,0,1
tests/kafkatest/tests/core/downgrade_test.py,tests/kafkatest/tests/core/downgrade_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",9,1,1,84,815,1,6,138,115,23,6,3.0,153,115,26,15,6,2,2,1,0,1
tests/kafkatest/tests/core/get_offset_shell_test.py,tests/kafkatest/tests/core/get_offset_shell_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",17,15,8,160,1452,12,13,260,162,26,10,1.0,294,179,29,34,17,3,2,1,0,1
tests/kafkatest/tests/core/replica_scale_test.py,tests/kafkatest/tests/core/replica_scale_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",13,12,10,107,880,7,5,139,139,23,6,2.5,159,139,26,20,10,3,1,0,1,1
tests/kafkatest/tests/end_to_end.py,tests/kafkatest/tests/end_to_end.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",21,3,3,109,771,2,13,150,151,38,4,1.5,157,151,39,7,3,2,1,0,1,1
tests/kafkatest/tests/kafka_test.py,tests/kafkatest/tests/kafka_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",4,6,4,25,153,2,2,47,45,24,2,2.5,51,45,26,4,4,2,2,1,0,1
tests/kafkatest/tests/streams/streams_application_upgrade_test.py,tests/kafkatest/tests/streams/streams_application_upgrade_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",31,2,0,220,2121,0,12,299,297,150,2,1.5,299,297,150,0,0,0,1,0,0,0
tests/kafkatest/tests/streams/streams_cooperative_rebalance_upgrade_test.py,tests/kafkatest/tests/streams/streams_cooperative_rebalance_upgrade_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",13,2,0,148,1080,0,6,212,209,35,6,1.0,217,209,36,5,2,1,1,0,1,1
tests/kafkatest/tests/streams/streams_optimized_test.py,tests/kafkatest/tests/streams/streams_optimized_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",13,2,1,107,863,0,7,150,150,25,6,2.5,195,150,32,45,26,8,2,1,0,1
tests/kafkatest/tests/streams/streams_shutdown_deadlock_test.py,tests/kafkatest/tests/streams/streams_shutdown_deadlock_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",2,2,0,18,134,0,2,47,46,16,3,2,49,46,16,2,2,1,2,1,0,1
tests/kafkatest/tests/streams/streams_standby_replica_test.py,tests/kafkatest/tests/streams/streams_standby_replica_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",3,2,0,112,829,0,3,201,166,25,8,1.5,282,166,35,81,62,10,2,1,0,1
tests/kafkatest/tests/streams/streams_static_membership_test.py,tests/kafkatest/tests/streams/streams_static_membership_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",11,2,0,75,527,0,4,110,108,37,3,2,112,108,37,2,2,1,2,1,0,1
tests/kafkatest/tests/streams/streams_upgrade_test.py,tests/kafkatest/tests/streams/streams_upgrade_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",51,2,0,427,3539,0,13,562,195,12,48,3.5,1072,271,22,510,86,11,2,1,0,1
tests/kafkatest/tests/tools/log4j_appender_test.py,tests/kafkatest/tests/tools/log4j_appender_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",11,9,7,61,498,5,7,97,62,9,11,3,136,62,12,39,15,4,2,1,0,1
tests/kafkatest/tests/tools/replica_verification_test.py,tests/kafkatest/tests/tools/replica_verification_test.py,"MINOR: Test the new KIP-500 quorum mode in ducktape (#10105)

Add the necessary test annotations to test the new KIP-500 quorum broker mode
in many of our ducktape tests. This mode is tested in addition to the classic
Apache ZooKeeper mode.

This PR also adds a new sanity_checks/bounce_test.py system test that runs
through a simple produce/bounce/produce series of events.

Finally, this PR adds @cluster annotations to dozens of system tests that were
missing them. The lack of this annotation was causing these tests to grab the
entire cluster of nodes.  Adding the @cluster annotation dramatically reduced
the time needed to run these tests.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",10,10,6,59,514,5,7,94,88,24,4,4.5,105,88,26,11,6,3,1,0,1,1
core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala,core/src/main/scala/kafka/raft/KafkaNetworkChannel.scala,"MINOR: Raft request thread should discover api versions (#10157)

We do not plan to rely on the IBP in order to determine API versions for raft requests. Instead, we want to discover them through the ApiVersions API. This patch enables the flag to do so. 

In addition, this patch adds unsupported version as well as authentication version checking to all of the derivatives of `InterBrokerSendThread` which rely on dynamic api version discovery. Test cases for these checks have been added.

Reviewers:  Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>",28,12,2,147,885,2,15,194,237,15,13,2,379,237,29,185,158,14,2,1,0,1
core/src/test/scala/unit/kafka/raft/KafkaNetworkChannelTest.scala,core/src/test/scala/unit/kafka/raft/KafkaNetworkChannelTest.scala,"MINOR: Raft request thread should discover api versions (#10157)

We do not plan to rely on the IBP in order to determine API versions for raft requests. Instead, we want to discover them through the ApiVersions API. This patch enables the flag to do so. 

In addition, this patch adds unsupported version as well as authentication version checking to all of the derivatives of `InterBrokerSendThread` which rely on dynamic api version discovery. Test cases for these checks have been added.

Reviewers:  Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>",43,14,1,225,1541,3,19,290,241,22,13,2,361,241,28,71,32,5,2,1,0,1
core/src/test/scala/unit/kafka/server/MockBrokerToControllerChannelManager.scala,core/src/test/scala/unit/kafka/server/MockBrokerToControllerChannelManager.scala,"MINOR: Raft request thread should discover api versions (#10157)

We do not plan to rely on the IBP in order to determine API versions for raft requests. Instead, we want to discover them through the ApiVersions API. This patch enables the flag to do so. 

In addition, this patch adds unsupported version as well as authentication version checking to all of the derivatives of `InterBrokerSendThread` which rely on dynamic api version discovery. Test cases for these checks have been added.

Reviewers:  Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>",16,3,1,68,450,1,6,97,95,48,2,1.0,98,95,49,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",53,4,3,205,1544,2,23,370,371,92,4,2.5,392,371,98,22,18,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredLoginCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredLoginCallbackHandler.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",64,3,6,235,2013,1,18,343,288,49,7,3,360,288,51,17,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredValidatorCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredValidatorCallbackHandler.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",28,1,2,127,1025,1,11,214,217,36,6,2.5,238,217,40,24,13,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerScopeUtilsTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerScopeUtilsTest.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,2,1,32,271,1,2,54,53,14,4,1.5,59,53,15,5,3,1,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Values.java,connect/api/src/main/java/org/apache/kafka/connect/data/Values.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",312,2,2,937,6652,2,48,1265,1117,97,13,2,1345,1117,103,80,31,6,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/health/AbstractState.java,connect/api/src/main/java/org/apache/kafka/connect/health/AbstractState.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,4,2,43,248,1,6,95,74,32,3,3,99,74,33,4,2,1,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorHealth.java,connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorHealth.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,3,2,62,340,1,8,114,86,38,3,2,117,86,39,3,2,1,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/ValuesTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/ValuesTest.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",103,2,1,817,7119,1,83,1003,350,84,12,3.0,1048,350,87,45,14,4,2,1,0,1
connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/PropertyFileLoginModule.java,connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/PropertyFileLoginModule.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",17,2,1,114,784,1,6,155,116,31,5,2,161,116,32,6,3,1,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",48,1,1,410,3413,1,24,554,163,16,35,3,783,269,22,229,106,7,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SinkConnectorConfig.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",23,4,3,126,1105,3,12,175,46,12,15,3,202,46,13,27,8,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",73,1,2,354,2311,1,26,467,217,31,15,2,587,217,39,120,54,8,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",70,4,3,352,2873,1,16,492,258,18,28,4.0,715,258,26,223,107,8,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampConverter.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",47,2,2,357,2890,1,18,462,452,92,5,2,489,452,98,27,21,5,2,1,0,1
core/src/main/scala/kafka/network/SocketServer.scala,core/src/main/scala/kafka/network/SocketServer.scala,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",238,2,2,1241,8750,1,107,1716,341,11,159,3,3779,341,24,2063,203,13,2,1,0,1
core/src/main/scala/kafka/utils/Log4jController.scala,core/src/main/scala/kafka/utils/Log4jController.scala,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,3,2,85,566,2,7,233,98,19,12,2.0,309,98,26,76,33,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Printed.java,streams/src/main/java/org/apache/kafka/streams/kstream/Printed.java,"MINOR: apply Utils.isBlank to code base (#10124)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,2,1,53,439,1,7,136,126,17,8,1.5,160,126,20,24,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/KafkaConsumerMetrics.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/KafkaConsumerMetrics.java,"MINOR: Enhance the documentation with the metric unit which is milliseconds (#10148)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",6,2,2,61,441,1,4,87,81,22,4,2.5,99,81,25,12,8,3,1,0,1,1
clients/src/main/java/org/apache/kafka/common/errors/BrokerIdNotRegisteredException.java,clients/src/main/java/org/apache/kafka/common/errors/BrokerIdNotRegisteredException.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",2,29,0,9,49,2,2,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/BrokerControlState.java,metadata/src/main/java/org/apache/kafka/controller/BrokerControlState.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",4,46,0,22,106,4,4,46,46,46,1,1,46,46,46,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/BrokerControlStates.java,metadata/src/main/java/org/apache/kafka/controller/BrokerControlStates.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",8,56,0,30,153,6,6,56,56,56,1,1,56,56,56,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/ControllerPurgatory.java,metadata/src/main/java/org/apache/kafka/controller/ControllerPurgatory.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",13,108,0,56,406,4,4,108,108,108,1,1,108,108,108,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/DeferredEvent.java,metadata/src/main/java/org/apache/kafka/controller/DeferredEvent.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",0,31,0,4,20,0,0,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/controller/Replicas.java,metadata/src/main/java/org/apache/kafka/controller/Replicas.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",30,180,0,92,738,8,8,180,180,180,1,1,180,180,180,0,0,0,2,1,0,1
metadata/src/main/java/org/apache/kafka/metadata/BrokerHeartbeatReply.java,metadata/src/main/java/org/apache/kafka/metadata/BrokerHeartbeatReply.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",12,14,1,50,229,5,8,93,80,46,2,4.0,94,80,47,1,1,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ControllerPurgatoryTest.java,metadata/src/test/java/org/apache/kafka/controller/ControllerPurgatoryTest.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",6,102,0,76,651,5,5,102,102,102,1,1,102,102,102,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/MockRandom.java,metadata/src/test/java/org/apache/kafka/controller/MockRandom.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",1,34,0,10,66,1,1,34,34,34,1,1,34,34,34,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ReplicasTest.java,metadata/src/test/java/org/apache/kafka/controller/ReplicasTest.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",8,96,0,67,962,8,8,96,96,96,1,1,96,96,96,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/controller/ResultOrErrorTest.java,metadata/src/test/java/org/apache/kafka/controller/ResultOrErrorTest.java,"KAFKA-12276: Add the quorum controller code (#10070)

The quorum controller stores metadata in the KIP-500 metadata log, not in Apache
ZooKeeper. Each controller node is a voter in the metadata quorum. The leader of the
quorum is the active controller, which processes write requests. The followers are standby
controllers, which replay the operations written to the log. If the active controller goes away,
a standby controller can take its place.

Like the ZooKeeper-based controller, the quorum controller is based on an event queue
backed by a single-threaded executor. However, unlike the ZK-based controller, the quorum
controller can have multiple operations in flight-- it does not need to wait for one operation
to be finished before starting another. Therefore, calls into the QuorumController return
CompleteableFuture objects which are completed with either a result or an error when the
operation is done. The QuorumController will also time out operations that have been
sitting on the queue too long without being processed. In this case, the future is completed
with a TimeoutException.

The controller uses timeline data structures to store multiple ""versions"" of its in-memory 
state simultaneously. ""Read operations"" read only committed state, which is slightly older
than the most up-to-date in-memory state. ""Write operations"" read and write the latest
in-memory state. However, we can not return a successful result for a write operation until
its state has been committed to the log. Therefore, if a client receives an RPC response, it
knows that the requested operation has been performed, and can not be undone by a
controller failover.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",3,65,0,42,378,3,3,65,65,65,1,1,65,65,65,0,0,0,2,1,0,1
metadata/src/test/java/org/apache/kafka/metadata/MetadataParserTest.java,metadata/src/test/java/org/apache/kafka/metadata/MetadataParserTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",8,1,1,111,964,1,7,155,155,78,2,1.0,156,155,78,1,1,0,1,0,1,1
shell/src/main/java/org/apache/kafka/shell/CatCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/CatCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",17,120,0,84,530,11,11,120,120,120,1,1,120,120,120,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/CdCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/CdCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",16,117,0,82,509,11,11,117,117,117,1,1,117,117,117,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/Commands.java,shell/src/main/java/org/apache/kafka/shell/Commands.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",15,154,0,98,670,5,5,154,154,154,1,1,154,154,154,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/ErroneousCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/ErroneousCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",7,58,0,31,170,5,5,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/ExitCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/ExitCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",11,88,0,53,269,10,10,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/FindCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/FindCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",19,121,0,85,552,12,12,121,121,121,1,1,121,121,121,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/GlobComponent.java,shell/src/main/java/org/apache/kafka/shell/GlobComponent.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",41,179,0,135,620,7,7,179,179,179,1,1,179,179,179,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/GlobVisitor.java,shell/src/main/java/org/apache/kafka/shell/GlobVisitor.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",26,148,0,114,820,11,11,148,148,148,1,1,148,148,148,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/HelpCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/HelpCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",11,88,0,53,271,10,10,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/HistoryCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/HistoryCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",15,108,0,72,426,11,11,108,108,108,1,1,108,108,108,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/InteractiveShell.java,shell/src/main/java/org/apache/kafka/shell/InteractiveShell.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",22,172,0,136,932,10,10,172,172,172,1,1,172,172,172,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/LsCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/LsCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",57,299,0,247,1719,25,25,299,299,299,1,1,299,299,299,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/ManCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/ManCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",15,109,0,74,442,11,11,109,109,109,1,1,109,109,109,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/NoOpCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/NoOpCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",4,43,0,19,91,3,3,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/NotDirectoryException.java,shell/src/main/java/org/apache/kafka/shell/NotDirectoryException.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",1,30,0,7,34,1,1,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/NotFileException.java,shell/src/main/java/org/apache/kafka/shell/NotFileException.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",1,30,0,7,34,1,1,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
shell/src/main/java/org/apache/kafka/shell/PwdCommandHandler.java,shell/src/main/java/org/apache/kafka/shell/PwdCommandHandler.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",11,89,0,54,272,10,10,89,89,89,1,1,89,89,89,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/CommandTest.java,shell/src/test/java/org/apache/kafka/shell/CommandTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",3,70,0,47,479,3,3,70,70,70,1,1,70,70,70,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/CommandUtilsTest.java,shell/src/test/java/org/apache/kafka/shell/CommandUtilsTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",1,37,0,16,127,1,1,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/GlobComponentTest.java,shell/src/test/java/org/apache/kafka/shell/GlobComponentTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",4,75,0,52,462,4,4,75,75,75,1,1,75,75,75,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/GlobVisitorTest.java,shell/src/test/java/org/apache/kafka/shell/GlobVisitorTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",11,144,0,113,988,8,8,144,144,144,1,1,144,144,144,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/LsCommandHandlerTest.java,shell/src/test/java/org/apache/kafka/shell/LsCommandHandlerTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",3,99,0,74,585,3,3,99,99,99,1,1,99,99,99,0,0,0,0,0,0,0
shell/src/test/java/org/apache/kafka/shell/MetadataNodeTest.java,shell/src/test/java/org/apache/kafka/shell/MetadataNodeTest.java,"KAFKA-12334: Add the KIP-500 metadata shell

The Kafka Metadata shell is a new command which allows users to
interactively examine the metadata stored in a KIP-500 cluster.
It can examine snapshot files that are specified via --snapshot.

The metadata tool works by replaying the log and storing the state into
in-memory nodes.  These nodes are presented in a fashion similar to
filesystem directories.

Reviewers: Jason Gustafson <jason@confluent.io>, David Arthur <mumrah@gmail.com>, Igor Soarez <soarez@apple.com>",3,73,0,50,512,3,3,73,73,73,1,1,73,73,73,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/internals/MetadataOperationContext.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/MetadataOperationContext.java,"KAFKA-12339: Add retry to admin client's listOffsets (#10152)

`KafkaAdmin.listOffsets` did not handle topic-level errors, hence the UnknownTopicOrPartitionException on topic-level can obstruct a Connect worker from running when the new internal topic is NOT synced to all brokers. The method did handle partition-level retriable errors by retrying, so this changes to handle topic-level retriable errors in the same way.

This allows a Connect worker to start up and have the admin client retry when the worker is trying to read to the end of the newly-created internal topics until the internal topic metadata is synced to all brokers.

Author: Chia-Ping Tsai <chia7712@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",13,1,0,60,435,1,9,97,96,32,3,1,99,96,33,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InconsistentClusterIdException.java,clients/src/main/java/org/apache/kafka/common/errors/InconsistentClusterIdException.java,"KAFKA-10817; Add clusterId validation to raft Fetch handling (#10129)

This patch adds clusterId validation in the `Fetch` API as documented in KIP-595. A new error code `INCONSISTENT_CLUSTER_ID` is returned if the request clusterId does not match the value on the server. If no clusterId is provided, the request is treated as valid.

Reviewers: Jason Gustafson <jason@confluent.io>",2,28,0,9,49,2,2,28,28,28,1,1,28,28,28,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java,streams/src/main/java/org/apache/kafka/streams/errors/TaskCorruptedException.java,"KAFKA-9274: Throw TaskCorruptedException instead of TimeoutException when TX commit times out (#10072)

Part of KIP-572: follow up work to PR #9800. It's not save to retry a TX commit after a timeout, because it's unclear if the commit was successful or not, and thus on retry we might get an IllegalStateException. Instead, we will throw a TaskCorruptedException to retry the TX if the commit failed.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",3,10,12,19,132,6,3,50,46,12,4,4.0,66,46,16,16,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java,"KAFKA-9274: Throw TaskCorruptedException instead of TimeoutException when TX commit times out (#10072)

Part of KIP-572: follow up work to PR #9800. It's not save to retry a TX commit after a timeout, because it's unclear if the commit was successful or not, and thus on retry we might get an IllegalStateException. Instead, we will throw a TaskCorruptedException to retry the TX if the commit failed.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",141,3,6,648,4928,1,49,956,420,15,65,3,1901,676,29,945,256,15,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java,"KAFKA-12343: Handle exceptions better in TopicAdmin, including UnsupportedVersionException (#10158)

Refactored the KafkaBasedLog logic to read end offsets into a separate method to make it easier to test. Also changed the TopicAdmin.endOffsets method to throw the original UnsupportedVersionException, LeaderNotAvailableException, and TimeoutException rather than wrapping, to better conform with the consumer method and how the KafkaBasedLog retries those exceptions.

Added new tests to verify various scenarios and errors.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",93,11,5,435,3361,1,39,719,263,51,14,3.5,762,263,54,43,11,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicAdminTest.java,"KAFKA-12343: Handle exceptions better in TopicAdmin, including UnsupportedVersionException (#10158)

Refactored the KafkaBasedLog logic to read end offsets into a separate method to make it easier to test. Also changed the TopicAdmin.endOffsets method to throw the original UnsupportedVersionException, LeaderNotAvailableException, and TimeoutException rather than wrapping, to better conform with the consumer method and how the KafkaBasedLog retries those exceptions.

Added new tests to verify various scenarios and errors.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",84,5,7,743,7375,4,60,856,265,45,19,5,991,270,52,135,38,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java,"MINOR: Correct warning about increasing capacity when insufficient nodes to assign standby tasks (#10151)

We should only recommend to increase the number of KafkaStreams instances, not the number of threads, since a standby task can never be placed on the same instance as an active task regardless of the thread count

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",34,1,1,205,1493,1,10,266,551,22,12,5.5,914,551,76,648,217,54,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InconsistentTopicIdException.java,clients/src/main/java/org/apache/kafka/common/errors/InconsistentTopicIdException.java,"KAFKA-12332; Error partitions from topics with invalid IDs in LISR requests (#10143)

Changes how invalid IDs are handled in LeaderAndIsr requests. The ID check now occurs before leader epoch. If the ID exists and is invalid, the partition is ignored and a new `INCONSISTENT_TOPIC_ID` error is returned in the response.

Reviewers: Jason Gustafson <jason@confluent.io>",1,27,0,7,39,1,1,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java,"KAFKA-12340: Fix potential resource leak in Kafka*BackingStore (#10153)

These Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor, which is not used within AK. However, this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the “default” supplier is always closed when the Kafka*BackingStore is stopped.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",88,17,2,527,4391,2,26,835,546,25,34,4.0,1148,546,34,313,50,9,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java,"KAFKA-12340: Fix potential resource leak in Kafka*BackingStore (#10153)

These Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor, which is not used within AK. However, this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the “default” supplier is always closed when the Kafka*BackingStore is stopped.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",36,17,2,212,1739,2,16,274,393,13,21,3,580,393,28,306,220,15,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java,"KAFKA-12340: Fix potential resource leak in Kafka*BackingStore (#10153)

These Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor, which is not used within AK. However, this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the “default” supplier is always closed when the Kafka*BackingStore is stopped.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",107,17,2,551,4470,2,47,685,461,31,22,2.0,796,461,36,111,19,5,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/BatchBuilderTest.java,raft/src/test/java/org/apache/kafka/raft/internals/BatchBuilderTest.java,"KAFKA-12258; Add support for splitting appending records (#10063)

1. Type `BatchAccumulator`. Add support for appending records into one or more batches.
2. Type `RaftClient`. Rename `scheduleAppend` to `scheduleAtomicAppend`.
3. Type `RaftClient`. Add a new method `scheduleAppend` which appends records to the log using as many batches as necessary.
4. Increase the batch size from 1MB to 8MB.

Reviewers: David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",3,3,6,92,721,2,2,126,129,63,2,3.0,132,129,66,6,6,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java,clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",34,5,5,144,1200,2,12,236,109,15,16,6.5,469,109,29,233,59,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/Protocol.java,clients/src/main/java/org/apache/kafka/common/protocol/Protocol.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",26,1,1,151,1301,1,6,194,264,3,74,3.0,2977,294,40,2783,2067,38,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",34,64,25,209,1373,12,19,274,116,9,30,4.0,689,116,23,415,144,14,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java,clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",18,19,17,136,1337,5,10,168,101,8,22,3.5,327,101,15,159,23,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/NioEchoServer.java,clients/src/test/java/org/apache/kafka/common/network/NioEchoServer.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",58,4,1,315,2410,1,25,384,145,10,37,2,557,145,15,173,34,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SaslChannelBuilderTest.java,clients/src/test/java/org/apache/kafka/common/network/SaslChannelBuilderTest.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",21,10,4,188,1726,3,14,239,97,17,14,2.0,282,99,20,43,9,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ApiVersionsResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/ApiVersionsResponseTest.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",7,20,31,107,1022,5,4,148,68,11,14,5.5,317,68,23,169,65,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticatorTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticatorTest.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",10,6,2,128,1353,1,6,170,119,8,20,3.5,277,119,14,107,34,5,2,1,0,1
core/src/test/scala/integration/kafka/admin/BrokerApiVersionsCommandTest.scala,core/src/test/scala/integration/kafka/admin/BrokerApiVersionsCommandTest.scala,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",4,1,1,51,409,1,1,78,61,5,15,1,105,61,7,27,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/ApiVersionManagerTest.scala,core/src/test/scala/unit/kafka/server/ApiVersionManagerTest.scala,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",4,115,0,84,596,4,4,115,115,115,1,1,115,115,115,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/ApiVersionsRequestTest.scala,core/src/test/scala/unit/kafka/server/ApiVersionsRequestTest.scala,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",7,4,4,58,513,4,7,86,50,4,20,3.0,188,50,9,102,28,5,2,1,0,1
generator/src/main/java/org/apache/kafka/message/ApiMessageTypeGenerator.java,generator/src/main/java/org/apache/kafka/message/ApiMessageTypeGenerator.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",52,67,2,366,2427,7,20,410,248,51,8,2.5,428,248,54,18,6,2,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MessageSpec.java,generator/src/main/java/org/apache/kafka/message/MessageSpec.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",23,15,1,94,587,3,13,136,77,19,7,1,141,77,20,5,1,1,2,1,0,1
generator/src/main/java/org/apache/kafka/message/RequestListenerType.java,generator/src/main/java/org/apache/kafka/message/RequestListenerType.java,"KAFKA-12278; Ensure exposed api versions are consistent within listener (#10666)

Previously all APIs were accessible on every listener exposed by the broker, but
with KIP-500, that is no longer true.  We now have more complex requirements for
API accessibility.

For example, the KIP-500 controller exposes some APIs which are not exposed by
brokers, such as BrokerHeartbeatRequest, and does not expose most client APIs,
such as JoinGroupRequest, etc.  Similarly, the KIP-500 broker does not implement
some APIs that the ZK-based broker does, such as LeaderAndIsrRequest and
UpdateFeaturesRequest.

All of this means that we need more sophistication in how we expose APIs and
keep them consistent with the ApiVersions API. Up until now, we have been
working around this using the controllerOnly flag inside ApiKeys, but this is
not rich enough to support all of the cases listed above.  This PR introduces a
new ""listeners"" field to the request schema definitions.  This field is an array
of strings which indicate the listener types in which the API should be exposed.
We currently support ""zkBroker"", ""broker"", and ""controller"".  (""broker""
indicates the KIP-500 broker, whereas zkBroker indicates the old broker).

This PR also creates ApiVersionManager to encapsulate the creation of the
ApiVersionsResponse based on the listener type.  Additionally, it modifies
SocketServer to check the listener type of received requests before forwarding
them to the request handler.

Finally, this PR also fixes a bug in the handling of the ApiVersionsResponse
prior to authentication. Previously a static response was sent, which means that
changes to features would not get reflected. This also meant that the logic to
ensure that only the intersection of version ranges supported by the controller
would get exposed did not work. I think this is important because some clients
rely on the initial pre-authenticated ApiVersions response rather than doing a
second round after authentication as the Java client does.

One final cleanup note: I have removed the expectation that envelope requests
are only allowed on ""privileged"" listeners.  This made sense initially because
we expected to use forwarding before the KIP-500 controller was available. That
is not the case anymore and we expect the Envelope API to only be exposed on the
controller listener. I have nevertheless preserved the existing workarounds to
allow verification of the forwarding behavior in integration testing.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",0,30,0,10,46,0,0,30,30,30,1,1,30,30,30,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/processor/ProcessorSupplier.java,"TRIVIAL: fix JavaDocs formatting (#10134)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Bill Bejeck <bill@confluent.io>",0,1,1,6,63,0,0,49,23,5,10,1.5,64,23,6,15,4,2,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java,"KAFKA-12303: Fix handling of null values by Flatten SMT (#10073)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Greg Harris <gregh@confluent.io>",63,1,1,218,1696,1,17,290,281,48,6,1.0,306,281,51,16,11,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java,"KAFKA-12303: Fix handling of null values by Flatten SMT (#10073)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Greg Harris <gregh@confluent.io>",16,44,0,280,2900,2,16,373,257,37,10,2.0,421,257,42,48,27,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ApiVersions.java,clients/src/main/java/org/apache/kafka/clients/ApiVersions.java,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",5,7,6,33,278,1,5,67,66,22,3,1,74,66,25,7,6,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/ApiVersionsTest.java,clients/src/test/java/org/apache/kafka/clients/ApiVersionsTest.java,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",2,17,0,31,293,1,2,58,46,10,6,1.5,67,46,11,9,5,2,2,1,0,1
core/src/main/scala/kafka/cluster/Broker.scala,core/src/main/scala/kafka/cluster/Broker.scala,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",8,1,1,57,557,0,8,94,70,2,42,2.0,483,94,12,389,126,9,2,1,0,1
core/src/main/scala/kafka/server/EnvelopeUtils.scala,core/src/main/scala/kafka/server/EnvelopeUtils.scala,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",16,137,0,101,494,5,5,137,137,137,1,1,137,137,137,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/NetworkChannel.java,raft/src/main/java/org/apache/kafka/raft/NetworkChannel.java,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",1,5,0,8,56,0,1,46,63,9,5,1,73,63,15,27,17,5,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockNetworkChannel.java,raft/src/test/java/org/apache/kafka/raft/MockNetworkChannel.java,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",14,5,0,64,478,1,9,95,123,16,6,5.0,219,123,36,124,74,21,2,1,0,1
tests/kafkatest/services/kafka/config_property.py,tests/kafkatest/services/kafka/config_property.py,"MINOR: Add KIP-500 BrokerServer and ControllerServer (#10113)

This PR adds the KIP-500 BrokerServer and ControllerServer classes and 
makes some related changes to get them working.  Note that the ControllerServer 
does not instantiate a QuorumController object yet, since that will be added in
PR #10070.

* Add BrokerServer and ControllerServer

* Change ApiVersions#computeMaxUsableProduceMagic so that it can handle
endpoints which do not support PRODUCE (such as KIP-500 controller nodes)

* KafkaAdminClientTest: fix some lingering references to decommissionBroker
that should be references to unregisterBroker.

* Make some changes to allow SocketServer to be used by ControllerServer as
we as by the broker.

* We now return a random active Broker ID as the Controller ID in
MetadataResponse for the Raft-based case as per KIP-590.

* Add the RaftControllerNodeProvider

* Add EnvelopeUtils

* Add MetaLogRaftShim

* In ducktape, in config_property.py: use a KIP-500 compatible cluster ID.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <mumrah@gmail.com>",0,1,1,172,121,0,0,198,177,12,16,1.0,205,177,13,7,2,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeClientQuotasOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeClientQuotasOptions.java,"MINOR: Import classes that is used in docs to fix warnings. (#10136)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,6,51,0,0,30,29,15,2,1.0,30,29,15,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeClientQuotasResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeClientQuotasResult.java,"MINOR: Import classes that is used in docs to fix warnings. (#10136)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,1,0,16,145,0,2,53,52,26,2,1.0,53,52,26,0,0,0,2,1,0,1
core/src/main/scala/kafka/tools/MirrorMaker.scala,core/src/main/scala/kafka/tools/MirrorMaker.scala,"MINOR: Fix typo (thread -> threads) in MirrorMaker (#10130)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",75,1,1,435,2984,0,26,747,270,8,96,3.0,2624,362,27,1877,463,20,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocol.java,"MINOR: use 'mapKey' to avoid unnecessary grouped data (#10082)

1. add 'mapKey=true' to DescribeLogDirsRequest
2. rename PartitionIndex to Partitions for DescribeLogDirsRequest
3. add 'mapKey=true' to ElectLeadersRequest
4. rename PartitionId to Partitions for ElectLeadersRequest
5. add 'mapKey=true' to ConsumerProtocolAssignment

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>",28,16,16,127,1015,2,11,176,184,9,20,4.0,624,215,31,448,240,22,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ElectLeadersRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ElectLeadersRequest.java,"MINOR: use 'mapKey' to avoid unnecessary grouped data (#10082)

1. add 'mapKey=true' to DescribeLogDirsRequest
2. rename PartitionIndex to Partitions for DescribeLogDirsRequest
3. add 'mapKey=true' to ElectLeadersRequest
4. rename PartitionId to Partitions for ElectLeadersRequest
5. add 'mapKey=true' to ConsumerProtocolAssignment

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>",15,9,6,91,713,2,8,128,134,32,4,2.5,146,134,36,18,12,4,2,1,0,1
core/src/main/scala/kafka/api/package.scala,core/src/main/scala/kafka/api/package.scala,"MINOR: use 'mapKey' to avoid unnecessary grouped data (#10082)

1. add 'mapKey=true' to DescribeLogDirsRequest
2. rename PartitionIndex to Partitions for DescribeLogDirsRequest
3. add 'mapKey=true' to ElectLeadersRequest
4. rename PartitionId to Partitions for ElectLeadersRequest
5. add 'mapKey=true' to ConsumerProtocolAssignment

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>",0,1,1,27,159,0,0,46,47,12,4,1.0,51,47,13,5,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/message/RecordsSerdeTest.java,clients/src/test/java/org/apache/kafka/common/message/RecordsSerdeTest.java,"MINOR: remove duplicate code of serializing auto-generated data (#10128)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,12,53,395,2,6,83,130,17,5,3,143,130,29,60,25,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java,clients/src/test/java/org/apache/kafka/common/message/SimpleExampleMessageTest.java,"MINOR: remove duplicate code of serializing auto-generated data (#10128)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",23,5,20,266,2235,5,23,360,104,21,17,4,503,104,30,143,72,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockInputStream.java,clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockInputStream.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",42,6,5,193,1315,0,12,291,234,26,11,4,456,234,41,165,89,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockOutputStream.java,clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockOutputStream.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",51,1,1,270,1693,0,32,423,392,38,11,3,496,392,45,73,35,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/compress/SnappyFactory.java,clients/src/main/java/org/apache/kafka/common/compress/SnappyFactory.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,50,0,26,170,3,3,50,50,50,1,1,50,50,50,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/CompressionType.java,clients/src/main/java/org/apache/kafka/common/record/CompressionType.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",22,16,66,126,799,3,8,193,145,9,22,3.0,391,146,18,198,66,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java,clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",112,1,0,527,3370,0,68,724,435,21,34,2.0,927,435,27,203,49,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java,clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",40,1,0,186,1217,0,21,259,166,18,14,3.5,494,199,35,235,141,17,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/MutableRecordBatch.java,clients/src/main/java/org/apache/kafka/common/record/MutableRecordBatch.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,11,98,0,0,68,45,11,6,1.0,71,45,12,3,2,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordBatch.java,clients/src/main/java/org/apache/kafka/common/record/RecordBatch.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,39,233,0,0,241,203,24,10,1.0,256,203,26,15,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/BufferSupplier.java,clients/src/main/java/org/apache/kafka/common/utils/BufferSupplier.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,1,1,72,413,0,8,128,96,43,3,1,129,96,43,1,1,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/compress/KafkaLZ4Test.java,clients/src/test/java/org/apache/kafka/common/compress/KafkaLZ4Test.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,3,2,281,2316,0,17,375,205,47,8,7.0,566,242,71,191,108,24,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/BufferSupplierTest.java,clients/src/test/java/org/apache/kafka/common/record/BufferSupplierTest.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,1,0,22,195,0,1,47,46,16,3,1,50,46,17,3,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/CompressionTypeTest.java,clients/src/test/java/org/apache/kafka/common/record/CompressionTypeTest.java,"KAFKA-12327: Remove MethodHandle usage in CompressionType (#10123)

We don't really need it and it causes problems in older Android versions
and GraalVM native image usage (there are workarounds for the latter).

Move the logic to separate classes that are only invoked when the
relevant compression library is actually used. Place such classes
in their own package and enforce via checkstyle that only these
classes refer to compression library packages.

To avoid cyclic dependencies, moved `BufferSupplier` to the `utils`
package.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,3,0,33,294,0,2,59,55,7,8,2.0,78,55,10,19,7,2,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMaker.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMaker.java,"KAFKA-12326: Corrected regresion in MirrorMaker 2 executable introduced with KAFKA-10021 (#10122)

Fixes the recent change to the `MirrorMaker` class (used only in the MirrorMaker 2 executable) that uses a `SharedTopicAdmin` client as part of Connect, so that the correct properties into the `SharedTopicAdmin`.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>",28,1,1,228,1805,1,13,318,309,40,8,1.0,330,309,41,12,4,2,2,1,0,1
generator/src/main/java/org/apache/kafka/message/FieldSpec.java,generator/src/main/java/org/apache/kafka/message/FieldSpec.java,"KAFKA-12321 the comparison function for uuid type should be 'equals' rather than '==' (#10098)

Reviewers: Ismael Juma <ismael@juma.me.uk>",139,1,1,517,3312,1,32,636,340,40,16,2.0,658,340,41,22,7,1,2,1,0,1
tests/kafkatest/services/kafka/config.py,tests/kafkatest/services/kafka/config.py,"MINOR: Support Raft-based metadata quorums in system tests (#10093)

We need to be able to run system tests with Raft-based metadata quorums -- both
co-located brokers and controllers as well as remote controllers -- in addition to the
ZooKepeer-based mode we run today. This PR adds this capability to KafkaService in a
backwards-compatible manner as follows.

If no changes are made to existing system tests then they function as they always do --
they instantiate ZooKeeper, and Kafka will use ZooKeeper. On the other hand, if we want
to use a Raft-based metadata quorum we can do so by introducing a metadata_quorum
argument to the test method and using @matrix to set it to the quorums we want to use for
the various runs of the test. We then also have to skip creating a ZooKeeperService when
the quorum is Raft-based.

This PR does not update any tests -- those will come later after all the KIP-500 code is
merged.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",6,0,3,21,123,0,2,46,53,6,8,1.0,60,53,8,14,5,2,2,1,0,1
metadata/src/main/java/org/apache/kafka/metadata/BrokerRegistration.java,metadata/src/main/java/org/apache/kafka/metadata/BrokerRegistration.java,"MINOR: add the MetaLogListener, LocalLogManager, and Controller interface. (#10106)

Add MetaLogListener, LocalLogManager, and related classes. These
classes are used by the KIP-500 controller and broker to interface with the
Raft log.

Also add the Controller interface. The implementation will be added in a separate PR.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>",21,153,0,117,800,13,13,153,153,153,1,1,153,153,153,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/BrokerRegistrationReply.java,metadata/src/main/java/org/apache/kafka/metadata/BrokerRegistrationReply.java,"MINOR: add the MetaLogListener, LocalLogManager, and Controller interface. (#10106)

Add MetaLogListener, LocalLogManager, and related classes. These
classes are used by the KIP-500 controller and broker to interface with the
Raft log.

Also add the Controller interface. The implementation will be added in a separate PR.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>",6,50,0,25,121,5,5,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/FeatureMap.java,metadata/src/main/java/org/apache/kafka/metadata/FeatureMap.java,"MINOR: add the MetaLogListener, LocalLogManager, and Controller interface. (#10106)

Add MetaLogListener, LocalLogManager, and related classes. These
classes are used by the KIP-500 controller and broker to interface with the
Raft log.

Also add the Controller interface. The implementation will be added in a separate PR.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>",7,67,0,38,271,6,6,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/FeatureMapAndEpoch.java,metadata/src/main/java/org/apache/kafka/metadata/FeatureMapAndEpoch.java,"MINOR: add the MetaLogListener, LocalLogManager, and Controller interface. (#10106)

Add MetaLogListener, LocalLogManager, and related classes. These
classes are used by the KIP-500 controller and broker to interface with the
Raft log.

Also add the Controller interface. The implementation will be added in a separate PR.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>",8,64,0,35,206,6,6,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/metadata/BrokerRegistrationTest.java,metadata/src/test/java/org/apache/kafka/metadata/BrokerRegistrationTest.java,"MINOR: add the MetaLogListener, LocalLogManager, and Controller interface. (#10106)

Add MetaLogListener, LocalLogManager, and related classes. These
classes are used by the KIP-500 controller and broker to interface with the
Raft log.

Also add the Controller interface. The implementation will be added in a separate PR.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, David Arthur <mumrah@gmail.com>",3,78,0,54,592,3,3,78,78,78,1,1,78,78,78,0,0,0,0,0,0,0
core/src/main/scala/kafka/tools/GetOffsetShell.scala,core/src/main/scala/kafka/tools/GetOffsetShell.scala,"KAFKA-5235: GetOffsetShell: Support for multiple topics and consumer configuration override (KIP-635) (#9430)

This patch implements KIP-635 which mainly adds support for querying offsets of multiple topics/partitions.

Reviewers: David Jacot <djacot@confluent.io>",39,154,76,169,1409,10,9,232,78,11,21,1,411,154,20,179,76,9,2,1,0,1
core/src/test/scala/kafka/tools/GetOffsetShellParsingTest.scala,core/src/test/scala/kafka/tools/GetOffsetShellParsingTest.scala,"KAFKA-5235: GetOffsetShell: Support for multiple topics and consumer configuration override (KIP-635) (#9430)

This patch implements KIP-635 which mainly adds support for querying offsets of multiple topics/partitions.

Reviewers: David Jacot <djacot@confluent.io>",18,207,0,163,1678,18,18,207,207,207,1,1,207,207,207,0,0,0,0,0,0,0
core/src/test/scala/kafka/tools/GetOffsetShellTest.scala,core/src/test/scala/kafka/tools/GetOffsetShellTest.scala,"KAFKA-5235: GetOffsetShell: Support for multiple topics and consumer configuration override (KIP-635) (#9430)

This patch implements KIP-635 which mainly adds support for querying offsets of multiple topics/partitions.

Reviewers: David Jacot <djacot@confluent.io>",28,207,0,157,1270,22,22,207,207,207,1,1,207,207,207,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/DelayedDeleteRecords.scala,core/src/main/scala/kafka/server/DelayedDeleteRecords.scala,"MINOR: Add RaftReplicaManager (#10069)

This adds the logic to apply partition metadata when consuming from the Raft-based
metadata log.

RaftReplicaManager extends ReplicaManager for now to minimize changes to existing
code for the 2.8 release. We will likely adjust this hierarchy at a later time (e.g. introducing
a trait and adding a helper to refactor common code). For now, we expose the necessary
fields and methods in ReplicaManager by changing their scope from private to protected,
and we refactor out a couple of pieces of logic that are shared between the two
implementation (stopping replicas and adding log dir fetchers).

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",16,1,1,79,545,1,4,136,129,10,14,1.0,178,129,13,42,12,3,2,1,0,1
core/src/test/scala/unit/kafka/server/metadata/ClientQuotaMetadataManagerTest.scala,core/src/test/scala/unit/kafka/server/metadata/ClientQuotaMetadataManagerTest.scala,"MINOR: Add ClientQuotaMetadataManager for processing QuotaRecord  (#10101)

This PR brings in the new broker metadata processor for handling QuotaRecord-s coming from the metadata log. Also included is a new cache class to allow for fast lookups of quotas on the broker for handling DescribeClientQuotaRequest.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",49,452,0,369,3272,23,23,452,452,452,1,1,452,452,452,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerOptions.java,"MINOR: Rename DecommissionBrokers to UnregisterBrokers (#10084)

Rename DecommissionBrokers to UnregisterBrokers. Fix an incorrect JavaDoc comment
for the Admin API. Make sure that UNREGISTER_BROKER is marked as forwardable and
not as a controller-only API (since it can received by brokers).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",0,2,2,5,38,0,0,29,29,14,2,1.5,31,29,16,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerResult.java,clients/src/main/java/org/apache/kafka/clients/admin/UnregisterBrokerResult.java,"MINOR: Rename DecommissionBrokers to UnregisterBrokers (#10084)

Rename DecommissionBrokers to UnregisterBrokers. Fix an incorrect JavaDoc comment
for the Admin API. Make sure that UNREGISTER_BROKER is marked as forwardable and
not as a controller-only API (since it can received by brokers).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",2,3,3,11,65,2,2,40,40,20,2,2.0,43,40,22,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/UnregisterBrokerRequest.java,clients/src/main/java/org/apache/kafka/common/requests/UnregisterBrokerRequest.java,"MINOR: Rename DecommissionBrokers to UnregisterBrokers (#10084)

Rename DecommissionBrokers to UnregisterBrokers. Fix an incorrect JavaDoc comment
for the Admin API. Make sure that UNREGISTER_BROKER is marked as forwardable and
not as a controller-only API (since it can received by brokers).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,17,17,40,272,12,6,67,67,34,2,6.0,84,67,42,17,17,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/UnregisterBrokerResponse.java,clients/src/main/java/org/apache/kafka/common/requests/UnregisterBrokerResponse.java,"MINOR: Rename DecommissionBrokers to UnregisterBrokers (#10084)

Rename DecommissionBrokers to UnregisterBrokers. Fix an incorrect JavaDoc comment
for the Admin API. Make sure that UNREGISTER_BROKER is marked as forwardable and
not as a controller-only API (since it can received by brokers).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",7,8,8,38,251,6,6,64,64,32,2,3.0,72,64,36,8,8,4,2,1,0,1
core/src/test/scala/unit/kafka/tools/ClusterToolTest.scala,core/src/test/scala/unit/kafka/tools/ClusterToolTest.scala,"MINOR: Rename DecommissionBrokers to UnregisterBrokers (#10084)

Rename DecommissionBrokers to UnregisterBrokers. Fix an incorrect JavaDoc comment
for the Admin API. Make sure that UNREGISTER_BROKER is marked as forwardable and
not as a controller-only API (since it can received by brokers).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",4,6,6,52,321,4,4,74,74,37,2,3.5,80,74,40,6,6,3,2,1,0,1
core/src/test/scala/unit/kafka/server/MetadataRequestTest.scala,core/src/test/scala/unit/kafka/server/MetadataRequestTest.scala,"MINOR: KafkaBroker.brokerState should be volatile instead of AtomicReference (#10080)

We don't need or use the additional functionality provided by
AtomicReference.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Arthur <mumrah@gmail.com>",22,2,2,278,2706,4,18,377,168,11,35,3,593,168,17,216,45,6,2,1,0,1
core/src/test/scala/unit/kafka/server/ServerStartupTest.scala,core/src/test/scala/unit/kafka/server/ServerStartupTest.scala,"MINOR: KafkaBroker.brokerState should be volatile instead of AtomicReference (#10080)

We don't need or use the additional functionality provided by
AtomicReference.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Arthur <mumrah@gmail.com>",7,1,1,69,557,1,6,108,54,4,30,3.0,235,54,8,127,22,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java,connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java,"KAFKA-10021: Changed Kafka backing stores to use shared admin client to get end offsets and create topics (#9780)

The existing `Kafka*BackingStore` classes used by Connect all use `KafkaBasedLog`, which needs to frequently get the end offsets for the internal topic to know whether they are caught up. `KafkaBasedLog` uses its consumer to get the end offsets and to consume the records from the topic.

However, the Connect internal topics are often written very infrequently. This means that when the `KafkaBasedLog` used in the `Kafka*BackingStore` classes is already caught up and its last consumer poll is waiting for new records to appear, the call to the consumer to fetch end offsets will block until the consumer returns after a new record is written (unlikely) or the consumer’s `fetch.max.wait.ms` setting (defaults to 500ms) ends and the consumer returns no more records. IOW, the call to `KafkaBasedLog.readToEnd()` may block for some period of time even though it’s already caught up to the end.

Instead, we want the `KafkaBasedLog.readToEnd()` to always return quickly when the log is already caught up. The best way to do this is to have the `KafkaBackingStore` use the admin client (rather than the consumer) to fetch end offsets for the internal topic. The consumer and the admin API both use the same `ListOffset` broker API, so the functionality is ultimately the same but we don't have to block for any ongoing consumer activity.

Each Connect distributed runtime includes three instances of the `Kafka*BackingStore` classes, which means we have three instances of `KafkaBasedLog`. We don't want three instances of the admin client, and should have all three instances of the `KafkaBasedLog` share a single admin client instance. In fact, each `Kafka*BackingStore` instance currently creates, uses and closes an admin client instance when it checks and initializes that store's internal topic. If we change `Kafka*BackingStores` to share one admin client instance, we can change that initialization logic to also reuse the supplied admin client instance.

The final challenge is that `KafkaBasedLog` has been used by projects outside of Apache Kafka. While `KafkaBasedLog` is definitely not in the public API for Connect, we can make these changes in ways that are backward compatible: create new constructors and deprecate the old constructors. Connect can be changed to only use the new constructors, and this will give time for any downstream users to make changes.

These changes are implemented as follows:
1. Add a `KafkaBasedLog` constructor to accept in its parameters a supplier from which it can get an admin instance, and deprecate the old constructor. We need a supplier rather than just passing an instance because `KafkaBasedLog` is instantiated before Connect starts up, so we need to create the admin instance only when needed. At the same time, we'll change the existing init function parameter from a no-arg function to accept an admin instance as an argument, allowing that init function to reuse the shared admin instance used by the `KafkaBasedLog`. Note: if no admin supplier is provided (in deprecated constructor that is no longer used in AK), the consumer is still used to get latest offsets.
2. Add to the `Kafka*BackingStore` classes a new constructor with the same parameters but with an admin supplier, and deprecate the old constructor. When the classes instantiate its `KafkaBasedLog` instance, it would pass the admin supplier and pass an init function that takes an admin instance.
3. Create a new `SharedTopicAdmin` that lazily creates the `TopicAdmin` (and underlying Admin client) when required, and closes the admin objects when the `SharedTopicAdmin` is closed.
4. Modify the existing `TopicAdmin` (used only in Connect) to encapsulate the logic of fetching end offsets using the admin client, simplifying the logic in `KafkaBasedLog` mentioned in #1 above. Doing this also makes it easier to test that logic.
5. Change `ConnectDistributed` to create a `SharedTopicAdmin` instance (that is `AutoCloseable`) before creating the `Kafka*BackingStore` instances, passing the `SharedTopicAdmin` (which is an admin supplier) to all three `Kafka*BackingStore objects`, and finally always closing the `SharedTopicAdmin` upon termination. (Shutdown of the worker occurs outside of the `ConnectDistributed` code, so modify `DistributedHerder` to take in its constructor additional `AutoCloseable` objects that should be closed when the herder is closed, and then modify `ConnectDistributed` to pass the `SharedTopicAdmin` as one of those `AutoCloseable` instances.)
6. Change `MirrorMaker` similarly to `ConnectDistributed`.
7. Change existing unit tests to no longer use deprecated constructors.
8. Add unit tests for new functionality.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>",7,14,4,96,889,1,2,150,87,6,27,3,326,87,12,176,44,7,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/SharedTopicAdmin.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/SharedTopicAdmin.java,"KAFKA-10021: Changed Kafka backing stores to use shared admin client to get end offsets and create topics (#9780)

The existing `Kafka*BackingStore` classes used by Connect all use `KafkaBasedLog`, which needs to frequently get the end offsets for the internal topic to know whether they are caught up. `KafkaBasedLog` uses its consumer to get the end offsets and to consume the records from the topic.

However, the Connect internal topics are often written very infrequently. This means that when the `KafkaBasedLog` used in the `Kafka*BackingStore` classes is already caught up and its last consumer poll is waiting for new records to appear, the call to the consumer to fetch end offsets will block until the consumer returns after a new record is written (unlikely) or the consumer’s `fetch.max.wait.ms` setting (defaults to 500ms) ends and the consumer returns no more records. IOW, the call to `KafkaBasedLog.readToEnd()` may block for some period of time even though it’s already caught up to the end.

Instead, we want the `KafkaBasedLog.readToEnd()` to always return quickly when the log is already caught up. The best way to do this is to have the `KafkaBackingStore` use the admin client (rather than the consumer) to fetch end offsets for the internal topic. The consumer and the admin API both use the same `ListOffset` broker API, so the functionality is ultimately the same but we don't have to block for any ongoing consumer activity.

Each Connect distributed runtime includes three instances of the `Kafka*BackingStore` classes, which means we have three instances of `KafkaBasedLog`. We don't want three instances of the admin client, and should have all three instances of the `KafkaBasedLog` share a single admin client instance. In fact, each `Kafka*BackingStore` instance currently creates, uses and closes an admin client instance when it checks and initializes that store's internal topic. If we change `Kafka*BackingStores` to share one admin client instance, we can change that initialization logic to also reuse the supplied admin client instance.

The final challenge is that `KafkaBasedLog` has been used by projects outside of Apache Kafka. While `KafkaBasedLog` is definitely not in the public API for Connect, we can make these changes in ways that are backward compatible: create new constructors and deprecate the old constructors. Connect can be changed to only use the new constructors, and this will give time for any downstream users to make changes.

These changes are implemented as follows:
1. Add a `KafkaBasedLog` constructor to accept in its parameters a supplier from which it can get an admin instance, and deprecate the old constructor. We need a supplier rather than just passing an instance because `KafkaBasedLog` is instantiated before Connect starts up, so we need to create the admin instance only when needed. At the same time, we'll change the existing init function parameter from a no-arg function to accept an admin instance as an argument, allowing that init function to reuse the shared admin instance used by the `KafkaBasedLog`. Note: if no admin supplier is provided (in deprecated constructor that is no longer used in AK), the consumer is still used to get latest offsets.
2. Add to the `Kafka*BackingStore` classes a new constructor with the same parameters but with an admin supplier, and deprecate the old constructor. When the classes instantiate its `KafkaBasedLog` instance, it would pass the admin supplier and pass an init function that takes an admin instance.
3. Create a new `SharedTopicAdmin` that lazily creates the `TopicAdmin` (and underlying Admin client) when required, and closes the admin objects when the `SharedTopicAdmin` is closed.
4. Modify the existing `TopicAdmin` (used only in Connect) to encapsulate the logic of fetching end offsets using the admin client, simplifying the logic in `KafkaBasedLog` mentioned in #1 above. Doing this also makes it easier to test that logic.
5. Change `ConnectDistributed` to create a `SharedTopicAdmin` instance (that is `AutoCloseable`) before creating the `Kafka*BackingStore` instances, passing the `SharedTopicAdmin` (which is an admin supplier) to all three `Kafka*BackingStore objects`, and finally always closing the `SharedTopicAdmin` upon termination. (Shutdown of the worker occurs outside of the `ConnectDistributed` code, so modify `DistributedHerder` to take in its constructor additional `AutoCloseable` objects that should be closed when the herder is closed, and then modify `ConnectDistributed` to pass the `SharedTopicAdmin` as one of those `AutoCloseable` instances.)
6. Change `MirrorMaker` similarly to `ConnectDistributed`.
7. Change existing unit tests to no longer use deprecated constructors.
8. Add unit tests for new functionality.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>",13,145,0,61,444,9,9,145,145,145,1,1,145,145,145,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/util/SharedTopicAdminTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/SharedTopicAdminTest.java,"KAFKA-10021: Changed Kafka backing stores to use shared admin client to get end offsets and create topics (#9780)

The existing `Kafka*BackingStore` classes used by Connect all use `KafkaBasedLog`, which needs to frequently get the end offsets for the internal topic to know whether they are caught up. `KafkaBasedLog` uses its consumer to get the end offsets and to consume the records from the topic.

However, the Connect internal topics are often written very infrequently. This means that when the `KafkaBasedLog` used in the `Kafka*BackingStore` classes is already caught up and its last consumer poll is waiting for new records to appear, the call to the consumer to fetch end offsets will block until the consumer returns after a new record is written (unlikely) or the consumer’s `fetch.max.wait.ms` setting (defaults to 500ms) ends and the consumer returns no more records. IOW, the call to `KafkaBasedLog.readToEnd()` may block for some period of time even though it’s already caught up to the end.

Instead, we want the `KafkaBasedLog.readToEnd()` to always return quickly when the log is already caught up. The best way to do this is to have the `KafkaBackingStore` use the admin client (rather than the consumer) to fetch end offsets for the internal topic. The consumer and the admin API both use the same `ListOffset` broker API, so the functionality is ultimately the same but we don't have to block for any ongoing consumer activity.

Each Connect distributed runtime includes three instances of the `Kafka*BackingStore` classes, which means we have three instances of `KafkaBasedLog`. We don't want three instances of the admin client, and should have all three instances of the `KafkaBasedLog` share a single admin client instance. In fact, each `Kafka*BackingStore` instance currently creates, uses and closes an admin client instance when it checks and initializes that store's internal topic. If we change `Kafka*BackingStores` to share one admin client instance, we can change that initialization logic to also reuse the supplied admin client instance.

The final challenge is that `KafkaBasedLog` has been used by projects outside of Apache Kafka. While `KafkaBasedLog` is definitely not in the public API for Connect, we can make these changes in ways that are backward compatible: create new constructors and deprecate the old constructors. Connect can be changed to only use the new constructors, and this will give time for any downstream users to make changes.

These changes are implemented as follows:
1. Add a `KafkaBasedLog` constructor to accept in its parameters a supplier from which it can get an admin instance, and deprecate the old constructor. We need a supplier rather than just passing an instance because `KafkaBasedLog` is instantiated before Connect starts up, so we need to create the admin instance only when needed. At the same time, we'll change the existing init function parameter from a no-arg function to accept an admin instance as an argument, allowing that init function to reuse the shared admin instance used by the `KafkaBasedLog`. Note: if no admin supplier is provided (in deprecated constructor that is no longer used in AK), the consumer is still used to get latest offsets.
2. Add to the `Kafka*BackingStore` classes a new constructor with the same parameters but with an admin supplier, and deprecate the old constructor. When the classes instantiate its `KafkaBasedLog` instance, it would pass the admin supplier and pass an init function that takes an admin instance.
3. Create a new `SharedTopicAdmin` that lazily creates the `TopicAdmin` (and underlying Admin client) when required, and closes the admin objects when the `SharedTopicAdmin` is closed.
4. Modify the existing `TopicAdmin` (used only in Connect) to encapsulate the logic of fetching end offsets using the admin client, simplifying the logic in `KafkaBasedLog` mentioned in #1 above. Doing this also makes it easier to test that logic.
5. Change `ConnectDistributed` to create a `SharedTopicAdmin` instance (that is `AutoCloseable`) before creating the `Kafka*BackingStore` instances, passing the `SharedTopicAdmin` (which is an admin supplier) to all three `Kafka*BackingStore objects`, and finally always closing the `SharedTopicAdmin` upon termination. (Shutdown of the worker occurs outside of the `ConnectDistributed` code, so modify `DistributedHerder` to take in its constructor additional `AutoCloseable` objects that should be closed when the herder is closed, and then modify `ConnectDistributed` to pass the `SharedTopicAdmin` as one of those `AutoCloseable` instances.)
6. Change `MirrorMaker` similarly to `ConnectDistributed`.
7. Change existing unit tests to no longer use deprecated constructors.
8. Add unit tests for new functionality.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>",9,112,0,71,520,8,8,112,112,112,1,1,112,112,112,0,0,0,0,0,0,0
core/src/test/java/kafka/test/ClusterGenerator.java,core/src/test/java/kafka/test/ClusterGenerator.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,25,0,5,26,0,0,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/test/java/kafka/test/annotation/AutoStart.java,core/src/test/java/kafka/test/annotation/AutoStart.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,24,0,6,17,0,0,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
core/src/test/java/kafka/test/annotation/ClusterConfigProperty.java,core/src/test/java/kafka/test/annotation/ClusterConfigProperty.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,32,0,13,86,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
core/src/test/java/kafka/test/annotation/ClusterTemplate.java,core/src/test/java/kafka/test/annotation/ClusterTemplate.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,55,0,16,110,0,0,55,55,55,1,1,55,55,55,0,0,0,0,0,0,0
core/src/test/java/kafka/test/annotation/ClusterTestDefaults.java,core/src/test/java/kafka/test/annotation/ClusterTestDefaults.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,42,0,16,117,0,0,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
core/src/test/java/kafka/test/annotation/ClusterTests.java,core/src/test/java/kafka/test/annotation/ClusterTests.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",0,35,0,14,98,0,0,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
core/src/test/java/kafka/test/junit/ClusterInstanceParameterResolver.java,core/src/test/java/kafka/test/junit/ClusterInstanceParameterResolver.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",5,68,0,31,220,3,3,68,68,68,1,1,68,68,68,0,0,0,0,0,0,0
core/src/test/java/kafka/test/junit/GenericParameterResolver.java,core/src/test/java/kafka/test/junit/GenericParameterResolver.java,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",3,51,0,20,141,3,3,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
core/src/test/scala/integration/kafka/api/SaslSetup.scala,core/src/test/scala/integration/kafka/api/SaslSetup.scala,"JUnit extensions for integration tests (#9986)

Adds JUnit 5 extension for running the same test with different types of clusters. 
See core/src/test/java/kafka/test/junit/README.md for details",33,1,1,160,1327,2,15,215,87,9,24,3.0,360,87,15,145,32,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/UtilsTest.java,"MINOR: add Utils.isBlank to check whitespace character string and empty string (#10012)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",73,8,0,690,6383,1,57,862,138,22,39,3,1071,139,27,209,72,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/StringDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/StringDeserializer.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,1,28,189,0,2,53,50,9,6,1.0,70,50,12,17,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/StringSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/StringSerializer.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,1,28,188,0,2,53,50,9,6,1.0,70,50,12,17,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/UUIDSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/UUIDSerializer.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,1,29,199,0,2,54,58,18,3,1,60,58,20,6,5,2,1,0,1,1
connect/api/src/main/java/org/apache/kafka/connect/storage/StringConverterConfig.java,connect/api/src/main/java/org/apache/kafka/connect/storage/StringConverterConfig.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,2,1,28,221,0,3,61,60,30,2,1.5,62,60,31,1,1,0,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/storage/StringConverterTest.java,connect/api/src/test/java/org/apache/kafka/connect/storage/StringConverterTest.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,19,18,67,585,7,11,102,83,15,7,4,142,83,20,40,18,6,2,1,0,1
connect/json/src/test/java/org/apache/kafka/connect/json/JsonConverterTest.java,connect/json/src/test/java/org/apache/kafka/connect/json/JsonConverterTest.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",85,2,3,757,7686,1,80,931,173,37,25,4,1187,196,47,256,109,10,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",23,1,1,155,1406,1,9,216,154,20,11,3,283,154,26,67,38,6,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestServerTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestServerTest.java,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",25,1,1,387,3699,1,18,488,150,23,21,4,667,150,32,179,80,9,2,1,0,1
core/src/main/scala/kafka/serializer/Decoder.scala,core/src/main/scala/kafka/serializer/Decoder.scala,"MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8 (#10079)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,3,2,30,228,0,5,73,36,8,9,2,112,36,12,39,10,4,2,1,0,1
core/src/main/scala/kafka/tools/TerseFailure.scala,core/src/main/scala/kafka/tools/TerseFailure.scala,"MINOR: Add ClusterTool as specified in KIP-631 (#10047)

Add ClusterTool as specified in KIP-631. It can report the current cluster ID, and also send the new RPC for removing broker registrations.

Reviewers: David Arthur <mumrah@gmail.com>",0,30,0,4,28,0,0,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/metadata/ZkConfigRepository.scala,core/src/main/scala/kafka/server/metadata/ZkConfigRepository.scala,"MINOR: Defer log recovery until LogManager startup (#10039)

Currently log recovery begins as soon as we instantiate `LogManager`, but when using a
Raft-based metadata quorum we won't have configs until after we catch up on the metadata
log.  We therefore defer log recovery until we actually invoke `startup()` on the `LogManager`
instance.  This timing difference has no effect when using ZooKeeper because we
immediately invoke `startup()` on the instantiated instance, but it gives us the necessary
flexibility for accurate log recovery with updated configs when using a Raft-based metadata
quorum.

The `LogCleaner` is currently instantiated during construction just after log recovery
completes, and then it is started in `startup()`.  As an extra precaution, since we are
no longer performing recovery during construction, we both instantiate and start the
log cleaner in `startup()` after log recovery completes.

We also convert `LogManager` to use a `ConfigRepository` to load topic configs
(which can override the default log configs) instead of having a hard-coded
dependency on ZooKeeper.  We retrieve the topic configs when we invoke `startup()`
-- which again is effectively no different from a timing perspective than what we do
today for the ZooKeeper case.

One subtlety is that currently we create the log configs for every topic at this point
-- if a topic has no config overrides then we associate a copy of the default
configuration with the topic inside a map, and we retrieve the log configs for that
topic's partitions from from that map during recovery.  This PR makes a change to
this series of events as follows.  We do not associate a copy of the the default
configuration with a topic in the map if the topic has no configs set when we query
for them.  This saves some memory -- we don't unnecessarily copy the default
config many times -- but it also means we have use the default log configs for
that topic later on when recovery for each of its partitions begins.

The difference is that the default configs are dynamically reconfigurable, and they
could potentially change between the time when we invoke `startup()` and when
log recovery begins (log recovery can begin quite some time after `startup()` is
invoked if shutdown was unclean).  Prior to this patch such a change would not
be used; with this patch they could be if they happen before recovery begins.
This actually is better -- we are performing log recovery with the most recent
known defaults when a topic had no overrides at all. Also, `Partition.createLog`
has logic to handle missed config updates, so the behavior is eventually the same.

The transition of the broker state from `STARTING` to `RECOVERY` currently
happens within the `LogManager`, and it only occurs if the shutdown was
unclean.  We move this transition into the broker as it avoids passing a
reference to the broker state into the `LogManager`.  We also now always
transition the broker into the `RECOVERY` state as dictated by [the KIP-631 broker state machine](https://cwiki.apache.org/confluence/display/KAFKA/KIP-631%3A+The+Quorumbased+Kafka+Controller#KIP631:TheQuorumbasedKafkaController-TheBrokerStateMachine).

Finally, a few clean-ups were included. One worth highlighting is that `Partition`
no longer requires a `ConfigRepository`.

Reviewers: David Arthur <david.arthur@confluent.io>, Ismael Juma <ismael@juma.me.uk>",5,5,8,20,148,1,2,42,45,21,2,1.5,50,45,25,8,8,4,2,1,0,1
core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala,core/src/test/scala/unit/kafka/cluster/AbstractPartitionTest.scala,"MINOR: Defer log recovery until LogManager startup (#10039)

Currently log recovery begins as soon as we instantiate `LogManager`, but when using a
Raft-based metadata quorum we won't have configs until after we catch up on the metadata
log.  We therefore defer log recovery until we actually invoke `startup()` on the `LogManager`
instance.  This timing difference has no effect when using ZooKeeper because we
immediately invoke `startup()` on the instantiated instance, but it gives us the necessary
flexibility for accurate log recovery with updated configs when using a Raft-based metadata
quorum.

The `LogCleaner` is currently instantiated during construction just after log recovery
completes, and then it is started in `startup()`.  As an extra precaution, since we are
no longer performing recovery during construction, we both instantiate and start the
log cleaner in `startup()` after log recovery completes.

We also convert `LogManager` to use a `ConfigRepository` to load topic configs
(which can override the default log configs) instead of having a hard-coded
dependency on ZooKeeper.  We retrieve the topic configs when we invoke `startup()`
-- which again is effectively no different from a timing perspective than what we do
today for the ZooKeeper case.

One subtlety is that currently we create the log configs for every topic at this point
-- if a topic has no config overrides then we associate a copy of the default
configuration with the topic inside a map, and we retrieve the log configs for that
topic's partitions from from that map during recovery.  This PR makes a change to
this series of events as follows.  We do not associate a copy of the the default
configuration with a topic in the map if the topic has no configs set when we query
for them.  This saves some memory -- we don't unnecessarily copy the default
config many times -- but it also means we have use the default log configs for
that topic later on when recovery for each of its partitions begins.

The difference is that the default configs are dynamically reconfigurable, and they
could potentially change between the time when we invoke `startup()` and when
log recovery begins (log recovery can begin quite some time after `startup()` is
invoked if shutdown was unclean).  Prior to this patch such a change would not
be used; with this patch they could be if they happen before recovery begins.
This actually is better -- we are performing log recovery with the most recent
known defaults when a topic had no overrides at all. Also, `Partition.createLog`
has logic to handle missed config updates, so the behavior is eventually the same.

The transition of the broker state from `STARTING` to `RECOVERY` currently
happens within the `LogManager`, and it only occurs if the shutdown was
unclean.  We move this transition into the broker as it avoids passing a
reference to the broker state into the `LogManager`.  We also now always
transition the broker into the `RECOVERY` state as dictated by [the KIP-631 broker state machine](https://cwiki.apache.org/confluence/display/KAFKA/KIP-631%3A+The+Quorumbased+Kafka+Controller#KIP631:TheQuorumbasedKafkaController-TheBrokerStateMachine).

Finally, a few clean-ups were included. One worth highlighting is that `Partition`
no longer requires a `ConfigRepository`.

Reviewers: David Arthur <david.arthur@confluent.io>, Ismael Juma <ismael@juma.me.uk>",5,10,8,78,565,1,3,105,96,13,8,4.0,126,96,16,21,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsRequest.java,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",17,10,0,82,588,2,9,114,231,5,21,3,535,231,25,421,284,20,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java,clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorRequest.java,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",16,2,1,76,472,1,11,111,145,10,11,3,226,145,21,115,89,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/FindCoordinatorRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/FindCoordinatorRequestTest.java,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",1,31,0,11,84,1,1,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
core/src/test/scala/integration/kafka/api/ConsumerTopicCreationTest.scala,core/src/test/scala/integration/kafka/api/ConsumerTopicCreationTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",5,23,5,77,664,2,3,122,107,15,8,1.0,223,107,28,101,56,13,2,1,0,1
core/src/test/scala/integration/kafka/api/MetricsTest.scala,core/src/test/scala/integration/kafka/api/MetricsTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",36,9,7,239,2109,1,19,307,288,13,23,2,414,288,18,107,20,5,2,1,0,1
core/src/test/scala/unit/kafka/server/AbstractCreateTopicsRequestTest.scala,core/src/test/scala/unit/kafka/server/AbstractCreateTopicsRequestTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",30,1,1,149,1182,2,10,191,147,9,22,2.0,311,147,14,120,30,5,2,1,0,1
core/src/test/scala/unit/kafka/server/AbstractMetadataRequestTest.scala,core/src/test/scala/unit/kafka/server/AbstractMetadataRequestTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",6,61,0,36,310,4,4,61,61,61,1,1,61,61,61,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/CreateTopicsRequestWithForwardingTest.scala,core/src/test/scala/unit/kafka/server/CreateTopicsRequestWithForwardingTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",1,0,1,14,111,1,1,37,42,7,5,1,48,42,10,11,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/MetadataRequestWithForwardingTest.scala,core/src/test/scala/unit/kafka/server/MetadataRequestWithForwardingTest.scala,"KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)

Consolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager, when handling FindCoordinator/Metadata request.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>",4,111,0,70,703,4,4,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/ZkAdminManager.scala,core/src/main/scala/kafka/server/ZkAdminManager.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",251,4,4,957,8238,1,44,1145,249,16,71,3,1948,251,27,803,180,11,2,1,0,1
core/src/main/scala/kafka/server/metadata/RaftMetadataCache.scala,core/src/main/scala/kafka/server/metadata/RaftMetadataCache.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",51,390,0,301,2433,25,25,390,390,390,1,1,390,390,390,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/IsrExpirationTest.scala,core/src/test/scala/unit/kafka/server/IsrExpirationTest.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",11,1,1,165,1398,1,7,253,184,4,57,4,704,184,12,451,104,8,2,1,0,1
core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala,core/src/test/scala/unit/kafka/server/MetadataCacheTest.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",16,55,48,456,3549,23,14,550,199,17,32,6.0,926,199,29,376,50,12,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala,core/src/test/scala/unit/kafka/server/ReplicaManagerQuotasTest.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",12,1,1,219,1925,1,10,278,154,7,42,3.0,515,154,12,237,31,6,2,1,0,1
core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala,core/src/test/scala/unit/kafka/server/epoch/OffsetsForLeaderEpochTest.scala,"Refactor the MetadataCache into two implementations (#10049)

Refactor the MetadataCache into two implementations that both implement a common trait.  This will let us
continue to use the existing implementation when using ZK, but use the new implementation when in kip-500 mode.

Reviewers: Colin McCabe <cmccabe@apache.org>, Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@confluent.io>",7,3,3,117,1020,3,7,169,98,8,20,3.5,284,98,14,115,38,6,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala,core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorConcurrencyTest.scala,"MINOR: Remove ZK dependency for coordinator topics' partition counts (#10008)

The group coordinator and the transaction state manager query ZooKeeper 
to retrieve the partition count for the topics they manager. Since ZooKeeper
won't be available when the broker is using a Raft-based metadata quorum,
this PR changes the startup function to provide an accessor function instead.
This will allow the ZK-based broker to continue using ZK, while the kip-500
broker will query the metadata provided by the metadata log.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, David Arthur <david.arthur@confluent.io>

Co-authored-by: Ismael Juma <ismael@juma.me.uk>",40,6,4,338,2711,2,33,408,310,23,18,3.5,494,310,27,86,20,5,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountDemo.java,"MINOR: Word count should account for extra whitespaces between words (#10044)

Reviewers: Matthias J. Sax <matthias@confluent.io>",7,1,1,66,595,1,3,112,103,4,30,2.0,257,103,9,145,26,5,2,1,0,1
streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountDemoTest.java,streams/examples/src/test/java/org/apache/kafka/streams/examples/wordcount/WordCountDemoTest.java,"MINOR: Word count should account for extra whitespaces between words (#10044)

Reviewers: Matthias J. Sax <matthias@confluent.io>",6,2,2,87,748,1,5,132,112,33,4,2.0,141,112,35,9,5,2,1,0,1,1
connect/api/src/main/java/org/apache/kafka/connect/connector/policy/ConnectorClientConfigRequest.java,connect/api/src/main/java/org/apache/kafka/connect/connector/policy/ConnectorClientConfigRequest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",6,1,1,41,211,0,6,101,101,50,2,1.0,102,101,51,1,1,0,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/connector/ConnectorReconfigurationTest.java,connect/api/src/test/java/org/apache/kafka/connect/connector/ConnectorReconfigurationTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",10,1,1,58,346,1,9,90,76,8,11,2,115,76,10,25,5,2,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/Checkpoint.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/Checkpoint.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",19,1,1,140,1036,0,19,184,184,92,2,1.0,185,184,92,1,1,0,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/Heartbeat.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/Heartbeat.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",15,1,1,105,767,0,15,145,145,72,2,1.0,146,145,73,1,1,0,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationSSLTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",1,2,2,34,448,1,1,66,67,22,3,2,73,67,24,7,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectStandalone.java,connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectStandalone.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",8,1,1,86,819,1,1,130,87,5,25,3,254,87,10,124,43,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/AbstractConnectorClientConfigOverridePolicy.java,connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/AbstractConnectorClientConfigOverridePolicy.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",5,1,1,30,244,1,4,57,57,28,2,1.0,58,57,29,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetricsRegistry.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetricsRegistry.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",13,1,1,355,1976,1,13,412,275,69,6,3.0,441,275,74,29,28,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceTaskOffsetCommitter.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceTaskOffsetCommitter.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",14,1,1,76,554,1,6,121,103,10,12,3.0,228,103,19,107,60,9,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",25,1,1,131,1094,1,8,210,158,35,6,1.5,221,158,37,11,6,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResource.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",48,2,2,347,3083,2,27,430,201,15,29,3,617,201,21,187,40,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",64,1,1,349,2637,1,33,446,257,11,39,3,995,257,26,549,125,14,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",28,1,1,130,948,1,16,178,154,25,7,3,195,154,28,17,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/TransformationIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/TransformationIntegrationTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",15,2,2,202,2018,2,9,327,321,109,3,2,346,321,115,19,17,6,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",46,5,5,712,7063,2,38,882,171,28,31,4,1053,172,34,171,24,6,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectMetricsTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectMetricsTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",14,1,1,125,1052,1,12,170,146,14,12,1.0,275,146,23,105,49,9,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ConnectorConfigTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",44,1,1,398,3609,2,41,487,185,49,10,2.0,522,199,52,35,14,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/MockConnectMetrics.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/MockConnectMetrics.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",26,2,2,95,694,2,18,210,97,19,11,2,237,97,22,27,13,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResourceTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorsResourceTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",51,5,5,755,8412,5,51,995,364,38,26,3.5,1197,364,46,202,41,8,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",37,6,7,599,6315,4,22,768,186,21,37,7,1350,191,36,582,111,16,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/FileOffsetBackingStoreTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/FileOffsetBackingStoreTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",7,1,1,91,758,1,7,128,117,12,11,3,162,117,15,34,12,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/OffsetStorageWriterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/OffsetStorageWriterTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",17,1,1,172,1438,1,11,269,242,22,12,4.0,364,242,30,95,28,8,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/SetSchemaMetadataTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",10,1,1,109,1075,1,9,150,80,19,8,3.5,174,80,22,24,8,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampRouterTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampRouterTest.java,"KAFKA-10834: Remove redundant type casts in Connect (#10053)

Cleanup up to remove redundant type casts in Connect and use the diamond operator when needed 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",2,1,1,25,164,1,2,48,43,10,5,3,62,43,12,14,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java,"KAFKA-10833: Expose task configurations in Connect REST API (KIP-661) (#9726)

This PR adds a new REST endpoint to Connect: GET /{connector}/tasks-config, that returns the configuration of all tasks for the connector.

Details in: 
https://cwiki.apache.org/confluence/display/KAFKA/KIP-661%3A+Expose+task+configurations+in+Connect+REST+API

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Oliver Dineen <dineeno@uk.ibm.com>

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",10,7,0,77,695,0,6,290,72,13,22,3.0,340,82,15,50,10,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Branched.java,streams/src/main/java/org/apache/kafka/streams/kstream/Branched.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",8,148,0,46,502,8,8,148,148,148,1,1,148,148,148,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/Predicate.java,streams/src/main/java/org/apache/kafka/streams/kstream/Predicate.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",0,1,1,5,44,0,0,44,24,4,12,2.0,71,24,6,27,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/BranchedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/BranchedInternal.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",6,49,0,25,188,6,6,49,49,49,1,1,49,49,49,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/BranchedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/BranchedKStreamImpl.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",17,119,0,86,857,9,9,119,119,119,1,1,119,119,119,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSplitTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSplitTest.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",6,143,0,108,1269,5,5,143,143,143,1,1,143,143,143,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/StreamsGraphTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/StreamsGraphTest.java,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",12,28,27,422,3009,1,11,517,186,43,12,4.5,589,186,49,72,27,6,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Branched.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Branched.scala,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",0,67,0,10,206,0,0,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/BranchedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/BranchedKStream.scala,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",6,116,0,23,307,6,6,116,116,116,1,1,116,116,116,0,0,0,0,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/package.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/package.scala,"KAFKA-5488: Add type-safe split() operator (#9107)

Implements KIP-418, that deprecated the `branch()` operator in favor of the newly added and type-safe `split()` operator.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",0,1,0,13,248,0,0,31,27,5,6,1.0,32,27,5,1,1,0,1,0,1,1
core/src/main/scala/kafka/server/metadata/ConfigRepository.scala,core/src/main/scala/kafka/server/metadata/ConfigRepository.scala,"MINOR: Add ConfigRepository, use in Partition and KafkaApis (#10005)

`Partition` objects are able to retrieve topic configs when creating their log, and currently they do so with an implementation of `trait TopicConfigFetcher` that uses ZooKeeper.  ZooKeeper is not available when using a Raft-based metadata log, so we need to abstract the retrieval of configs so it can work either with or without ZooKeeper.  This PR introduces `trait ConfigRepository` with `ZkConfigRepository` and `CachedConfigRepository` implementations.  `Partition` objects now use a provided `ConfigRepository` to retrieve topic configs, and we eliminate `TopicConfigFetcher` as it is no longer needed.

`ReplicaManager` now contains an instance of `ConfigRepository` so it can provide it when creating `Partition` instances.

`KafkaApis` needs to be able to handle describe-config requests; it currently delegates that to `ZkAdminManager`, which of course queries ZooKeeper.  To make this work with or without ZooKeeper we move the logic from `ZkAdminManager` into a new `ConfigHelper` class that goes through a `ConfigRepository` instance.  We provide `KafkaApis` with such an instance, and it creates an instance of the helper so it can use that instead of going through `ZkAdminManager`.

Existing tests are sufficient to identify bugs and regressions in `Partition`, `ReplicaManager`, `KafkaApis`, and `ConfigHelper`.  The `ConfigRepository` implementations have their own unit tests.

Reviewers: Jason Gustafson <jason@confluent.io>",2,52,0,13,101,2,2,52,52,52,1,1,52,52,52,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/metadata/CachedConfigRepositoryTest.scala,core/src/test/scala/unit/kafka/server/metadata/CachedConfigRepositoryTest.scala,"MINOR: Add ConfigRepository, use in Partition and KafkaApis (#10005)

`Partition` objects are able to retrieve topic configs when creating their log, and currently they do so with an implementation of `trait TopicConfigFetcher` that uses ZooKeeper.  ZooKeeper is not available when using a Raft-based metadata log, so we need to abstract the retrieval of configs so it can work either with or without ZooKeeper.  This PR introduces `trait ConfigRepository` with `ZkConfigRepository` and `CachedConfigRepository` implementations.  `Partition` objects now use a provided `ConfigRepository` to retrieve topic configs, and we eliminate `TopicConfigFetcher` as it is no longer needed.

`ReplicaManager` now contains an instance of `ConfigRepository` so it can provide it when creating `Partition` instances.

`KafkaApis` needs to be able to handle describe-config requests; it currently delegates that to `ZkAdminManager`, which of course queries ZooKeeper.  To make this work with or without ZooKeeper we move the logic from `ZkAdminManager` into a new `ConfigHelper` class that goes through a `ConfigRepository` instance.  We provide `KafkaApis` with such an instance, and it creates an instance of the helper so it can use that instead of going through `ZkAdminManager`.

Existing tests are sufficient to identify bugs and regressions in `Partition`, `ReplicaManager`, `KafkaApis`, and `ConfigHelper`.  The `ConfigRepository` implementations have their own unit tests.

Reviewers: Jason Gustafson <jason@confluent.io>",3,78,0,50,386,3,3,78,78,78,1,1,78,78,78,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/metadata/ZkConfigRepositoryTest.scala,core/src/test/scala/unit/kafka/server/metadata/ZkConfigRepositoryTest.scala,"MINOR: Add ConfigRepository, use in Partition and KafkaApis (#10005)

`Partition` objects are able to retrieve topic configs when creating their log, and currently they do so with an implementation of `trait TopicConfigFetcher` that uses ZooKeeper.  ZooKeeper is not available when using a Raft-based metadata log, so we need to abstract the retrieval of configs so it can work either with or without ZooKeeper.  This PR introduces `trait ConfigRepository` with `ZkConfigRepository` and `CachedConfigRepository` implementations.  `Partition` objects now use a provided `ConfigRepository` to retrieve topic configs, and we eliminate `TopicConfigFetcher` as it is no longer needed.

`ReplicaManager` now contains an instance of `ConfigRepository` so it can provide it when creating `Partition` instances.

`KafkaApis` needs to be able to handle describe-config requests; it currently delegates that to `ZkAdminManager`, which of course queries ZooKeeper.  To make this work with or without ZooKeeper we move the logic from `ZkAdminManager` into a new `ConfigHelper` class that goes through a `ConfigRepository` instance.  We provide `KafkaApis` with such an instance, and it creates an instance of the helper so it can use that instead of going through `ZkAdminManager`.

Existing tests are sufficient to identify bugs and regressions in `Partition`, `ReplicaManager`, `KafkaApis`, and `ConfigHelper`.  The `ConfigRepository` implementations have their own unit tests.

Reviewers: Jason Gustafson <jason@confluent.io>",4,54,0,33,300,2,2,54,54,54,1,1,54,54,54,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/utils/ReplicationUtilsTest.scala,core/src/test/scala/unit/kafka/utils/ReplicationUtilsTest.scala,"MINOR: Add ConfigRepository, use in Partition and KafkaApis (#10005)

`Partition` objects are able to retrieve topic configs when creating their log, and currently they do so with an implementation of `trait TopicConfigFetcher` that uses ZooKeeper.  ZooKeeper is not available when using a Raft-based metadata log, so we need to abstract the retrieval of configs so it can work either with or without ZooKeeper.  This PR introduces `trait ConfigRepository` with `ZkConfigRepository` and `CachedConfigRepository` implementations.  `Partition` objects now use a provided `ConfigRepository` to retrieve topic configs, and we eliminate `TopicConfigFetcher` as it is no longer needed.

`ReplicaManager` now contains an instance of `ConfigRepository` so it can provide it when creating `Partition` instances.

`KafkaApis` needs to be able to handle describe-config requests; it currently delegates that to `ZkAdminManager`, which of course queries ZooKeeper.  To make this work with or without ZooKeeper we move the logic from `ZkAdminManager` into a new `ConfigHelper` class that goes through a `ConfigRepository` instance.  We provide `KafkaApis` with such an instance, and it creates an instance of the helper so it can use that instead of going through `ZkAdminManager`.

Existing tests are sufficient to identify bugs and regressions in `Partition`, `ReplicaManager`, `KafkaApis`, and `ConfigHelper`.  The `ConfigRepository` implementations have their own unit tests.

Reviewers: Jason Gustafson <jason@confluent.io>",2,0,20,45,361,1,2,75,110,4,20,2.5,234,110,12,159,59,8,2,1,0,1
core/src/test/scala/unit/kafka/metrics/MetricsTest.scala,core/src/test/scala/unit/kafka/metrics/MetricsTest.scala,"MINOR: Introduce KafkaBroker trait for use in dynamic reconfiguration (#10019)

Dynamic broker reconfiguration needs to occur for both ZooKeeper-based
brokers and brokers that use a Raft-based metadata quorum.  DynamicBrokerConfig
currently operates on KafkaServer, but it needs to operate on BrokerServer
(the broker implementation that will use the Raft metadata log) as well.
This PR introduces a KafkaBroker trait to allow dynamic reconfiguration to
work with either implementation.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Colin Patrick McCabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>",17,31,4,167,1492,6,16,240,72,6,38,3.0,419,72,11,179,38,5,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/ConnectSchemaTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/ConnectSchemaTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",31,2,1,257,2742,1,31,333,272,33,10,3.0,475,272,48,142,60,14,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java,connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",48,6,14,462,4221,2,42,575,547,96,6,3.0,604,547,101,29,14,5,2,1,0,1
connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java,connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverter.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",155,59,111,610,5026,0,17,741,265,25,30,2.0,1316,332,44,575,119,19,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointConnector.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointConnector.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",21,1,1,124,905,1,14,169,156,42,4,1.5,174,156,44,5,3,1,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",39,1,1,252,2000,1,19,320,193,46,7,1,336,193,48,16,9,2,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMakerConfig.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMakerConfig.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",25,4,3,197,1854,3,13,290,255,48,6,2.5,305,255,51,15,6,2,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMetrics.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorMetrics.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",11,1,1,159,1489,1,11,208,208,52,4,1.5,226,208,56,18,15,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",14,3,6,74,658,1,6,106,71,13,8,1.5,131,71,16,25,15,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",117,1,6,690,5139,1,53,868,226,11,78,3.0,1574,226,20,706,106,9,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginUtils.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginUtils.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",36,2,7,263,1314,1,11,373,147,19,20,1.0,443,164,22,70,24,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryOffsetBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryOffsetBackingStore.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",13,12,20,71,500,2,7,107,113,11,10,2.5,170,113,17,63,20,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageWriter.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageWriter.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",18,4,7,99,740,1,7,212,208,19,11,2,296,208,27,84,30,8,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockConnector.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",11,3,6,68,465,1,6,115,111,29,4,2.5,128,111,32,13,7,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTaskTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",13,8,15,255,1671,1,11,342,147,23,15,4,435,153,29,93,15,6,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedConfigTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",25,6,18,242,2257,6,20,297,201,74,4,4.0,323,201,81,26,18,6,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImplTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImplTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",5,10,20,84,626,3,5,110,87,22,5,3,138,87,28,28,20,6,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampRouter.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampRouter.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",5,5,8,66,526,1,4,99,95,14,7,1,133,95,19,34,15,5,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKeyTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKeyTest.java,"KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect (#9867)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",12,1,1,86,748,1,12,123,123,41,3,1,136,123,45,13,12,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/DefaultHostResolver.java,clients/src/main/java/org/apache/kafka/clients/DefaultHostResolver.java,"KAFKA-12193: Re-resolve IPs after a client disconnects (#9902)

This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Satish Duggana <satishd@apache.org>, David Jacot <djacot@confluent.io>",1,29,0,9,53,1,1,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/HostResolver.java,clients/src/main/java/org/apache/kafka/clients/HostResolver.java,"KAFKA-12193: Re-resolve IPs after a client disconnects (#9902)

This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Satish Duggana <satishd@apache.org>, David Jacot <djacot@confluent.io>",0,26,0,6,39,0,0,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/AddressChangeHostResolver.java,clients/src/test/java/org/apache/kafka/clients/AddressChangeHostResolver.java,"KAFKA-12193: Re-resolve IPs after a client disconnects (#9902)

This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Satish Duggana <satishd@apache.org>, David Jacot <djacot@confluent.io>",6,49,0,26,125,5,5,49,49,49,1,1,49,49,49,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/test/MockSelector.java,clients/src/test/java/org/apache/kafka/test/MockSelector.java,"KAFKA-12193: Re-resolve IPs after a client disconnects (#9902)

This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Satish Duggana <satishd@apache.org>, David Jacot <djacot@confluent.io>",38,10,1,166,1074,3,29,234,87,11,22,3.0,335,87,15,101,17,5,2,1,0,1
core/src/test/scala/unit/kafka/server/TopicIdWithOldInterBrokerProtocolTest.scala,core/src/test/scala/unit/kafka/server/TopicIdWithOldInterBrokerProtocolTest.scala,"MINOR: remove unused import in TopicIdWithOldInterBrokerProtocolTest (#10037)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,0,1,84,763,0,8,115,66,38,3,1,121,66,40,6,5,2,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MetadataRecordTypeGenerator.java,generator/src/main/java/org/apache/kafka/message/MetadataRecordTypeGenerator.java,"KAFKA-12250; Add metadata record serde for KIP-631 (#9998)

This patch adds a `RecordSerde` implementation for the metadata record format expected by KIP-631. 

Reviewers: Colin McCabe <cmccabe@apache.org>, Ismael Juma <mlists@juma.me.uk>",19,12,2,158,1065,4,13,190,180,95,2,3.5,192,180,96,2,2,1,1,0,1,1
core/src/test/scala/kafka/server/metadata/MetadataBrokersTest.scala,core/src/test/scala/kafka/server/metadata/MetadataBrokersTest.scala,"KAFKA-12271 Immutable container classes to support new MetadataCache (#10018)

Three new classes are added to support the upcoming changes to MetadataCache
required for handling Raft metadata records.

Reviewers: Jason Gustafson <jason@confluent.io>

Co-authored-by: David Arthur <mumrah@gmail.com>",5,90,0,64,650,3,3,90,90,90,1,1,90,90,90,0,0,0,0,0,0,0
connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/BasicAuthSecurityRestExtension.java,connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/BasicAuthSecurityRestExtension.java,"KAFKA-10895: Gracefully handle invalid JAAS configs (follow up fix) (#9987)

Fixes a regression introduced by https://github.com/apache/kafka/pull/9806 in the original fix for KAFKA-10895

It was discovered that if an invalid JAAS config is present on the worker, invoking Configuration::getConfiguration throws an exception. The changes from #9806 cause that exception to be thrown during plugin scanning, which causes the worker to fail even if it is not configured to use the basic auth extension at all.

This follow-up handles invalid JAAS configurations more gracefully, and only throws them if the worker is actually configured to use the basic auth extension, at the time that the extension is instantiated and configured.

Two unit tests are added.

Reviewers: Greg Harris <gregh@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",8,30,3,49,339,5,7,117,79,23,5,3,124,79,25,7,3,1,2,1,0,1
connect/basic-auth-extension/src/test/java/org/apache/kafka/connect/rest/basic/auth/extension/BasicAuthSecurityRestExtensionTest.java,connect/basic-auth-extension/src/test/java/org/apache/kafka/connect/rest/basic/auth/extension/BasicAuthSecurityRestExtensionTest.java,"KAFKA-10895: Gracefully handle invalid JAAS configs (follow up fix) (#9987)

Fixes a regression introduced by https://github.com/apache/kafka/pull/9806 in the original fix for KAFKA-10895

It was discovered that if an invalid JAAS config is present on the worker, invoking Configuration::getConfiguration throws an exception. The changes from #9806 cause that exception to be thrown during plugin scanning, which causes the worker to fail even if it is not configured to use the basic auth extension at all.

This follow-up handles invalid JAAS configurations more gracefully, and only throws them if the worker is actually configured to use the basic auth extension, at the time that the extension is instantiated and configured.

Two unit tests are added.

Reviewers: Greg Harris <gregh@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",7,49,0,77,652,3,6,115,69,38,3,4,129,69,43,14,14,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java,"KAFKA-10648: Add Prefix Scan support to State Stores (#9508)

Add prefix scan support to State stores. Currently, only RocksDB and InMemory key value stores are being supported.

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",40,11,0,307,2186,1,23,368,219,9,43,3,739,219,17,371,97,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java,streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java,"KAFKA-10648: Add Prefix Scan support to State Stores (#9508)

Add prefix scan support to State stores. Currently, only RocksDB and InMemory key value stores are being supported.

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",66,2,2,297,1774,2,39,381,298,13,30,2.0,639,298,21,258,74,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBRangeIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBRangeIterator.java,"KAFKA-10648: Add Prefix Scan support to State Stores (#9508)

Add prefix scan support to State stores. Currently, only RocksDB and InMemory key value stores are being supported.

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,7,2,60,407,3,2,85,62,28,3,5,98,62,33,13,11,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java,"KAFKA-10648: Add Prefix Scan support to State Stores (#9508)

Add prefix scan support to State stores. Currently, only RocksDB and InMemory key value stores are being supported.

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",17,19,0,352,4399,1,7,483,293,48,10,2.5,640,293,64,157,117,16,2,1,0,1
core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala,core/src/test/scala/integration/kafka/admin/ReassignPartitionsIntegrationTest.scala,"MINOR: Add HostedPartition.Deferred state in ReplicaManager (#10003)

Brokers receive metadata from the Raft metadata quorum very differently than they do from
ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker
reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple
messages.  In normal operation the multiple messages associated with a state change will all
appear in a single batch, so they can and will be coalesced and applied together.  There are
circumstances where messages associated with partition state changes will appear across
multiple batches and we will be forced to coalesce these multiple batches together.  The
circumstances when this occurs are as follows:

- When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the
broker will see multiple partition state changes for a single partition across different
batches while it is catching up.  For example, it will see the `TopicRecord` and the
`PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords`
that may have been recorded since the creation.  The broker does not know the state of
the topic partitions until it reads and coalesces all the messages.
- The broker will have to ""catch up"" on the metadata log if it becomes fenced and then
regains its lease and resumes communication with the metadata quorum.
- A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so
long that the point at which it needs to resume fetching the metadata log has been
subsumed into a metadata snapshot and is no longer independently fetchable.  A soft
restart will entail some kind of metadata reset based on the latest available snapshot
plus a catchup phase to fetch after the snapshot end point.

The first case -- during startup -- occurs before clients are able to connect to the broker.
Clients are able to connect to the broker in the second case.  It is unclear if clients will be
able to to connect to the broker during a soft restart (the third case).

We need a way to defer the application of topic partition metadata in all of the above cases,
and while we are deferring the application of the metadata the broker will not service clients
for the affected partitions.

As a side note, it is arguable if the broker should be able to service clients while catching up
or not.  The decision to not service clients has no impact in the startup case -- clients can't
connect yet at that point anyway.  In the third case it is not yet clear what we are going to do,
but being unable to service clients while performing a soft reset seems reasonable.  In the
second case it is most likely true that we will catch up quickly; it would be unusual to
reestablish communication with the metadata quorum such that we gain a new lease and
begin to catch up only to lose our lease again.

So we need a way to defer the application of partition metadata and make those partitions
unavailable while deferring state changes.  This PR adds a new internal partition state to
ReplicaManager to accomplish this.  Currently the available partition states are simple
`Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it). 
We add a new `Deferred` state.  We also rename a couple of methods that refer to
""nonOffline"" partitions to instead refer to ""online"" partitions.

**The new `Deferred` state never happens when using ZooKeeper for metadata storage.**
Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum
and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing
tests to confirm that there is no functionality change in the ZooKeeper case.  We will add
the logic for deferring/applying/reacting to deferred partition state in separate PRs since
that code will never be invoked in the ZooKeeper world.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",39,1,1,551,4551,1,31,719,427,40,18,1.5,1014,564,56,295,137,16,2,1,0,1
core/src/test/scala/unit/kafka/server/LogDirFailureTest.scala,core/src/test/scala/unit/kafka/server/LogDirFailureTest.scala,"MINOR: Add HostedPartition.Deferred state in ReplicaManager (#10003)

Brokers receive metadata from the Raft metadata quorum very differently than they do from
ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker
reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple
messages.  In normal operation the multiple messages associated with a state change will all
appear in a single batch, so they can and will be coalesced and applied together.  There are
circumstances where messages associated with partition state changes will appear across
multiple batches and we will be forced to coalesce these multiple batches together.  The
circumstances when this occurs are as follows:

- When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the
broker will see multiple partition state changes for a single partition across different
batches while it is catching up.  For example, it will see the `TopicRecord` and the
`PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords`
that may have been recorded since the creation.  The broker does not know the state of
the topic partitions until it reads and coalesces all the messages.
- The broker will have to ""catch up"" on the metadata log if it becomes fenced and then
regains its lease and resumes communication with the metadata quorum.
- A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so
long that the point at which it needs to resume fetching the metadata log has been
subsumed into a metadata snapshot and is no longer independently fetchable.  A soft
restart will entail some kind of metadata reset based on the latest available snapshot
plus a catchup phase to fetch after the snapshot end point.

The first case -- during startup -- occurs before clients are able to connect to the broker.
Clients are able to connect to the broker in the second case.  It is unclear if clients will be
able to to connect to the broker during a soft restart (the third case).

We need a way to defer the application of topic partition metadata in all of the above cases,
and while we are deferring the application of the metadata the broker will not service clients
for the affected partitions.

As a side note, it is arguable if the broker should be able to service clients while catching up
or not.  The decision to not service clients has no impact in the startup case -- clients can't
connect yet at that point anyway.  In the third case it is not yet clear what we are going to do,
but being unable to service clients while performing a soft reset seems reasonable.  In the
second case it is most likely true that we will catch up quickly; it would be unusual to
reestablish communication with the metadata quorum such that we gain a new lease and
begin to catch up only to lose our lease again.

So we need a way to defer the application of partition metadata and make those partitions
unavailable while deferring state changes.  This PR adds a new internal partition state to
ReplicaManager to accomplish this.  Currently the available partition states are simple
`Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it). 
We add a new `Deferred` state.  We also rename a couple of methods that refer to
""nonOffline"" partitions to instead refer to ""online"" partitions.

**The new `Deferred` state never happens when using ZooKeeper for metadata storage.**
Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum
and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing
tests to confirm that there is no functionality change in the ZooKeeper case.  We will add
the logic for deferring/applying/reacting to deferred partition state in separate PRs since
that code will never be invoked in the ZooKeeper world.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",12,2,2,133,1257,1,10,204,126,6,34,2.5,387,126,11,183,35,5,2,1,0,1
core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala,core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala,"MINOR: Add HostedPartition.Deferred state in ReplicaManager (#10003)

Brokers receive metadata from the Raft metadata quorum very differently than they do from
ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker
reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple
messages.  In normal operation the multiple messages associated with a state change will all
appear in a single batch, so they can and will be coalesced and applied together.  There are
circumstances where messages associated with partition state changes will appear across
multiple batches and we will be forced to coalesce these multiple batches together.  The
circumstances when this occurs are as follows:

- When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the
broker will see multiple partition state changes for a single partition across different
batches while it is catching up.  For example, it will see the `TopicRecord` and the
`PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords`
that may have been recorded since the creation.  The broker does not know the state of
the topic partitions until it reads and coalesces all the messages.
- The broker will have to ""catch up"" on the metadata log if it becomes fenced and then
regains its lease and resumes communication with the metadata quorum.
- A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so
long that the point at which it needs to resume fetching the metadata log has been
subsumed into a metadata snapshot and is no longer independently fetchable.  A soft
restart will entail some kind of metadata reset based on the latest available snapshot
plus a catchup phase to fetch after the snapshot end point.

The first case -- during startup -- occurs before clients are able to connect to the broker.
Clients are able to connect to the broker in the second case.  It is unclear if clients will be
able to to connect to the broker during a soft restart (the third case).

We need a way to defer the application of topic partition metadata in all of the above cases,
and while we are deferring the application of the metadata the broker will not service clients
for the affected partitions.

As a side note, it is arguable if the broker should be able to service clients while catching up
or not.  The decision to not service clients has no impact in the startup case -- clients can't
connect yet at that point anyway.  In the third case it is not yet clear what we are going to do,
but being unable to service clients while performing a soft reset seems reasonable.  In the
second case it is most likely true that we will catch up quickly; it would be unusual to
reestablish communication with the metadata quorum such that we gain a new lease and
begin to catch up only to lose our lease again.

So we need a way to defer the application of partition metadata and make those partitions
unavailable while deferring state changes.  This PR adds a new internal partition state to
ReplicaManager to accomplish this.  Currently the available partition states are simple
`Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it). 
We add a new `Deferred` state.  We also rename a couple of methods that refer to
""nonOffline"" partitions to instead refer to ""online"" partitions.

**The new `Deferred` state never happens when using ZooKeeper for metadata storage.**
Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum
and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing
tests to confirm that there is no functionality change in the ZooKeeper case.  We will add
the logic for deferring/applying/reacting to deferred partition state in separate PRs since
that code will never be invoked in the ZooKeeper world.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",11,1,1,157,1278,1,8,246,288,4,61,4,782,288,13,536,72,9,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala,core/src/test/scala/unit/kafka/server/ReplicaAlterLogDirsThreadTest.scala,"MINOR: Add HostedPartition.Deferred state in ReplicaManager (#10003)

Brokers receive metadata from the Raft metadata quorum very differently than they do from
ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker
reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple
messages.  In normal operation the multiple messages associated with a state change will all
appear in a single batch, so they can and will be coalesced and applied together.  There are
circumstances where messages associated with partition state changes will appear across
multiple batches and we will be forced to coalesce these multiple batches together.  The
circumstances when this occurs are as follows:

- When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the
broker will see multiple partition state changes for a single partition across different
batches while it is catching up.  For example, it will see the `TopicRecord` and the
`PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords`
that may have been recorded since the creation.  The broker does not know the state of
the topic partitions until it reads and coalesces all the messages.
- The broker will have to ""catch up"" on the metadata log if it becomes fenced and then
regains its lease and resumes communication with the metadata quorum.
- A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so
long that the point at which it needs to resume fetching the metadata log has been
subsumed into a metadata snapshot and is no longer independently fetchable.  A soft
restart will entail some kind of metadata reset based on the latest available snapshot
plus a catchup phase to fetch after the snapshot end point.

The first case -- during startup -- occurs before clients are able to connect to the broker.
Clients are able to connect to the broker in the second case.  It is unclear if clients will be
able to to connect to the broker during a soft restart (the third case).

We need a way to defer the application of topic partition metadata in all of the above cases,
and while we are deferring the application of the metadata the broker will not service clients
for the affected partitions.

As a side note, it is arguable if the broker should be able to service clients while catching up
or not.  The decision to not service clients has no impact in the startup case -- clients can't
connect yet at that point anyway.  In the third case it is not yet clear what we are going to do,
but being unable to service clients while performing a soft reset seems reasonable.  In the
second case it is most likely true that we will catch up quickly; it would be unusual to
reestablish communication with the metadata quorum such that we gain a new lease and
begin to catch up only to lose our lease again.

So we need a way to defer the application of partition metadata and make those partitions
unavailable while deferring state changes.  This PR adds a new internal partition state to
ReplicaManager to accomplish this.  Currently the available partition states are simple
`Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it). 
We add a new `Deferred` state.  We also rename a couple of methods that refer to
""nonOffline"" partitions to instead refer to ""online"" partitions.

**The new `Deferred` state never happens when using ZooKeeper for metadata storage.**
Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum
and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing
tests to confirm that there is no functionality change in the ZooKeeper case.  We will add
the logic for deferring/applying/reacting to deferred partition state in separate PRs since
that code will never be invoked in the ZooKeeper world.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",16,4,4,731,6565,3,16,945,526,32,30,6.0,1416,526,47,471,112,16,2,1,0,1
core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala,core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceTest.scala,"MINOR: Add HostedPartition.Deferred state in ReplicaManager (#10003)

Brokers receive metadata from the Raft metadata quorum very differently than they do from
ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker
reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple
messages.  In normal operation the multiple messages associated with a state change will all
appear in a single batch, so they can and will be coalesced and applied together.  There are
circumstances where messages associated with partition state changes will appear across
multiple batches and we will be forced to coalesce these multiple batches together.  The
circumstances when this occurs are as follows:

- When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the
broker will see multiple partition state changes for a single partition across different
batches while it is catching up.  For example, it will see the `TopicRecord` and the
`PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords`
that may have been recorded since the creation.  The broker does not know the state of
the topic partitions until it reads and coalesces all the messages.
- The broker will have to ""catch up"" on the metadata log if it becomes fenced and then
regains its lease and resumes communication with the metadata quorum.
- A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so
long that the point at which it needs to resume fetching the metadata log has been
subsumed into a metadata snapshot and is no longer independently fetchable.  A soft
restart will entail some kind of metadata reset based on the latest available snapshot
plus a catchup phase to fetch after the snapshot end point.

The first case -- during startup -- occurs before clients are able to connect to the broker.
Clients are able to connect to the broker in the second case.  It is unclear if clients will be
able to to connect to the broker during a soft restart (the third case).

We need a way to defer the application of topic partition metadata in all of the above cases,
and while we are deferring the application of the metadata the broker will not service clients
for the affected partitions.

As a side note, it is arguable if the broker should be able to service clients while catching up
or not.  The decision to not service clients has no impact in the startup case -- clients can't
connect yet at that point anyway.  In the third case it is not yet clear what we are going to do,
but being unable to service clients while performing a soft reset seems reasonable.  In the
second case it is most likely true that we will catch up quickly; it would be unusual to
reestablish communication with the metadata quorum such that we gain a new lease and
begin to catch up only to lose our lease again.

So we need a way to defer the application of partition metadata and make those partitions
unavailable while deferring state changes.  This PR adds a new internal partition state to
ReplicaManager to accomplish this.  Currently the available partition states are simple
`Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it). 
We add a new `Deferred` state.  We also rename a couple of methods that refer to
""nonOffline"" partitions to instead refer to ""online"" partitions.

**The new `Deferred` state never happens when using ZooKeeper for metadata storage.**
Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum
and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing
tests to confirm that there is no functionality change in the ZooKeeper case.  We will add
the logic for deferring/applying/reacting to deferred partition state in separate PRs since
that code will never be invoked in the ZooKeeper world.

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",20,1,1,282,2925,1,20,474,410,18,26,2.0,606,410,23,132,22,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/DuplicateBrokerRegistrationException.java,clients/src/main/java/org/apache/kafka/common/errors/DuplicateBrokerRegistrationException.java,"MINOR: Add DuplicateBrokerRegistrationException (#10029)

Add DuplicateBrokerRegistrationException, as specified in KIP-631.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",2,29,0,9,49,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/config/internals/QuotaConfigs.java,clients/src/main/java/org/apache/kafka/common/config/internals/QuotaConfigs.java,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",5,96,0,61,478,5,5,96,96,96,1,1,96,96,96,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaEntity.java,clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaEntity.java,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",11,6,0,35,230,1,6,77,70,26,3,1,77,70,26,0,0,0,1,0,0,0
core/src/main/scala/kafka/admin/ConfigCommand.scala,core/src/main/scala/kafka/admin/ConfigCommand.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",217,4,4,738,6942,1,29,920,174,14,66,3.0,1572,174,24,652,98,10,2,1,0,1
core/src/main/scala/kafka/server/ConfigHandler.scala,core/src/main/scala/kafka/server/ConfigHandler.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",44,10,10,182,1532,2,15,254,69,7,39,3,461,78,12,207,24,5,2,1,0,1
core/src/main/scala/kafka/server/DynamicConfig.scala,core/src/main/scala/kafka/server/DynamicConfig.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",11,4,47,74,590,2,7,127,86,7,17,2,219,86,13,92,47,5,2,1,0,1
core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala,core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",64,7,6,303,2803,3,26,387,195,14,28,4.0,670,195,24,283,96,10,2,1,0,1
core/src/test/scala/unit/kafka/server/ControllerMutationQuotaTest.scala,core/src/test/scala/unit/kafka/server/ControllerMutationQuotaTest.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",37,2,2,302,2613,1,25,420,366,60,7,3,454,366,65,34,12,5,2,1,0,1
core/src/test/scala/unit/kafka/server/DynamicConfigChangeTest.scala,core/src/test/scala/unit/kafka/server/DynamicConfigChangeTest.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",29,15,15,290,2571,5,23,387,77,8,47,3,642,80,14,255,49,5,2,1,0,1
core/src/test/scala/unit/kafka/server/DynamicConfigTest.scala,core/src/test/scala/unit/kafka/server/DynamicConfigTest.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",7,4,3,47,359,3,7,72,58,6,12,3.0,133,58,11,61,20,5,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaMetricReporterExceptionHandlingTest.scala,core/src/test/scala/unit/kafka/server/KafkaMetricReporterExceptionHandlingTest.scala,"MINOR: Upstream QuotaConfigs

This PR moves static property definitions for user client quotas into a
new class called QuotaConfigs in the clients module under the
o.a.k.common.config.internals package. This is needed to support the
client quotas work in the quorum based controller.

Reviewers: Colin McCabe <cmccabe@apache.org>",12,2,1,79,585,1,10,119,116,17,7,2,147,116,21,28,9,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java,"KAFKA-10413: Allow for even distribution of lost/new tasks when multiple Connect workers join at the same time (#9319)

First issue: When more than one workers join the Connect group the incremental cooperative assignor revokes and reassigns at most average number of tasks per worker.
Side-effect: This results in the additional workers joining the group stay idle and would require more future rebalances to happen to have even distribution of tasks.
Fix: As part of task assignment calculation following a deployment, the reassignment of tasks are calculated by revoking all the tasks above the rounded up (ceil) average number of tasks.

Second issue: When more than one worker is lost and rejoins the group at most one worker will be re assigned with the lost tasks from all the workers that left the group.
Side-effect: In scenarios where more than one worker is lost and rejoins the group only one among them gets assigned all the partitions that were lost in the past. The additional workers that have joined would not get any task assigned to them until a rebalance that happens in future.
Fix: As part fo lost task re assignment all the new workers that have joined the group would be considered for task assignment and would be assigned in a round robin fashion with the new tasks.

Testing strategy : 
* System testing in a Kubernetes environment completed
* New integration tests to test for balanced tasks
* Updated unit tests. 

Co-authored-by: Rameshkrishnan Muthusamy <rameshkrishnan_muthusamy@apple.com>
Co-authored-by: Randall Hauch <rhauch@gmail.com>
Co-authored-by: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",47,15,9,1164,11328,1,44,1469,1079,210,7,8,1576,1079,225,107,53,15,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinatorIncrementalTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinatorIncrementalTest.java,"KAFKA-10413: Allow for even distribution of lost/new tasks when multiple Connect workers join at the same time (#9319)

First issue: When more than one workers join the Connect group the incremental cooperative assignor revokes and reassigns at most average number of tasks per worker.
Side-effect: This results in the additional workers joining the group stay idle and would require more future rebalances to happen to have even distribution of tasks.
Fix: As part of task assignment calculation following a deployment, the reassignment of tasks are calculated by revoking all the tasks above the rounded up (ceil) average number of tasks.

Second issue: When more than one worker is lost and rejoins the group at most one worker will be re assigned with the lost tasks from all the workers that left the group.
Side-effect: In scenarios where more than one worker is lost and rejoins the group only one among them gets assigned all the partitions that were lost in the past. The additional workers that have joined would not get any task assigned to them until a rebalance that happens in future.
Fix: As part fo lost task re assignment all the new workers that have joined the group would be considered for task assignment and would be assigned in a round robin fashion with the new tasks.

Testing strategy : 
* System testing in a Kubernetes environment completed
* New integration tests to test for balanced tasks
* Updated unit tests. 

Co-authored-by: Rameshkrishnan Muthusamy <rameshkrishnan_muthusamy@apple.com>
Co-authored-by: Randall Hauch <rhauch@gmail.com>
Co-authored-by: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <k.karantasis@gmail.com>",16,10,9,447,3540,1,14,584,573,97,6,3.5,618,573,103,34,16,6,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/timeline/TimelineHashMapBenchmark.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",8,90,0,66,553,3,3,90,90,90,1,1,90,90,90,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/timeline/Delta.java,metadata/src/main/java/org/apache/kafka/timeline/Delta.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",0,31,0,4,23,0,0,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/timeline/Revertable.java,metadata/src/main/java/org/apache/kafka/timeline/Revertable.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",0,32,0,4,23,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/timeline/Snapshot.java,metadata/src/main/java/org/apache/kafka/timeline/Snapshot.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",13,98,0,55,330,10,10,98,98,98,1,1,98,98,98,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/timeline/TimelineHashSet.java,metadata/src/main/java/org/apache/kafka/timeline/TimelineHashSet.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",49,261,0,202,1096,30,30,261,261,261,1,1,261,261,261,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/timeline/SnapshotRegistryTest.java,metadata/src/test/java/org/apache/kafka/timeline/SnapshotRegistryTest.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",7,88,0,62,504,5,5,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/timeline/SnapshottableHashTableTest.java,metadata/src/test/java/org/apache/kafka/timeline/SnapshottableHashTableTest.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",19,236,0,189,1694,13,13,236,236,236,1,1,236,236,236,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/timeline/TimelineHashSetTest.java,metadata/src/test/java/org/apache/kafka/timeline/TimelineHashSetTest.java,"KAFKA-12209: Add the timeline data structures for the KIP-631 controller (#9901)

Reviewers: Jun Rao <junrao@gmail.com>",5,113,0,87,915,5,5,113,113,113,1,1,113,113,113,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/SaslAuthenticationContext.java,clients/src/main/java/org/apache/kafka/common/security/auth/SaslAuthenticationContext.java,"KAFKA-10700 - Support mutual TLS authentication for SASL_SSL listeners (KIP-684) (#10007)

mTLS is enabled if listener-prefixed ssl.client.auth is configured for SASL_SSL listeners. Broker-wide ssl.client.auth is not applied to SASL_SSL listeners as before, but we now print a warning.

Reviewers: David Jacot <djacot@confluent.io>",7,20,0,43,226,3,7,74,48,18,4,3.5,79,48,20,5,4,1,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslPlainSslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SaslPlainSslEndToEndAuthorizationTest.scala,"KAFKA-10700 - Support mutual TLS authentication for SASL_SSL listeners (KIP-684) (#10007)

mTLS is enabled if listener-prefixed ssl.client.auth is configured for SASL_SSL listeners. Broker-wide ssl.client.auth is not applied to SASL_SSL listeners as before, but we now print a warning.

Reviewers: David Jacot <djacot@confluent.io>",21,22,5,108,945,3,10,156,77,10,16,2.5,214,82,13,58,16,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/AbstractConfigTest.java,clients/src/test/java/org/apache/kafka/common/config/AbstractConfigTest.java,"MINOR: AbstractConfigTest.testClassConfigs should reset the class loa… (#10010)

Reviewers: Jason Gustafson <jason@confluent.io>, Luke Chen <showuon@gmail.com>",48,42,45,478,4401,1,32,611,137,25,24,2.5,807,137,34,196,63,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/BrokerHeartbeatRequest.java,clients/src/main/java/org/apache/kafka/common/requests/BrokerHeartbeatRequest.java,"KAFKA-12248; Add BrokerHeartbeat/BrokerRegistration RPCs for KIP-500 (#9994)

This patch adds the schemas and request/response objects for the `BrokerHeartbeat` and `BrokerRegistration` APIs that were added as part of KIP-631. These APIs are only exposed by the KIP-500 controller and not advertised by the broker.

Reviewers: Jason Gustafson <jason@confluent.io>",7,72,0,44,288,7,7,72,72,72,1,1,72,72,72,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/BrokerHeartbeatResponse.java,clients/src/main/java/org/apache/kafka/common/requests/BrokerHeartbeatResponse.java,"KAFKA-12248; Add BrokerHeartbeat/BrokerRegistration RPCs for KIP-500 (#9994)

This patch adds the schemas and request/response objects for the `BrokerHeartbeat` and `BrokerRegistration` APIs that were added as part of KIP-631. These APIs are only exposed by the KIP-500 controller and not advertised by the broker.

Reviewers: Jason Gustafson <jason@confluent.io>",6,62,0,36,239,6,6,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/BrokerRegistrationRequest.java,clients/src/main/java/org/apache/kafka/common/requests/BrokerRegistrationRequest.java,"KAFKA-12248; Add BrokerHeartbeat/BrokerRegistration RPCs for KIP-500 (#9994)

This patch adds the schemas and request/response objects for the `BrokerHeartbeat` and `BrokerRegistration` APIs that were added as part of KIP-631. These APIs are only exposed by the KIP-500 controller and not advertised by the broker.

Reviewers: Jason Gustafson <jason@confluent.io>",7,72,0,44,288,7,7,72,72,72,1,1,72,72,72,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/BrokerRegistrationResponse.java,clients/src/main/java/org/apache/kafka/common/requests/BrokerRegistrationResponse.java,"KAFKA-12248; Add BrokerHeartbeat/BrokerRegistration RPCs for KIP-500 (#9994)

This patch adds the schemas and request/response objects for the `BrokerHeartbeat` and `BrokerRegistration` APIs that were added as part of KIP-631. These APIs are only exposed by the KIP-500 controller and not advertised by the broker.

Reviewers: Jason Gustafson <jason@confluent.io>",7,67,0,40,255,7,7,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
streams/examples/src/main/java/org/apache/kafka/streams/examples/temperature/TemperatureDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/temperature/TemperatureDemo.java,"KAFKA-10366 & KAFKA-9649: Implement KIP-659 to allow TimeWindowedDeserializer and TimeWindowedSerde to handle window size (#9253)

See KIP details and discussions here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-659%3A+Improve+TimeWindowedDeserializer+and+TimeWindowedSerde+to+handle+window+size

Deprecates methods that allow users to skip setting a window size when one is needed. Adds a window size streams config to allow the timeWindowedDeserializer to calculate window end time.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",3,1,1,61,570,1,1,127,144,12,11,3,201,144,18,74,26,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/WindowedSerdes.java,streams/src/main/java/org/apache/kafka/streams/kstream/WindowedSerdes.java,"KAFKA-10366 & KAFKA-9649: Implement KIP-659 to allow TimeWindowedDeserializer and TimeWindowedSerde to handle window size (#9253)

See KIP details and discussions here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-659%3A+Improve+TimeWindowedDeserializer+and+TimeWindowedSerde+to+handle+window+size

Deprecates methods that allow users to skip setting a window size when one is needed. Adds a window size streams config to allow the timeWindowedDeserializer to calculate window end time.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",13,2,0,58,522,0,11,101,59,17,6,2.0,104,59,17,3,2,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/WindowedSerdesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/WindowedSerdesTest.java,"KAFKA-10366 & KAFKA-9649: Implement KIP-659 to allow TimeWindowedDeserializer and TimeWindowedSerde to handle window size (#9253)

See KIP details and discussions here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-659%3A+Improve+TimeWindowedDeserializer+and+TimeWindowedSerde+to+handle+window+size

Deprecates methods that allow users to skip setting a window size when one is needed. Adds a window size streams config to allow the timeWindowedDeserializer to calculate window end time.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",14,2,2,129,1078,2,14,164,96,41,4,2.0,167,96,42,3,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedCogroupedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedCogroupedKStreamImplTest.java,"KAFKA-10366 & KAFKA-9649: Implement KIP-659 to allow TimeWindowedDeserializer and TimeWindowedSerde to handle window size (#9253)

See KIP details and discussions here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-659%3A+Improve+TimeWindowedDeserializer+and+TimeWindowedSerde+to+handle+window+size

Deprecates methods that allow users to skip setting a window size when one is needed. Adds a window size streams config to allow the timeWindowedDeserializer to calculate window end time.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",16,8,7,259,2804,7,16,323,297,65,5,4,350,297,70,27,16,5,2,1,0,1
core/src/test/scala/unit/kafka/admin/AddPartitionsTest.scala,core/src/test/scala/unit/kafka/admin/AddPartitionsTest.scala,"KAFKA-7940: Fix flaky test case in `CustomQuotaCallbackTest` (#9777)

This patch attempts to fix `CustomQuotaCallbackTest#testCustomQuotaCallback`. The test creates 99 partitions in a topic, and expects that we can get the partition info for all of them after 15 seconds. If we cannot, then we'll get the error:
```
org.scalatest.exceptions.TestFailedException: Partition [group1_largeTopic,69] metadata not propagated after 15000 ms
```
15 secs is not enough to complete the 99 partitions creation on a slow system. So, we fix it by explicitly wait until we've got the expected partition size before retrieving each partition info and we increase the wait time to 60s for all partition metadata to be propagated.

Reviewers: Jason Gustafson <jason@confluent.io>",13,12,12,139,1565,4,8,187,251,4,44,4.0,633,251,14,446,76,10,2,1,0,1
core/src/test/scala/unit/kafka/integration/UncleanLeaderElectionTest.scala,core/src/test/scala/unit/kafka/integration/UncleanLeaderElectionTest.scala,"KAFKA-7940: Fix flaky test case in `CustomQuotaCallbackTest` (#9777)

This patch attempts to fix `CustomQuotaCallbackTest#testCustomQuotaCallback`. The test creates 99 partitions in a topic, and expects that we can get the partition info for all of them after 15 seconds. If we cannot, then we'll get the error:
```
org.scalatest.exceptions.TestFailedException: Partition [group1_largeTopic,69] metadata not propagated after 15000 ms
```
15 secs is not enough to complete the 99 partitions creation on a slow system. So, we fix it by explicitly wait until we've got the expected partition size before retrieving each partition info and we increase the wait time to 60s for all partition metadata to be propagated.

Reviewers: Jason Gustafson <jason@confluent.io>",24,3,3,222,2247,3,15,359,290,8,47,3,614,290,13,255,31,5,2,1,0,1
core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala,core/src/test/scala/unit/kafka/server/CreateTopicsRequestTest.scala,"KAFKA-7940: Fix flaky test case in `CustomQuotaCallbackTest` (#9777)

This patch attempts to fix `CustomQuotaCallbackTest#testCustomQuotaCallback`. The test creates 99 partitions in a topic, and expects that we can get the partition info for all of them after 15 seconds. If we cannot, then we'll get the error:
```
org.scalatest.exceptions.TestFailedException: Partition [group1_largeTopic,69] metadata not propagated after 15000 ms
```
15 secs is not enough to complete the 99 partitions creation on a slow system. So, we fix it by explicitly wait until we've got the expected partition size before retrieving each partition info and we increase the wait time to 60s for all partition metadata to be propagated.

Reviewers: Jason Gustafson <jason@confluent.io>",8,3,3,139,1439,1,5,181,233,10,19,1,452,233,24,271,116,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java,"MINOR: Reorder modifiers and Replace Map.get with Map.computeIfAbsent (#9991)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",101,2,6,449,2970,1,50,725,296,12,60,3.0,1372,296,23,647,116,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java,"MINOR: fix @link tags in javadoc (#9939)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,25,319,0,0,301,268,19,16,1.5,354,268,22,53,17,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java,"MINOR: fix @link tags in javadoc (#9939)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,36,513,0,0,575,338,13,44,6.0,2648,486,60,2073,1340,47,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedCogroupedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedCogroupedKStream.java,"MINOR: fix @link tags in javadoc (#9939)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,21,255,0,0,254,251,21,12,2.0,401,251,33,147,120,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindowedKStream.java,"MINOR: fix @link tags in javadoc (#9939)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,1,0,37,540,0,0,651,278,34,19,6,978,278,51,327,229,17,2,1,0,1
core/src/main/scala/kafka/common/InconsistentNodeIdException.scala,core/src/main/scala/kafka/common/InconsistentNodeIdException.scala,"KAFKA-12236; New meta.properties logic for KIP-500 (#9967)

This patch contains the new handling of `meta.properties` required by the KIP-500 server as specified in KIP-631. When using the self-managed quorum, the `meta.properties` file is required in each log directory with the new `version` property set to 1. It must include the `cluster.id` property and it must have a `node.id` matching that in the configuration.

The behavior of `meta.properties` for the Zookeeper-based `KafkaServer` does not change. We treat `meta.properties` as optional and as if it were `version=0`. We continue to generate the clusterId and/or the brokerId through Zookeeper as needed.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",1,22,0,4,38,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/ServerGenerateBrokerIdTest.scala,core/src/test/scala/unit/kafka/server/ServerGenerateBrokerIdTest.scala,"KAFKA-12236; New meta.properties logic for KIP-500 (#9967)

This patch contains the new handling of `meta.properties` required by the KIP-500 server as specified in KIP-631. When using the self-managed quorum, the `meta.properties` file is required in each log directory with the new `version` property set to 1. It must include the `cluster.id` property and it must have a `node.id` matching that in the configuration.

The behavior of `meta.properties` for the Zookeeper-based `KafkaServer` does not change. We treat `meta.properties` as optional and as if it were `version=0`. We continue to generate the clusterId and/or the brokerId through Zookeeper as needed.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",15,3,2,147,1227,1,9,193,127,9,22,2.0,315,127,14,122,39,6,2,1,0,1
core/src/test/scala/unit/kafka/server/ServerGenerateClusterIdTest.scala,core/src/test/scala/unit/kafka/server/ServerGenerateClusterIdTest.scala,"KAFKA-12236; New meta.properties logic for KIP-500 (#9967)

This patch contains the new handling of `meta.properties` required by the KIP-500 server as specified in KIP-631. When using the self-managed quorum, the `meta.properties` file is required in each log directory with the new `version` property set to 1. It must include the `cluster.id` property and it must have a `node.id` matching that in the configuration.

The behavior of `meta.properties` for the Zookeeper-based `KafkaServer` does not change. We treat `meta.properties` as optional and as if it were `version=0`. We continue to generate the clusterId and/or the brokerId through Zookeeper as needed.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",16,5,3,149,1220,2,11,234,140,20,12,3.0,301,140,25,67,22,6,2,1,0,1
core/src/test/scala/unit/kafka/server/ServerTest.scala,core/src/test/scala/unit/kafka/server/ServerTest.scala,"KAFKA-12236; New meta.properties logic for KIP-500 (#9967)

This patch contains the new handling of `meta.properties` required by the KIP-500 server as specified in KIP-631. When using the self-managed quorum, the `meta.properties` file is required in each log directory with the new `version` property set to 1. It must include the `cluster.id` property and it must have a `node.id` matching that in the configuration.

The behavior of `meta.properties` for the Zookeeper-based `KafkaServer` does not change. We treat `meta.properties` as optional and as if it were `version=0`. We continue to generate the clusterId and/or the brokerId through Zookeeper as needed.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",2,66,0,39,280,2,2,66,66,66,1,1,66,66,66,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/DescribeProducersRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeProducersRequest.java,"KAFKA-12238; Implement `DescribeProducers` API from KIP-664 (#9979)

Implements the `DescribeProducers` API specified by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: David Jacot <djacot@confluent.io>",11,97,0,67,455,9,9,97,97,97,1,1,97,97,97,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/DescribeProducersResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeProducersResponse.java,"KAFKA-12238; Implement `DescribeProducers` API from KIP-664 (#9979)

Implements the `DescribeProducers` API specified by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.

Reviewers: David Jacot <djacot@confluent.io>",8,69,0,43,295,6,6,69,69,69,1,1,69,69,69,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/log/LogCleanerParameterizedIntegrationTest.scala,core/src/test/scala/unit/kafka/log/LogCleanerParameterizedIntegrationTest.scala,"KAFKA-10761; Kafka Raft update log start offset (#9816)

Adds support for nonzero log start offsets.

Changes to `Log`:
1. Add a new ""reason"" for increasing the log start offset. This is used by `KafkaMetadataLog` when a snapshot is generated.
2. `LogAppendInfo` should return if it was rolled because of an records append. A log is rolled when a new segment is created. This is used by `KafkaMetadataLog` to in some cases delete the created segment based on the log start offset.

Changes to `KafkaMetadataLog`:
1. Update both append functions to delete old segments based on the log start offset whenever the log is rolled.
2. Update `lastFetchedEpoch` to return the epoch of the latest snapshot whenever the log is empty.
3. Add a function that empties the log whenever the latest snapshot is greater than the replicated log. This is used when first loading the `KafkaMetadataLog` and whenever the `KafkaRaftClient` downloads a snapshot from the leader.

Changes to `KafkaRaftClient`:
1. Improve `validateFetchOffsetAndEpoch` so that it can handle fetch offset and last fetched epoch that are smaller than the log start offset. This is in addition to the existing code that check for a diverging log. This is used by the raft client to determine if the Fetch response should include a diverging epoch or a snapshot id. 
2. When a follower finishes fetching a snapshot from the leader fully truncate the local log.
3. When polling the current state the raft client should check if the state machine has generated a new snapshot and update the log start offset accordingly.

Reviewers: Jason Gustafson <jason@confluent.io>",21,3,3,229,2492,3,14,330,326,28,12,2.0,396,326,33,66,47,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsWithIdsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsWithIdsResult.java,"KAFKA-10764: Add support for returning topic IDs on create, supplying topic IDs for delete (#9684)

Updated CreateTopicResponse, DeleteTopicsRequest/Response and added some new AdminClient methods and classes. Now the newly created topic ID will be returned in CreateTopicsResult and found in TopicAndMetadataConfig, and topics can be deleted by supplying topic IDs through deleteTopicsWithIds which will return DeleteTopicsWithIdsResult.

Reviewers: dengziming <dengziming1993@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",3,54,0,19,151,3,3,54,54,54,1,1,54,54,54,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/UnknownTopicIdException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownTopicIdException.java,"KAFKA-10764: Add support for returning topic IDs on create, supplying topic IDs for delete (#9684)

Updated CreateTopicResponse, DeleteTopicsRequest/Response and added some new AdminClient methods and classes. Now the newly created topic ID will be returned in CreateTopicsResult and found in TopicAndMetadataConfig, and topics can be deleted by supplying topic IDs through deleteTopicsWithIds which will return DeleteTopicsWithIdsResult.

Reviewers: dengziming <dengziming1993@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",1,27,0,7,39,1,1,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
core/src/test/scala/integration/kafka/api/BaseAdminIntegrationTest.scala,core/src/test/scala/integration/kafka/api/BaseAdminIntegrationTest.scala,"KAFKA-10764: Add support for returning topic IDs on create, supplying topic IDs for delete (#9684)

Updated CreateTopicResponse, DeleteTopicsRequest/Response and added some new AdminClient methods and classes. Now the newly created topic ID will be returned in CreateTopicsResult and found in TopicAndMetadataConfig, and topics can be deleted by supplying topic IDs through deleteTopicsWithIds which will return DeleteTopicsWithIdsResult.

Reviewers: dengziming <dengziming1993@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",15,7,0,181,1794,1,8,239,246,24,10,3.0,283,246,28,44,16,4,2,1,0,1
core/src/test/scala/unit/kafka/server/DeleteTopicsRequestTest.scala,core/src/test/scala/unit/kafka/server/DeleteTopicsRequestTest.scala,"KAFKA-10764: Add support for returning topic IDs on create, supplying topic IDs for delete (#9684)

Updated CreateTopicResponse, DeleteTopicsRequest/Response and added some new AdminClient methods and classes. Now the newly created topic ID will be returned in CreateTopicsResult and found in TopicAndMetadataConfig, and topics can be deleted by supplying topic IDs through deleteTopicsWithIds which will return DeleteTopicsWithIdsResult.

Reviewers: dengziming <dengziming1993@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",14,56,1,140,1190,7,9,192,121,12,16,2.5,265,121,17,73,18,5,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/SessionWindowedCogroupedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/SessionWindowedCogroupedKStream.scala,"KAFKA-8744: Update Scala API to give names to processors (#9738)

As it's only API extension to match the java API with Named object with lots of duplication, I only tested the logic once.

Reviewers: Bill Bejeck <bbejeck@apache.org>",2,20,4,14,208,1,2,64,48,32,2,2.5,68,48,34,4,4,2,1,0,1,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/TimeWindowedCogroupedKStream.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/TimeWindowedCogroupedKStream.scala,"KAFKA-8744: Update Scala API to give names to processors (#9738)

As it's only API extension to match the java API with Named object with lots of duplication, I only tested the logic once.

Reviewers: Bill Bejeck <bbejeck@apache.org>",2,19,5,14,172,1,2,62,48,31,2,2.5,67,48,34,5,5,2,1,0,1,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java,"KAFKA-10658 ErrantRecordReporter.report always return completed futur… (#9525)

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",33,2,3,135,938,1,20,252,219,63,4,2.5,259,219,65,7,3,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/testutil/LogCaptureAppender.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/testutil/LogCaptureAppender.java,"KAFKA-10867: Improved task idling (#9840)

Use the new ConsumerRecords.metadata() API to implement
improved task idling as described in KIP-695

Reviewers: Guozhang Wang <guozhang@apache.org>",18,4,0,93,590,1,14,130,66,22,6,1.5,137,66,23,7,7,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchedRecords.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetchedRecords.java,"KAFKA-10866: Add metadata to ConsumerRecords (#9836)

Expose fetched metadata via the ConsumerRecords
object as described in KIP-695.

Reviewers: Guozhang Wang <guozhang@apache.org>",13,102,0,67,452,11,11,102,102,102,1,1,102,102,102,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/data/ConnectSchema.java,connect/api/src/main/java/org/apache/kafka/connect/data/ConnectSchema.java,"MINOR: Remove redundant casting and if condition from ConnectSchema (#9959)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",69,21,25,235,1815,2,24,349,235,17,21,3,458,235,22,109,25,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java,clients/src/test/java/org/apache/kafka/common/record/FileRecordsTest.java,"KAFKA-12233: Align the length passed to FileChannel by `FileRecords.writeTo` (#9970)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",61,23,0,540,5464,1,34,709,410,26,27,5,1048,410,39,339,109,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/SendBuilder.java,clients/src/main/java/org/apache/kafka/common/protocol/SendBuilder.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",26,4,0,158,937,1,21,228,194,46,5,6,279,194,56,51,24,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java,clients/src/main/java/org/apache/kafka/common/record/AbstractRecords.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",31,2,2,102,762,1,12,160,92,8,20,3.0,384,136,19,224,105,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/DefaultRecordsSend.java,clients/src/main/java/org/apache/kafka/common/record/DefaultRecordsSend.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",3,3,3,15,116,4,3,36,35,9,4,2.0,45,35,11,9,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecordsSend.java,clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecordsSend.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",9,1,1,57,446,1,5,106,99,13,8,2.0,163,99,20,57,23,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/Records.java,clients/src/main/java/org/apache/kafka/common/record/Records.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",0,1,13,20,157,0,0,105,42,6,17,3,203,53,12,98,28,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/TransferableRecords.java,clients/src/main/java/org/apache/kafka/common/record/TransferableRecords.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",0,39,0,6,53,0,0,39,39,39,1,1,39,39,39,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/UnalignedFileRecords.java,clients/src/main/java/org/apache/kafka/common/record/UnalignedFileRecords.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",3,50,0,24,162,3,3,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/UnalignedMemoryRecords.java,clients/src/main/java/org/apache/kafka/common/record/UnalignedMemoryRecords.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",6,56,0,28,205,4,4,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/UnalignedRecords.java,clients/src/main/java/org/apache/kafka/common/record/UnalignedRecords.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",1,29,0,7,45,1,1,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/protocol/SendBuilderTest.java,clients/src/test/java/org/apache/kafka/common/protocol/SendBuilderTest.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",7,33,0,116,982,1,7,166,133,42,4,1.5,170,133,42,4,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/UnalignedFileRecordsTest.java,clients/src/test/java/org/apache/kafka/common/record/UnalignedFileRecordsTest.java,"KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)

This patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API, records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Jason Gustafson <jason@confluent.io>",6,79,0,47,430,4,4,79,79,79,1,1,79,79,79,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/JsonConverterGenerator.java,generator/src/main/java/org/apache/kafka/message/JsonConverterGenerator.java,"MINOR: set initial capacity of ArrayList for all json converters (#9962)

Reviewers: Ismael Juma <ismael@juma.me.uk>",46,1,1,399,3451,1,11,442,412,88,5,1,449,412,90,7,3,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/protocol/Message.java,clients/src/main/java/org/apache/kafka/common/protocol/Message.java,"MINOR: Remove `toStruct` and `fromStruct` methods from generated protocol classes (#9960)

Update few classes that were still using the removed methods (including
tests that are no longer required).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,0,25,17,133,0,1,104,92,15,7,2,177,92,25,73,42,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/ControlRecordUtilsTest.java,clients/src/test/java/org/apache/kafka/common/record/ControlRecordUtilsTest.java,"MINOR: Remove `toStruct` and `fromStruct` methods from generated protocol classes (#9960)

Update few classes that were still using the removed methods (including
tests that are no longer required).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,1,41,369,1,3,70,65,18,4,2.5,78,65,20,8,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java,clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java,"MINOR: MessageUtil: remove some deadcode (#9931)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",43,1,3,173,1201,0,14,218,103,18,12,2.0,250,103,21,32,14,3,2,1,0,1
core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala,core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala,"MINOR: Optimize assertions in unit tests (#9955)

Reviewers: David Jacot <djacot@confluent.io>",53,2,2,528,5487,1,44,731,233,22,34,3.0,984,233,29,253,44,7,2,1,0,1
core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerTest.scala,core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerTest.scala,"MINOR: Optimize assertions in unit tests (#9955)

Reviewers: David Jacot <djacot@confluent.io>",92,2,2,837,8217,1,64,1082,877,57,19,2,1250,877,66,168,53,9,2,1,0,1
core/src/test/scala/unit/kafka/security/token/delegation/DelegationTokenManagerTest.scala,core/src/test/scala/unit/kafka/security/token/delegation/DelegationTokenManagerTest.scala,"MINOR: Optimize assertions in unit tests (#9955)

Reviewers: David Jacot <djacot@confluent.io>",18,1,1,253,2593,1,16,363,311,18,20,2.0,439,311,22,76,21,4,2,1,0,1
core/src/main/scala/kafka/coordinator/group/DelayedHeartbeat.scala,core/src/main/scala/kafka/coordinator/group/DelayedHeartbeat.scala,"MINOR: A few small group coordinator cleanups (#9952)

A few small cleanups in `GroupCoordinator` and related classes:

- Remove redundant `groupId` field from `MemberMetadata`
- Remove redundant `isStaticMember` field from `MemberMetadata`
- Fix broken log message in `GroupMetadata.loadGroup` and apply it to all loaded members
- Remove ancient TODOs and no-op methods from `GroupCoordinator`
- Move removal of static members into `GroupMetadata.remove`

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",3,2,2,12,102,3,3,36,44,2,15,1,93,44,6,57,23,4,2,1,0,1
core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala,core/src/main/scala/kafka/coordinator/group/DelayedJoin.scala,"MINOR: A few small group coordinator cleanups (#9952)

A few small cleanups in `GroupCoordinator` and related classes:

- Remove redundant `groupId` field from `MemberMetadata`
- Remove redundant `isStaticMember` field from `MemberMetadata`
- Fix broken log message in `GroupMetadata.loadGroup` and apply it to all loaded members
- Remove ancient TODOs and no-op methods from `GroupCoordinator`
- Move removal of static members into `GroupMetadata.remove`

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",8,0,1,38,264,1,6,84,62,5,16,1.0,146,62,9,62,31,4,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/group/MemberMetadataTest.scala,core/src/test/scala/unit/kafka/coordinator/group/MemberMetadataTest.scala,"MINOR: A few small group coordinator cleanups (#9952)

A few small cleanups in `GroupCoordinator` and related classes:

- Remove redundant `groupId` field from `MemberMetadata`
- Remove redundant `isStaticMember` field from `MemberMetadata`
- Fix broken log message in `GroupMetadata.loadGroup` and apply it to all loaded members
- Remove ancient TODOs and no-op methods from `GroupCoordinator`
- Move removal of static members into `GroupMetadata.remove`

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, David Jacot <djacot@confluent.io>",6,6,6,62,637,6,6,94,90,9,11,6,160,90,15,66,20,6,2,1,0,1
tests/setup.py,tests/setup.py,"MINOR: Upgrade ducktape to version 0.8.1  (#9933)

ducktape 0.8.1 was updated to include the following changes/fixes from 0.7.x branch:
* Junit reporting support
* fix for an issue where unicode characters in exception message would cause test runner to hang on py27.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",3,1,1,33,208,0,3,57,27,2,25,1,84,27,3,27,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/PartitionInfo.java,clients/src/main/java/org/apache/kafka/common/PartitionInfo.java,"KAFKA-10357: Extract setup of repartition topics from Streams partition assignor (#9848)

KIP-698: extract the code for the setup of the repartition topics from the Streams partition assignor so that it can also be called outside of a rebalance.

Reviewers: Leah Thomas <lthomas@confluent.io> , Guozhang Wang <guozhang@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",14,0,2,65,363,0,10,115,58,8,15,1,175,58,12,60,14,4,2,1,0,1
core/src/main/scala/kafka/Kafka.scala,core/src/main/scala/kafka/Kafka.scala,"MINOR: Drop enable.metadata.quorum config (#9934)

The primary purpose of this patch is to remove the internal `enable.metadata.quorum` configuration. Instead, we rely on `process.roles` to determine if the self-managed quorum has been enabled. As a part of this, I've done the following:

1. Replace the notion of ""disabled"" APIs with ""controller-only"" APIs. We previously marked some APIs which were intended only for the KIP-500 as ""disabled"" so that they would not be unintentionally exposed. For example, the Raft quorum APIs were disabled. Marking them as ""controller-only"" carries the same effect, but makes the intent that they should be only exposed by the KIP-500 controller clearer.
2. Make `ForwardingManager` optional in `KafkaServer` and `KafkaApis`. Previously we used `null` if forwarding was enabled and relied on the metadata quorum check.
3. Make `zookeeper.connect` an optional configuration if `process.roles` is defined.
4. Update raft README to remove reference to `zookeeper.conntect`

Reviewers: Colin Patrick McCabe <cmccabe@confluent.io>, Boyang Chen <boyang@confluent.io>",19,3,2,85,515,1,3,114,64,3,40,2.0,284,64,7,170,26,4,2,1,0,1
core/src/test/scala/integration/kafka/api/DescribeAuthorizedOperationsTest.scala,core/src/test/scala/integration/kafka/api/DescribeAuthorizedOperationsTest.scala,"MINOR: Refactor DescribeAuthorizedOperationsTest (#9938)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,79,67,153,1475,9,8,209,121,14,15,2,328,121,22,119,67,8,2,1,0,1
generator/src/test/java/org/apache/kafka/message/CodeBufferTest.java,generator/src/test/java/org/apache/kafka/message/CodeBufferTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",3,17,18,48,341,3,3,72,73,36,2,5.5,90,73,45,18,18,9,2,1,0,1
generator/src/test/java/org/apache/kafka/message/EntityTypeTest.java,generator/src/test/java/org/apache/kafka/message/EntityTypeTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",5,5,11,59,460,1,4,83,89,42,2,3.0,94,89,47,11,11,6,1,0,1,1
generator/src/test/java/org/apache/kafka/message/IsNullConditionalTest.java,generator/src/test/java/org/apache/kafka/message/IsNullConditionalTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",5,8,10,96,514,5,5,120,122,40,3,4,134,122,45,14,10,5,2,1,0,1
generator/src/test/java/org/apache/kafka/message/MessageDataGeneratorTest.java,generator/src/test/java/org/apache/kafka/message/MessageDataGeneratorTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",13,8,10,218,1037,1,13,257,166,64,4,3.5,284,180,71,27,14,7,1,0,1,1
generator/src/test/java/org/apache/kafka/message/MessageGeneratorTest.java,generator/src/test/java/org/apache/kafka/message/MessageGeneratorTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",6,7,9,45,356,0,5,70,59,18,4,1.5,79,59,20,9,9,2,2,1,0,1
generator/src/test/java/org/apache/kafka/message/StructRegistryTest.java,generator/src/test/java/org/apache/kafka/message/StructRegistryTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",6,12,15,121,670,2,4,148,124,37,4,1.5,164,124,41,16,15,4,2,1,0,1
generator/src/test/java/org/apache/kafka/message/VersionConditionalTest.java,generator/src/test/java/org/apache/kafka/message/VersionConditionalTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",15,18,20,209,1160,13,13,243,245,81,3,7,270,245,90,27,20,9,2,1,0,1
generator/src/test/java/org/apache/kafka/message/VersionsTest.java,generator/src/test/java/org/apache/kafka/message/VersionsTest.java,"KAFKA-12175 Migrate generator module to junit5 (#9926)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",11,13,20,80,916,2,7,107,85,21,5,1,127,85,25,20,20,4,2,1,0,1
core/src/main/scala/kafka/zk/AdminZkClient.scala,core/src/main/scala/kafka/zk/AdminZkClient.scala,"KAFKA-10869: Gate topic IDs behind IBP 2.8 (KIP-516) (#9814)

Topics processed by the controller and topics newly created will only be given topic IDs if the inter-broker protocol version on the controller is greater than 2.8. This PR also adds a kafka config to specify whether the IBP is greater or equal to 2.8. System tests have been modified to include topic ID checks for upgrade/downgrade tests. This PR also adds a new integration test file for requests/responses that are not gated by IBP (ex: metadata) 

Reviewers: dengziming <dengziming1993@gmail.com>, Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",87,13,8,294,2384,8,27,536,417,27,20,3.0,667,417,33,131,53,7,2,1,0,1
core/src/test/scala/unit/kafka/security/auth/ZkAuthorizationTest.scala,core/src/test/scala/unit/kafka/security/auth/ZkAuthorizationTest.scala,"KAFKA-10869: Gate topic IDs behind IBP 2.8 (KIP-516) (#9814)

Topics processed by the controller and topics newly created will only be given topic IDs if the inter-broker protocol version on the controller is greater than 2.8. This PR also adds a kafka config to specify whether the IBP is greater or equal to 2.8. System tests have been modified to include topic ID checks for upgrade/downgrade tests. This PR also adds a new integration test file for requests/responses that are not gated by IBP (ex: metadata) 

Reviewers: dengziming <dengziming1993@gmail.com>, Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",41,1,1,228,1773,1,16,337,317,12,29,3,595,317,21,258,88,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsTest.java,"HOTFIX: fix RocksDBMetricsTest (#9935)

Reviewers: Ismael Juma <ismael@juma.me.uk>",48,3,3,482,2401,3,47,562,283,80,7,3,597,283,85,35,10,5,2,1,0,1
core/src/test/scala/integration/kafka/api/AbstractConsumerTest.scala,core/src/test/scala/integration/kafka/api/AbstractConsumerTest.scala,"KAFKA-8460: produce records with current timestamp (#9877)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",44,4,2,295,2308,2,26,443,437,55,8,3.0,468,437,58,25,6,3,2,1,0,1
core/src/test/scala/integration/kafka/api/BaseConsumerTest.scala,core/src/test/scala/integration/kafka/api/BaseConsumerTest.scala,"KAFKA-8460: produce records with current timestamp (#9877)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",3,3,2,43,351,1,2,370,301,5,74,3.0,1836,301,25,1466,422,20,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslMultiMechanismConsumerTest.scala,core/src/test/scala/integration/kafka/api/SaslMultiMechanismConsumerTest.scala,"KAFKA-8460: produce records with current timestamp (#9877)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",3,16,8,66,503,1,3,95,86,7,13,2,145,86,11,50,13,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetrics.java,"KAFKA-9924: Add docs for RocksDB properties-based metrics (#9895)

Document the new properties-based metrics for RocksDB

Reviewers: Leah Thomas <lthomas@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",54,3,3,734,3234,0,49,803,382,100,8,11.5,919,382,115,116,38,14,1,0,1,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeClusterRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeClusterRequest.java,"KAFKA-12204; Implement DescribeCluster API in the broker (KIP-700) (#9903)

This PR implements the DescribeCluster API in the broker.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",8,76,0,46,305,8,8,76,76,76,1,1,76,76,76,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/DescribeClusterResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeClusterResponse.java,"KAFKA-12204; Implement DescribeCluster API in the broker (KIP-700) (#9903)

This PR implements the DescribeCluster API in the broker.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",6,63,0,37,304,6,6,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/DescribeClusterRequestTest.scala,core/src/test/scala/unit/kafka/server/DescribeClusterRequestTest.scala,"KAFKA-12204; Implement DescribeCluster API in the broker (KIP-700) (#9903)

This PR implements the DescribeCluster API in the broker.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",8,92,0,63,503,6,6,92,92,92,1,1,92,92,92,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsRequest.java,"MINOR: Replace ApiVersion by auto-generated protocol (#9746)

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,3,7,75,531,1,11,122,55,7,17,3,221,55,13,99,32,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java,"MINOR: Replace ApiVersion by auto-generated protocol (#9746)

Reviewers: Ismael Juma <ismael@juma.me.uk>",109,3,3,531,3430,1,36,711,259,17,42,3.5,1132,259,27,421,79,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/RequestContextTest.java,clients/src/test/java/org/apache/kafka/common/requests/RequestContextTest.java,"MINOR: Replace ApiVersion by auto-generated protocol (#9746)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,2,2,72,724,1,2,107,75,6,19,2,146,75,8,39,11,2,2,1,0,1
core/src/main/scala/kafka/server/DelayedFetch.scala,core/src/main/scala/kafka/server/DelayedFetch.scala,"KAFKA-10841: Extract conversion from LogReadResult to FetchPartitionData (#9743)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",29,1,10,132,866,1,3,201,91,5,37,2,411,91,11,210,45,6,2,1,0,1
core/src/test/scala/integration/kafka/api/AdminClientWithPoliciesIntegrationTest.scala,core/src/test/scala/integration/kafka/api/AdminClientWithPoliciesIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,9,17,138,1427,2,8,203,209,16,13,2,261,209,20,58,17,4,2,1,0,1
core/src/test/scala/integration/kafka/api/DelegationTokenEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/DelegationTokenEndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,3,4,84,724,1,6,125,96,7,19,1,189,96,10,64,27,3,2,1,0,1
core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/EndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",38,19,39,425,3341,8,31,553,273,11,49,3,1070,273,22,517,76,11,2,1,0,1
core/src/test/scala/integration/kafka/api/EndToEndClusterIdTest.scala,core/src/test/scala/integration/kafka/api/EndToEndClusterIdTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,3,4,145,1183,0,7,213,225,15,14,1.5,259,225,18,46,13,3,2,1,0,1
core/src/test/scala/integration/kafka/api/GroupCoordinatorIntegrationTest.scala,core/src/test/scala/integration/kafka/api/GroupCoordinatorIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,3,3,41,350,1,1,62,63,4,16,2.0,95,63,6,33,5,2,2,1,0,1
core/src/test/scala/integration/kafka/api/IntegrationTestHarness.scala,core/src/test/scala/integration/kafka/api/IntegrationTestHarness.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,3,3,115,954,2,7,164,73,3,49,2,351,73,7,187,41,4,2,1,0,1
core/src/test/scala/integration/kafka/api/LogAppendTimeTest.scala,core/src/test/scala/integration/kafka/api/LogAppendTimeTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,3,4,47,412,0,2,77,86,10,8,1.5,97,86,12,20,9,2,2,1,0,1
core/src/test/scala/integration/kafka/api/PlaintextEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/PlaintextEndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,3,3,59,418,0,4,88,71,10,9,1,101,71,11,13,3,1,2,1,0,1
core/src/test/scala/integration/kafka/api/ProducerCompressionTest.scala,core/src/test/scala/integration/kafka/api/ProducerCompressionTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,22,25,76,614,4,3,117,126,4,26,2.0,232,126,9,115,49,4,2,1,0,1
core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala,core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",20,19,42,153,1446,6,14,260,284,5,56,3.0,858,284,15,598,76,11,2,1,0,1
core/src/test/scala/integration/kafka/api/RackAwareAutoTopicCreationTest.scala,core/src/test/scala/integration/kafka/api/RackAwareAutoTopicCreationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,3,3,39,352,1,1,65,65,8,8,1.0,76,65,10,11,3,1,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslClientsWithInvalidCredentialsTest.scala,core/src/test/scala/integration/kafka/api/SaslClientsWithInvalidCredentialsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",25,9,20,197,1376,5,19,245,176,11,23,2,365,176,16,120,21,5,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SaslEndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,5,4,44,340,1,2,81,89,4,22,1.0,142,89,6,61,16,3,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslPlainPlaintextConsumerTest.scala,core/src/test/scala/integration/kafka/api/SaslPlainPlaintextConsumerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,3,3,35,253,1,3,58,27,3,17,1,92,27,5,34,7,2,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslPlaintextConsumerTest.scala,core/src/test/scala/integration/kafka/api/SaslPlaintextConsumerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,3,3,17,112,1,2,34,19,7,5,1,40,19,8,6,3,1,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslScramSslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SaslScramSslEndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,2,2,35,277,1,4,61,49,3,18,2.0,102,49,6,41,11,2,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslSslConsumerTest.scala,core/src/test/scala/integration/kafka/api/SaslSslConsumerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,3,3,21,153,1,2,39,22,8,5,3,45,22,9,6,3,1,2,1,0,1
core/src/test/scala/integration/kafka/api/SslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SslEndToEndAuthorizationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,47,371,0,3,83,29,6,14,1.0,119,29,8,36,10,3,2,1,0,1
core/src/test/scala/integration/kafka/api/TransactionsExpirationTest.scala,core/src/test/scala/integration/kafka/api/TransactionsExpirationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,3,3,80,718,1,4,122,122,41,3,1,126,122,42,4,3,1,2,1,0,1
core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala,core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,2,2,170,1284,0,9,216,121,13,17,1,362,121,21,146,113,9,2,1,0,1
core/src/test/scala/integration/kafka/server/MultipleListenersWithSameSecurityProtocolBaseTest.scala,core/src/test/scala/integration/kafka/server/MultipleListenersWithSameSecurityProtocolBaseTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,7,7,134,1191,1,6,191,124,13,15,2,273,124,18,82,24,5,2,1,0,1
core/src/test/scala/integration/kafka/server/ScramServerStartupTest.scala,core/src/test/scala/integration/kafka/server/ScramServerStartupTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,3,3,30,242,1,2,65,73,9,7,1,82,73,12,17,9,2,2,1,0,1
core/src/test/scala/integration/kafka/tools/MirrorMakerIntegrationTest.scala,core/src/test/scala/integration/kafka/tools/MirrorMakerIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,7,7,94,788,2,5,125,73,12,10,2.0,146,73,15,21,7,2,2,1,0,1
core/src/test/scala/kafka/metrics/LinuxIoMetricsCollectorTest.scala,core/src/test/scala/kafka/metrics/LinuxIoMetricsCollectorTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,3,6,50,420,0,3,81,84,27,3,1,88,84,29,7,6,2,2,1,0,1
core/src/test/scala/kafka/security/auth/ResourceTest.scala,core/src/test/scala/kafka/security/auth/ResourceTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,2,2,41,328,0,7,68,61,10,7,1,88,61,13,20,8,3,2,1,0,1
core/src/test/scala/kafka/security/minikdc/MiniKdcTest.scala,core/src/test/scala/kafka/security/minikdc/MiniKdcTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,3,3,26,187,1,1,46,46,23,2,1.5,49,46,24,3,3,2,2,1,0,1
core/src/test/scala/kafka/tools/CustomDeserializerTest.scala,core/src/test/scala/kafka/tools/CustomDeserializerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,4,5,38,302,2,4,65,53,9,7,3,96,53,14,31,15,4,2,1,0,1
core/src/test/scala/kafka/utils/ExitTest.scala,core/src/test/scala/kafka/utils/ExitTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,2,2,87,669,0,9,121,122,30,4,1.5,131,122,33,10,6,2,2,1,0,1
core/src/test/scala/kafka/utils/LoggingTest.scala,core/src/test/scala/kafka/utils/LoggingTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,3,3,54,349,1,5,88,37,13,7,2,113,37,16,25,20,4,2,1,0,1
core/src/test/scala/kafka/utils/ToolsUtilsTest.scala,core/src/test/scala/kafka/utils/ToolsUtilsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,2,2,23,221,0,1,45,45,22,2,1.0,47,45,24,2,2,1,2,1,0,1
core/src/test/scala/kafka/zk/ExtendedAclStoreTest.scala,core/src/test/scala/kafka/zk/ExtendedAclStoreTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,40,300,0,6,68,67,14,5,1,80,67,16,12,5,2,2,1,0,1
core/src/test/scala/kafka/zk/FeatureZNodeTest.scala,core/src/test/scala/kafka/zk/FeatureZNodeTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,9,30,70,650,2,4,102,123,34,3,1,133,123,44,31,30,10,1,0,1,1
core/src/test/scala/kafka/zk/LiteralAclStoreTest.scala,core/src/test/scala/kafka/zk/LiteralAclStoreTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,45,354,0,6,75,62,12,6,2.5,91,62,15,16,6,3,2,1,0,1
core/src/test/scala/unit/kafka/KafkaConfigTest.scala,core/src/test/scala/unit/kafka/KafkaConfigTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",26,4,4,239,2072,1,25,310,157,17,18,2.0,392,157,22,82,28,5,2,1,0,1
core/src/test/scala/unit/kafka/admin/AclCommandTest.scala,core/src/test/scala/unit/kafka/admin/AclCommandTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",38,10,12,269,2670,5,23,332,134,9,35,3,566,134,16,234,46,7,2,1,0,1
core/src/test/scala/unit/kafka/admin/AdminRackAwareTest.scala,core/src/test/scala/unit/kafka/admin/AdminRackAwareTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",19,2,2,184,1713,0,15,225,196,28,8,2.5,256,196,32,31,15,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/ConsumerGroupCommandTest.scala,core/src/test/scala/unit/kafka/admin/ConsumerGroupCommandTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",21,3,3,156,1199,1,16,211,197,13,16,2.5,276,197,17,65,18,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/DelegationTokenCommandTest.scala,core/src/test/scala/unit/kafka/admin/DelegationTokenCommandTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,4,4,99,898,1,7,146,147,13,11,2,165,147,15,19,4,2,2,1,0,1
core/src/test/scala/unit/kafka/admin/DeleteConsumerGroupsTest.scala,core/src/test/scala/unit/kafka/admin/DeleteConsumerGroupsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",32,23,25,170,1462,9,11,246,251,16,15,5,408,251,27,162,27,11,2,1,0,1
core/src/test/scala/unit/kafka/admin/DeleteOffsetsConsumerGroupCommandIntegrationTest.scala,core/src/test/scala/unit/kafka/admin/DeleteOffsetsConsumerGroupCommandIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",21,2,2,153,1019,0,18,197,199,66,3,1,209,199,70,12,10,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/DescribeConsumerGroupTest.scala,core/src/test/scala/unit/kafka/admin/DescribeConsumerGroupTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",128,30,52,506,4436,12,32,698,425,25,28,5.0,1517,511,54,819,165,29,2,1,0,1
core/src/test/scala/unit/kafka/admin/FeatureCommandTest.scala,core/src/test/scala/unit/kafka/admin/FeatureCommandTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,2,2,154,1003,0,7,241,244,60,4,1.0,249,244,62,8,5,2,2,1,0,1
core/src/test/scala/unit/kafka/admin/ListConsumerGroupTest.scala,core/src/test/scala/unit/kafka/admin/ListConsumerGroupTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,2,2,87,735,0,5,127,89,9,14,1.5,243,90,17,116,48,8,2,1,0,1
core/src/test/scala/unit/kafka/admin/RackAwareTest.scala,core/src/test/scala/unit/kafka/admin/RackAwareTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,9,6,58,523,1,3,85,82,28,3,1,92,82,31,7,6,2,2,1,0,1
core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandArgsTest.scala,core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandArgsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",32,8,13,257,1124,2,32,314,232,29,11,2,393,232,36,79,43,7,2,1,0,1
core/src/test/scala/unit/kafka/admin/UserScramCredentialsCommandTest.scala,core/src/test/scala/unit/kafka/admin/UserScramCredentialsCommandTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,5,5,100,802,2,4,137,137,68,2,2.0,142,137,71,5,5,2,1,0,1,1
core/src/test/scala/unit/kafka/api/ApiUtilsTest.scala,core/src/test/scala/unit/kafka/api/ApiUtilsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,5,15,40,357,1,2,71,84,10,7,2,100,84,14,29,15,4,2,1,0,1
core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala,core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,2,240,946,0,3,272,124,16,17,2,398,124,23,126,57,7,2,1,0,1
core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala,core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,3,3,67,555,1,7,111,61,6,18,3.0,198,61,11,87,21,5,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala,core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",24,4,4,166,1206,1,19,224,94,16,14,3.5,279,94,20,55,18,4,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala,core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,5,5,65,490,1,4,99,187,4,25,2,289,187,12,190,112,8,2,1,0,1
core/src/test/scala/unit/kafka/controller/PartitionLeaderElectionAlgorithmsTest.scala,core/src/test/scala/unit/kafka/controller/PartitionLeaderElectionAlgorithmsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,3,3,155,873,0,14,187,176,47,4,2.0,199,176,50,12,7,3,2,1,0,1
core/src/test/scala/unit/kafka/controller/PartitionStateMachineTest.scala,core/src/test/scala/unit/kafka/controller/PartitionStateMachineTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",30,13,8,430,3591,3,29,533,311,24,22,7.0,767,311,35,234,77,11,2,1,0,1
core/src/test/scala/unit/kafka/controller/ReplicaStateMachineTest.scala,core/src/test/scala/unit/kafka/controller/ReplicaStateMachineTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",49,3,3,342,2525,0,49,414,371,22,19,3,520,371,27,106,33,6,2,1,0,1
core/src/test/scala/unit/kafka/controller/TopicDeletionManagerTest.scala,core/src/test/scala/unit/kafka/controller/TopicDeletionManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,196,1811,0,5,274,232,46,6,1.0,285,232,48,11,5,2,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionLogTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionLogTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,2,2,93,733,0,4,138,109,14,10,2.0,180,109,18,42,22,4,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerRequestCompletionHandlerTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerRequestCompletionHandlerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",25,6,16,190,1459,2,25,262,164,15,18,3.0,395,164,22,133,52,7,2,1,0,1
core/src/test/scala/unit/kafka/integration/MetricsDuringTopicCreationDeletionTest.scala,core/src/test/scala/unit/kafka/integration/MetricsDuringTopicCreationDeletionTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",22,2,2,94,600,0,4,153,161,10,15,2,217,161,14,64,16,4,2,1,0,1
core/src/test/scala/unit/kafka/integration/MinIsrConfigTest.scala,core/src/test/scala/unit/kafka/integration/MinIsrConfigTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,1,1,14,103,0,1,37,37,4,9,1,46,37,5,9,2,1,2,1,0,1
core/src/test/scala/unit/kafka/log/LogCleanerIntegrationTest.scala,core/src/test/scala/unit/kafka/log/LogCleanerIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,14,13,148,1421,3,7,217,117,5,46,3.0,868,132,19,651,321,14,2,1,0,1
core/src/test/scala/unit/kafka/log/LogCleanerLagIntegrationTest.scala,core/src/test/scala/unit/kafka/log/LogCleanerLagIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,14,24,76,730,2,3,124,188,8,16,1.5,250,188,16,126,57,8,2,1,0,1
core/src/test/scala/unit/kafka/log/LogConfigTest.scala,core/src/test/scala/unit/kafka/log/LogConfigTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",24,8,8,131,982,6,13,182,93,6,30,2.0,304,93,10,122,22,4,2,1,0,1
core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala,core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",26,31,23,181,1589,4,15,240,182,11,22,2.0,373,182,17,133,31,6,2,1,0,1
core/src/test/scala/unit/kafka/log/OffsetMapTest.scala,core/src/test/scala/unit/kafka/log/OffsetMapTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",13,2,2,61,467,0,6,88,87,9,10,1.5,129,87,13,41,27,4,2,1,0,1
core/src/test/scala/unit/kafka/log/TimeIndexTest.scala,core/src/test/scala/unit/kafka/log/TimeIndexTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",13,4,4,99,732,1,10,147,97,16,9,2,176,97,20,29,13,3,2,1,0,1
core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala,core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,2,2,33,222,0,4,60,60,5,11,2,93,60,8,33,5,3,2,1,0,1
core/src/test/scala/unit/kafka/security/auth/OperationTest.scala,core/src/test/scala/unit/kafka/security/auth/OperationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,2,17,102,0,1,40,39,4,9,1,61,39,7,21,13,2,2,1,0,1
core/src/test/scala/unit/kafka/security/auth/PermissionTypeTest.scala,core/src/test/scala/unit/kafka/security/auth/PermissionTypeTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,3,8,24,153,1,2,49,39,5,10,1.0,67,39,7,18,8,2,2,1,0,1
core/src/test/scala/unit/kafka/security/auth/ResourceTypeTest.scala,core/src/test/scala/unit/kafka/security/auth/ResourceTypeTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,3,9,24,155,1,2,48,39,5,10,1.0,67,39,7,19,9,2,2,1,0,1
core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerWithZkSaslTest.scala,core/src/test/scala/unit/kafka/security/authorizer/AclAuthorizerWithZkSaslTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,4,4,142,1254,1,13,186,186,62,3,1,191,186,64,5,4,2,2,1,0,1
core/src/test/scala/unit/kafka/security/authorizer/AclEntryTest.scala,core/src/test/scala/unit/kafka/security/authorizer/AclEntryTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,4,3,23,293,1,1,47,44,5,10,1.5,76,44,8,29,9,3,2,1,0,1
core/src/test/scala/unit/kafka/security/authorizer/AuthorizerInterfaceDefaultTest.scala,core/src/test/scala/unit/kafka/security/authorizer/AuthorizerInterfaceDefaultTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,3,3,57,465,1,9,94,94,47,2,2.0,97,94,48,3,3,2,1,0,1,1
core/src/test/scala/unit/kafka/security/authorizer/AuthorizerWrapperTest.scala,core/src/test/scala/unit/kafka/security/authorizer/AuthorizerWrapperTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,15,15,74,570,3,5,106,106,53,2,4.0,121,106,60,15,15,8,1,0,1,1
core/src/test/scala/unit/kafka/security/authorizer/BaseAuthorizerTest.scala,core/src/test/scala/unit/kafka/security/authorizer/BaseAuthorizerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",16,86,86,297,3046,9,13,375,375,188,2,17.5,461,375,230,86,86,43,1,0,1,1
core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala,core/src/test/scala/unit/kafka/server/AbstractFetcherManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,3,3,94,757,0,8,136,61,12,11,2,152,61,14,16,4,1,2,1,0,1
core/src/test/scala/unit/kafka/server/AddPartitionsToTxnRequestServerTest.scala,core/src/test/scala/unit/kafka/server/AddPartitionsToTxnRequestServerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,3,3,40,311,0,3,72,76,8,9,2,102,76,11,30,12,3,1,0,1,1
core/src/test/scala/unit/kafka/server/AdvertiseBrokerTest.scala,core/src/test/scala/unit/kafka/server/AdvertiseBrokerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,3,3,64,517,0,4,93,52,6,16,2.0,147,58,9,54,20,3,2,1,0,1
core/src/test/scala/unit/kafka/server/AlterReplicaLogDirsRequestTest.scala,core/src/test/scala/unit/kafka/server/AlterReplicaLogDirsRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,2,2,95,1107,0,4,137,83,10,14,1.5,194,83,14,57,17,4,2,1,0,1
core/src/test/scala/unit/kafka/server/AlterUserScramCredentialsRequestNotAuthorizedTest.scala,core/src/test/scala/unit/kafka/server/AlterUserScramCredentialsRequestNotAuthorizedTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,79,0,49,473,5,5,79,79,79,1,1,79,79,79,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/AlterUserScramCredentialsRequestTest.scala,core/src/test/scala/unit/kafka/server/AlterUserScramCredentialsRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",37,21,57,328,3177,11,23,397,433,198,2,7.0,454,433,227,57,57,28,1,0,1,1
core/src/test/scala/unit/kafka/server/AuthHelperTest.scala,core/src/test/scala/unit/kafka/server/AuthHelperTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,2,2,97,818,0,4,142,142,71,2,1.0,144,142,72,2,2,1,1,0,1,1
core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala,core/src/test/scala/unit/kafka/server/BrokerFeaturesTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,2,2,75,635,0,5,106,106,53,2,1.0,108,106,54,2,2,1,1,0,1,1
core/src/test/scala/unit/kafka/server/ClientRequestQuotaManagerTest.scala,core/src/test/scala/unit/kafka/server/ClientRequestQuotaManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,7,7,49,443,1,2,89,89,44,2,2.5,96,89,48,7,7,4,1,0,1,1
core/src/test/scala/unit/kafka/server/ControllerMutationQuotaManagerTest.scala,core/src/test/scala/unit/kafka/server/ControllerMutationQuotaManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,8,9,157,1222,1,9,235,216,78,3,5,263,216,88,28,19,9,1,0,1,1
core/src/test/scala/unit/kafka/server/CreateTopicsRequestWithPolicyTest.scala,core/src/test/scala/unit/kafka/server/CreateTopicsRequestWithPolicyTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,1,1,109,981,0,6,161,81,12,14,2.0,231,81,16,70,38,5,2,1,0,1
core/src/test/scala/unit/kafka/server/DelayedOperationTest.scala,core/src/test/scala/unit/kafka/server/DelayedOperationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",41,42,42,316,2413,10,33,404,94,11,36,3.0,761,167,21,357,79,10,2,1,0,1
core/src/test/scala/unit/kafka/server/DelegationTokenRequestsOnPlainTextTest.scala,core/src/test/scala/unit/kafka/server/DelegationTokenRequestsOnPlainTextTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,4,4,42,390,1,3,72,76,6,12,1.5,106,76,9,34,15,3,2,1,0,1
core/src/test/scala/unit/kafka/server/DelegationTokenRequestsTest.scala,core/src/test/scala/unit/kafka/server/DelegationTokenRequestsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,5,5,87,895,1,3,133,116,11,12,1.5,196,116,16,63,42,5,2,1,0,1
core/src/test/scala/unit/kafka/server/DelegationTokenRequestsWithDisableTokenFeatureTest.scala,core/src/test/scala/unit/kafka/server/DelegationTokenRequestsWithDisableTokenFeatureTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,7,10,51,488,1,3,79,87,7,12,1.5,125,87,10,46,19,4,2,1,0,1
core/src/test/scala/unit/kafka/server/DeleteTopicsRequestWithDeletionDisabledTest.scala,core/src/test/scala/unit/kafka/server/DeleteTopicsRequestWithDeletionDisabledTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,2,2,38,309,0,2,64,59,11,6,2.0,82,59,14,18,7,3,2,1,0,1
core/src/test/scala/unit/kafka/server/DescribeLogDirsRequestTest.scala,core/src/test/scala/unit/kafka/server/DescribeLogDirsRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,3,3,49,538,1,1,76,64,6,12,1.0,105,64,9,29,9,2,2,1,0,1
core/src/test/scala/unit/kafka/server/DescribeUserScramCredentialsRequestNotAuthorizedTest.scala,core/src/test/scala/unit/kafka/server/DescribeUserScramCredentialsRequestNotAuthorizedTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,51,0,26,226,3,3,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/DescribeUserScramCredentialsRequestTest.scala,core/src/test/scala/unit/kafka/server/DescribeUserScramCredentialsRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",12,13,36,99,840,7,10,139,149,46,3,1,175,149,58,36,36,12,1,0,1,1
core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala,core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",54,12,28,341,3423,8,45,431,153,23,19,3,523,153,28,92,29,5,2,1,0,1
core/src/test/scala/unit/kafka/server/FinalizedFeatureCacheTest.scala,core/src/test/scala/unit/kafka/server/FinalizedFeatureCacheTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,4,8,75,660,2,5,114,116,38,3,3,160,116,53,46,38,15,1,0,1,1
core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala,core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,4,8,185,1464,1,11,269,225,67,4,2.0,326,225,82,57,48,14,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaMetricReporterClusterIdTest.scala,core/src/test/scala/unit/kafka/server/KafkaMetricReporterClusterIdTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,4,4,77,578,1,7,118,93,17,7,3,132,93,19,14,4,2,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaMetricsReporterTest.scala,core/src/test/scala/unit/kafka/server/KafkaMetricsReporterTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,5,5,59,500,1,9,95,95,32,3,2,102,95,34,7,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/KafkaServerTest.scala,core/src/test/scala/unit/kafka/server/KafkaServerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",31,3,4,96,785,1,11,136,62,19,7,2,150,62,21,14,7,2,2,1,0,1
core/src/test/scala/unit/kafka/server/LeaderElectionTest.scala,core/src/test/scala/unit/kafka/server/LeaderElectionTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,13,13,123,1090,3,5,179,98,3,71,2,653,98,9,474,63,7,2,1,0,1
core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala,core/src/test/scala/unit/kafka/server/OffsetsForLeaderEpochRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,87,838,0,6,126,65,11,11,2,152,65,14,26,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicaFetchTest.scala,core/src/test/scala/unit/kafka/server/ReplicaFetchTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,3,3,53,412,1,4,81,84,2,39,1,199,84,5,118,34,3,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicationQuotaManagerTest.scala,core/src/test/scala/unit/kafka/server/ReplicationQuotaManagerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,3,3,67,596,0,6,124,123,12,10,1.5,155,123,16,31,8,3,2,1,0,1
core/src/test/scala/unit/kafka/server/ReplicationQuotasTest.scala,core/src/test/scala/unit/kafka/server/ReplicationQuotasTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",17,12,12,146,1387,2,13,241,242,10,25,2,373,242,15,132,45,5,2,1,0,1
core/src/test/scala/unit/kafka/server/ServerMetricsTest.scala,core/src/test/scala/unit/kafka/server/ServerMetricsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,5,7,24,188,1,1,48,51,10,5,2,62,51,12,14,7,3,2,1,0,1
core/src/test/scala/unit/kafka/server/StopReplicaRequestTest.scala,core/src/test/scala/unit/kafka/server/StopReplicaRequestTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,2,2,53,488,0,1,78,57,10,8,1.0,96,57,12,18,5,2,2,1,0,1
core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala,core/src/test/scala/unit/kafka/server/checkpoints/LeaderEpochCheckpointFileTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,2,2,30,263,0,2,70,72,18,4,1.0,76,72,19,6,3,2,2,1,0,1
core/src/test/scala/unit/kafka/server/checkpoints/OffsetCheckpointFileTest.scala,core/src/test/scala/unit/kafka/server/checkpoints/OffsetCheckpointFileTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,3,4,75,717,1,6,134,89,19,7,2,154,89,22,20,6,3,2,1,0,1
core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala,core/src/test/scala/unit/kafka/server/epoch/LeaderEpochIntegrationTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",18,3,3,199,2109,0,11,301,283,10,31,2,399,283,13,98,19,3,2,1,0,1
core/src/test/scala/unit/kafka/tools/ConsoleProducerTest.scala,core/src/test/scala/unit/kafka/tools/ConsoleProducerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",6,2,2,91,517,0,6,118,79,8,15,2,174,79,12,56,12,4,2,1,0,1
core/src/test/scala/unit/kafka/tools/ConsumerPerformanceTest.scala,core/src/test/scala/unit/kafka/tools/ConsumerPerformanceTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,2,2,81,527,0,7,127,52,13,10,2.0,191,53,19,64,32,6,2,1,0,1
core/src/test/scala/unit/kafka/tools/MirrorMakerTest.scala,core/src/test/scala/unit/kafka/tools/MirrorMakerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,2,53,452,0,3,82,42,10,8,2.0,92,42,12,10,3,1,2,1,0,1
core/src/test/scala/unit/kafka/utils/CommandLineUtilsTest.scala,core/src/test/scala/unit/kafka/utils/CommandLineUtilsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,10,10,169,1326,4,11,223,147,20,11,2,260,147,24,37,10,3,2,1,0,1
core/src/test/scala/unit/kafka/utils/CoreUtilsTest.scala,core/src/test/scala/unit/kafka/utils/CoreUtilsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",23,6,6,211,1634,3,14,280,51,9,30,2.0,422,51,14,142,53,5,2,1,0,1
core/src/test/scala/unit/kafka/utils/JsonTest.scala,core/src/test/scala/unit/kafka/utils/JsonTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,2,2,104,1079,0,2,134,49,10,13,3,205,68,16,71,24,5,2,1,0,1
core/src/test/scala/unit/kafka/utils/PasswordEncoderTest.scala,core/src/test/scala/unit/kafka/utils/PasswordEncoderTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,5,5,92,693,1,5,124,127,31,4,1.5,137,127,34,13,7,3,2,1,0,1
core/src/test/scala/unit/kafka/utils/PoolTest.scala,core/src/test/scala/unit/kafka/utils/PoolTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,2,2,18,130,0,1,40,40,20,2,1.0,42,40,21,2,2,1,1,0,1,1
core/src/test/scala/unit/kafka/utils/QuotaUtilsTest.scala,core/src/test/scala/unit/kafka/utils/QuotaUtilsTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",12,5,6,87,762,2,12,134,138,45,3,3,147,138,49,13,7,4,2,1,0,1
core/src/test/scala/unit/kafka/utils/ShutdownableThreadTest.scala,core/src/test/scala/unit/kafka/utils/ShutdownableThreadTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,4,4,30,208,1,3,54,54,18,3,1,59,54,20,5,4,2,2,1,0,1
core/src/test/scala/unit/kafka/utils/ThrottlerTest.scala,core/src/test/scala/unit/kafka/utils/ThrottlerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,2,3,31,216,0,1,61,63,15,4,1.0,67,63,17,6,3,2,2,1,0,1
core/src/test/scala/unit/kafka/utils/TopicFilterTest.scala,core/src/test/scala/unit/kafka/utils/TopicFilterTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,2,2,24,228,0,1,100,51,4,23,3,232,51,10,132,37,6,2,1,0,1
core/src/test/scala/unit/kafka/utils/json/JsonValueTest.scala,core/src/test/scala/unit/kafka/utils/json/JsonValueTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",20,2,2,161,1313,0,20,212,210,53,4,1.0,234,210,58,22,20,6,2,1,0,1
core/src/test/scala/unit/kafka/utils/timer/TimerTaskListTest.scala,core/src/test/scala/unit/kafka/utils/timer/TimerTaskListTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,2,57,442,0,3,93,95,16,6,1.5,106,95,18,13,6,2,2,1,0,1
core/src/test/scala/unit/kafka/utils/timer/TimerTest.scala,core/src/test/scala/unit/kafka/utils/timer/TimerTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,7,7,69,536,3,5,107,111,18,6,1.5,134,111,22,27,16,4,2,1,0,1
core/src/test/scala/unit/kafka/zk/AdminZkClientTest.scala,core/src/test/scala/unit/kafka/zk/AdminZkClientTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",24,18,28,245,2529,10,15,348,323,17,21,1,444,323,21,96,28,5,2,1,0,1
core/src/test/scala/unit/kafka/zk/ReassignPartitionsZNodeTest.scala,core/src/test/scala/unit/kafka/zk/ReassignPartitionsZNodeTest.scala,"KAFKA-7341 Migrate core module to JUnit 5 (#9855)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,2,31,256,0,3,55,56,11,5,2,67,56,13,12,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotResponse.java,clients/src/main/java/org/apache/kafka/common/requests/FetchSnapshotResponse.java,"MINOR: Make data in FetchSnapshotRequest and FetchSnapshotRespponse private (#9820)

Reviewers: José Armando García Sancio <jsancio@gmail.com>, David Jacot <djacot@confluent.io>",12,8,9,76,531,2,8,129,130,64,2,3.0,138,130,69,9,9,4,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/connector/ConnectorTest.java,connect/api/src/test/java/org/apache/kafka/connect/connector/ConnectorTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",4,3,3,55,351,0,4,88,88,44,2,1.5,91,88,46,3,3,2,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/data/DateTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/DateTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,3,3,53,478,0,6,81,78,16,5,2,97,78,19,16,6,3,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/DecimalTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/DecimalTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",3,3,3,37,342,0,3,62,63,16,4,1.5,71,63,18,9,5,2,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/FieldTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/FieldTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",1,3,3,18,167,0,1,39,32,8,5,2,57,32,11,18,9,4,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/SchemaBuilderTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/SchemaBuilderTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",40,4,4,296,2921,0,38,381,287,32,12,1.0,460,287,38,79,55,7,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/SchemaProjectorTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/SchemaProjectorTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",29,45,98,381,4825,9,15,477,495,53,9,3,618,495,69,141,98,16,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/StructTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/StructTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",18,5,5,272,2452,0,18,344,222,34,10,2.0,388,222,39,44,20,4,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/TimeTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/TimeTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,3,3,54,489,0,6,83,80,17,5,2,99,80,20,16,6,3,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/TimestampTest.java,connect/api/src/test/java/org/apache/kafka/connect/data/TimestampTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",5,3,3,48,440,0,5,77,75,15,5,2,91,75,18,14,5,3,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeaderTest.java,connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeaderTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",8,7,7,76,585,0,8,108,108,54,2,2.0,115,108,58,7,7,4,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/sink/SinkConnectorTest.java,connect/api/src/test/java/org/apache/kafka/connect/sink/SinkConnectorTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",18,3,3,102,589,0,18,146,146,73,2,1.0,149,146,74,3,3,2,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/sink/SinkRecordTest.java,connect/api/src/test/java/org/apache/kafka/connect/sink/SinkRecordTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,10,10,98,1022,0,6,128,128,64,2,2.0,138,128,69,10,10,5,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/source/SourceConnectorTest.java,connect/api/src/test/java/org/apache/kafka/connect/source/SourceConnectorTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",19,3,3,107,612,0,19,152,152,76,2,1.0,155,152,78,3,3,2,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/source/SourceRecordTest.java,connect/api/src/test/java/org/apache/kafka/connect/source/SourceRecordTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",6,10,10,99,1045,0,6,129,129,64,2,2.0,139,129,70,10,10,5,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/storage/ConverterTypeTest.java,connect/api/src/test/java/org/apache/kafka/connect/storage/ConverterTypeTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",2,2,2,11,79,0,1,31,31,16,2,1.5,33,31,16,2,2,1,1,0,1,1
connect/api/src/test/java/org/apache/kafka/connect/storage/SimpleHeaderConverterTest.java,connect/api/src/test/java/org/apache/kafka/connect/storage/SimpleHeaderConverterTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",23,7,7,183,1539,0,22,236,220,79,3,3,250,220,83,14,7,5,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/util/ConnectorUtilsTest.java,connect/api/src/test/java/org/apache/kafka/connect/util/ConnectorUtilsTest.java,"KAFKA-12196: Migrate connect:api module to JUnit 5 (#9909)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",2,3,3,40,378,0,2,68,67,11,6,1.5,81,67,14,13,5,2,2,1,0,1
connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSinkConnectorTest.java,connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSinkConnectorTest.java,"KAFKA-12200: Migrate connect:file module to JUnit 5 (#9917)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",7,7,7,68,540,1,5,103,85,11,9,3,145,85,16,42,16,5,2,1,0,1
connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceConnectorTest.java,connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceConnectorTest.java,"KAFKA-12200: Migrate connect:file module to JUnit 5 (#9917)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",10,8,8,95,749,1,9,136,104,15,9,4,189,104,21,53,13,6,2,1,0,1
connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java,connect/file/src/test/java/org/apache/kafka/connect/file/FileStreamSourceTaskTest.java,"KAFKA-12200: Migrate connect:file module to JUnit 5 (#9917)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Ismael Juma <ismael@juma.me.uk>",15,7,7,179,1775,0,11,236,140,14,17,4,318,140,19,82,16,5,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/integration/MirrorConnectorsIntegrationTest.java,"KAFKA-12202 Migrate connect:mirror module to JUnit 5 (#9894)

1. Replace junit 4 APIs by junit 5
2. Remove the dependencies of junit 4 from `EmbeddedKafkaCluster`

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,2,3,5,36,0,0,23,24,12,2,1.5,26,24,13,3,3,2,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ExtractFieldTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,6,6,77,776,0,7,116,59,19,6,3.0,132,59,22,16,8,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HoistFieldTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/HoistFieldTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,5,5,33,349,0,3,61,44,12,5,3,79,44,16,18,8,4,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/InsertFieldTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",28,7,7,143,1740,0,8,203,113,23,9,4,254,113,28,51,13,6,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/MaskFieldTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",16,25,25,211,2303,6,13,253,156,51,5,4,348,159,70,95,62,19,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/RegexRouterTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/RegexRouterTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,2,2,42,316,0,7,70,70,18,4,1.5,82,70,20,12,8,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ReplaceFieldTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ReplaceFieldTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,5,5,119,1148,0,7,171,92,24,7,3,193,92,28,22,8,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/TimestampConverterTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",47,6,6,415,3822,0,47,548,370,91,6,16.5,679,370,113,131,72,22,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ValueToKeyTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/ValueToKeyTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,6,6,71,672,0,4,109,87,22,5,3,125,87,25,16,8,3,2,1,0,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatchesTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatchesTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,10,10,63,645,0,7,93,93,46,2,2.0,103,93,52,10,10,5,1,0,1,1
connect/transforms/src/test/java/org/apache/kafka/connect/transforms/util/NonEmptyListValidatorTest.java,connect/transforms/src/test/java/org/apache/kafka/connect/transforms/util/NonEmptyListValidatorTest.java,"KAFKA-12197: Migrate connect:transforms module to JUnit 5 (#9907)

Reviewers: Ismael Juma <ismael@juma.me.uk>",3,2,3,20,155,0,3,43,40,14,3,2,50,40,17,7,4,2,2,1,0,1
core/src/main/scala/kafka/server/AuthHelper.scala,core/src/main/scala/kafka/server/AuthHelper.scala,"MINOR: Move a few more methods to AuthHelper (#9913)

And move some tests to `AuthHelperTest`.

Reviewers: David Arthur <mumrah@gmail.com>",7,62,2,105,946,2,4,133,73,66,2,3.0,135,73,68,2,2,1,1,0,1,1
core/src/main/scala/kafka/utils/CoreUtils.scala,core/src/main/scala/kafka/utils/CoreUtils.scala,"MINOR: Move a few more methods to AuthHelper (#9913)

And move some tests to `AuthHelperTest`.

Reviewers: David Arthur <mumrah@gmail.com>",42,1,2,185,1625,0,17,455,684,4,116,2.0,1985,684,17,1530,288,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/ApiVersion.java,clients/src/main/java/org/apache/kafka/common/protocol/ApiVersion.java,"KAFKA-10674: Controller API version bond with forwardable APIs (#9600)

Get controller api version intersection setup for client queries. When the unsupported exception was hit in the EnvelopeResponse, close the client connection to let it rediscover the api version.

Reviewers: Jason Gustafson <jason@confluent.io>",7,13,2,38,267,1,5,67,56,34,2,2.0,69,56,34,2,2,1,0,0,0,0
core/src/main/scala/kafka/metrics/KafkaMetricsReporter.scala,core/src/main/scala/kafka/metrics/KafkaMetricsReporter.scala,"MINOR: Generalize server startup to make way for KIP-500 (#9883)

This patch attempts to generalize server initialization for KIP-500. It adds a `Server` trait which `KafkaServer` extends for the legacy Zookeeper server, and a new `KafkaRaftServer` for the new server. I have also added stubs for `KafkaRaftBroker` and `KafkaRaftController` to give a clearer idea how this will be used.

Note that this patch removes `KafkaServerStartable`, which was intended to enable custom startup logic, but was not codified into an official API and is not planned to be supported after KIP-500. 

Reviewers: Ismael Juma <ismael@juma.me.uk>, Colin P. McCabe <cmccabe@apache.org>",6,2,2,38,215,2,2,80,47,8,10,2.0,98,47,10,18,4,2,2,1,0,1
core/src/main/scala/kafka/utils/VerifiableProperties.scala,core/src/main/scala/kafka/utils/VerifiableProperties.scala,"MINOR: Generalize server startup to make way for KIP-500 (#9883)

This patch attempts to generalize server initialization for KIP-500. It adds a `Server` trait which `KafkaServer` extends for the legacy Zookeeper server, and a new `KafkaRaftServer` for the new server. I have also added stubs for `KafkaRaftBroker` and `KafkaRaftController` to give a clearer idea how this will be used.

Note that this patch removes `KafkaServerStartable`, which was intended to enable custom startup logic, but was not codified into an official API and is not planned to be supported after KIP-500. 

Reviewers: Ismael Juma <ismael@juma.me.uk>, Colin P. McCabe <cmccabe@apache.org>",42,8,0,126,975,1,22,238,171,13,18,2.0,272,171,15,34,9,2,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/KafkaRaftMetrics.java,raft/src/main/java/org/apache/kafka/raft/internals/KafkaRaftMetrics.java,"KAFKA-12161; Support raft observers with optional id (#9871)

We would like to be able to use `KafkaRaftClient` for tooling/debugging use cases. For this, we need the localId to be optional so that the client can be used more like a consumer. This is already supported in the `Fetch` protocol by setting `replicaId=-1`, which the Raft implementation checks for. We just need to alter `QuorumState` so that the `localId` is optional. The main benefit of doing this is that it saves tools the need to generate an arbitrary id (which might cause conflicts given limited Int32 space) and it lets the leader avoid any local state for these observers (such as `ReplicaState` inside `LeaderState`).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Boyang Chen <boyang@confluent.io>",21,1,1,155,1366,1,11,204,204,68,3,1,206,204,69,2,1,1,2,1,0,1
core/src/main/scala/kafka/server/DelayedCreatePartitions.scala,core/src/main/scala/kafka/server/DelayedCreatePartitions.scala,"KAFKA-12208: Rename AdminManager to ZkAdminManager (#9900)

Rename AdminManager to ZkAdminManager to emphasize the fact that it is not used by the KIP-500 code paths.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Boyang Chen <boyang@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",14,1,1,56,447,0,8,102,95,13,8,1.0,148,95,18,46,38,6,2,1,0,1
core/src/main/scala/kafka/server/DelayedDeleteTopics.scala,core/src/main/scala/kafka/server/DelayedDeleteTopics.scala,"KAFKA-12208: Rename AdminManager to ZkAdminManager (#9900)

Rename AdminManager to ZkAdminManager to emphasize the fact that it is not used by the KIP-500 code paths.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Boyang Chen <boyang@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",9,1,1,40,262,0,5,83,77,21,4,1.0,85,77,21,2,1,0,2,1,0,1
connect/json/src/test/java/org/apache/kafka/connect/json/JsonConverterConfigTest.java,connect/json/src/test/java/org/apache/kafka/connect/json/JsonConverterConfigTest.java,"KAFKA-12198: Migrate connect:json module to JUnit 5 (#9890)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",1,5,4,17,157,0,1,40,39,20,2,1.5,44,39,22,4,4,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java,clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java,"MINOR: remove unused flag 'hasIdempotentRecords' (#9884)

Reviewers: Ismael Juma <ismael@juma.me.uk>",12,13,29,47,397,4,5,80,82,4,18,5.0,315,82,18,235,80,13,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SslTransportTls12Tls13Test.java,clients/src/test/java/org/apache/kafka/common/network/SslTransportTls12Tls13Test.java,"KAFKA-12191 SslTransportTls12Tls13Test can replace 'assumeTrue' by (junit 5) conditional test (#9899)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Chia-Ping Tsai <chia7712@gmail.com>",11,5,9,111,1082,3,9,165,169,55,3,4,180,169,60,15,9,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ShellTest.java,clients/src/test/java/org/apache/kafka/common/utils/ShellTest.java,"KAFKA-12189 ShellTest can replace 'assumeTrue' by (junit 5) conditional test (#9898)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,6,12,39,338,4,4,64,45,9,7,3,99,45,14,35,18,5,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/RaftConfigTest.java,raft/src/test/java/org/apache/kafka/raft/RaftConfigTest.java,"MINOR: Initialize QuorumState lazily in RaftClient.initialize() (#9881)

It is helpful to delay initialization of the `RaftClient` configuration including the voter string until after construction. This helps in integration test cases where the voter ports may not be known until sockets are bound.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",4,0,1,50,376,0,4,76,77,25,3,1,80,77,27,4,3,1,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/RaftTestUtil.java,raft/src/test/java/org/apache/kafka/raft/RaftTestUtil.java,"MINOR: Initialize QuorumState lazily in RaftClient.initialize() (#9881)

It is helpful to delay initialization of the `RaftClient` configuration including the voter string until after construction. This helps in integration test cases where the voter ports may not be known until sockets are bound.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",3,65,0,43,347,2,2,65,65,65,1,1,65,65,65,0,0,0,0,0,0,0
connect/basic-auth-extension/src/test/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilterTest.java,connect/basic-auth-extension/src/test/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilterTest.java,"KAFKA-12201: Migrate connect:basic-auth-extensio module to JUnit 5 (#9892)

Also:
* Remove unused powermock dependency
* Remove ""examples"" from the JUnit 4 list since one module was already
converted and the other has no tests

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,2,7,166,1396,0,15,214,168,27,8,4.0,314,168,39,100,84,12,2,1,0,1
core/src/main/scala/kafka/log/TimeIndex.scala,core/src/main/scala/kafka/log/TimeIndex.scala,"MINOR: fix typo in TimeIndex (#9834)

fix typos in TimeIndex

Co-authored-by: wenbingshen <oliver.shen999@gmail.com>
Reviewers: Boyang Chen <boyang@confluent.io>",27,3,3,117,759,1,11,229,208,11,21,2,331,208,16,102,35,5,2,1,0,1
metadata/src/main/java/org/apache/kafka/metadata/BrokerState.java,metadata/src/main/java/org/apache/kafka/metadata/BrokerState.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",5,110,0,34,210,4,4,110,110,110,1,1,110,110,110,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/MetadataParseException.java,metadata/src/main/java/org/apache/kafka/metadata/MetadataParseException.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",1,29,0,7,37,1,1,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/MetadataParser.java,metadata/src/main/java/org/apache/kafka/metadata/MetadataParser.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",10,115,0,66,491,4,4,115,115,115,1,1,115,115,115,0,0,0,0,0,0,0
metadata/src/main/java/org/apache/kafka/metadata/VersionRange.java,metadata/src/main/java/org/apache/kafka/metadata/VersionRange.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",12,70,0,40,244,7,7,70,70,70,1,1,70,70,70,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/metadata/BrokerStateTest.java,metadata/src/test/java/org/apache/kafka/metadata/BrokerStateTest.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",3,43,0,21,155,2,2,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
metadata/src/test/java/org/apache/kafka/metadata/VersionRangeTest.java,metadata/src/test/java/org/apache/kafka/metadata/VersionRangeTest.java,"KAFKA-12183: Add the KIP-631 metadata record definitions (#9876)

Add the metadata gradle module, which will contain the metadata record
definitions, and other metadata-related broker-side code.

Add MetadataParser, MetadataParseException, etc.

Reviewers: José Armando García Sancio <jsancio@gmail.com>, Ismael Juma <ismael@juma.me.uk>, David Arthur <mumrah@gmail.com>",4,61,0,39,424,4,4,61,61,61,1,1,61,61,61,0,0,0,0,0,0,0
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/TestUtils.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/TestUtils.java,"KAFKA-10304: Refactor MM2 integration tests (#9224)

Co-authored-by: Ning Zhang <nzhang1220@fb.com>
Reviewers: Mickael Maison <mickael.maison@gmail.com>",4,11,0,23,197,1,2,46,35,23,2,1.0,46,35,23,0,0,0,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/MockTimeTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/MockTimeTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,3,3,33,257,0,4,58,56,19,3,2,63,56,21,5,3,2,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/TestTopicsTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/TestTopicsTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",36,10,10,384,4131,4,30,454,412,57,8,2.5,559,412,70,105,66,13,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverAtLeastOnceTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverAtLeastOnceTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",1,26,0,7,44,1,1,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
streams/test-utils/src/test/java/org/apache/kafka/streams/test/ConsumerRecordFactoryTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/ConsumerRecordFactoryTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",31,4,4,206,1885,0,25,273,265,46,6,2.0,307,265,51,34,26,6,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/MockProcessorContextAPITest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/MockProcessorContextAPITest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",39,1,1,272,3011,0,7,353,353,176,2,1.0,354,353,177,1,1,0,1,0,1,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/MockProcessorContextStateStoreTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/MockProcessorContextStateStoreTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",30,17,30,154,1260,5,2,187,200,94,2,7.0,217,200,108,30,30,15,1,0,1,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/OutputVerifierTest.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/OutputVerifierTest.java,"KAFKA-12171: Migrate streams:test-utils module to JUnit 5 (#9856)

* replace `org.junit.Assert` by `org.junit.jupiter.api.Assertions`
* replace `org.junit` by `org.junit.jupiter.api`
* replace `org.junit.runners.Parameterized` by `org.junit.jupiter.params.ParameterizedTest`
* replace `org.junit.runners.Parameterized.Parameters` by `org.junit.jupiter.params.provider.{Arguments, MethodSource}`
* replace `Before` by `BeforeEach`
* replace `After` by `AfterEach`

Reviewers: Ismael Juma <ismael@juma.me.uk>",67,2,2,362,2619,0,67,452,584,113,4,1.5,729,584,182,277,275,69,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/CommonClientConfigsTest.java,clients/src/test/java/org/apache/kafka/clients/CommonClientConfigsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,2,2,59,441,0,3,85,85,28,3,1,88,85,29,3,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/InFlightRequestsTest.java,clients/src/test/java/org/apache/kafka/clients/InFlightRequestsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,5,5,86,777,0,9,128,60,26,5,5,150,60,30,22,11,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/MetadataCacheTest.java,clients/src/test/java/org/apache/kafka/clients/MetadataCacheTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,4,4,53,507,0,1,85,85,42,2,1.5,89,85,44,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/MetadataTest.java,clients/src/test/java/org/apache/kafka/clients/MetadataTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",33,15,15,655,7032,2,26,891,212,18,49,4,1599,212,33,708,288,14,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsResultTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,6,6,84,721,0,7,118,118,59,2,2.0,124,118,62,6,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsResultTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,6,6,88,794,2,3,118,118,59,2,3.0,124,118,62,6,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/MemberDescriptionTest.java,clients/src/test/java/org/apache/kafka/clients/admin/MemberDescriptionTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,67,407,0,3,102,102,51,2,1.5,105,102,52,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupOptionsTest.java,clients/src/test/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupOptionsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,3,3,15,129,0,1,39,38,10,4,2.0,50,38,12,11,8,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupResultTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,6,6,85,732,0,7,119,119,60,2,2.0,125,119,62,6,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/internals/AdminMetadataManagerTest.java,clients/src/test/java/org/apache/kafka/clients/admin/internals/AdminMetadataManagerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,6,12,66,690,1,4,101,107,50,2,2.0,113,107,56,12,12,6,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadataTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,5,5,55,408,0,5,87,44,29,3,2,93,44,31,6,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,5,6,58,536,1,4,104,105,35,3,1,111,105,37,7,6,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/OffsetAndMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/OffsetAndMetadataTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,3,3,36,302,0,5,67,61,13,5,3,104,61,21,37,22,7,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",23,5,5,253,2870,0,20,341,217,43,8,3.0,472,217,59,131,54,16,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,3,3,251,2787,0,17,338,209,34,10,2.0,510,209,51,172,71,17,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,2,60,579,0,4,89,89,44,2,1.5,91,89,46,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",230,24,37,2448,23320,8,158,3198,284,22,144,3.0,6000,364,42,2802,259,19,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerMetadataTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",12,4,4,128,1361,0,9,175,164,18,10,2.0,198,164,20,23,6,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",26,8,8,338,2960,1,21,429,125,12,35,3,591,125,17,162,41,5,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocolTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerProtocolTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,5,5,174,1827,0,11,226,118,21,11,6,373,118,34,147,36,13,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/CooperativeConsumerCoordinatorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/CooperativeConsumerCoordinatorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,25,0,7,48,1,1,25,25,25,1,1,25,25,25,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/EagerConsumerCoordinatorTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/EagerConsumerCoordinatorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,26,0,7,48,1,1,26,26,26,1,1,26,26,26,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/HeartbeatTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/HeartbeatTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,6,6,84,674,0,8,122,45,12,10,3.0,154,45,15,32,8,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/OffsetForLeaderEpochClientTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/OffsetForLeaderEpochClientTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,4,4,137,1370,0,8,181,166,30,6,2.0,205,166,34,24,13,4,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapterTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,4,4,122,1202,0,12,171,173,43,4,1.5,178,173,44,7,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/RequestFutureTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/RequestFutureTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",22,5,5,176,1341,0,22,242,180,48,5,3,272,182,54,30,14,6,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java,clients/src/test/java/org/apache/kafka/clients/producer/ProducerRecordTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,4,4,45,442,0,2,78,52,11,7,2,100,52,14,22,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/RoundRobinPartitionerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/RoundRobinPartitionerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",12,5,5,91,1010,1,3,128,124,43,3,4,161,124,54,33,28,11,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/UniformStickyPartitionerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/UniformStickyPartitionerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",27,6,6,149,1512,1,3,207,207,104,2,3.0,213,207,106,6,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/BufferPoolTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/BufferPoolTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",38,25,25,322,2580,4,21,432,170,13,33,5,694,170,21,262,35,8,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/DefaultPartitionerTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/DefaultPartitionerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,3,3,30,336,1,1,51,54,3,18,3.0,214,54,12,163,65,9,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",22,24,24,221,1964,5,13,310,205,44,7,4,368,205,53,58,24,8,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/StickyPartitionCacheTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/StickyPartitionCacheTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,3,3,74,789,0,2,111,110,37,3,2,118,110,39,7,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/ClusterTest.java,clients/src/test/java/org/apache/kafka/common/ClusterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,66,747,0,2,92,46,23,4,2.5,106,48,26,14,9,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/KafkaFutureTest.java,clients/src/test/java/org/apache/kafka/common/KafkaFutureTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,11,19,158,1409,1,10,199,164,25,8,3.0,250,164,31,51,26,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/PartitionInfoTest.java,clients/src/test/java/org/apache/kafka/common/PartitionInfoTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,5,4,20,198,1,1,42,36,10,4,3.0,62,36,16,20,9,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/TopicPartitionTest.java,clients/src/test/java/org/apache/kafka/common/TopicPartitionTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,5,5,29,245,1,3,63,61,16,4,2.0,80,61,20,17,10,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java,clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,8,8,112,1076,0,8,145,110,16,9,7,219,110,24,74,17,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/acl/AclOperationTest.java,clients/src/test/java/org/apache/kafka/common/acl/AclOperationTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,10,11,64,575,4,5,89,86,13,7,1,104,86,15,15,11,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/acl/AclPermissionTypeTest.java,clients/src/test/java/org/apache/kafka/common/acl/AclPermissionTypeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,8,10,54,451,3,5,80,80,16,5,1,92,80,18,12,10,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/acl/ResourcePatternFilterTest.java,clients/src/test/java/org/apache/kafka/common/acl/ResourcePatternFilterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",24,4,4,133,1034,0,24,176,164,59,3,4,208,164,69,32,28,11,2,1,0,1
clients/src/test/java/org/apache/kafka/common/acl/ResourcePatternTest.java,clients/src/test/java/org/apache/kafka/common/acl/ResourcePatternTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,3,3,25,212,0,4,49,40,12,4,4.0,65,40,16,16,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/cache/LRUCacheTest.java,clients/src/test/java/org/apache/kafka/common/cache/LRUCacheTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,58,554,0,3,92,93,31,3,2,100,93,33,8,5,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/ConfigDefTest.java,clients/src/test/java/org/apache/kafka/common/config/ConfigDefTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",60,7,7,592,6253,0,44,714,180,22,32,2.0,809,191,25,95,29,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/ConfigResourceTest.java,clients/src/test/java/org/apache/kafka/common/config/ConfigResourceTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,4,4,22,223,1,3,45,45,15,3,1,50,45,17,5,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java,clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,6,6,112,930,0,13,149,117,30,5,1,156,117,31,7,6,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/SaslConfigsTest.java,clients/src/test/java/org/apache/kafka/common/config/SaslConfigsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,3,3,95,979,0,11,125,124,42,3,2,144,124,48,19,16,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/provider/DirectoryConfigProviderTest.java,clients/src/test/java/org/apache/kafka/common/config/provider/DirectoryConfigProviderTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,8,8,114,1050,0,11,149,149,50,3,3,160,149,53,11,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/provider/FileConfigProviderTest.java,clients/src/test/java/org/apache/kafka/common/config/provider/FileConfigProviderTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,6,6,67,517,0,8,97,95,24,4,2.5,110,95,28,13,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/config/provider/MockFileConfigProvider.java,clients/src/test/java/org/apache/kafka/common/config/provider/MockFileConfigProvider.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,2,2,40,290,0,4,64,35,21,3,1,66,35,22,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java,clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,6,6,127,1326,0,11,173,173,86,2,1.5,179,173,90,6,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/feature/FinalizedVersionRangeTest.java,clients/src/test/java/org/apache/kafka/common/feature/FinalizedVersionRangeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,4,4,46,555,0,4,80,80,40,2,1.5,84,80,42,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/feature/SupportedVersionRangeTest.java,clients/src/test/java/org/apache/kafka/common/feature/SupportedVersionRangeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,5,5,95,806,0,6,142,142,71,2,1.5,147,142,74,5,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java,clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,7,7,154,1399,0,11,236,224,59,4,2.5,253,224,63,17,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/internals/PartitionStatesTest.java,clients/src/test/java/org/apache/kafka/common/internals/PartitionStatesTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,2,2,164,1835,0,8,211,219,53,4,3.0,227,219,57,16,10,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/internals/TopicTest.java,clients/src/test/java/org/apache/kafka/common/internals/TopicTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,4,4,67,594,0,5,105,84,35,3,2,143,84,48,38,34,13,2,1,0,1
clients/src/test/java/org/apache/kafka/common/memory/GarbageCollectedMemoryPoolTest.java,clients/src/test/java/org/apache/kafka/common/memory/GarbageCollectedMemoryPoolTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,24,28,126,957,3,14,175,163,44,4,7.0,244,163,61,69,41,17,2,1,0,1
clients/src/test/java/org/apache/kafka/common/message/ApiMessageTypeTest.java,clients/src/test/java/org/apache/kafka/common/message/ApiMessageTypeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,13,16,84,837,2,6,123,76,25,5,2,143,76,29,20,16,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/JmxReporterTest.java,clients/src/test/java/org/apache/kafka/common/metrics/JmxReporterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,4,4,147,1628,0,5,197,61,12,17,3,256,64,15,59,13,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/KafkaMbeanTest.java,clients/src/test/java/org/apache/kafka/common/metrics/KafkaMbeanTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,17,27,119,997,3,13,155,165,39,4,2.5,187,165,47,32,27,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/KafkaMetricsContextTest.java,clients/src/test/java/org/apache/kafka/common/metrics/KafkaMetricsContextTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,7,7,56,472,0,6,89,89,44,2,3.0,96,89,48,7,7,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java,clients/src/test/java/org/apache/kafka/common/metrics/SensorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,13,15,274,2820,2,13,373,94,31,12,3.0,440,94,37,67,19,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/TokenBucketTest.java,clients/src/test/java/org/apache/kafka/common/metrics/TokenBucketTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,4,4,49,529,0,3,93,93,46,2,2.0,97,93,48,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/internals/IntGaugeSuiteTest.java,clients/src/test/java/org/apache/kafka/common/metrics/internals/IntGaugeSuiteTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,3,3,72,627,0,4,97,97,48,2,1.5,100,97,50,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/internals/MetricsUtilsTest.java,clients/src/test/java/org/apache/kafka/common/metrics/internals/MetricsUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,3,3,18,163,0,2,40,40,20,2,1.5,43,40,22,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/stats/FrequenciesTest.java,clients/src/test/java/org/apache/kafka/common/metrics/stats/FrequenciesTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,7,7,117,1313,0,9,159,159,23,7,3,191,159,27,32,12,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/stats/HistogramTest.java,clients/src/test/java/org/apache/kafka/common/metrics/stats/HistogramTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,39,39,146,1878,3,6,176,74,22,8,3.0,236,80,30,60,39,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/stats/MeterTest.java,clients/src/test/java/org/apache/kafka/common/metrics/stats/MeterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,2,2,47,456,0,1,73,73,24,3,1,76,73,25,3,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/KafkaChannelTest.java,clients/src/test/java/org/apache/kafka/common/network/KafkaChannelTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,6,6,79,871,0,2,112,107,28,4,3.0,124,107,31,12,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/NetworkReceiveTest.java,clients/src/test/java/org/apache/kafka/common/network/NetworkReceiveTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,4,4,43,416,0,1,72,72,36,2,1.5,76,72,38,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/NetworkTestUtils.java,clients/src/test/java/org/apache/kafka/common/network/NetworkTestUtils.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,4,4,88,843,2,9,122,80,7,17,2,175,80,10,53,9,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java,clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",131,50,56,919,8148,14,65,1173,292,19,61,2,1672,292,27,499,68,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SslVersionsTransportLayerTest.java,clients/src/test/java/org/apache/kafka/common/network/SslVersionsTransportLayerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,30,41,108,1309,5,6,172,183,57,3,1,214,183,71,42,41,14,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java,clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",12,10,10,60,467,6,7,90,63,13,7,2,106,63,15,16,10,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/MessageUtilTest.java,clients/src/test/java/org/apache/kafka/common/protocol/MessageUtilTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,8,10,55,571,0,4,80,59,13,6,2.5,114,59,19,34,20,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/ProtoUtilsTest.java,clients/src/test/java/org/apache/kafka/common/protocol/ProtoUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,9,11,30,153,1,1,76,35,10,8,1.0,100,35,12,24,11,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/types/ProtocolSerializationTest.java,clients/src/test/java/org/apache/kafka/common/protocol/types/ProtocolSerializationTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",40,21,22,382,3439,8,22,437,96,20,22,3.0,527,96,24,90,27,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/types/RawTaggedFieldWriterTest.java,clients/src/test/java/org/apache/kafka/common/protocol/types/RawTaggedFieldWriterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,6,8,72,702,0,4,97,99,48,2,2.5,105,99,52,8,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/types/StructTest.java,clients/src/test/java/org/apache/kafka/common/protocol/types/StructTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,3,3,63,625,0,1,88,84,29,3,2,91,84,30,3,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/types/TypeTest.java,clients/src/test/java/org/apache/kafka/common/protocol/types/TypeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,4,4,66,593,0,6,93,93,46,2,2.5,97,93,48,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/AbstractLegacyRecordBatchTest.java,clients/src/test/java/org/apache/kafka/common/record/AbstractLegacyRecordBatchTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",18,5,5,187,1967,0,11,251,167,42,6,2.0,269,167,45,18,12,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/ByteBufferLogInputStreamTest.java,clients/src/test/java/org/apache/kafka/common/record/ByteBufferLogInputStreamTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,6,6,83,934,0,4,122,110,15,8,4.5,210,110,26,88,51,11,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/CompressionRatioEstimatorTest.java,clients/src/test/java/org/apache/kafka/common/record/CompressionRatioEstimatorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,2,2,29,211,0,1,53,53,26,2,1.0,55,53,28,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/ControlRecordTypeTest.java,clients/src/test/java/org/apache/kafka/common/record/ControlRecordTypeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,2,2,25,183,0,2,48,37,10,5,2,69,37,14,21,12,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/DefaultRecordTest.java,clients/src/test/java/org/apache/kafka/common/record/DefaultRecordTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",21,7,7,401,3447,0,20,484,235,44,11,2,577,235,52,93,50,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/EndTransactionMarkerTest.java,clients/src/test/java/org/apache/kafka/common/record/EndTransactionMarkerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,3,3,49,380,0,6,75,70,19,4,1.5,86,70,22,11,8,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java,clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",30,73,42,250,2419,17,13,321,205,40,8,4.5,387,214,48,66,42,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/LazyDownConversionRecordsTest.java,clients/src/test/java/org/apache/kafka/common/record/LazyDownConversionRecordsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,85,98,184,1881,9,7,243,203,40,6,5.0,455,203,76,212,99,35,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/LegacyRecordTest.java,clients/src/test/java/org/apache/kafka/common/record/LegacyRecordTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,77,65,112,1030,11,7,142,87,12,12,2.5,268,87,22,126,65,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/MultiRecordsSendTest.java,clients/src/test/java/org/apache/kafka/common/record/MultiRecordsSendTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,3,3,48,392,0,3,80,79,16,5,2,91,79,18,11,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java,clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,3,3,55,511,0,4,87,87,14,6,2.0,107,87,18,20,12,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/replica/ReplicaSelectorTest.java,clients/src/test/java/org/apache/kafka/common/replica/ReplicaSelectorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,7,7,56,547,1,5,88,88,44,2,3.0,95,88,48,7,7,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,2,2,66,664,0,2,88,88,44,2,1.5,90,88,45,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,2,2,35,299,0,1,57,57,28,2,1.5,59,57,30,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ByteBufferChannelTest.java,clients/src/test/java/org/apache/kafka/common/requests/ByteBufferChannelTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,1,1,60,567,0,3,87,48,29,3,1,88,48,29,1,1,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/CreateAclsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/CreateAclsRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,4,4,65,715,1,6,101,91,13,8,5.0,140,91,18,39,11,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/DeleteAclsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/DeleteAclsRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,4,4,78,821,1,7,117,87,15,8,5.5,157,87,20,40,14,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/DeleteAclsResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/DeleteAclsResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,3,3,95,787,0,4,130,107,19,7,6,231,107,33,101,64,14,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/DeleteGroupsResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/DeleteGroupsResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,53,417,0,3,80,79,27,3,2,85,79,28,5,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/DescribeAclsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/DescribeAclsRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,3,3,73,744,0,8,107,96,12,9,6,161,96,18,54,19,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/DescribeAclsResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/DescribeAclsResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,3,3,122,1018,0,9,164,86,16,10,4.5,219,88,22,55,23,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/HeartbeatRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/HeartbeatRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,2,2,16,120,0,1,36,34,12,3,2,45,34,15,9,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ListOffsetsRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/ListOffsetsRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,3,3,121,1034,0,4,148,147,30,5,2,196,147,39,48,42,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/MetadataRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/MetadataRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,5,5,43,415,0,3,68,55,17,4,2.0,85,55,21,17,12,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/RequestHeaderTest.java,clients/src/test/java/org/apache/kafka/common/requests/RequestHeaderTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,2,2,63,532,0,4,96,76,8,12,1.5,147,76,12,51,14,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/SyncGroupRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/SyncGroupRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,2,2,16,120,0,1,36,34,12,3,2,45,34,15,9,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/UpdateFeaturesRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/UpdateFeaturesRequestTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,2,2,30,250,0,1,56,56,28,2,1.5,58,56,29,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/UpdateFeaturesResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/UpdateFeaturesResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,2,2,34,286,0,1,61,61,30,2,1.5,63,61,32,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/WriteTxnMarkersResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/WriteTxnMarkersResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,4,4,34,332,0,2,60,60,20,3,3,68,60,23,8,4,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/resource/ResourceFilterTest.java,clients/src/test/java/org/apache/kafka/common/resource/ResourceFilterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,4,4,54,382,0,9,82,163,27,3,4,188,163,63,106,102,35,2,1,0,1
clients/src/test/java/org/apache/kafka/common/resource/ResourceTypeTest.java,clients/src/test/java/org/apache/kafka/common/resource/ResourceTypeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,10,10,59,485,3,5,84,81,10,8,1.0,103,81,13,19,10,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/JaasContextTest.java,clients/src/test/java/org/apache/kafka/common/security/JaasContextTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",37,10,10,242,2019,1,28,303,257,28,11,3,375,257,34,72,20,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/SaslExtensionsTest.java,clients/src/test/java/org/apache/kafka/common/security/SaslExtensionsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,5,5,29,247,0,3,53,52,18,3,3,60,52,20,7,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/auth/KafkaPrincipalTest.java,clients/src/test/java/org/apache/kafka/common/security/auth/KafkaPrincipalTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,5,4,13,105,1,1,34,42,8,4,1.5,53,42,13,19,9,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,6,6,110,1076,0,8,145,114,13,11,2,195,114,18,50,25,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/LoginManagerTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/LoginManagerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,8,8,93,834,0,5,129,129,32,4,2.5,150,129,38,21,12,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailureNoDelayTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailureNoDelayTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailurePositiveDelayTest.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorFailurePositiveDelayTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/kerberos/KerberosNameTest.java,clients/src/test/java/org/apache/kafka/common/security/kerberos/KerberosNameTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,4,4,99,752,0,5,141,59,20,7,2,160,59,23,19,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/kerberos/KerberosRuleTest.java,clients/src/test/java/org/apache/kafka/common/security/kerberos/KerberosRuleTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,25,267,0,1,49,49,24,2,1.5,52,49,26,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerExtensionsValidatorCallbackTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerExtensionsValidatorCallbackTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,5,5,56,564,0,4,91,90,30,3,2,98,90,33,7,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModuleTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModuleTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,6,6,307,3071,0,11,439,308,73,6,7.5,512,308,85,73,28,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerSaslClienCallbackHandlerTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerSaslClienCallbackHandlerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,3,3,74,491,0,4,102,102,34,3,2,113,102,38,11,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenCallbackTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenCallbackTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,4,4,52,312,0,3,78,78,39,2,1.5,82,78,41,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerValidatorCallbackTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerValidatorCallbackTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,5,5,52,322,0,3,78,78,39,2,1.5,83,78,42,5,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,4,4,85,740,0,10,125,65,25,5,2,136,65,27,11,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,3,3,110,787,0,9,151,125,38,4,2.0,160,125,40,9,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServerTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,13,13,173,1455,4,13,225,113,25,9,4,264,113,29,39,13,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshConfigTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshConfigTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,3,3,23,231,0,1,43,43,14,3,1,47,43,16,4,3,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshingLoginTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshingLoginTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",48,4,4,489,3887,0,26,775,615,111,7,2,843,615,120,68,58,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJwsTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJwsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,5,5,129,1334,0,10,159,156,40,4,1.5,169,156,42,10,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredLoginCallbackHandlerTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredLoginCallbackHandlerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,6,6,129,1349,2,7,161,127,27,6,2.5,177,127,30,16,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredValidatorCallbackHandlerTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredValidatorCallbackHandlerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,4,4,148,1283,0,13,182,182,46,4,1.5,191,182,48,9,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationUtilsTest.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",60,11,13,211,1969,2,14,245,248,61,4,2.5,263,248,66,18,13,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/plain/internals/PlainSaslServerTest.java,clients/src/test/java/org/apache/kafka/common/security/plain/internals/PlainSaslServerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,5,5,80,808,0,6,117,75,13,9,1,133,75,15,16,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramCredentialUtilsTest.java,clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramCredentialUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,17,16,66,684,2,7,98,97,16,6,4.5,133,97,22,35,16,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramFormatterTest.java,clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramFormatterTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,3,3,56,620,0,2,93,94,12,8,3.5,127,94,16,34,7,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramMessagesTest.java,clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramMessagesTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",35,11,11,250,2211,3,17,353,348,29,12,3.5,411,348,34,58,11,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramSaslServerTest.java,clients/src/test/java/org/apache/kafka/common/security/scram/internals/ScramSaslServerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,7,7,48,469,2,5,77,74,10,8,2.0,100,74,12,23,7,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactoryTest.java,clients/src/test/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactoryTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,20,20,269,1460,5,14,324,324,162,2,4.5,344,324,172,20,20,10,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/SslFactoryTest.java,clients/src/test/java/org/apache/kafka/common/security/ssl/SslFactoryTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",38,44,68,449,3759,17,27,546,71,22,25,4,857,90,34,311,85,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/SslPrincipalMapperTest.java,clients/src/test/java/org/apache/kafka/common/security/ssl/SslPrincipalMapperTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,3,3,89,526,0,8,128,85,32,4,2.0,179,85,45,51,48,13,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/Tls12SslFactoryTest.java,clients/src/test/java/org/apache/kafka/common/security/ssl/Tls12SslFactoryTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/Tls13SslFactoryTest.java,clients/src/test/java/org/apache/kafka/common/security/ssl/Tls13SslFactoryTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,27,0,9,64,1,1,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/AbstractIteratorTest.java,clients/src/test/java/org/apache/kafka/common/utils/AbstractIteratorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,5,5,46,356,0,4,71,54,8,9,2,92,54,10,21,7,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/AppInfoParserTest.java,clients/src/test/java/org/apache/kafka/common/utils/AppInfoParserTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,9,9,57,500,0,6,88,88,44,2,2.5,97,88,48,9,9,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ByteBufferInputStreamTest.java,clients/src/test/java/org/apache/kafka/common/utils/ByteBufferInputStreamTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,3,3,24,241,0,1,49,49,24,2,2.5,52,49,26,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ByteBufferOutputStreamTest.java,clients/src/test/java/org/apache/kafka/common/utils/ByteBufferOutputStreamTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,3,3,73,551,0,9,106,101,26,4,1.5,112,101,28,6,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ByteBufferUnmapperTest.java,clients/src/test/java/org/apache/kafka/common/utils/ByteBufferUnmapperTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,1,1,16,130,0,1,41,41,14,3,1,44,41,15,3,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ByteUtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/ByteUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,4,4,261,3379,0,15,311,222,52,6,2.0,319,222,53,8,4,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/BytesTest.java,clients/src/test/java/org/apache/kafka/common/utils/BytesTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,4,4,52,621,0,3,84,84,42,2,1.5,88,84,44,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ChecksumsTest.java,clients/src/test/java/org/apache/kafka/common/utils/ChecksumsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,4,4,58,554,2,6,93,93,46,2,2.5,97,93,48,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/CircularIteratorTest.java,clients/src/test/java/org/apache/kafka/common/utils/CircularIteratorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,6,6,39,365,0,3,67,66,22,3,2,75,66,25,8,6,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/CollectionUtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/CollectionUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,4,4,39,377,0,2,65,65,32,2,1.5,69,65,34,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/Crc32CTest.java,clients/src/test/java/org/apache/kafka/common/utils/Crc32CTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,4,4,26,244,1,2,52,59,10,5,3,81,59,16,29,18,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ExitTest.java,clients/src/test/java/org/apache/kafka/common/utils/ExitTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,2,2,65,428,0,4,88,88,44,2,1.5,90,88,45,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/FixedOrderMapTest.java,clients/src/test/java/org/apache/kafka/common/utils/FixedOrderMapTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,13,23,39,379,3,3,62,86,21,3,1,99,86,33,37,23,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/FlattenedIteratorTest.java,clients/src/test/java/org/apache/kafka/common/utils/FlattenedIteratorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,2,2,72,762,0,6,115,115,58,2,1.5,117,115,58,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashMultiCollectionTest.java,clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashMultiCollectionTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",16,18,21,139,1244,2,8,170,163,42,4,5.0,199,163,50,29,21,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/JavaTest.java,clients/src/test/java/org/apache/kafka/common/utils/JavaTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,8,8,59,435,0,5,89,56,22,4,4.5,107,56,27,18,8,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/LoggingSignalHandlerTest.java,clients/src/test/java/org/apache/kafka/common/utils/LoggingSignalHandlerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,1,1,8,47,0,1,28,28,14,2,1.0,29,28,14,1,1,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/MappedIteratorTest.java,clients/src/test/java/org/apache/kafka/common/utils/MappedIteratorTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,2,2,32,323,0,2,61,61,30,2,1.5,63,61,32,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/MockTimeTest.java,clients/src/test/java/org/apache/kafka/common/utils/MockTimeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,4,7,28,213,0,3,50,50,12,4,3.0,73,50,18,23,14,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/SanitizerTest.java,clients/src/test/java/org/apache/kafka/common/utils/SanitizerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,4,4,55,382,0,4,82,48,16,5,2,89,48,18,7,4,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,10,10,70,563,1,7,106,63,26,4,3.0,117,63,29,11,10,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/ThreadUtilsTest.java,clients/src/test/java/org/apache/kafka/common/utils/ThreadUtilsTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,3,3,59,380,0,4,89,89,44,2,1.5,92,89,46,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/TimeTest.java,clients/src/test/java/org/apache/kafka/common/utils/TimeTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,4,4,54,405,0,2,83,83,42,2,1.5,87,83,44,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/TimerTest.java,clients/src/test/java/org/apache/kafka/common/utils/TimerTest.java,"KAFKA-7340: Migrate clients module to JUnit 5 (#9874)

* Use the packages/classes from JUnit 5
* Move description in `assert` methods to last parameter
* Convert parameterized tests so that they work with JUnit 5
* Remove `hamcrest`, it didn't seem to add much value
* Fix `Utils.mkEntry` to have correct `equals` implementation
* Add a missing `@Test` annotation in `SslSelectorTest` override
* Adjust regex in `SaslAuthenticatorTest` due to small change in the
assert failure string in JUnit 5

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,4,4,104,898,0,7,151,127,50,3,2,155,127,52,4,4,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsRebalanceListenerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsRebalanceListenerTest.java,"MINOR: Add restoration time tracking (#9830)

Add Stream restoration time tracking log

Reviewers: John Roesler <vvcephei@apache.org>",11,5,1,139,1068,2,11,185,74,37,5,4,188,74,38,3,2,1,2,1,0,1
log4j-appender/src/test/java/org/apache/kafka/log4jappender/KafkaLog4jAppenderTest.java,log4j-appender/src/test/java/org/apache/kafka/log4jappender/KafkaLog4jAppenderTest.java,"MINOR: Remove unnecessary assertDoesNotThrow (#9854)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,2,3,162,1294,2,17,218,98,31,7,3,295,103,42,77,55,11,2,1,0,1
streams/examples/src/test/java/org/apache/kafka/streams/examples/docs/DeveloperGuideTesting.java,streams/examples/src/test/java/org/apache/kafka/streams/examples/docs/DeveloperGuideTesting.java,"KAFKA-12172 Migrate streams:examples module to JUnit 5 (#9857)

This PR includes following changes.
1. replace org.junit.Assert by org.junit.jupiter.api.Assertions
2. replace org.junit by org.junit.jupiter.api
3. replace Before by BeforeEach
4. replace After by AfterEach

Reviewers: Ismael Juma <ismael@confluent.io>",15,5,5,136,1351,0,12,183,187,20,9,2,219,187,24,36,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/PrincipalDeserializationException.java,clients/src/main/java/org/apache/kafka/common/errors/PrincipalDeserializationException.java,"KAFKA-12168; Move envelope request parsing out of SocketServer (#9850)

Prior to this patch, envelope handling was a shared responsibility between `SocketServer` and `KafkaApis`.  The former was responsible for parsing and validation, while the latter was responsible for authorization. This patch consolidates logic in `KafkaApis` so that envelope requests follow the normal request flow.

Reviewers: David Jacot <djacot@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",2,5,0,10,57,1,2,34,29,17,2,1.0,34,29,17,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/Readable.java,clients/src/main/java/org/apache/kafka/common/protocol/Readable.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",7,4,0,48,323,1,5,77,57,8,10,2.0,104,57,10,27,21,3,1,0,1,1
clients/src/main/java/org/apache/kafka/common/protocol/Writable.java,clients/src/main/java/org/apache/kafka/common/protocol/Writable.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",4,6,0,32,226,1,3,57,71,6,10,2.0,108,71,11,51,48,5,1,0,1,1
clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java,clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",30,6,0,141,1041,1,29,196,48,13,15,1,234,48,16,38,34,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java,clients/src/main/java/org/apache/kafka/common/protocol/types/Struct.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",135,20,0,417,2975,4,90,606,227,14,42,2.0,738,227,18,132,33,3,2,1,0,1
generator/src/main/java/org/apache/kafka/message/FieldType.java,generator/src/main/java/org/apache/kafka/message/FieldType.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",82,22,0,353,1499,4,67,487,281,35,14,2.0,522,281,37,35,24,2,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MessageSpecType.java,generator/src/main/java/org/apache/kafka/message/MessageSpecType.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",0,18,0,14,60,0,0,52,31,13,4,1.0,53,31,13,1,1,0,2,1,0,1
generator/src/main/java/org/apache/kafka/message/MetadataJsonConvertersGenerator.java,generator/src/main/java/org/apache/kafka/message/MetadataJsonConvertersGenerator.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",10,123,0,95,660,6,6,123,123,123,1,1,123,123,123,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/SchemaGenerator.java,generator/src/main/java/org/apache/kafka/message/SchemaGenerator.java,"KAFKA-12180: Implement the KIP-631 message generator changes

* Implement the uint16 type
* Implement MetadataRecordType and MetadataJsonConverters

Reviewers: Jason Gustafson <jason@confluent.io>",73,6,0,280,2073,1,10,371,243,29,13,1,400,243,31,29,15,2,2,1,0,1
connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilter.java,connect/basic-auth-extension/src/main/java/org/apache/kafka/connect/rest/basic/auth/extension/JaasBasicAuthFilter.java,"KAFKA-10895: Attempt to prevent JAAS config from being overwritten for basic auth filter in Connect (#9806)

If a connector, converter, etc. invokes [Configuration::setConfiguration](https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/Configuration.html#setConfiguration-javax.security.auth.login.Configuration-), it will cause the Connect basic auth filter to use that JAAS config instead of the one configured at startup with the `-Djava.security.auth.login.config` JVM property. This can cause requests to the worker to succeed initially but start to be rejected after the JVM's global JAAS config is altered.

To alleviate this the current PR instructs the Connect Worker to cache the JVM's global JAAS configuration at the beginning (as soon as the `BasicAuthSecurityRestExtension` class is loaded), and use that for all future authentication. 

Existing tests for the JAAS basic auth filter are modified to work with the new internal logic. The `testEmptyCredentialsFile` test is corrected to actually operate on an empty credentials file (instead of a non-existent credentials file, which it currently operates on). A new test is added to ensure that, even if the global JAAS config is overwritten, the basic auth filter will use the first one it could find.

Reviewers: Greg Harris <gregh@confluent.io>, Konstantine Karantasis <k.karantasis@gmail.com>",17,13,3,117,803,2,5,156,100,31,5,3,192,100,38,36,25,7,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/converters/NumberConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/NumberConverterTest.java,"MINOR: Substitute assertEquals(null) with assertNull (#9852)

Reviewers: David Jacot <djacot@confluent.io>",9,1,1,71,568,1,8,107,105,27,4,1.0,117,105,29,10,8,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaStatusBackingStoreFormatTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaStatusBackingStoreFormatTest.java,"MINOR: Substitute assertEquals(null) with assertNull (#9852)

Reviewers: David Jacot <djacot@confluent.io>",9,4,3,239,2460,3,9,303,227,101,3,4,314,235,105,11,8,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ConnectUtilsTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ConnectUtilsTest.java,"MINOR: Substitute assertEquals(null) with assertNull (#9852)

Reviewers: David Jacot <djacot@confluent.io>",5,1,1,75,747,1,5,104,63,21,5,3,112,63,22,8,5,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/LoggingContextTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/LoggingContextTest.java,"MINOR: Substitute assertEquals(null) with assertNull (#9852)

Reviewers: David Jacot <djacot@confluent.io>",18,1,1,156,1076,1,14,208,207,69,3,1,215,207,72,7,6,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/converters/ByteArrayConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/ByteArrayConverterTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",8,7,4,63,442,2,8,93,90,31,3,5,102,90,34,9,5,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",10,4,4,87,589,2,8,122,74,41,3,4,126,74,42,4,4,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",24,3,2,274,2739,1,21,367,198,41,9,3,408,198,45,41,20,5,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperatorTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",41,12,11,361,2957,5,29,457,308,57,8,3.0,517,308,65,60,47,8,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoaderTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoaderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",13,3,2,95,659,1,9,132,67,22,6,2.0,138,69,23,6,2,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",39,6,5,380,2789,2,38,483,191,69,7,6,532,192,76,49,17,7,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/InternalRequestSignatureTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/InternalRequestSignatureTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",10,9,6,115,983,3,10,154,151,77,2,4.0,160,151,80,6,6,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/LoggingResourceTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/LoggingResourceTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",8,7,6,167,1718,3,8,196,195,98,2,4.0,202,195,101,6,6,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ConvertingFutureCallbackTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ConvertingFutureCallbackTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",27,8,12,199,1327,3,17,238,242,119,2,3.5,250,242,125,12,12,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/NamedTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/NamedTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",4,3,2,25,180,1,2,49,48,24,2,2.0,51,48,26,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/WindowTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/WindowTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",10,5,4,58,386,2,10,89,85,22,4,1.0,110,85,28,21,17,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/WindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/WindowsTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",6,5,4,36,202,2,6,63,62,8,8,2.0,103,62,13,40,15,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedTableImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedTableImplTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",24,45,45,324,2616,15,24,459,133,14,32,4.5,884,141,28,425,93,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",7,3,2,69,797,1,7,123,124,41,3,1,142,124,47,19,17,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedCogroupedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedCogroupedKStreamImplTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",18,28,20,273,2934,10,18,337,332,84,4,1.0,361,332,90,24,20,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImplTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",22,29,28,250,2341,12,22,305,264,19,16,3.0,510,264,32,205,62,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",9,5,4,79,850,2,9,137,122,23,6,2.0,161,122,27,24,17,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImplTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",18,27,26,273,2732,10,18,325,144,18,18,3.5,537,144,30,212,58,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/UnlimitedWindowTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/UnlimitedWindowTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",2,3,2,19,156,1,2,41,42,14,3,1,60,42,20,19,17,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySchemaTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySchemaTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",5,7,6,62,524,3,5,90,73,22,4,5.5,112,73,28,22,10,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerdeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerdeTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",12,4,2,98,949,1,12,136,91,45,3,3,148,91,49,12,10,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerdeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerdeTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",5,6,8,58,639,2,5,84,86,21,4,4.0,100,86,25,16,8,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/FailOnInvalidTimestampTest.java,streams/src/test/java/org/apache/kafka/streams/processor/FailOnInvalidTimestampTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",2,4,2,17,129,1,2,38,36,13,3,3,46,36,15,8,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsEnforcerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/CopartitionedTopicsEnforcerTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",12,6,7,156,1433,2,12,209,137,17,12,5.0,335,137,28,126,25,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopicConfigTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",10,7,6,77,736,3,10,116,122,12,10,3.0,229,122,23,113,79,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestoreCallbackAdapterTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestoreCallbackAdapterTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",7,5,4,106,1023,2,6,148,147,74,2,3.0,152,147,76,4,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",51,2,2,941,10881,1,29,1175,258,27,44,4.5,2632,739,60,1457,722,33,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",26,11,10,298,2896,5,24,369,262,15,24,3.5,591,262,25,222,94,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentInfoTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",14,7,4,139,1378,2,14,176,107,10,17,2,313,107,18,137,60,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",26,7,6,350,2383,2,22,401,184,20,20,3.5,651,207,33,250,56,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/StateSerdesTest.java,streams/src/test/java/org/apache/kafka/streams/state/StateSerdesTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",14,16,16,105,822,8,12,139,89,23,6,2.0,165,89,28,26,16,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlySessionStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlySessionStoreTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",13,11,10,124,1259,5,12,164,121,12,14,3.5,219,121,16,55,10,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/FilteredCacheIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/FilteredCacheIteratorTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",13,3,2,99,758,1,8,134,118,17,8,2.5,153,118,19,19,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/GlobalStateStoreProviderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/GlobalStateStoreProviderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",20,4,2,204,1741,1,13,239,123,22,11,3,267,129,24,28,7,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueStoreBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueStoreBuilderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",12,9,8,120,1081,4,12,156,141,26,6,6.5,190,141,32,34,11,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",17,4,2,215,2826,1,16,271,189,14,20,3.5,437,189,22,166,32,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",13,12,8,100,828,4,12,134,75,11,12,6.5,207,75,17,73,13,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreBuilderTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",13,10,8,143,1226,4,13,182,135,30,6,6.0,214,135,36,32,16,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/SystemTestUtilTest.java,streams/src/test/java/org/apache/kafka/streams/tests/SystemTestUtilTest.java,"MINOR: replace test ""expected"" parameter by assertThrows (#9520)

This PR includes following changes.

1. @Test(expected = Exception.class) is replaced by assertThrows
2. remove reference to org.scalatest.Assertions
3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD
4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX
5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout

Reviewers: Ismael Juma <ismael@confluent.io>",6,7,6,46,332,3,6,76,75,38,2,4.0,82,75,41,6,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java,"MINOR: revise error message from TransactionalRequestResult#await (#9843)

Reviewers: Gwen Shapira <cshapi@gmail.com>",15,1,1,59,350,1,9,93,64,13,7,1,107,64,15,14,9,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/DeserializationExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/DeserializationExceptionHandler.java,"KAFKA-9566: Improve DeserializationExceptionHandler JavaDocs (#9837)

Reviewers: John Roesler <john@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",1,6,0,19,132,0,1,66,60,33,2,1.5,66,60,33,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/Cluster.java,clients/src/main/java/org/apache/kafka/common/Cluster.java,"KAFKA-10894; Ensure PartitionInfo replicas are not null in client quota callback (#9802)

Previously offline replicas were included as `null` in the array of replicas in `PartitionInfo` when populated by the `MetadataCache` for the purpose of the client quota callback. This patch instead initializes empty non-null nodes, which is consistent with how `PartitionInfo` is constructed by the clients in `MetadataResponse`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",53,2,2,232,1978,0,29,383,102,11,36,3.0,661,102,18,278,49,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ByteBufferInputStream.java,clients/src/main/java/org/apache/kafka/common/utils/ByteBufferInputStream.java,"KAFKA-10768 Make ByteBufferInputStream.read(byte[], int, int) to follow the contract (#9761)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,3,0,26,155,1,3,51,49,6,8,1.5,96,49,12,45,22,6,1,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java,clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesRequest.java,"MINOR: Use top-level error in `UpdateFeaturesRequest.getErrorResponse` (#9781)

The current `getErrorResponse` sets all of the feature errors, but does not set a top-level error. It seems like the whole point of having the top-level error is so that it could be used in cases like this. This patch also updates `errorCounts` in `UpdateFeaturesResponse` so that the top-level error is included.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,9,19,46,283,1,8,76,95,19,4,2.5,108,95,27,32,19,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java,clients/src/main/java/org/apache/kafka/common/requests/UpdateFeaturesResponse.java,"MINOR: Use top-level error in `UpdateFeaturesRequest.getErrorResponse` (#9781)

The current `getErrorResponse` sets all of the feature errors, but does not set a top-level error. It seems like the whole point of having the top-level error is so that it could be used in cases like this. This patch also updates `errorCounts` in `UpdateFeaturesResponse` so that the top-level error is included.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,11,10,63,496,3,8,99,109,25,4,2.5,125,109,31,26,16,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/PositionOutOfRangeException.java,clients/src/main/java/org/apache/kafka/common/errors/PositionOutOfRangeException.java,"KAFKA-10427:  Fetch snapshot API (#9553)

Implements the code necessary for the leader to response to fetch snapshot requests and for the follower to fetch snapshots. This API is described in more detail in KIP-630: https://cwiki.apache.org/confluence/display/KAFKA/KIP-630%3A+Kafka+Raft+Snapshot.  More specifically, this patch includes the following changes:

Leader Changes:
1. Raft leader response to FetchSnapshot request by reading the local snapshot and sending the requested bytes in the response. This implementation currently copies the bytes to memory. This will be fixed in a future PR.

Follower Changes:
1. Raft followers will start fetching snapshot if the leader sends a Fetch response that includes a SnapshotId.

2. Raft followers send FetchSnapshot requests if there is a pending download. The same timer is used for both Fetch and FetchSnapshot requests.

3. Raft follower handle FetchSnapshot responses by comping the bytes to the pending SnapshotWriter. This implementation doesn't fix the replicated log after the snapshot has been downloaded. This will be implemented in a future PR.

Reviewers: Jason Gustafson <jason@confluent.io>",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/SnapshotNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/SnapshotNotFoundException.java,"KAFKA-10427:  Fetch snapshot API (#9553)

Implements the code necessary for the leader to response to fetch snapshot requests and for the follower to fetch snapshots. This API is described in more detail in KIP-630: https://cwiki.apache.org/confluence/display/KAFKA/KIP-630%3A+Kafka+Raft+Snapshot.  More specifically, this patch includes the following changes:

Leader Changes:
1. Raft leader response to FetchSnapshot request by reading the local snapshot and sending the requested bytes in the response. This implementation currently copies the bytes to memory. This will be fixed in a future PR.

Follower Changes:
1. Raft followers will start fetching snapshot if the leader sends a Fetch response that includes a SnapshotId.

2. Raft followers send FetchSnapshot requests if there is a pending download. The same timer is used for both Fetch and FetchSnapshot requests.

3. Raft follower handle FetchSnapshot responses by comping the bytes to the pending SnapshotWriter. This implementation doesn't fix the replicated log after the snapshot has been downloaded. This will be implemented in a future PR.

Reviewers: Jason Gustafson <jason@confluent.io>",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java,"MINOR: Use ApiUtils' methods static imported consistently (#9763)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",13,3,3,56,338,1,7,140,140,47,3,1,144,140,48,4,3,1,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java,streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java,"KAFKA-10815 EosTestDriver#verifyAllTransactionFinished should break loop if all partitions are verified (#9706)

Reviewers: Matthias J. Sax <mjsax@apache.org>",99,1,1,525,5831,1,16,654,470,38,17,4,1068,470,63,414,132,24,2,1,0,1
streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"MINOR: Kafka Streams updates for 2.7.0 release (#9773)

Reviewer: Matthias J. Sax <matthias@confluent.io>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"MINOR: Kafka Streams updates for 2.7.0 release (#9773)

Reviewer: Matthias J. Sax <matthias@confluent.io>",87,622,0,507,4649,24,24,622,622,622,1,1,622,622,622,0,0,0,0,0,0,0
streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"MINOR: Kafka Streams updates for 2.7.0 release (#9773)

Reviewer: Matthias J. Sax <matthias@confluent.io>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-27/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Kafka Streams updates for 2.7.0 release (#9773)

Reviewer: Matthias J. Sax <matthias@confluent.io>",9,100,0,60,487,1,1,100,100,100,1,1,100,100,100,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/AbstractRequestResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AbstractRequestResponse.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",0,6,1,5,34,0,0,24,66,3,8,1.0,101,66,13,77,48,10,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/RaftMessageQueue.java,raft/src/main/java/org/apache/kafka/raft/RaftMessageQueue.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",0,58,0,7,38,0,0,58,58,58,1,1,58,58,58,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/RaftRequest.java,raft/src/main/java/org/apache/kafka/raft/RaftRequest.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",9,5,0,58,296,0,9,88,83,44,2,2.0,88,83,44,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/RequestManager.java,raft/src/main/java/org/apache/kafka/raft/RequestManager.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",39,11,7,164,945,6,20,214,210,107,2,5.0,221,210,110,7,7,4,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/BlockingMessageQueue.java,raft/src/main/java/org/apache/kafka/raft/internals/BlockingMessageQueue.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",8,76,0,50,294,5,5,76,76,76,1,1,76,76,76,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockMessageQueue.java,raft/src/test/java/org/apache/kafka/raft/MockMessageQueue.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",7,67,0,40,235,6,6,67,67,67,1,1,67,67,67,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/RequestManagerTest.java,raft/src/test/java/org/apache/kafka/raft/RequestManagerTest.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",5,2,3,92,737,3,5,137,138,46,3,2,143,138,48,6,3,2,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/BlockingMessageQueueTest.java,raft/src/test/java/org/apache/kafka/raft/internals/BlockingMessageQueueTest.java,"KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel (#9732)

This patch contains the following improvements:

- Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling
- Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings
- Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel` 
- Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.

Reviewers: David Arthur <mumrah@gmail.com>",2,59,0,34,291,2,2,59,59,59,1,1,59,59,59,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java,clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",37,1,1,147,901,1,19,194,207,24,8,1.0,233,207,29,39,18,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Frequencies.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Frequencies.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",24,1,1,112,816,1,10,192,192,64,3,1,195,192,65,3,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslClient.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslClient.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",36,1,1,192,1377,1,14,247,239,27,9,3,283,239,31,36,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslServer.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslServer.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",37,1,1,207,1547,1,14,265,212,19,14,3.5,325,212,23,60,16,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/util/SSLUtils.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/util/SSLUtils.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",21,2,2,96,927,1,9,171,152,17,10,2.0,207,152,21,36,15,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableAbstractJoinValueGetterSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableAbstractJoinValueGetterSupplier.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,1,1,22,194,1,2,43,45,7,6,2.0,63,45,10,20,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinMerger.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinMerger.java,"MINOR: Replace Collection.toArray(new T[size]) by Collection.toArray(new T[0]) (#9750)

This PR is based on the research of https://shipilev.net/blog/2016/arrays-wisdom-ancients

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,1,1,96,754,1,9,131,56,10,13,3,185,56,14,54,13,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/TopicDescription.java,"KAFKA-10547; Add TopicId in MetadataResponse (#9622)

Includes:
- Bump the version of MetadataRequest and MetadataResponse, add topicId in MetadataResponse
- Alter describeTopic in AdminClientTopicService and ZookeeperTopicService
- TopicMetadata is cached in MetadataCache, so we need to add topicId to MetadataCache
- MetadataCache is updated by UpdateMetadataRequest, bump the version of UpdateMetadataReq and UpdateMetadataResp, add topicId in UpdateMetadataReq.

Reviewers: Justine Olshan <jolshan@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",17,12,0,65,452,3,11,130,56,12,11,2,158,56,14,28,13,3,1,0,1,1
clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java,clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java,"KAFKA-10547; Add TopicId in MetadataResponse (#9622)

Includes:
- Bump the version of MetadataRequest and MetadataResponse, add topicId in MetadataResponse
- Alter describeTopic in AdminClientTopicService and ZookeeperTopicService
- TopicMetadata is cached in MetadataCache, so we need to add topicId to MetadataCache
- MetadataCache is updated by UpdateMetadataRequest, bump the version of UpdateMetadataReq and UpdateMetadataResp, add topicId in UpdateMetadataReq.

Reviewers: Justine Olshan <jolshan@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",69,18,3,354,2509,7,44,489,86,10,51,3,1615,176,32,1126,314,22,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java,clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataRequest.java,"KAFKA-10547; Add TopicId in MetadataResponse (#9622)

Includes:
- Bump the version of MetadataRequest and MetadataResponse, add topicId in MetadataResponse
- Alter describeTopic in AdminClientTopicService and ZookeeperTopicService
- TopicMetadata is cached in MetadataCache, so we need to add topicId to MetadataCache
- MetadataCache is updated by UpdateMetadataRequest, bump the version of UpdateMetadataReq and UpdateMetadataResp, add topicId in UpdateMetadataReq.

Reviewers: Justine Olshan <jolshan@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",36,20,4,173,1373,6,16,227,291,7,33,2,1149,291,35,922,396,28,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/SecurityUtils.java,clients/src/main/java/org/apache/kafka/common/utils/SecurityUtils.java,"KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic (KIP-679) (#9485)

Includes:
- New API to authorize by resource type
- Default implementation for the method that supports super users and ACLs
- Optimized implementation in AclAuthorizer that supports ACLs, super users and allow.everyone.if.no.acl.found
- Benchmarks and tests
- InitProducerIdRequest authorized for Cluster:IdempotentWrite or WRITE to any topic, ProduceRequest authorized only for topic even if idempotent

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",34,30,0,135,1078,2,15,179,44,26,7,2,186,44,27,7,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/server/authorizer/Authorizer.java,clients/src/main/java/org/apache/kafka/server/authorizer/Authorizer.java,"KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic (KIP-679) (#9485)

Includes:
- New API to authorize by resource type
- Default implementation for the method that supports super users and ACLs
- Optimized implementation in AclAuthorizer that supports ACLs, super users and allow.everyone.if.no.acl.found
- Benchmarks and tests
- InitProducerIdRequest authorized for Cluster:IdempotentWrite or WRITE to any topic, ProduceRequest authorized only for topic even if idempotent

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",25,150,5,120,1113,1,1,287,145,72,4,3.0,300,150,75,13,8,3,1,0,1,1
core/src/main/scala/kafka/security/authorizer/AuthorizerWrapper.scala,core/src/main/scala/kafka/security/authorizer/AuthorizerWrapper.scala,"KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic (KIP-679) (#9485)

Includes:
- New API to authorize by resource type
- Default implementation for the method that supports super users and ACLs
- Optimized implementation in AclAuthorizer that supports ACLs, super users and allow.everyone.if.no.acl.found
- Benchmarks and tests
- InitProducerIdRequest authorized for Cluster:IdempotentWrite or WRITE to any topic, ProduceRequest authorized only for topic even if idempotent

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",45,51,6,175,1742,7,15,223,136,37,6,4.5,244,136,41,21,6,4,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/acl/AclAuthorizerBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/acl/AclAuthorizerBenchmark.java,"KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic (KIP-679) (#9485)

Includes:
- New API to authorize by resource type
- Default implementation for the method that supports super users and ACLs
- Optimized implementation in AclAuthorizer that supports ACLs, super users and allow.everyone.if.no.acl.found
- Benchmarks and tests
- InitProducerIdRequest authorized for Cluster:IdempotentWrite or WRITE to any topic, ProduceRequest authorized only for topic even if idempotent

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",25,97,28,181,1751,9,10,236,119,34,7,1,288,119,41,52,28,7,2,1,0,1
connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java,connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java,"KAFKA-10846: Grow buffer in FileSourceStreamTask only when needed (#9735)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",45,24,7,195,1318,4,11,251,176,13,19,3,343,176,18,92,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochRequest.java,"KAFKA-10740; Replace OffsetsForLeaderEpochRequest.PartitionData with automated protocol (#9689)

This patch follows up https://github.com/apache/kafka/pull/9547. It refactors AbstractFetcherThread and its descendants to use `OffsetForLeaderEpochRequestData.OffsetForLeaderPartition` instead of `OffsetsForLeaderEpochRequest.PartitionData`. The patch relies on existing tests.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",13,2,40,81,648,4,11,134,175,6,21,3,486,175,23,352,133,17,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamJoinedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamJoinedInternal.java,"KAFKA-9126: KIP-689: StreamJoined changelog configuration (#9708)

Add withLoggingEnabled and withLoggingDisabled for StreamJoined
to give StreamJoined the same flexibility as Materialized

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",10,9,0,37,204,2,10,69,60,34,2,1.5,69,60,34,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsRequest.java,"MINOR: Simplify ApiKeys by relying on ApiMessageType (#9748)

* The naming for `ListOffsets` was inconsistent, in some places it was `ListOffset` and in others
it was `ListOffsets`. Picked the latter since it was used in metrics and the protocol documentation
and made it consistent.
* Removed unused methods in ApiKeys.
* Deleted `CommonFields`.
* Added `lowestSupportedVersion` and `highestSupportedVersion` to `ApiMessageType`
* Removed tests in `MessageTest` that are no longer relevant.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",25,38,38,141,1091,20,16,183,114,5,35,3,813,131,23,630,224,18,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ListOffsetsResponse.java,"MINOR: Simplify ApiKeys by relying on ApiMessageType (#9748)

* The naming for `ListOffsets` was inconsistent, in some places it was `ListOffset` and in others
it was `ListOffsets`. Picked the latter since it was used in metrics and the protocol documentation
and made it consistent.
* Removed unused methods in ApiKeys.
* Deleted `CommonFields`.
* Added `lowestSupportedVersion` and `highestSupportedVersion` to `ApiMessageType`
* Removed tests in `MessageTest` that are no longer relevant.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,13,13,66,471,10,9,111,108,4,31,2,530,108,17,419,222,14,2,1,0,1
core/src/main/scala/kafka/log/IndexEntry.scala,core/src/main/scala/kafka/log/IndexEntry.scala,"MINOR: Simplify ApiKeys by relying on ApiMessageType (#9748)

* The naming for `ListOffsets` was inconsistent, in some places it was `ListOffset` and in others
it was `ListOffsets`. Picked the latter since it was used in metrics and the protocol documentation
and made it consistent.
* Removed unused methods in ApiKeys.
* Deleted `CommonFields`.
* Added `lowestSupportedVersion` and `highestSupportedVersion` to `ApiMessageType`
* Removed tests in `MessageTest` that are no longer relevant.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,2,2,17,100,0,0,52,40,6,8,2.0,84,40,10,32,21,4,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ListOffsetRequestBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ListOffsetRequestBenchmark.java,"MINOR: Simplify ApiKeys by relying on ApiMessageType (#9748)

* The naming for `ListOffsets` was inconsistent, in some places it was `ListOffset` and in others
it was `ListOffsets`. Picked the latter since it was used in metrics and the protocol documentation
and made it consistent.
* Removed unused methods in ApiKeys.
* Deleted `CommonFields`.
* Added `lowestSupportedVersion` and `highestSupportedVersion` to `ApiMessageType`
* Removed tests in `MessageTest` that are no longer relevant.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",4,6,6,54,479,1,2,80,80,40,2,3.5,86,80,43,6,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ByteBufferSend.java,clients/src/main/java/org/apache/kafka/common/network/ByteBufferSend.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",11,2,3,54,288,2,8,84,54,5,17,2,154,54,9,70,15,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/NetworkSend.java,clients/src/main/java/org/apache/kafka/common/network/NetworkSend.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",5,1,2,25,121,2,5,49,26,5,10,1.0,87,26,9,38,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/Send.java,clients/src/main/java/org/apache/kafka/common/network/Send.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,1,2,7,42,0,0,45,41,4,11,2,99,41,9,54,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/TransferableChannel.java,clients/src/main/java/org/apache/kafka/common/network/TransferableChannel.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,51,0,8,63,0,0,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java,clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,5,28,21,146,0,0,93,86,6,15,2,146,86,10,53,28,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/MultiRecordsSend.java,clients/src/main/java/org/apache/kafka/common/record/MultiRecordsSend.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",19,3,2,87,525,2,9,141,100,13,11,4,208,100,19,67,19,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordsSend.java,clients/src/main/java/org/apache/kafka/common/record/RecordsSend.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",10,5,6,44,274,2,5,84,77,14,6,4.5,123,77,20,39,9,6,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ByteBufferChannel.java,clients/src/test/java/org/apache/kafka/common/requests/ByteBufferChannel.java,"MINOR: make Send and Receive work with TransferableChannel rather than Gat… (#9516)

This PR introduces a new interface 'TransferableChannel' to replace GatheringByteChannel to avoid casting in write path. `TransferableChannel ` extends GatheringByteChannel with the minimal set of methods required by the Send interface. Supporting TLS and efficient zero copy transfers are the main reasons for the additional methods.

Co-authored-by: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,20,12,51,340,4,9,79,71,26,3,6,98,71,33,19,12,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedStreamAggregateBuilder.java,"KAFKA-10417: Update Cogrouped processor to work with suppress() and joins (#9727)

Correct the implementation of the Cogroup
processor to implement KTableProcessorSupplier.

Change the cogrouped processor from PassThrough to
KTablePassThrough to allow for sending old values.
KTablePassThrough extends KTableProcessorSupplier instead of
ProcessorSupplier to implement sending old values and the view() method.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>",52,48,11,255,2157,6,9,296,146,33,9,8,430,146,48,134,62,15,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java,"KAFKA-10417: Update Cogrouped processor to work with suppress() and joins (#9727)

Correct the implementation of the Cogroup
processor to implement KTableProcessorSupplier.

Change the cogrouped processor from PassThrough to
KTablePassThrough to allow for sending old values.
KTablePassThrough extends KTableProcessorSupplier instead of
ProcessorSupplier to implement sending old values and the view() method.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>",37,42,0,1074,7984,1,36,1264,652,181,7,3,1300,653,186,36,24,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java,"KAFKA-10417: Update Cogrouped processor to work with suppress() and joins (#9727)

Correct the implementation of the Cogroup
processor to implement KTableProcessorSupplier.

Change the cogrouped processor from PassThrough to
KTablePassThrough to allow for sending old values.
KTablePassThrough extends KTableProcessorSupplier instead of
ProcessorSupplier to implement sending old values and the view() method.

Reviewers: Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>",21,17,0,724,6714,1,15,856,410,45,19,5,1136,410,60,280,101,15,2,1,0,1
core/src/main/scala/kafka/utils/Json.scala,core/src/main/scala/kafka/utils/Json.scala,"KAFKA-6084: Propagate JSON parsing errors in ReassignPartitionsCommand (#4090)

Reviewers: Ismael Juma <ismael@juma.me.uk>",7,16,4,32,408,2,6,92,31,6,15,2,185,31,12,93,30,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,2,2,42,282,1,7,70,107,5,13,2,190,107,15,120,79,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AddOffsetsToTxnResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,2,2,37,229,1,7,77,60,6,14,2.0,153,60,11,76,52,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",17,2,2,95,677,1,10,131,136,10,13,2,284,136,22,153,106,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AddPartitionsToTxnResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",14,2,2,84,622,1,9,137,60,9,16,1.5,258,72,16,121,87,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterClientQuotasRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterClientQuotasRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",19,1,1,102,917,1,9,139,133,46,3,1,155,133,52,16,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterClientQuotasResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterClientQuotasResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",14,1,1,81,700,1,8,113,124,23,5,1,160,124,32,47,38,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",16,1,1,98,760,1,13,138,179,13,11,2,289,179,26,151,104,14,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterIsrRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterIsrRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,42,283,0,7,75,73,25,3,1,81,73,27,6,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterIsrResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterIsrResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",5,1,0,35,264,0,5,61,58,20,3,1,67,58,22,6,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterPartitionReassignmentsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterPartitionReassignmentsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,41,275,0,6,69,84,14,5,1,95,84,19,26,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,1,1,62,524,1,8,94,148,9,10,3.0,241,148,24,147,98,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterUserScramCredentialsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterUserScramCredentialsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,57,440,0,7,85,102,28,3,1,106,102,35,21,21,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterUserScramCredentialsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterUserScramCredentialsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,33,226,0,6,59,70,20,3,1,75,70,25,16,16,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/BeginQuorumEpochRequest.java,clients/src/main/java/org/apache/kafka/common/requests/BeginQuorumEpochRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,2,2,64,412,1,9,94,94,31,3,2,104,94,35,10,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/BeginQuorumEpochResponse.java,clients/src/main/java/org/apache/kafka/common/requests/BeginQuorumEpochResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,2,2,60,396,1,6,102,108,26,4,1.5,125,108,31,23,17,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,43,288,0,7,72,69,3,22,2.0,219,69,10,147,52,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ControlledShutdownResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,47,337,0,7,83,91,5,18,2.0,216,91,12,133,61,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",18,1,1,102,888,1,12,137,133,11,13,3,310,133,24,173,118,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,1,37,252,1,7,63,106,6,11,4,185,106,17,122,75,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java,clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,41,275,0,7,69,136,12,6,2.0,163,136,27,94,79,16,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenResponse.java,clients/src/main/java/org/apache/kafka/common/requests/CreateDelegationTokenResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,1,0,64,412,0,10,94,168,16,6,3.0,229,168,38,135,121,22,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,1,0,51,360,0,7,82,213,8,10,2.5,311,213,31,229,179,23,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/CreatePartitionsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,38,250,0,6,65,105,9,7,2,151,105,22,86,68,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/CreateTopicsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,38,250,0,6,80,96,4,20,2.5,296,96,15,216,80,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",19,1,1,107,936,1,11,146,111,10,15,2,288,111,19,142,91,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",19,1,1,107,1037,1,13,143,182,10,15,4,450,182,30,307,193,20,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,53,359,1,7,81,117,12,7,2,173,117,25,92,59,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteGroupsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,2,2,53,386,1,8,92,129,13,7,2,197,129,28,105,84,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,41,270,0,6,78,135,6,14,1.0,232,135,17,154,118,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteTopicsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,2,1,38,250,0,6,75,88,5,16,3.0,209,88,13,134,75,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",16,1,0,89,733,0,9,124,90,7,17,3,262,90,15,138,59,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",25,1,1,124,1092,1,13,161,128,11,15,9,421,128,28,260,131,17,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeClientQuotasRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeClientQuotasRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",16,1,1,95,694,1,8,128,121,32,4,1.0,140,121,35,12,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeClientQuotasResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeClientQuotasResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",14,1,1,85,809,1,7,119,119,24,5,2,172,119,34,53,48,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,1,0,52,372,0,8,81,132,14,6,2.0,174,132,29,93,78,16,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeDelegationTokenResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",11,1,1,86,729,1,11,118,187,17,7,1,258,187,37,140,131,20,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,1,0,44,312,0,7,72,68,5,14,3.0,186,68,13,114,53,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeGroupsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",12,2,1,109,714,0,11,151,224,7,23,2,483,224,21,332,226,14,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,1,0,82,491,0,10,138,184,12,12,2.5,316,184,26,178,126,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeQuorumRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeQuorumRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",11,2,2,73,531,1,10,105,109,35,3,2,117,109,39,12,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeQuorumResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeQuorumResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,58,419,1,6,96,98,24,4,2.0,115,98,29,19,11,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeUserScramCredentialsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeUserScramCredentialsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,2,1,45,294,0,7,73,87,24,3,2,92,87,31,19,18,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeUserScramCredentialsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeUserScramCredentialsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,33,226,0,6,59,70,20,3,1,75,70,25,16,16,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ElectLeadersResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ElectLeadersResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",12,1,0,72,524,0,8,104,129,21,5,1,145,129,29,41,33,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EndQuorumEpochRequest.java,clients/src/main/java/org/apache/kafka/common/requests/EndQuorumEpochRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,2,2,68,438,1,9,98,101,24,4,3.0,112,101,28,14,8,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EndQuorumEpochResponse.java,clients/src/main/java/org/apache/kafka/common/requests/EndQuorumEpochResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,59,380,1,6,100,107,25,4,2.0,124,107,31,24,16,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java,clients/src/main/java/org/apache/kafka/common/requests/EndTxnRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,2,2,49,309,1,8,78,107,6,13,2,196,107,15,118,80,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EndTxnResponse.java,clients/src/main/java/org/apache/kafka/common/requests/EndTxnResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,40,241,1,8,81,59,6,13,2,149,59,11,68,43,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EnvelopeRequest.java,clients/src/main/java/org/apache/kafka/common/requests/EnvelopeRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,1,0,55,352,0,10,87,94,22,4,2.5,112,94,28,25,20,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EnvelopeResponse.java,clients/src/main/java/org/apache/kafka/common/requests/EnvelopeResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,0,44,275,0,9,74,62,18,4,1.5,86,62,22,12,12,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,1,50,316,1,9,80,112,11,7,2,150,112,21,70,55,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ExpireDelegationTokenResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,1,43,255,1,9,72,96,10,7,2,137,96,20,65,53,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorResponse.java,clients/src/main/java/org/apache/kafka/common/requests/FindCoordinatorResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",11,1,0,56,373,0,11,98,69,4,25,3,284,69,11,186,85,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java,clients/src/main/java/org/apache/kafka/common/requests/HeartbeatRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,2,2,50,340,1,7,78,64,3,24,3.0,252,64,10,174,65,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/HeartbeatResponse.java,clients/src/main/java/org/apache/kafka/common/requests/HeartbeatResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,1,36,225,1,7,73,45,3,23,2,174,45,8,101,39,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",12,1,0,81,635,0,9,112,91,22,5,2,127,91,25,15,13,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,0,67,516,0,8,97,99,16,6,2.0,132,99,22,35,12,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java,clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,3,2,50,365,1,7,80,81,7,12,3.0,181,81,15,101,63,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdResponse.java,clients/src/main/java/org/apache/kafka/common/requests/InitProducerIdResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,40,248,1,8,78,80,5,15,2,204,80,14,126,74,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/JoinGroupResponse.java,clients/src/main/java/org/apache/kafka/common/requests/JoinGroupResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,0,43,271,0,9,72,102,2,31,2,425,102,14,353,172,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java,clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",14,1,0,80,586,0,9,116,71,6,20,3.0,268,71,13,152,63,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupResponse.java,clients/src/main/java/org/apache/kafka/common/requests/LeaveGroupResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",20,2,2,95,609,1,14,153,70,7,23,2,277,78,12,124,46,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ListGroupsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ListGroupsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,33,213,0,6,59,107,3,21,3,245,107,12,186,103,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ListPartitionReassignmentsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ListPartitionReassignmentsRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,0,63,460,0,7,93,104,23,4,2.0,121,104,30,28,20,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ListPartitionReassignmentsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ListPartitionReassignmentsResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,0,34,211,0,6,60,75,12,5,1,82,75,16,22,15,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java,clients/src/main/java/org/apache/kafka/common/requests/MetadataRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",27,1,0,117,855,0,18,162,50,5,30,3.0,439,80,15,277,113,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",15,1,0,90,664,0,9,124,180,4,30,4.0,806,180,27,682,276,23,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetCommitResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",10,1,0,65,502,0,9,114,87,3,33,2,385,87,12,271,91,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetDeleteRequest.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetDeleteRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,47,303,1,8,77,83,26,3,2,89,83,30,12,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetDeleteResponse.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetDeleteResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,2,2,41,277,1,6,84,95,21,4,2.0,108,95,27,24,13,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",28,2,2,136,914,1,14,181,98,6,28,4.0,538,98,19,357,138,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",29,2,2,161,1177,1,17,230,107,7,35,2,650,116,19,420,139,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java,clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,44,283,0,7,72,112,10,7,2,149,112,21,77,60,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenResponse.java,clients/src/main/java/org/apache/kafka/common/requests/RenewDelegationTokenResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,1,43,255,1,9,72,96,10,7,2,137,96,20,65,53,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java,clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,44,284,0,7,82,102,9,9,3,153,102,17,71,40,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateResponse.java,clients/src/main/java/org/apache/kafka/common/requests/SaslAuthenticateResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,1,41,256,1,9,78,88,8,10,2.0,171,88,17,93,67,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java,clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,0,41,276,0,7,80,83,5,15,3,194,83,13,114,49,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeResponse.java,clients/src/main/java/org/apache/kafka/common/requests/SaslHandshakeResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",7,1,1,36,236,1,7,72,85,6,13,3,171,85,13,99,45,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java,clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",26,1,1,169,1332,1,12,217,120,9,23,3,528,120,23,311,105,14,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/StopReplicaResponse.java,clients/src/main/java/org/apache/kafka/common/requests/StopReplicaResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,1,1,46,353,1,8,83,101,4,19,2,252,101,13,169,68,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java,clients/src/main/java/org/apache/kafka/common/requests/SyncGroupRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",14,2,2,63,449,1,9,98,118,5,18,3.0,311,118,17,213,110,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/SyncGroupResponse.java,clients/src/main/java/org/apache/kafka/common/requests/SyncGroupResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,40,248,1,8,68,71,3,22,2.0,190,71,9,122,65,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java,clients/src/main/java/org/apache/kafka/common/requests/TxnOffsetCommitResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",12,2,2,72,570,1,9,121,102,8,16,1.0,279,102,17,158,97,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataResponse.java,clients/src/main/java/org/apache/kafka/common/requests/UpdateMetadataResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",6,1,1,32,205,1,6,59,59,4,14,3.0,140,59,10,81,32,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/VoteRequest.java,clients/src/main/java/org/apache/kafka/common/requests/VoteRequest.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,2,2,75,438,1,9,106,105,35,3,2,116,105,39,10,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/VoteResponse.java,clients/src/main/java/org/apache/kafka/common/requests/VoteResponse.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",8,2,2,57,388,1,6,98,105,24,4,2.0,122,105,30,24,16,6,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchRequestBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchRequestBenchmark.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",9,6,0,98,863,1,7,134,129,34,4,2.5,143,129,36,9,6,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ProduceRequestBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/ProduceRequestBenchmark.java,"KAFKA-10525: Emit JSONs with new auto-generated schema (KIP-673) (#9526)

This patch updates the request logger to output request and response payloads in JSON. Payloads are converted to JSON based on their auto-generated schema.

Reviewers:  Lucas Bradstreet <lucas@confluent.io>, David Mao <dmao@confluent.io>, David Jacot <djacot@confluent.io>",2,64,0,40,338,2,2,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/StreamsUncaughtExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/StreamsUncaughtExceptionHandler.java,"KAFKA-10810: Replace stream threads (#9697)

StreamThreads can now be replaced in the streams uncaught exception handler

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>, Leah Thomas <lthomas@confluent.io>",1,1,0,15,83,0,1,45,44,22,2,1.0,45,44,22,0,0,0,2,1,0,1
core/src/main/scala/kafka/controller/ReplicaStateMachine.scala,core/src/main/scala/kafka/controller/ReplicaStateMachine.scala,"MINOR: Fix some java docs of ReplicaStateMachine (#8552)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",58,2,2,335,2313,1,12,484,232,6,85,3,1598,266,19,1114,167,13,2,1,0,1
tests/kafkatest/services/connect.py,tests/kafkatest/services/connect.py,"KAFKA-10289; Fix failed connect_distributed_test.py (ConnectDistributedTest.test_bounce) (#9673)

In Python 3, `filter` functions return iterators rather than `list` so it can traverse only once. Hence, the following loop will only see ""empty"" and then validation fails.

```python
        src_messages = self.source.committed_messages() # return iterator
        sink_messages = self.sink.flushed_messages()) # return iterator
        for task in range(num_tasks):
            # only first task can ""see"" the result. following tasks see empty result
            src_seqnos = [msg['seqno'] for msg in src_messages if msg['task'] == task]
```

Reference: https://portingguide.readthedocs.io/en/latest/iterators.html#new-behavior-of-map-and-filter.

Reviewers: Jason Gustafson <jason@confluent.io>",101,4,4,384,3460,4,56,522,95,15,35,4,786,124,22,264,36,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java,clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",103,4,4,357,2008,3,46,674,230,21,32,3.5,823,237,26,149,28,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/Selectable.java,clients/src/main/java/org/apache/kafka/common/network/Selectable.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",0,2,2,24,173,0,0,123,68,7,17,3,205,68,12,82,18,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/ByteBufferAccessor.java,clients/src/main/java/org/apache/kafka/common/protocol/ByteBufferAccessor.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",23,7,0,98,471,2,23,143,78,20,7,1,145,78,21,2,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/MessageSizeAccumulator.java,clients/src/main/java/org/apache/kafka/common/protocol/MessageSizeAccumulator.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",6,8,0,25,115,1,6,68,60,34,2,1.0,68,60,34,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/BaseRecords.java,clients/src/main/java/org/apache/kafka/common/record/BaseRecords.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",0,1,1,5,31,0,0,34,34,11,3,1,36,34,12,2,1,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java,clients/src/main/java/org/apache/kafka/common/record/LazyDownConversionRecords.java,"MINOR: Remove connection id from Send and consolidate request/message utils (#9714)

Connection id is now only present in `NetworkSend`, which is now
the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which
works well since `NetworkReceive` is the class used for
received data).

The previous `NetworkSend` was also responsible for adding a size
prefix. This logic is already present in `SendBuilder`, but for the
minority of cases where `SendBuilder` is not used (including
a number of tests), we now have `ByteBufferSend.sizePrefixed()`.

With regards to the request/message utilities:
* Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to
`toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity.
* Introduced new `MessageUtil.toByteBuffer` that does not include
the version as the prefix.
* Renamed `serializeBody` in `AbstractRequest/Response` to
`serialize` for symmetry with `parse`.
* Introduced `RequestTestUtils` and moved relevant methods from
`TestUtils`.
* Moved `serializeWithHeader` methods that were only used in
tests to `RequestTestUtils`.
* Deleted `MessageTestUtil`.

Finally, a couple of changes to simplify coding patterns:
* Added `flip()` and `buffer()` to `ByteBufferAccessor`.
* Added `MessageSizeAccumulator.sizeExcludingZeroCopy`.
* Used lambdas instead of `TestCondition`.
* Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",22,2,2,110,703,2,10,189,168,27,7,1,215,168,31,26,18,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedTable.java,streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedTable.java,"MINOR: fix typo ""intervall"" to ""interval"" (#5435)

Co-authored-by: Chia-Ping Tsai <chia7712@gmail.com>

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,11,11,39,534,0,0,700,349,23,30,5.0,1572,444,52,872,581,29,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedCogroupedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedCogroupedKStream.java,"MINOR: fix typo ""intervall"" to ""interval"" (#5435)

Co-authored-by: Chia-Ping Tsai <chia7712@gmail.com>

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,4,4,23,273,0,0,265,159,53,5,4,338,170,68,73,64,15,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedKStream.java,streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindowedKStream.java,"MINOR: fix typo ""intervall"" to ""interval"" (#5435)

Co-authored-by: Chia-Ping Tsai <chia7712@gmail.com>

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",0,12,12,39,558,0,0,646,268,59,11,5,909,313,83,263,225,24,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/StickyPartitionCache.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/StickyPartitionCache.java,"MINOR: Using primitive data types for loop index (#9705)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,1,1,47,394,1,3,76,76,25,3,1,79,76,26,3,2,1,1,0,1,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/SchemaSourceConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/SchemaSourceConnector.java,"MINOR: Using primitive data types for loop index (#9705)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,2,41,283,1,6,67,68,22,3,2,74,68,25,7,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSinkConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSinkConnector.java,"MINOR: Using primitive data types for loop index (#9705)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,2,41,283,1,6,69,64,14,5,2,76,64,15,7,5,1,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSourceConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSourceConnector.java,"MINOR: Using primitive data types for loop index (#9705)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,2,2,41,283,1,6,69,64,14,5,2,76,64,15,7,5,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/RequestCompletionHandler.java,clients/src/main/java/org/apache/kafka/clients/RequestCompletionHandler.java,"KAFKA-10667: add timeout for forwarding requests (#9564)

add total timeout for forwarding, including the underlying broker-to-controller channel timeout setting.

Reviewers: David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",0,0,1,4,21,0,0,26,31,5,5,1,52,31,10,26,13,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ClientRequest.java,clients/src/main/java/org/apache/kafka/clients/ClientRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,3,3,77,372,1,11,119,61,6,20,2.0,252,61,13,133,39,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterConfigsResponse.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,5,12,43,307,5,7,71,88,8,9,4,167,88,19,96,69,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterPartitionReassignmentsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AlterPartitionReassignmentsRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,3,21,65,480,4,7,96,114,48,2,3.0,117,114,58,21,21,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/AlterReplicaLogDirsResponse.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,8,15,38,261,4,6,73,114,7,11,4,210,114,19,137,91,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ApiError.java,clients/src/main/java/org/apache/kafka/common/requests/ApiError.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",19,0,16,65,403,2,14,116,99,14,8,2.0,146,99,18,30,16,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DeleteRecordsRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",9,2,11,55,419,3,7,85,151,8,10,2.5,230,151,23,145,110,14,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,3,12,46,346,3,6,73,142,7,11,3,286,142,26,213,155,19,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeConfigsResponse.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,9,11,214,1684,5,33,275,186,21,13,5,599,186,46,324,252,25,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,3,12,42,272,3,8,72,145,9,8,2.5,195,145,24,123,93,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ListGroupsRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",10,4,13,52,361,4,7,88,57,5,17,3,189,57,11,101,24,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java,clients/src/main/java/org/apache/kafka/common/requests/OffsetsForLeaderEpochResponse.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",6,4,11,40,291,4,6,79,132,5,16,3.5,367,132,23,288,112,18,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",43,5,20,200,1498,5,20,276,74,6,43,4,848,124,20,572,265,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/RequestHeader.java,clients/src/main/java/org/apache/kafka/common/requests/RequestHeader.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",20,19,21,87,581,7,15,126,68,5,24,2.5,400,68,17,274,94,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ResponseHeader.java,clients/src/main/java/org/apache/kafka/common/requests/ResponseHeader.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,15,11,56,356,6,11,89,45,7,13,3,158,45,12,69,19,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java,clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkersResponse.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",15,28,23,81,698,10,7,126,130,10,13,1,283,130,22,157,105,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/protocol/MessageTestUtil.java,clients/src/test/java/org/apache/kafka/common/protocol/MessageTestUtil.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,1,5,12,91,2,1,31,39,8,4,1.0,51,39,13,20,15,5,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerRequestCompletionHandler.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerRequestCompletionHandler.scala,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",26,2,1,141,769,1,1,204,95,9,22,1.0,305,95,14,101,43,5,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerRequestBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerRequestBenchmark.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",3,0,6,62,578,1,3,87,93,44,2,1.5,93,93,46,6,6,3,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerResponseBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerResponseBenchmark.java,"KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses (#7409)

Generated request/response classes have code to serialize/deserialize directly to
`ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them.
We have recently completed the transition to generated request/response classes,
so we can also remove the `Struct` based fallbacks.

Additional noteworthy changes:
* `AbstractRequest.parseRequest` has a more efficient computation of request size that
relies on the received buffer instead of the parsed `Struct`.
* Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass
implementation final and removed the overrides that are no longer necessary.
* Removed request/response constructors that assume latest version as they are unsafe
outside of tests.
* Removed redundant version fields in requests/responses.
* Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2.
* Made `AbstractResponse.throttleTimeMs()` abstract.
* Using `toSend` in `SaslClientAuthenticator` instead of `serialize`.
* Various changes in Request/Response classes to make them more consistent and to
rely on the Data classes as much as possible when it comes to their state.
* Remove the version argument from `AbstractResponse.toString`.
* Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to
use `ApiError` which processes the error message sent back to the clients. This was
uncovered by an accidental fix to a `RequestResponseTest` test (it was calling
`AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).

Rely on existing protocol tests to ensure this refactoring does not change 
observed behavior (aside from improved performance).

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",2,0,9,52,421,1,2,79,88,40,2,2.0,88,88,44,9,9,4,2,1,0,1
connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java,connect/mirror/src/test/java/org/apache/kafka/connect/mirror/MirrorConnectorsIntegrationTest.java,"KAFKA-10811: Correct the MirrorConnectorsIntegrationTest to correctly mask the exit procedures (#9698)

Normally the `EmbeddedConnectCluster` class masks the `Exit` procedures using within the Connect worker. This normally works great when a single instance of the embedded cluster is used. However, the `MirrorConnectorsIntegrationTest` uses two `EmbeddedConnectCluster` instances, and when the first one is stopped it would reset the (static) exit procedures, and any problems during shutdown of the second embedded Connect cluster would cause the worker to shut down the JVM running the tests.

Instead, the `MirrorConnectorsIntegrationTest` class should mask the `Exit` procedures and instruct the `EmbeddedConnectClusters` instances (via the existing builder method) to not mask the procedures.

Author: Randall Hauch <rhauch@gmail.com>
Reviewers: Mickael Maison <mickael.maison@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",41,57,17,448,4237,3,16,595,302,50,12,9.5,851,302,71,256,128,21,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/AbstractStreamTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/AbstractStreamTest.java,"KAFKA-10629: TopologyTestDriver should not require a Properties argument (#9660)

Implements KIP-680.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Matthias J. Sax <matthias@confluent.io>",18,3,7,97,910,1,8,134,99,5,26,3.0,240,99,9,106,19,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamTransformTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamTransformTest.java,"KAFKA-10629: TopologyTestDriver should not require a Properties argument (#9660)

Implements KIP-680.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Matthias J. Sax <matthias@confluent.io>",6,0,1,121,1260,1,2,168,97,5,31,3,454,102,15,286,65,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/internals/metrics/ClientMetrics.java,streams/src/main/java/org/apache/kafka/streams/internals/metrics/ClientMetrics.java,"KAFKA-10500: Add failed-stream-threads metric for adding + removing stream threads (#9614)

Part of KIP-663.

Reviewer: Bruno Cadonna <bruno@confluent.io>, Walker Carlson <wcarlson@confluent.io>, Matthias J. Sax <matthias@confluent.io>",12,19,0,114,640,1,12,147,116,49,3,3,147,116,49,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/internals/metrics/ClientMetricsTest.java,streams/src/test/java/org/apache/kafka/streams/internals/metrics/ClientMetricsTest.java,"KAFKA-10500: Add failed-stream-threads metric for adding + removing stream threads (#9614)

Part of KIP-663.

Reviewer: Bruno Cadonna <bruno@confluent.io>, Walker Carlson <wcarlson@confluent.io>, Matthias J. Sax <matthias@confluent.io>",10,46,2,142,903,2,10,179,137,45,4,8.0,238,137,60,59,47,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Histogram.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Histogram.java,"MINOR: Make Histogram#clear more readable (#9679)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",29,3,2,129,894,1,14,213,137,21,10,1.0,256,137,26,43,29,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsForLeaderEpochClient.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsForLeaderEpochClient.java,"KAFKA-10739; Replace EpochEndOffset with automated protocol (#9630)

This patch follows up https://github.com/apache/kafka/pull/9547. It refactors KafkaApis, ReplicaManager and Partition to use `OffsetForLeaderEpochResponseData.EpochEndOffset` instead of `EpochEndOffset`. In the mean time, it removes `OffsetsForLeaderEpochRequest#epochsByTopicPartition` and `OffsetsForLeaderEpochResponse#responses` and replaces their usages to use the automated protocol directly. Finally, it removes old constructors in `OffsetsForLeaderEpochResponse`. The patch relies on existing tests.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jason Gustafson <jason@confluent.io>",21,6,6,115,842,4,6,149,128,17,9,1,235,128,26,86,46,10,2,1,0,1
core/src/main/scala/kafka/server/AbstractFetcherManager.scala,core/src/main/scala/kafka/server/AbstractFetcherManager.scala,"KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response (#9382)

From IBP 2.7 onwards, fetch responses include diverging epoch and offset in fetch responses if lastFetchedEpoch is provided in the fetch request. This PR uses that information for truncation and avoids the additional OffsetForLeaderEpoch requests in followers when lastFetchedEpoch is known.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Nikhil Bhatia <rite2nikhil@gmail.com>",29,17,15,163,1130,6,14,234,85,5,43,3,472,85,11,238,46,6,2,1,0,1
core/src/main/scala/kafka/server/ReplicaAlterLogDirsManager.scala,core/src/main/scala/kafka/server/ReplicaAlterLogDirsManager.scala,"KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response (#9382)

From IBP 2.7 onwards, fetch responses include diverging epoch and offset in fetch responses if lastFetchedEpoch is provided in the fetch request. This PR uses that information for truncation and avoids the additional OffsetForLeaderEpoch requests in followers when lastFetchedEpoch is known.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Nikhil Bhatia <rite2nikhil@gmail.com>",6,1,1,34,205,2,3,58,40,10,6,1.0,66,40,11,8,5,1,2,1,0,1
core/src/test/scala/unit/kafka/server/AbstractFetcherThreadWithIbp26Test.scala,core/src/test/scala/unit/kafka/server/AbstractFetcherThreadWithIbp26Test.scala,"KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response (#9382)

From IBP 2.7 onwards, fetch responses include diverging epoch and offset in fetch responses if lastFetchedEpoch is provided in the fetch request. This PR uses that information for truncation and avoids the additional OffsetForLeaderEpoch requests in followers when lastFetchedEpoch is known.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Nikhil Bhatia <rite2nikhil@gmail.com>",0,23,0,4,15,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceWithIbp26Test.scala,core/src/test/scala/unit/kafka/server/epoch/EpochDrivenReplicationProtocolAcceptanceWithIbp26Test.scala,"KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response (#9382)

From IBP 2.7 onwards, fetch responses include diverging epoch and offset in fetch responses if lastFetchedEpoch is provided in the fetch request. This PR uses that information for truncation and avoids the additional OffsetForLeaderEpoch requests in followers when lastFetchedEpoch is known.

Co-authored-by: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Nikhil Bhatia <rite2nikhil@gmail.com>",0,29,0,5,23,0,0,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",13,6,6,117,933,5,10,148,107,21,7,2,177,107,25,29,14,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",9,6,6,87,672,3,3,133,76,6,21,3,255,76,12,122,28,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",23,8,8,201,1631,6,17,254,180,7,36,6.0,1022,180,28,768,373,21,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedTableImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedTableImpl.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",22,5,5,181,1746,3,14,247,172,7,37,4,839,172,23,592,123,16,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImpl.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",12,3,3,113,972,2,6,144,144,72,2,2.0,147,144,74,3,3,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImpl.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",30,3,3,190,1917,2,16,236,236,118,2,1.5,239,236,120,3,3,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/BaseRepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/BaseRepartitionNode.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",19,1,1,108,760,0,15,147,76,18,8,1.5,161,79,20,14,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GraphGraceSearchUtil.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/GraphGraceSearchUtil.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",14,9,9,65,484,6,4,95,89,19,5,1,106,89,21,11,9,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/SourceGraphNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/SourceGraphNode.java,"KAFKA-6687: restrict DSL to allow only Streams from the same source topics (#9609)

Followup to PR #9582, need to restrict DSL so only KStreams can be created from the same set of topic(s)s but not KTables, which can be tackled as followup work

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <cadonna@confluent.io>",7,74,0,44,292,7,7,74,74,74,1,1,74,74,74,0,0,0,0,0,0,0
tests/kafkatest/tests/streams/base_streams_test.py,tests/kafkatest/tests/streams/base_streams_test.py,"MINOR: fix reading SSH output in Streams system tests (#9665)

SSH outputs in system tests originating from paramiko are bytes. However, the logger in the system tests does not accept bytes and instead throws an exception. That means, the bytes returned as SSH output from paramiko need to converted to a type that the logger (or other objects) can process.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",10,1,1,71,480,1,9,105,99,12,9,1,116,99,13,11,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java,clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java,"KAFKA-10770: Remove duplicate defination of Metrics#getTags (#9659)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",74,3,12,322,2716,3,41,671,190,17,39,2,854,190,22,183,39,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/internals/MetricsUtils.java,clients/src/main/java/org/apache/kafka/common/metrics/internals/MetricsUtils.java,"KAFKA-10770: Remove duplicate defination of Metrics#getTags (#9659)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",11,20,0,35,276,1,2,66,46,33,2,1.5,66,46,33,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetrics.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectMetrics.java,"KAFKA-10770: Remove duplicate defination of Metrics#getTags (#9659)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",45,2,17,213,1740,2,30,446,296,34,13,3,620,296,48,174,45,13,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/transforms/Transformation.java,connect/api/src/main/java/org/apache/kafka/connect/transforms/Transformation.java,"KAFKA-10720: Document prohibition on header mutation by SMTs (#9597)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Chris Egerton <fearthecellos@gmail.com>",0,5,0,11,91,0,0,52,48,17,3,1,57,48,19,5,5,2,2,1,0,1
core/src/main/scala/kafka/tools/ConsoleProducer.scala,core/src/main/scala/kafka/tools/ConsoleProducer.scala,"KAFKA-10565: Only print console producer prompt with a tty (#9644)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",30,3,1,256,1966,1,7,298,151,4,67,3,881,206,13,583,128,9,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConnector.java,"MINOR: Remove unnecessary statement from WorkerConnector#doRun (#9653)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",78,0,1,413,2298,1,41,527,206,28,19,2,624,206,33,97,24,5,2,1,0,1
core/src/main/scala/kafka/zookeeper/ZooKeeperClient.scala,core/src/main/scala/kafka/zookeeper/ZooKeeperClient.scala,"MINOR: Upgrade to Scala 2.13.4 (#9643)

Scala 2.13.4 restores default global `ExecutionContext` to 2.12 behavior
(to fix a perf regression in some use cases) and improves pattern matching
(especially exhaustiveness checking). Most of the changes are related
to the latter as I have enabled the newly introduced `-Xlint:strict-unsealed-patmat`.

More details on the code changes:
* Don't swallow exception in `ReassignPartitionsCommand.topicDescriptionFutureToState`.
* `RequestChannel.Response` should be `sealed`.
* Introduce sealed ClientQuotaManager.BaseUserEntity to avoid false positive
exhaustiveness warning.
* Handle a number of cases where pattern matches were not exhaustive:
either by marking them with @unchecked or by adding a catch-all clause.
* Workaround scalac bug related to exhaustiveness warnings in ZooKeeperClient
* Remove warning suppression annotations related to the optimizer that are no
longer needed in ConsumerGroupCommand and AclAuthorizer.
* Use `forKeyValue` in `AclAuthorizer.acls` as the scala bug preventing us from
using it seems to be fixed.
* Also update scalaCollectionCompat to 2.3.0, which includes minor improvements.

Full release notes:
* https://github.com/scala/scala/releases/tag/v2.13.4
* https://github.com/scala/scala-collection-compat/releases/tag/v2.3.0

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",60,4,1,426,3531,0,32,615,286,21,29,5,1111,286,38,496,108,17,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ScramMechanism.java,clients/src/main/java/org/apache/kafka/clients/admin/ScramMechanism.java,"MINOR: Update build and test dependencies (#9645)

The spotbugs upgrade means we can re-enable
RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE and RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE.
These uncovered one bug, one unnecessary null check and one
false positive. Addressed them all, including a test for the bug.

* gradle (6.7.0 -> 6.7.1): minor fixes.
* gradle versions plugin (0.29.0 -> 0.36.0): minor fixes.
* grgit (4.0.2 -> 4.1.0): a few small fixes and dependency bumps.
* owasp dependency checker plugin (5.3.2.1 -> 6.0.3): improved db
schema, data and several fixes. 
* scoverage plugin (4.0.2 -> 5.0.0): support Scala 2.13.
* shadow plugin (6.0.0 -> 6.1.0): require Java 8, support for Java 16.
* spotbugs plugin (4.4.4 -> 4.6.0): support SARIF reporting standard.
* spotbugs (4.0.6 -> 4.1.4): support for Java 16 and various fixes including
try with resources false positive.
* spotless plugin (5.1.0 -> 5.8.2): minor fixes.
* test retry plugin (1.1.6 -> 1.1.9): newer gradle and java version compatibility
fixes.
* mockito (3.5.7 -> 3.6.0): minor fixes.
* powermock (2.0.7 -> 2.0.9): minor fixes.

Release notes links:
* https://docs.gradle.org/6.7.1/release-notes.html
* https://github.com/spotbugs/spotbugs/blob/4.1.4/CHANGELOG.md
* https://github.com/scoverage/gradle-scoverage/releases/tag/5.0.0
* https://github.com/johnrengelman/shadow/releases/tag/6.1.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.5.0
* https://github.com/ben-manes/gradle-versions-plugin/releases
* https://github.com/ajoberstar/grgit/releases/tag/4.1.0
* https://github.com/jeremylong/DependencyCheck/blob/main/RELEASE_NOTES.md#version-603-2020-11-03
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.8
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.9
* https://github.com/mockito/mockito/blob/v3.6.0/doc/release-notes/official.md
* https://github.com/gradle/test-retry-gradle-plugin/releases
* https://github.com/diffplug/spotless/blob/main/plugin-gradle/CHANGES.md

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,9,3,34,198,2,5,87,81,44,2,2.5,90,81,45,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java,clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java,"MINOR: Update build and test dependencies (#9645)

The spotbugs upgrade means we can re-enable
RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE and RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE.
These uncovered one bug, one unnecessary null check and one
false positive. Addressed them all, including a test for the bug.

* gradle (6.7.0 -> 6.7.1): minor fixes.
* gradle versions plugin (0.29.0 -> 0.36.0): minor fixes.
* grgit (4.0.2 -> 4.1.0): a few small fixes and dependency bumps.
* owasp dependency checker plugin (5.3.2.1 -> 6.0.3): improved db
schema, data and several fixes. 
* scoverage plugin (4.0.2 -> 5.0.0): support Scala 2.13.
* shadow plugin (6.0.0 -> 6.1.0): require Java 8, support for Java 16.
* spotbugs plugin (4.4.4 -> 4.6.0): support SARIF reporting standard.
* spotbugs (4.0.6 -> 4.1.4): support for Java 16 and various fixes including
try with resources false positive.
* spotless plugin (5.1.0 -> 5.8.2): minor fixes.
* test retry plugin (1.1.6 -> 1.1.9): newer gradle and java version compatibility
fixes.
* mockito (3.5.7 -> 3.6.0): minor fixes.
* powermock (2.0.7 -> 2.0.9): minor fixes.

Release notes links:
* https://docs.gradle.org/6.7.1/release-notes.html
* https://github.com/spotbugs/spotbugs/blob/4.1.4/CHANGELOG.md
* https://github.com/scoverage/gradle-scoverage/releases/tag/5.0.0
* https://github.com/johnrengelman/shadow/releases/tag/6.1.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.5.0
* https://github.com/ben-manes/gradle-versions-plugin/releases
* https://github.com/ajoberstar/grgit/releases/tag/4.1.0
* https://github.com/jeremylong/DependencyCheck/blob/main/RELEASE_NOTES.md#version-603-2020-11-03
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.8
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.9
* https://github.com/mockito/mockito/blob/v3.6.0/doc/release-notes/official.md
* https://github.com/gradle/test-retry-gradle-plugin/releases
* https://github.com/diffplug/spotless/blob/main/plugin-gradle/CHANGES.md

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,1,1,55,368,1,7,83,64,17,5,1,107,64,21,24,15,5,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/ScramMechanismTest.java,clients/src/test/java/org/apache/kafka/clients/admin/ScramMechanismTest.java,"MINOR: Update build and test dependencies (#9645)

The spotbugs upgrade means we can re-enable
RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE and RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE.
These uncovered one bug, one unnecessary null check and one
false positive. Addressed them all, including a test for the bug.

* gradle (6.7.0 -> 6.7.1): minor fixes.
* gradle versions plugin (0.29.0 -> 0.36.0): minor fixes.
* grgit (4.0.2 -> 4.1.0): a few small fixes and dependency bumps.
* owasp dependency checker plugin (5.3.2.1 -> 6.0.3): improved db
schema, data and several fixes. 
* scoverage plugin (4.0.2 -> 5.0.0): support Scala 2.13.
* shadow plugin (6.0.0 -> 6.1.0): require Java 8, support for Java 16.
* spotbugs plugin (4.4.4 -> 4.6.0): support SARIF reporting standard.
* spotbugs (4.0.6 -> 4.1.4): support for Java 16 and various fixes including
try with resources false positive.
* spotless plugin (5.1.0 -> 5.8.2): minor fixes.
* test retry plugin (1.1.6 -> 1.1.9): newer gradle and java version compatibility
fixes.
* mockito (3.5.7 -> 3.6.0): minor fixes.
* powermock (2.0.7 -> 2.0.9): minor fixes.

Release notes links:
* https://docs.gradle.org/6.7.1/release-notes.html
* https://github.com/spotbugs/spotbugs/blob/4.1.4/CHANGELOG.md
* https://github.com/scoverage/gradle-scoverage/releases/tag/5.0.0
* https://github.com/johnrengelman/shadow/releases/tag/6.1.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0
* https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.5.0
* https://github.com/ben-manes/gradle-versions-plugin/releases
* https://github.com/ajoberstar/grgit/releases/tag/4.1.0
* https://github.com/jeremylong/DependencyCheck/blob/main/RELEASE_NOTES.md#version-603-2020-11-03
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.8
* https://github.com/powermock/powermock/releases/tag/powermock-2.0.9
* https://github.com/mockito/mockito/blob/v3.6.0/doc/release-notes/official.md
* https://github.com/gradle/test-retry-gradle-plugin/releases
* https://github.com/diffplug/spotless/blob/main/plugin-gradle/CHANGES.md

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",1,35,0,13,119,1,1,35,35,35,1,1,35,35,35,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosError.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosError.java,"KAFKA-10727; Handle Kerberos error during re-login as transient failure in clients (#9605)

We use a background thread for Kerberos to perform re-login before tickets expire. The thread performs logout() followed by login(), relying on the Java library to clear and then populate credentials in Subject. This leaves a timing window where clients fail to authenticate because credentials are not available. We cannot introduce any form of locking since authentication is performed on the network thread. So this commit treats NO_CRED as a transient failure rather than a fatal authentication exception in clients.

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Manikumar Reddy <manikumar.reddy@gmail.com>",19,19,0,84,526,1,9,131,100,44,3,2,132,100,44,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/CorrelationIdMismatchException.java,clients/src/main/java/org/apache/kafka/common/requests/CorrelationIdMismatchException.java,"MINOR: Factor out common response parsing logic (#9617)

This patch factors out some common parsing logic from `NetworkClient.parseResponse` and `AbstractResponse.parseResponse`. As a result of this refactor, we are now verifying the correlationId in forwarded requests. This patch also adds a test case to verify handling in this case.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>",3,45,0,20,79,3,3,45,45,45,1,1,45,45,45,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/DynamicConfigManager.scala,core/src/main/scala/kafka/server/DynamicConfigManager.scala,"KAFKA-10024: Add dynamic configuration and enforce quota for per-IP connection rate limits (KIP-612, part 2) (#9386)

This PR implements the part of KIP-612 for adding IP throttling enforcement, and a ZK entity for configuring dynamic IP throttles.

Reviewers: Anna Povzner <anna@confluent.io>, David Jacot <djacot@confluent.io>",17,2,1,101,813,0,5,184,133,6,32,2.0,492,133,15,308,78,10,2,1,0,1
core/src/main/scala/kafka/server/DelegationTokenManager.scala,core/src/main/scala/kafka/server/DelegationTokenManager.scala,"KAFKA-10692: Add delegation.token.secret.key, deprecate ...master.key (#9623)


Reviewers: Mickael Maison <mickael.maison@gmail.com>",84,1,1,346,2531,0,29,512,515,51,10,1.5,549,515,55,37,12,4,2,1,0,1
core/src/main/scala/kafka/tools/ConsumerPerformance.scala,core/src/main/scala/kafka/tools/ConsumerPerformance.scala,KAFKA-10701 First line of detailed stats from consumer-perf-test.sh incorrect (#9598),13,1,1,251,1890,0,7,462,199,8,55,3,970,199,18,508,158,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java,"KAFKA-10628: remove all the unnecessary parameters from the tests which are using TopologyTestDriver (#9507)

1. remove unneeded javadoc content.
2. Replace containsKey/setProperty with putIfAbsent
3. refactor the constructor of TopologyTestDriverTest

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",17,0,2,177,1845,1,9,232,292,14,17,3,467,292,27,235,122,14,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinMaterializationIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinMaterializationIntegrationTest.java,"KAFKA-10628: remove all the unnecessary parameters from the tests which are using TopologyTestDriver (#9507)

1. remove unneeded javadoc content.
2. Replace containsKey/setProperty with putIfAbsent
3. refactor the constructor of TopologyTestDriverTest

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",14,0,2,155,1209,1,6,204,192,41,5,3,217,192,43,13,8,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableAggregateTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableAggregateTest.java,"KAFKA-10628: remove all the unnecessary parameters from the tests which are using TopologyTestDriver (#9507)

1. remove unneeded javadoc content.
2. Replace containsKey/setProperty with putIfAbsent
3. refactor the constructor of TopologyTestDriverTest

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",8,7,20,226,2045,4,6,274,122,6,43,6,1179,144,27,905,267,21,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableForeignKeyJoinScenarioTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableKTableForeignKeyJoinScenarioTest.java,"KAFKA-10628: remove all the unnecessary parameters from the tests which are using TopologyTestDriver (#9507)

1. remove unneeded javadoc content.
2. Replace containsKey/setProperty with putIfAbsent
3. refactor the constructor of TopologyTestDriverTest

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",7,0,1,198,1920,1,7,257,178,32,8,3.0,333,178,42,76,51,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TaskTimeoutExceptions.java,streams/src/main/java/org/apache/kafka/streams/errors/TaskTimeoutExceptions.java,"KAFKA-9274: Handle TimeoutException on commit (#9570)

- part of KIP-572
 - when KafkaStreams commits a task, a TimeoutException should not kill
   the thread but `task.timeout.ms` should be triggered and the commit
   should be retried in the next loop

Reviewer: John Roesler <john@confluent.io>",5,58,0,32,191,5,5,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java,clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java,"KAFKA-9628; Replace Produce request/response with automated protocol (#9401)

This patch rewrites `ProduceRequest` and `ProduceResponse` using the generated protocols. We have also added several new benchmarks to verify no regression in performance. A summary of results is included below:

### Benchmark

1. loop **30** times
1. calculate average

#### kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_throughput

> @cluster(num_nodes=5)
> @parametrize(acks=-1, topic=TOPIC_REP_THREE)

- +0.3144915325 %
- 28.08766667 ->  28.1715625 (mb_per_sec)

> @cluster(num_nodes=5)
> @matrix(acks=[1], topic=[TOPIC_REP_THREE], message_size=[100000],compression_type=[""none""], security_protocol=['PLAINTEXT'])

- +4.220730323 %
- 157.145 -> 163.7776667 (mb_per_sec)

> @cluster(num_nodes=7)
> @parametrize(acks=1, topic=TOPIC_REP_THREE, num_producers=3)

- +5.996241145%
- 57.64166667 -> 61.098 (mb_per_sec)

> @cluster(num_nodes=5)
> @parametrize(acks=1, topic=TOPIC_REP_THREE)

- +0.3979572536%
- 44.05833333 -> 44.23366667 (mb_per_sec)

> @cluster(num_nodes=5)
> @parametrize(acks=1, topic= TOPIC_REP_ONE)

- +2.228235226%
- 69.23266667 -> 70.77533333 (mb_per_sec)

### JMH results

In short, most ops performance are regression since we have to convert data to protocol data. The cost is inevitable (like other request/response) before we use protocol data directly.

### JMH for ProduceRequest

1. construction regression:
    - 281.474 -> 454.935 ns/op
    - 296.000 -> 1888.000 B/op
1. toErrorResponse regression:
    - 41.942 -> 107.528 ns/op
    - 1216.000 -> 1616.000 B/op
1. toStruct improvement:
    - 255.185 -> 90.728 ns/op
    - 864.000 -> 304.000 B/op

**BEFORE**
```
Benchmark                                                                        Mode  Cnt     Score    Error   Units
ProducerRequestBenchmark.constructorErrorResponse                                avgt   15    41.942 ±  0.036   ns/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.alloc.rate                 avgt   15  6409.263 ±  5.478  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.alloc.rate.norm            avgt   15   296.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Eden_Space        avgt   15  6416.420 ± 76.071  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Eden_Space.norm   avgt   15   296.331 ±  3.539    B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Old_Gen           avgt   15     0.002 ±  0.002  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Old_Gen.norm      avgt   15    ≈ 10⁻⁴             B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.count                      avgt   15   698.000           counts
ProducerRequestBenchmark.constructorErrorResponse:·gc.time                       avgt   15   378.000               ms
ProducerRequestBenchmark.constructorProduceRequest                               avgt   15   281.474 ±  3.286   ns/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.alloc.rate                avgt   15  3923.868 ± 46.303  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.alloc.rate.norm           avgt   15  1216.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Eden_Space       avgt   15  3923.375 ± 59.568  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Eden_Space.norm  avgt   15  1215.844 ± 11.184    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Old_Gen          avgt   15     0.004 ±  0.001  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Old_Gen.norm     avgt   15     0.001 ±  0.001    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.count                     avgt   15   515.000           counts
ProducerRequestBenchmark.constructorProduceRequest:·gc.time                      avgt   15   279.000               ms
ProducerRequestBenchmark.constructorStruct                                       avgt   15   255.185 ±  0.069   ns/op
ProducerRequestBenchmark.constructorStruct:·gc.alloc.rate                        avgt   15  3074.889 ±  0.823  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.alloc.rate.norm                   avgt   15   864.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Eden_Space               avgt   15  3077.737 ± 31.537  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Eden_Space.norm          avgt   15   864.800 ±  8.823    B/op
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Old_Gen                  avgt   15     0.003 ±  0.001  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Old_Gen.norm             avgt   15     0.001 ±  0.001    B/op
ProducerRequestBenchmark.constructorStruct:·gc.count                             avgt   15   404.000           counts
ProducerRequestBenchmark.constructorStruct:·gc.time                              avgt   15   214.000               ms
```

**AFTER**
```
Benchmark                                                                        Mode  Cnt     Score    Error   Units
ProducerRequestBenchmark.constructorErrorResponse                                avgt   15   107.528 ±  0.270   ns/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.alloc.rate                 avgt   15  4864.899 ± 12.132  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.alloc.rate.norm            avgt   15   576.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Eden_Space        avgt   15  4868.023 ± 61.943  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Eden_Space.norm   avgt   15   576.371 ±  7.331    B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Old_Gen           avgt   15     0.005 ±  0.001  MB/sec
ProducerRequestBenchmark.constructorErrorResponse:·gc.churn.G1_Old_Gen.norm      avgt   15     0.001 ±  0.001    B/op
ProducerRequestBenchmark.constructorErrorResponse:·gc.count                      avgt   15   639.000           counts
ProducerRequestBenchmark.constructorErrorResponse:·gc.time                       avgt   15   339.000               ms
ProducerRequestBenchmark.constructorProduceRequest                               avgt   15   454.935 ±  0.332   ns/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.alloc.rate                avgt   15  3769.014 ±  2.767  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.alloc.rate.norm           avgt   15  1888.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Eden_Space       avgt   15  3763.407 ± 31.530  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Eden_Space.norm  avgt   15  1885.190 ± 15.594    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Old_Gen          avgt   15     0.004 ±  0.001  MB/sec
ProducerRequestBenchmark.constructorProduceRequest:·gc.churn.G1_Old_Gen.norm     avgt   15     0.002 ±  0.001    B/op
ProducerRequestBenchmark.constructorProduceRequest:·gc.count                     avgt   15   494.000           counts
ProducerRequestBenchmark.constructorProduceRequest:·gc.time                      avgt   15   264.000               ms
ProducerRequestBenchmark.constructorStruct                                       avgt   15    90.728 ±  0.695   ns/op
ProducerRequestBenchmark.constructorStruct:·gc.alloc.rate                        avgt   15  3043.140 ± 23.246  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.alloc.rate.norm                   avgt   15   304.000 ±  0.001    B/op
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Eden_Space               avgt   15  3047.251 ± 59.638  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Eden_Space.norm          avgt   15   304.404 ±  5.034    B/op
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Old_Gen                  avgt   15     0.003 ±  0.001  MB/sec
ProducerRequestBenchmark.constructorStruct:·gc.churn.G1_Old_Gen.norm             avgt   15    ≈ 10⁻⁴             B/op
ProducerRequestBenchmark.constructorStruct:·gc.count                             avgt   15   400.000           counts
ProducerRequestBenchmark.constructorStruct:·gc.time                              avgt   15   205.000               ms
```
### JMH for ProduceResponse

1. construction regression:
    - 3.293 -> 303.226 ns/op
    - 24.000 -> 1848.000 B/op
1. toStruct improvement:
    - 825.889 -> 311.725 ns/op
    - 2208.000 -> 896.000 B/op

**BEFORE**

```
Benchmark                                                                          Mode  Cnt     Score    Error   Units
ProducerResponseBenchmark.constructorProduceResponse                               avgt   15     3.293 ±  0.004   ns/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.alloc.rate                avgt   15  6619.731 ±  9.075  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.alloc.rate.norm           avgt   15    24.000 ±  0.001    B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Eden_Space       avgt   15  6618.648 ±  0.153  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Eden_Space.norm  avgt   15    23.996 ±  0.033    B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Old_Gen          avgt   15     0.003 ±  0.002  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Old_Gen.norm     avgt   15    ≈ 10⁻⁵             B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.count                     avgt   15   720.000           counts
ProducerResponseBenchmark.constructorProduceResponse:·gc.time                      avgt   15   383.000               ms
ProducerResponseBenchmark.constructorStruct                                        avgt   15   825.889 ±  0.638   ns/op
ProducerResponseBenchmark.constructorStruct:·gc.alloc.rate                         avgt   15  2428.000 ±  1.899  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.alloc.rate.norm                    avgt   15  2208.000 ±  0.001    B/op
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Eden_Space                avgt   15  2430.196 ± 55.894  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Eden_Space.norm           avgt   15  2210.001 ± 51.009    B/op
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Old_Gen                   avgt   15     0.003 ±  0.001  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Old_Gen.norm              avgt   15     0.002 ±  0.001    B/op
ProducerResponseBenchmark.constructorStruct:·gc.count                              avgt   15   319.000           counts
ProducerResponseBenchmark.constructorStruct:·gc.time                               avgt   15   166.000               ms
```

**AFTER**

```
Benchmark                                                                          Mode  Cnt     Score    Error   Units
ProducerResponseBenchmark.constructorProduceResponse                               avgt   15   303.226 ±  0.517   ns/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.alloc.rate                avgt   15  5534.940 ±  9.439  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.alloc.rate.norm           avgt   15  1848.000 ±  0.001    B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Eden_Space       avgt   15  5534.046 ± 51.849  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Eden_Space.norm  avgt   15  1847.710 ± 18.105    B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Old_Gen          avgt   15     0.007 ±  0.001  MB/sec
ProducerResponseBenchmark.constructorProduceResponse:·gc.churn.G1_Old_Gen.norm     avgt   15     0.002 ±  0.001    B/op
ProducerResponseBenchmark.constructorProduceResponse:·gc.count                     avgt   15   602.000           counts
ProducerResponseBenchmark.constructorProduceResponse:·gc.time                      avgt   15   318.000               ms
ProducerResponseBenchmark.constructorStruct                                        avgt   15   311.725 ±  3.132   ns/op
ProducerResponseBenchmark.constructorStruct:·gc.alloc.rate                         avgt   15  2610.602 ± 25.964  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.alloc.rate.norm                    avgt   15   896.000 ±  0.001    B/op
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Eden_Space                avgt   15  2613.021 ± 42.965  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Eden_Space.norm           avgt   15   896.824 ± 11.331    B/op
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Old_Gen                   avgt   15     0.003 ±  0.001  MB/sec
ProducerResponseBenchmark.constructorStruct:·gc.churn.G1_Old_Gen.norm              avgt   15     0.001 ±  0.001    B/op
ProducerResponseBenchmark.constructorStruct:·gc.count                              avgt   15   343.000           counts
ProducerResponseBenchmark.constructorStruct:·gc.time                               avgt   15   194.000               ms
```

Reviewers: David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>",0,0,10,17,167,0,0,35,40,3,12,1.0,81,40,7,46,27,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorError.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorError.java,"KAFKA-9331: Add a streams specific uncaught exception handler (#9487)

This PR introduces a streams specific uncaught exception handler that currently has the option to close the client or the application. If the new handler is set as well as the old handler (java thread handler) will be ignored and an error will be logged.
The application shutdown is achieved through the rebalance protocol.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Leah Thomas <lthomas@confluent.io>, John Roesler <john@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",2,3,1,15,74,0,2,38,34,8,5,1,44,34,9,6,3,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/ShutdownDeadlockTest.java,streams/src/test/java/org/apache/kafka/streams/tests/ShutdownDeadlockTest.java,"KAFKA-9331: Add a streams specific uncaught exception handler (#9487)

This PR introduces a streams specific uncaught exception handler that currently has the option to close the client or the application. If the new handler is set as well as the old handler (java thread handler) will be ignored and an error will be logged.
The application shutdown is achieved through the rebalance protocol.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Leah Thomas <lthomas@confluent.io>, John Roesler <john@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",3,4,5,57,525,1,2,91,100,8,12,2.0,124,100,10,33,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-9331: Add a streams specific uncaught exception handler (#9487)

This PR introduces a streams specific uncaught exception handler that currently has the option to close the client or the application. If the new handler is set as well as the old handler (java thread handler) will be ignored and an error will be logged.
The application shutdown is achieved through the rebalance protocol.

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Leah Thomas <lthomas@confluent.io>, John Roesler <john@confluent.io>, Anna Sophie Blee-Goldman <ableegoldman@apache.org>",24,4,3,227,2236,1,10,293,241,6,46,3.5,768,241,17,475,113,10,2,1,0,1
core/src/main/scala/kafka/common/OffsetAndMetadata.scala,core/src/main/scala/kafka/common/OffsetAndMetadata.scala,"KAFKA-10497 Convert group coordinator metadata schemas to use generat… (#9318)

Reviewers: David Jacot <djacot@confluent.io>",2,0,4,24,153,2,2,48,52,24,2,1.0,52,52,26,4,4,2,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/producer/Callback.java,clients/src/main/java/org/apache/kafka/clients/producer/Callback.java,"KAFKA-10687: make ProduceRespone only returns INVALID_PRODUCER_EPOCH (#9569)

Ensures INVALID_PRODUCER_EPOCH recognizable from client side, and ensure the ProduceResponse always uses the old error code as INVALID_PRODUCER_EPOCH.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",0,1,0,4,26,0,0,56,19,6,9,1,70,19,8,14,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidProducerEpochException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidProducerEpochException.java,"KAFKA-10687: make ProduceRespone only returns INVALID_PRODUCER_EPOCH (#9569)

Ensures INVALID_PRODUCER_EPOCH recognizable from client side, and ensure the ProduceResponse always uses the old error code as INVALID_PRODUCER_EPOCH.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",1,5,6,7,39,0,1,32,33,16,2,2.0,38,33,19,6,6,3,2,1,0,1
core/src/main/scala/kafka/server/FinalizedFeatureCache.scala,core/src/main/scala/kafka/server/FinalizedFeatureCache.scala,"MINOR: Use string interpolation in FinalizedFeatureCache (#9602)

Reviewers: David Jacot <djacot@confluent.io>",13,7,7,79,456,2,5,162,99,54,3,4,183,99,61,21,14,7,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/KeyValueBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/KeyValueBytesStoreSupplier.java,"Improve JavaDoc (#9594)

In the JavaDoc, the implemented interface was described inaccurately.
Also, the ordered list was formatted as plain text, not as html ""ol"".

Reviewers: Boyang Chen <boyang@confluent.io>",0,7,5,4,41,0,0,33,26,7,5,1,41,26,8,8,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/ObjectSerializationCache.java,clients/src/main/java/org/apache/kafka/common/protocol/ObjectSerializationCache.java,"KAFKA-10684; Avoid additional envelope copies during network transmission (#9563)

This patch creates a new `SendBuilder` class which allows us to avoid copying ""zero copy"" types when transmitting an api message over the network. This generalizes the pattern that was previously used only for `FetchResponse`. Initially we only apply this optimization to the `Envelope` types and `FetchResponse`, but in the future, it can be the default implementation for `toSend`.

The patch also contains a few minor cleanups such as moving envelope parsing logic into `RequestContext`.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",5,4,6,21,144,3,5,55,57,28,2,1.5,61,57,30,6,6,3,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilderTest.java,"KAFKA-6687: rewrite topology to allow reading the same topic multiple times in the DSL (#9582)

Rewrite DSL topology to allow reading a topic or pattern multiple times

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Leah Thomas <lthomas@confluent.io>",25,0,5,293,3112,1,23,374,370,16,24,4.5,665,370,28,291,58,12,2,1,0,1
core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala,core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala,"MINOR: Fix comment in FinalizedFeatureChangeListener.initOrThrow (#9562)

Fixed the param doc in FinalizedFeatureChangeListener.initOrThrow method. The parameter waitOnceForCacheUpdateMs is expected to be > 0, but the doc was incorrect.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",24,1,1,129,739,0,10,257,251,64,4,2.0,269,251,67,12,7,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/Authenticator.java,clients/src/main/java/org/apache/kafka/common/network/Authenticator.java,"KAFKA-10181: Use Envelope RPC to do redirection for (Incremental)AlterConfig, AlterClientQuota and CreateTopics (#9103)

This PR adds support for forwarding of the following RPCs:

AlterConfigs
IncrementalAlterConfigs
AlterClientQuotas
CreateTopics

Co-authored-by: Jason Gustafson <jason@confluent.io>
Reviewers: Jason Gustafson <jason@confluent.io>",7,6,0,32,186,0,7,166,102,14,12,2.5,222,102,18,56,16,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/AbstractResponseTest.java,clients/src/test/java/org/apache/kafka/common/requests/AbstractResponseTest.java,"KAFKA-10181: Use Envelope RPC to do redirection for (Incremental)AlterConfig, AlterClientQuota and CreateTopics (#9103)

This PR adds support for forwarding of the following RPCs:

AlterConfigs
IncrementalAlterConfigs
AlterClientQuotas
CreateTopics

Co-authored-by: Jason Gustafson <jason@confluent.io>
Reviewers: Jason Gustafson <jason@confluent.io>",1,53,0,30,237,1,1,53,53,53,1,1,53,53,53,0,0,0,0,0,0,0
core/src/main/scala/kafka/controller/PartitionStateMachine.scala,core/src/main/scala/kafka/controller/PartitionStateMachine.scala,"MINOR: remove duplicate code in PartitionStateMachine.doHandleStateChanges (#9546)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",78,2,9,399,2810,2,22,575,416,7,85,4,2147,416,25,1572,226,18,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/internals/ApiUtils.java,streams/src/main/java/org/apache/kafka/streams/internals/ApiUtils.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",12,25,0,50,369,2,6,103,60,21,5,4,118,60,24,15,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/TransformerSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/TransformerSupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",0,13,3,6,71,0,0,53,24,5,10,2.0,67,24,7,14,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerSupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",0,11,3,5,46,0,0,52,24,5,11,2,68,24,6,16,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerWithKeySupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerWithKeySupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",0,20,1,6,71,0,0,54,33,18,3,2,56,33,19,2,1,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/api/ProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/processor/api/ProcessorSupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",0,13,3,8,92,0,0,53,43,26,2,3.0,56,43,28,3,3,2,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamTransformValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamTransformValuesTest.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",5,2,2,111,1078,1,3,152,92,5,31,3,470,92,15,318,85,10,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockApiProcessorSupplier.java,streams/src/test/java/org/apache/kafka/test/MockApiProcessorSupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",8,6,1,40,329,1,7,74,69,37,2,1.0,75,69,38,1,1,0,1,0,1,1
streams/src/test/java/org/apache/kafka/test/MockProcessorSupplier.java,streams/src/test/java/org/apache/kafka/test/MockProcessorSupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",8,6,1,40,297,1,7,74,58,5,15,2,186,58,12,112,73,7,2,1,0,1
streams/src/test/java/org/apache/kafka/test/NoOpValueTransformerWithKeySupplier.java,streams/src/test/java/org/apache/kafka/test/NoOpValueTransformerWithKeySupplier.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",1,16,17,23,150,3,1,45,46,22,2,3.5,62,46,31,17,17,8,0,0,0,0
streams/src/test/java/org/apache/kafka/test/NoopValueTransformer.java,streams/src/test/java/org/apache/kafka/test/NoopValueTransformer.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",3,35,0,15,90,3,3,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/test/NoopValueTransformerWithKey.java,streams/src/test/java/org/apache/kafka/test/NoopValueTransformerWithKey.java,"KAFKA-10036: Improve handling and documentation of Suppliers (#9000)

Reviewer: Matthias J. Sax <matthias@confluent.io>",3,35,0,15,96,3,3,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesOptions.java,"MINOR: KIP-584: Remove admin client facility to read features from controller (#9536)

In this PR, I have eliminated the facility in Admin#describeFeatures API and it's implementation to be able to optionally send a describeFeatures request to the controller. This feature was not seen to be particularly useful, and besides it also poses some hindrance to post KIP-500 world where no client would be able to access the controller directly.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Jun Rao <junrao@gmail.com>",0,0,20,5,38,2,0,28,48,14,2,1.0,48,48,24,20,20,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",20,2,2,109,793,1,9,146,76,9,16,1.0,232,76,14,86,23,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java,clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",22,130,0,98,451,12,12,130,130,130,1,1,130,130,130,0,0,0,2,1,0,1
core/src/main/scala/kafka/raft/TimingWheelExpirationService.scala,core/src/main/scala/kafka/raft/TimingWheelExpirationService.scala,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",3,64,0,37,230,3,3,64,64,64,1,1,64,64,64,0,0,0,2,1,0,1
core/src/main/scala/kafka/utils/timer/Timer.scala,core/src/main/scala/kafka/utils/timer/Timer.scala,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",9,2,5,68,440,2,5,125,86,12,10,2.0,151,86,15,26,5,3,2,1,0,1
core/src/main/scala/kafka/utils/timer/TimerTaskList.scala,core/src/main/scala/kafka/utils/timer/TimerTaskList.scala,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",16,1,1,99,591,2,9,159,132,20,8,2.0,194,132,24,35,18,4,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ElectionState.java,raft/src/main/java/org/apache/kafka/raft/ElectionState.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",27,1,1,87,608,2,14,125,125,62,2,1.0,126,125,63,1,1,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/ExpirationService.java,raft/src/main/java/org/apache/kafka/raft/ExpirationService.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",0,12,7,5,36,1,0,32,27,16,2,1.5,39,27,20,7,7,4,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/CloseListener.java,raft/src/main/java/org/apache/kafka/raft/internals/CloseListener.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",0,3,6,4,28,1,0,23,26,12,2,2.5,29,26,14,6,6,3,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/FuturePurgatory.java,raft/src/main/java/org/apache/kafka/raft/internals/FuturePurgatory.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",0,91,0,9,75,0,0,91,91,91,1,1,91,91,91,0,0,0,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/ThresholdPurgatory.java,raft/src/main/java/org/apache/kafka/raft/internals/ThresholdPurgatory.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",12,94,0,65,489,8,8,94,94,94,1,1,94,94,94,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockExpirationService.java,raft/src/test/java/org/apache/kafka/raft/MockExpirationService.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",9,78,0,52,368,5,5,78,78,78,1,1,78,78,78,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockExpirationServiceTest.java,raft/src/test/java/org/apache/kafka/raft/MockExpirationServiceTest.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",1,55,0,29,267,1,1,55,55,55,1,1,55,55,55,0,0,0,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/internals/ThresholdPurgatoryTest.java,raft/src/test/java/org/apache/kafka/raft/internals/ThresholdPurgatoryTest.java,"KAFKA-10632; Raft client should push all committed data to state machines (#9482)

In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.

Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Boyang Chen <boyang@confluent.io>",5,162,0,121,1182,5,5,162,162,162,1,1,162,162,162,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/OffsetCheckpoint.java,streams/src/main/java/org/apache/kafka/streams/state/internals/OffsetCheckpoint.java,"KAFKA-10664: Delete existing checkpoint when writing empty offsets (#9534)

Delete the existing checkpoint file if told to write empty offsets map to ensure that corrupted offsets are not re-initialized from

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@apache.org>",20,3,1,133,993,1,9,220,172,12,19,3,317,172,17,97,37,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/OffsetCheckpointTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/OffsetCheckpointTest.java,"KAFKA-10664: Delete existing checkpoint when writing empty offsets (#9534)

Delete the existing checkpoint file if told to write empty offsets map to ensure that corrupted offsets are not re-initialized from

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@apache.org>",9,20,1,132,1196,2,8,189,61,19,10,3.0,209,61,21,20,7,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java,connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java,"MINOR: improve `null` checks for headers (#9513)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Luke Chen @showuon",132,2,1,405,2539,2,42,497,519,99,5,2,530,519,106,33,27,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/QueryableStoreType.java,streams/src/main/java/org/apache/kafka/streams/state/QueryableStoreType.java,"KAFKA-10638: Fix QueryableStateIntegrationTest (#9521)

This test has been observed to have flaky failures.
Apparently, in the failed runs, Streams had entered a rebalance
before some of the assertions were made. We recently made
IQ a little stricter on whether it would return errors instead of
null responses in such cases:
KAFKA-10598: Improve IQ name and type checks (#9408)

As a result, we have started seeing failures now instead of
silently executing an invalid test (I.e., it was asserting the
return to be null, but the result was null for the wrong
reason).

Now, if the test discovers that Streams is no longer running,
it will repeat the verification until it actually gets a valid
positive or negative result.

Reviewers: Chia-Ping Tsai <chia7712@apache.org>",0,1,1,9,78,0,0,51,48,7,7,2,78,48,11,27,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/KafkaClient.java,clients/src/main/java/org/apache/kafka/clients/KafkaClient.java,"Revert ""KAFKA-9705 part 1: add KIP-590 request header fields (#9144)"" (#9523)

This reverts commit 21dc5231ce9c7398c7ede4dbefa2f2202e06b2d4 as we decide to use Envelope for redirection instead of initial principal.

Reviewers: Jason Gustafson <jason@confluent.io>",0,2,4,35,255,0,0,216,83,9,25,2,314,83,13,98,24,4,2,1,0,1
clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizableRequestContext.java,clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizableRequestContext.java,"Revert ""KAFKA-9705 part 1: add KIP-590 request header fields (#9144)"" (#9523)

This reverts commit 21dc5231ce9c7398c7ede4dbefa2f2202e06b2d4 as we decide to use Envelope for redirection instead of initial principal.

Reviewers: Jason Gustafson <jason@confluent.io>",0,0,9,16,110,1,0,72,72,18,4,1.0,82,72,20,10,9,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/protocol/DataOutputStreamWritable.java,clients/src/main/java/org/apache/kafka/common/protocol/DataOutputStreamWritable.java,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",26,146,0,113,561,13,13,146,146,146,1,1,146,146,146,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/RecordsReadable.java,clients/src/main/java/org/apache/kafka/common/protocol/RecordsReadable.java,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",13,11,0,62,317,2,12,103,92,52,2,1.5,103,92,52,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/RecordsWritable.java,clients/src/main/java/org/apache/kafka/common/protocol/RecordsWritable.java,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",14,10,0,79,475,2,13,149,139,74,2,1.0,149,139,74,0,0,0,1,0,0,0
core/src/main/scala/kafka/common/RecordValidationException.scala,core/src/main/scala/kafka/common/RecordValidationException.scala,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",0,2,1,8,59,0,0,28,25,9,3,1,30,25,10,2,1,1,2,1,0,1
raft/src/main/java/org/apache/kafka/raft/internals/BatchMemoryPool.java,raft/src/main/java/org/apache/kafka/raft/internals/BatchMemoryPool.java,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",10,107,0,74,389,6,6,107,107,107,1,1,107,107,107,0,0,0,0,0,0,0
raft/src/test/java/org/apache/kafka/raft/internals/BatchMemoryPoolTest.java,raft/src/test/java/org/apache/kafka/raft/internals/BatchMemoryPoolTest.java,"KAFKA-10601; Add support for append linger to Raft implementation (#9418)

The patch adds `quorum.append.linger.ms` behavior to the raft implementation. This gives users a powerful knob to tune the impact of fsync.  When an append is accepted from the state machine, it is held in an accumulator (similar to the producer) until the configured linger time is exceeded. This allows the implementation to amortize fsync overhead at the expense of some write latency.

The patch also improves our methodology for testing performance. Up to now, we have relied on the producer performance test, but it is difficult to simulate expected controller loads because producer performance is limited by other factors such as the number of producer clients and head-of-line blocking. Instead, this patch adds a workload generator which runs on the leader after election.

Finally, this patch brings us nearer to the write semantics expected by the KIP-500 controller. It makes the following changes:

- Introduce `RecordSerde<T>` interface which abstracts the underlying log implementation from `RaftClient`. The generic type is carried over to `RaftClient<T>` and is exposed through the read/write APIs.
- `RaftClient.append` is changed to `RaftClient.scheduleAppend` and returns the last offset of the expected log append.
- `RaftClient.scheduleAppend` accepts a list of records and ensures that the full set are included in a single batch.
- Introduce `RaftClient.Listener` with a single `handleCommit` API which will eventually replace `RaftClient.read` in order to surface committed data to the controller state machine. Currently `handleCommit` is only used for records appended by the leader.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",5,107,0,69,566,4,4,107,107,107,1,1,107,107,107,0,0,0,0,0,0,0
tests/kafkatest/tests/client/quota_test.py,tests/kafkatest/tests/client/quota_test.py,"MINOR: fix error in quota_test.py system tests

quota_test.py tests are failing with below error.

```
23:24:42 [INFO:2020-10-24 17:54:42,366]: RunnerClient: kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=False: FAIL: not enough arguments for format string
23:24:42 Traceback (most recent call last):
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/tests/runner_client.py"", line 134, in run
23:24:42     data = self.run_test()
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/tests/runner_client.py"", line 192, in run_test
23:24:42     return self.test_context.function(self.test)
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/mark/_mark.py"", line 429, in wrapper
23:24:42     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 141, in test_quota
23:24:42     self.quota_config = QuotaConfig(quota_type, override_quota, self.kafka)
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 60, in __init__
23:24:42     self.configure_quota(kafka, self.producer_quota, self.consumer_quota, ['users', None])
23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 83, in configure_quota
23:24:42     (kafka.kafka_configs_cmd_with_optional_security_settings(node, force_use_zk_conection), producer_byte_rate, consumer_byte_rate)
23:24:42 TypeError: not enough arguments for format string
23:24:42
```

ran thee tests locally.

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: David Jacot <djacot@confluent.io>, Ron Dagostino <rndgstn@gmail.com>

Closes #9496 from omkreddy/quota-tests",25,1,1,173,1569,1,8,237,180,13,18,1.0,310,180,17,73,34,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/internals/ConsumerGroupOperationContext.java,clients/src/main/java/org/apache/kafka/clients/admin/internals/ConsumerGroupOperationContext.java,"MINOR: simplify implementation of ConsumerGroupOperationContext.hasCo… (#9449)

Reviewers: David Jacot <djacot@confluent.io>",11,9,5,53,345,4,10,91,87,46,2,2.0,96,87,48,5,5,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/LagInfo.java,streams/src/main/java/org/apache/kafka/streams/LagInfo.java,"MINOR: Clean-up streams javadoc warnings (#9461)

Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>",10,1,1,43,214,0,7,91,91,46,2,1.0,92,91,46,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/StateStore.java,streams/src/main/java/org/apache/kafka/streams/processor/StateStore.java,"MINOR: Clean-up streams javadoc warnings (#9461)

Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>",1,1,1,16,132,0,1,122,52,9,14,1.0,139,52,10,17,7,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/api/Record.java,streams/src/main/java/org/apache/kafka/streams/processor/api/Record.java,"MINOR: Clean-up streams javadoc warnings (#9461)

Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>",24,2,2,73,509,0,13,193,165,64,3,2,195,165,65,2,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableSerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableSerde.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",6,68,0,42,323,6,6,68,68,68,1,1,68,68,68,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableUtils.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableUtils.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",20,97,0,66,698,11,11,97,97,97,1,1,97,97,97,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerde.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",17,8,18,111,917,3,7,157,119,26,6,6.5,207,119,34,50,18,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedWindowStore.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",3,6,30,28,253,3,2,57,58,7,8,3.0,112,58,14,55,30,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampDeserializer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampDeserializer.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",9,10,1,60,454,1,8,93,84,46,2,2.0,94,84,47,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerde.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerde.java,"KAFKA-10515: Properly initialize nullable Serdes with default values (#9338)

Also introduced the notion of WrappingNullableSerdes (aligned to the concept
of WrappingNullableSerializer and WrappingNullableDeserializer) and centralized
initialization in WrappingNullables.

The added integeration test KTableKTableForeignKeyJoinDistributedTest tests
whether all serdes are now correctly set on all stream clients.

Reviewers: John Roesler <vvcephei@apache.org>",1,7,34,13,133,5,1,32,59,11,3,1,67,59,22,35,34,12,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java,clients/src/main/java/org/apache/kafka/clients/admin/FeatureUpdate.java,"MINOR: Clean-up client javadoc warnings (#9463)

Reviewers: Boyang Chen <boyang@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",11,1,1,40,205,0,6,78,78,39,2,1.0,79,78,40,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java,"MINOR: Clean-up client javadoc warnings (#9463)

Reviewers: Boyang Chen <boyang@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",10,1,1,50,493,0,3,117,97,15,8,2.5,154,97,19,37,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/SslEngineFactory.java,clients/src/main/java/org/apache/kafka/common/security/auth/SslEngineFactory.java,"MINOR: Clean-up client javadoc warnings (#9463)

Reviewers: Boyang Chen <boyang@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>",0,1,1,15,123,0,0,89,88,30,3,1,114,88,38,25,24,8,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultConfigPropertyFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultConfigPropertyFilter.java,"KAFKA-10572 mirror-maker config changes for KIP-629 (#9429)

Author: Xavier Léauté <xavier@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",7,33,24,56,355,8,6,86,77,43,2,5.0,110,77,55,24,24,12,1,0,1,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultGroupFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultGroupFilter.java,"KAFKA-10572 mirror-maker config changes for KIP-629 (#9429)

Author: Xavier Léauté <xavier@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",11,42,33,66,436,12,8,100,91,50,2,6.5,133,91,66,33,33,16,1,0,1,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultTopicFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultTopicFilter.java,"KAFKA-10572 mirror-maker config changes for KIP-629 (#9429)

Author: Xavier Léauté <xavier@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",11,40,32,66,436,12,8,99,91,50,2,7.0,131,91,66,32,32,16,1,0,1,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java,"KAFKA-10572 mirror-maker config changes for KIP-629 (#9429)

Author: Xavier Léauté <xavier@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",43,52,26,556,3272,1,35,671,601,96,7,4,710,601,101,39,26,6,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeader.java,connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeader.java,"MINOR: Remove unnecessary assertion from ConnectHeader (#9452)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>",17,0,1,63,415,1,9,96,97,48,2,1.0,97,97,48,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogReader.java,"KAFKA-9274: Add timeout handling for state restore and StandbyTasks (#9368)

* Part of KIP-572
* If a TimeoutException happens during restore of active tasks, or updating standby tasks, we need to trigger task.timeout.ms timeout.

Reviewers: John Roesler <john@confluent.io>",0,3,1,13,99,0,0,58,52,4,13,2,112,52,9,54,16,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ClientUtils.java,"KAFKA-9274: Add timeout handling for state restore and StandbyTasks (#9368)

* Part of KIP-572
* If a TimeoutException happens during restore of active tasks, or updating standby tasks, we need to trigger task.timeout.ms timeout.

Reviewers: John Roesler <john@confluent.io>",26,6,6,115,1005,1,14,164,111,20,8,3.5,219,111,27,55,17,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/MockChangelogReader.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/MockChangelogReader.java,"KAFKA-9274: Add timeout handling for state restore and StandbyTasks (#9368)

* Part of KIP-572
* If a TimeoutException happens during restore of active tasks, or updating standby tasks, we need to trigger task.timeout.ms timeout.

Reviewers: John Roesler <john@confluent.io>",11,2,1,47,276,2,9,80,54,5,15,1,129,54,9,49,18,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ReferenceContainer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ReferenceContainer.java,"MINOR: remove explicit passing of AdminClient into StreamsPartitionAssignor (#9384)

Currently, we pass multiple object reference (AdminClient,TaskManager, and a few more) into StreamsPartitionAssignor. Furthermore, we (miss)use TaskManager#mainConsumer() to get access to the main consumer (we need to do this, to avoid a cyclic dependency).

This PR unifies how object references are passed into a single ReferenceContainer class to
 - not ""miss use"" the TaskManager as reference container
 - unify how object references are passes

Note: we need to use a reference container to avoid cyclic dependencies, instead of using a config for each passed reference individually.

Reviewers: John Roesler <john@confluent.io>",0,36,0,17,163,0,0,36,36,36,1,1,36,36,36,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsAssignmentScaleTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsAssignmentScaleTest.java,"MINOR: remove explicit passing of AdminClient into StreamsPartitionAssignor (#9384)

Currently, we pass multiple object reference (AdminClient,TaskManager, and a few more) into StreamsPartitionAssignor. Furthermore, we (miss)use TaskManager#mainConsumer() to get access to the main consumer (we need to do this, to avoid a cyclic dependency).

This PR unifies how object references are passed into a single ReferenceContainer class to
 - not ""miss use"" the TaskManager as reference container
 - unify how object references are passes

Note: we need to use a reference container to avoid cyclic dependencies, instead of using a config for each passed reference individually.

Reviewers: John Roesler <john@confluent.io>",22,27,28,198,1884,1,14,255,256,128,2,4.5,283,256,142,28,28,14,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/RetryWithToleranceOperator.java,"KAFKA-10602: Make RetryWithToleranceOperator thread safe (#9422)

ErrantRecordReporter uses a RetryWithToleranceOperator instance, which is necessarily stateful, having a ProcessingContext of which there's supposed to be one per task. That ProcessingContext is used by both RetryWithToleranceOperator.executeFailed() and execute(), so it's not enough to just synchronize executeFailed().

So make all public methods of RetryWithToleranceOperator synchronized so that RetryWithToleranceOperator is now threadsafe.

Tested with the addition of a multithreaded test case that fails consistently if the methods are not properly synchronized. 

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>",35,21,11,191,1256,12,18,314,328,45,7,2,411,328,59,97,81,14,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java,clients/src/main/java/org/apache/kafka/common/security/ssl/DefaultSslEngineFactory.java,"MINOR: Fix typos in DefaultSslEngineFactory javadoc (#9413)

Fix comment typos.

Reviewers: Boyang Chen <boyang@confluent.io>, Lee Dongjin <dongjin@apache.org>",115,2,2,481,3577,0,36,584,316,73,8,4.0,725,316,91,141,96,18,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ReplaceField.java,"KAFKA-10573 Update connect transforms configs for KIP-629 (#9403)

Changes the Connect `ReplaceField` SMT's configuration properties, deprecating and replacing `blacklist` with `exclude`, and `whitelist` with `include`. The old configurations are still allowed (ensuring backward compatibility), but warning messages are written to the log to suggest users change to `include` and `exclude`.

This is part of KIP-629.

Author: Xavier Léauté <xvrl@apache.org>
Reviewer: Randall Hauch <rhauch@gmail.com>",33,26,11,184,1629,2,18,249,230,42,6,1.5,271,230,45,22,11,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/SafeObjectInputStream.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/SafeObjectInputStream.java,"MINOR internal KIP-629 changes to methods and variables

cc gwenshap

Author: Xavier Léauté <xvrl@apache.org>

Reviewers: Gwen Shapira

Closes #9405 from xvrl/minor-kip-629-vars",6,2,2,43,263,3,3,71,71,36,2,1.5,73,71,36,2,2,1,1,0,1,1
core/src/main/scala/kafka/tools/JmxTool.scala,core/src/main/scala/kafka/tools/JmxTool.scala,"MINOR internal KIP-629 changes to methods and variables

cc gwenshap

Author: Xavier Léauté <xvrl@apache.org>

Reviewers: Gwen Shapira

Closes #9405 from xvrl/minor-kip-629-vars",33,10,10,221,1705,3,3,275,110,9,29,2,388,110,13,113,25,4,2,1,0,1
core/src/main/scala/kafka/utils/TopicFilter.scala,core/src/main/scala/kafka/utils/TopicFilter.scala,"MINOR rename kafka.utils.Whitelist to IncludeList

rename internal classes, methods, and related constants for KIP-629

Author: Xavier Léauté <xvrl@apache.org>

Reviewers: Gwen Shapira

Closes #9400 from xvrl/rename-topic-includelist",0,1,1,33,92,0,0,131,76,12,11,3,172,76,16,41,17,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java,clients/src/main/java/org/apache/kafka/common/metrics/JmxReporter.java,"KAFKA-10570; Rename JMXReporter configs for KIP-629

* rename whitelist/blacklist to include/exclude
* add utility methods to translate deprecated configs

Author: Xavier Léauté <xvrl@apache.org>

Reviewers: Gwen Shapira

Closes #9367 from xvrl/kafka-10570",58,28,18,276,1890,2,28,356,184,14,25,2,557,184,22,201,27,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableFilterTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableFilterTest.java,"KAFKA-10494: Eager handling of sending old values (#9415)

Nodes that are materialized should not forward requests to `enableSendingOldValues` to parent nodes, as they themselves can handle fulfilling this request. However, some instances of `KTableProcessorSupplier` were still forwarding requests to parent nodes, which was causing unnecessary materialization of table sources.

The following instances of `KTableProcessorSupplier` have been updated to not forward `enableSendingOldValues` to parent nodes if they themselves are materialized and can handle sending old values downstream:

 * `KTableFilter`
 * `KTableMapValues`
 * `KTableTransformValues`

Other instances of `KTableProcessorSupplier` have not be modified for reasons given below:
 * `KTableSuppressProcessorSupplier`: though it has a `storeName` field, it didn't seem right for this to handle sending old values itself. Its only job is to suppress output.
 * `KTableKTableAbstractJoin`: doesn't have a store name, i.e. it is never materialized, so can't handle the call itself.
 * `KTableKTableJoinMerger`: table-table joins already have materialized sources, which are sending old values. It would be an unnecessary performance hit to have this class do a lookup to retrieve the old value from its store.
 * `KTableReduce`: is always materialized and already handling the call without forwarding
 * `KTableAggregate`: is always materialized and already handling the call without forwarding

Reviewer: Matthias J. Sax <matthias@confluent.io>",23,41,12,383,4552,4,18,508,147,13,40,9.0,1431,194,36,923,164,23,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableMapValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableMapValuesTest.java,"KAFKA-10494: Eager handling of sending old values (#9415)

Nodes that are materialized should not forward requests to `enableSendingOldValues` to parent nodes, as they themselves can handle fulfilling this request. However, some instances of `KTableProcessorSupplier` were still forwarding requests to parent nodes, which was causing unnecessary materialization of table sources.

The following instances of `KTableProcessorSupplier` have been updated to not forward `enableSendingOldValues` to parent nodes if they themselves are materialized and can handle sending old values downstream:

 * `KTableFilter`
 * `KTableMapValues`
 * `KTableTransformValues`

Other instances of `KTableProcessorSupplier` have not be modified for reasons given below:
 * `KTableSuppressProcessorSupplier`: though it has a `storeName` field, it didn't seem right for this to handle sending old values itself. Its only job is to suppress output.
 * `KTableKTableAbstractJoin`: doesn't have a store name, i.e. it is never materialized, so can't handle the call itself.
 * `KTableKTableJoinMerger`: table-table joins already have materialized sources, which are sending old values. It would be an unnecessary performance hit to have this class do a lookup to retrieve the old value from its store.
 * `KTableReduce`: is always materialized and already handling the call without forwarding
 * `KTableAggregate`: is always materialized and already handling the call without forwarding

Reviewer: Matthias J. Sax <matthias@confluent.io>",9,52,13,247,2997,4,9,323,198,8,38,7.0,1165,198,31,842,191,22,2,1,0,1
streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorSupplier.java,streams/test-utils/src/test/java/org/apache/kafka/streams/test/wordcount/WindowedWordCountProcessorSupplier.java,"KAFKA-10437: Implement new PAPI support for test-utils (#9396)

Implements KIP-478 for the test-utils module:
* adds mocks of the new ProcessorContext and StateStoreContext
* adds tests that all stores and store builders are usable with the new mock
* adds tests that the new Processor api is usable with the new mock
* updates the demonstration Processor to the new api

Reviewers: Guozhang Wang <guozhang@apache.org>",4,14,19,46,458,1,1,73,78,36,2,5.0,92,78,46,19,19,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Suppressed.java,streams/src/main/java/org/apache/kafka/streams/kstream/Suppressed.java,"KAFKA-10598: Improve IQ name and type checks (#9408)

Previously, we would throw a confusing error, ""the store has migrated,""
when users ask for a store that is not in the topology at all, or when the
type of the store doesn't match the QueryableStoreType parameter.

Adds an up-front check that the requested store is registered and also
a better error message when the QueryableStoreType parameter
doesn't match the store's type.

Reviewers: Guozhang Wang <guozhang@apache.org>",5,1,1,39,317,0,5,197,160,22,9,2,218,160,24,21,12,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/MaterializedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/MaterializedInternal.java,"KAFKA-10598: Improve IQ name and type checks (#9408)

Previously, we would throw a confusing error, ""the store has migrated,""
when users ask for a store that is not in the topology at all, or when the
type of the store doesn't match the QueryableStoreType parameter.

Adds an up-front check that the requested store is registered and also
a better error message when the QueryableStoreType parameter
doesn't match the store's type.

Reviewers: Guozhang Wang <guozhang@apache.org>",15,4,4,52,312,2,11,86,62,10,9,3,124,62,14,38,10,4,2,1,0,1
tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py,tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py,"MINOR: ACLs for secured cluster system tests (#9378)

This PR adds missing broker ACLs required to create topics and SCRAM credentials when ACLs are enabled for a system test. This PR also adds support for using PLAINTEXT as the inter broker security protocol when using SCRAM from the client in a system test with a secured cluster-- without this it would always be necessary to set both the inter-broker and client mechanisms to a SCRAM mechanism. Also contains some refactoring to make assumptions clearer.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",14,2,1,68,541,1,7,112,103,6,18,1.0,147,103,8,35,11,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java,"KAFKA-10271: Performance regression while fetching a key from a single partition (#9020)

StreamThreadStateStoreProvider excessive loop over calling internalTopologyBuilder.topicGroups(), which is synchronized, thus causing significant performance degradation to the caller, especially when store has many partitions.

Reviewers: John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",3,0,20,27,208,1,2,63,54,4,14,2.0,135,54,10,72,20,5,2,1,0,1
streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java,streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java,"KAFKA-10271: Performance regression while fetching a key from a single partition (#9020)

StreamThreadStateStoreProvider excessive loop over calling internalTopologyBuilder.topicGroups(), which is synchronized, thus causing significant performance degradation to the caller, especially when store has many partitions.

Reviewers: John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",9,1,1,52,460,1,4,78,43,9,9,3,109,43,12,31,11,3,2,1,0,1
core/src/main/scala/kafka/metrics/LinuxIoMetricsCollector.scala,core/src/main/scala/kafka/metrics/LinuxIoMetricsCollector.scala,"MINOR: correct package of LinuxIoMetricsCollector (#9271)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Lee Dongjin <dongjin@apache.org>",11,1,1,59,327,0,4,102,102,51,2,1.0,103,102,52,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheSessionStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheSessionStoreIterator.java,"KAFKA-9929: Support backward iterator on SessionStore (#9139)

Implements KIP-617 for `SessionStore`

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",6,3,2,40,343,2,6,70,71,6,11,1,133,71,12,63,17,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingSessionBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingSessionBytesStoreTest.java,"KAFKA-9929: Support backward iterator on SessionStore (#9139)

Implements KIP-617 for `SessionStore`

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",16,40,0,156,1195,4,16,226,177,19,12,2.0,298,177,25,72,29,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedSessionStoreIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedSessionStoreIteratorTest.java,"KAFKA-9929: Support backward iterator on SessionStore (#9139)

Implements KIP-617 for `SessionStore`

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",16,54,9,115,1206,16,16,155,113,13,12,3.0,235,113,20,80,24,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueSegments.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",5,1,1,34,253,1,3,61,45,7,9,1,79,45,9,18,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueToTimestampedKeyValueByteStoreAdapter.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueToTimestampedKeyValueByteStoreAdapter.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",23,7,0,100,695,1,18,149,131,50,3,2,149,131,50,0,0,0,1,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryLRUCache.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryLRUCache.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",26,12,0,132,859,1,20,199,151,8,26,2.5,376,151,14,177,54,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilder.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",28,7,0,146,939,1,22,190,96,32,6,2.5,194,98,32,4,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimestampedSegments.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",5,1,1,34,253,1,3,61,48,8,8,1.0,75,48,9,14,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedStateStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedStateStore.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",18,7,0,74,431,1,13,109,90,9,12,3.0,206,90,17,97,60,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueLoggedStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueLoggedStoreTest.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",1,2,2,21,206,2,1,43,49,5,9,3,99,49,11,56,20,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreTest.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",8,2,2,77,613,2,4,111,49,4,25,2,252,50,10,141,34,6,2,1,0,1
streams/src/test/java/org/apache/kafka/test/GenericInMemoryKeyValueStore.java,streams/src/test/java/org/apache/kafka/test/GenericInMemoryKeyValueStore.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",25,1,0,131,861,0,21,185,174,46,4,1.0,198,174,50,13,8,3,2,1,0,1
streams/src/test/java/org/apache/kafka/test/GenericInMemoryTimestampedKeyValueStore.java,streams/src/test/java/org/apache/kafka/test/GenericInMemoryTimestampedKeyValueStore.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",25,1,0,132,931,0,21,186,190,62,3,1,191,190,64,5,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java,streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",17,1,0,97,584,0,17,137,84,15,9,4,169,84,19,32,22,4,2,1,0,1
streams/src/test/java/org/apache/kafka/test/NoOpReadOnlyStore.java,streams/src/test/java/org/apache/kafka/test/NoOpReadOnlyStore.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",14,1,0,71,390,0,13,105,91,15,7,1,117,91,17,12,6,2,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/internals/KeyValueStoreFacade.java,streams/test-utils/src/main/java/org/apache/kafka/streams/internals/KeyValueStoreFacade.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",13,7,0,67,476,1,12,99,92,33,3,3,103,92,34,4,4,1,1,0,1,1
streams/test-utils/src/main/java/org/apache/kafka/streams/internals/WindowStoreFacade.java,streams/test-utils/src/main/java/org/apache/kafka/streams/internals/WindowStoreFacade.java,"KAFKA-10562: Properly invoke new StateStoreContext init (#9388)

* all wrapping stores should pass StateStoreContext init through to the same
  method on the wrapped store and not translate it to ProcessorContext init
* base-level stores should handle StateStoreContext init so that callers passing
  a non-InternalProcessorContext implementation will be able to initialize the store
* extra tests are added to verify the desired behavior

Reviewers: Guozhang Wang <guozhang@apache.org>",13,7,0,77,551,1,13,109,76,27,4,2.0,109,76,27,0,0,0,1,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java,connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",1,1,1,17,119,0,1,126,59,10,13,2,170,59,13,44,21,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java,connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,1,1,7,51,0,0,41,40,5,8,2.0,61,40,8,20,10,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",31,0,1,175,1147,1,12,225,121,15,15,2,293,129,20,68,17,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueStoreBuilder.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,2,1,40,311,1,4,64,62,13,5,1,72,62,14,8,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SessionStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SessionStoreBuilder.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",7,2,1,42,329,1,5,68,63,14,5,1,82,63,16,14,9,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/TimestampedKeyValueStoreMaterializerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/TimestampedKeyValueStoreMaterializerTest.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,1,0,96,1174,1,5,126,116,14,9,6,197,116,22,71,24,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SessionStoreBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SessionStoreBuilderTest.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",12,18,5,129,1179,3,12,166,141,42,4,5.5,198,141,50,32,19,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilderTest.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",14,10,1,142,1262,2,14,183,143,30,6,3.5,204,143,34,21,10,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilderTest.java,"MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc. (#8130)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",15,21,0,181,1454,2,15,224,183,45,5,4,236,183,47,12,8,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeFeaturesResult.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",2,37,0,11,64,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java,clients/src/main/java/org/apache/kafka/clients/admin/FeatureMetadata.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",12,111,0,62,414,8,8,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java,clients/src/main/java/org/apache/kafka/clients/admin/FinalizedVersionRange.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",12,84,0,46,215,6,6,84,84,84,1,1,84,84,84,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/SupportedVersionRange.java,clients/src/main/java/org/apache/kafka/clients/admin/SupportedVersionRange.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",13,82,0,42,218,6,6,82,82,82,1,1,82,82,82,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesOptions.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",0,29,0,6,45,0,0,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java,clients/src/main/java/org/apache/kafka/clients/admin/UpdateFeaturesResult.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",3,48,0,15,118,3,3,48,48,48,1,1,48,48,48,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/FeatureUpdateFailedException.java,clients/src/main/java/org/apache/kafka/common/errors/FeatureUpdateFailedException.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",2,29,0,10,60,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java,clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",21,17,4,76,511,4,9,137,124,68,2,3.0,141,124,70,4,4,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java,clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",4,2,2,17,131,0,3,53,53,26,2,1.5,55,53,28,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java,clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",3,7,3,17,119,3,3,44,40,22,2,1.5,47,40,24,3,3,2,1,0,1,1
core/src/main/scala/kafka/controller/ControllerState.scala,core/src/main/scala/kafka/controller/ControllerState.scala,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",0,6,1,73,331,0,0,122,83,12,10,2.0,133,83,13,11,3,1,2,1,0,1
core/src/main/scala/kafka/server/BrokerFeatures.scala,core/src/main/scala/kafka/server/BrokerFeatures.scala,"KAFKA-10028: Implement write path for feature versioning system (KIP-584) (#9001)

Summary:
In this PR, I have implemented the write path of the feature versioning system (KIP-584). Here is a summary of what's in this PR:

New APIs in org.apache.kafka.clients.admin.Admin interface, and their client and server implementations. These APIs can be used to describe features and update finalized features. These APIs are: Admin#describeFeatures and Admin#updateFeatures.
The write path is provided by the Admin#updateFeatures API. The corresponding server-side implementation is provided in KafkaApis and KafkaController classes. This can be a good place to start the code review.
The write path is supplemented by Admin#describeFeatures client API. This does not translate 1:1 to a server-side API. Instead, under the hood the API makes an explicit ApiVersionsRequest to the Broker to fetch the supported and finalized features.
Implemented a suite of integration tests in UpdateFeaturesTest.scala that thoroughly exercises the various cases in the write path.

Other changes:

The data type of the FinalizedFeaturesEpoch field in ApiVersionsResponse has been modified from int32 to int64. This change is to conform with the latest changes to the KIP explained in the voting thread.
Along the way, the class SupportedFeatures has been renamed to be called BrokerFeatures, and, it now holds both supported features as well as default minimum version levels.
For the purpose of testing, both the BrokerFeatures and FinalizedFeatureCache classes have been changed to be no longer singleton in implementation. Instead, these are now instantiated once and maintained in KafkaServer. The singleton instances are passed around to various classes, as needed.

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",13,116,0,51,414,5,5,116,116,116,1,1,116,116,116,0,0,0,0,0,0,0
tests/kafkatest/benchmarks/core/benchmark_test.py,tests/kafkatest/benchmarks/core/benchmark_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",9,2,2,237,1831,0,9,281,274,20,14,8.5,598,274,43,317,161,23,2,1,0,1
tests/kafkatest/services/kafka/__init__.py,tests/kafkatest/services/kafka/__init__.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",0,3,3,3,15,0,0,18,16,4,4,1.0,21,16,5,3,3,1,2,1,0,1
tests/kafkatest/services/monitor/http.py,tests/kafkatest/services/monitor/http.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",37,3,3,146,1123,2,14,228,226,76,3,2,231,226,77,3,3,1,1,0,1,1
tests/kafkatest/services/monitor/jmx.py,tests/kafkatest/services/monitor/jmx.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",33,1,1,106,931,1,10,156,81,10,16,3.0,207,81,13,51,11,3,2,1,0,1
tests/kafkatest/services/performance/__init__.py,tests/kafkatest/services/performance/__init__.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",0,4,4,4,26,0,0,19,19,6,3,1,25,19,8,6,4,2,1,0,1,1
tests/kafkatest/services/security/minikdc.py,tests/kafkatest/services/security/minikdc.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",13,0,2,78,692,0,7,134,79,11,12,2.0,176,79,15,42,18,4,2,1,0,1
tests/kafkatest/services/trogdor/task_spec.py,tests/kafkatest/services/trogdor/task_spec.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",5,1,1,24,95,1,3,54,45,14,4,3.0,66,45,16,12,6,3,2,1,0,1
tests/kafkatest/services/verifiable_client.py,tests/kafkatest/services/verifiable_client.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",35,0,1,240,928,0,17,346,330,69,5,1,353,330,71,7,5,1,2,1,0,1
tests/kafkatest/services/verifiable_consumer.py,tests/kafkatest/services/verifiable_consumer.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",94,10,10,324,2458,9,34,418,222,16,26,3.5,621,222,24,203,72,8,2,1,0,1
tests/kafkatest/services/zookeeper.py,tests/kafkatest/services/zookeeper.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",47,0,1,176,1566,0,23,257,64,11,24,3.0,342,68,14,85,15,4,2,1,0,1
tests/kafkatest/tests/core/mirror_maker_test.py,tests/kafkatest/tests/core/mirror_maker_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",13,1,1,87,896,0,7,171,164,16,11,1,212,164,19,41,25,4,2,1,0,1
tests/kafkatest/tests/core/network_degrade_test.py,tests/kafkatest/tests/core/network_degrade_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",18,1,1,85,754,1,5,138,138,69,2,1.0,139,138,70,1,1,0,2,1,0,1
tests/kafkatest/tests/core/throttling_test.py,tests/kafkatest/tests/core/throttling_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",11,1,1,130,837,1,6,180,174,14,13,1,193,174,15,13,3,1,2,1,0,1
tests/kafkatest/tests/core/zookeeper_tls_encrypt_only_test.py,tests/kafkatest/tests/core/zookeeper_tls_encrypt_only_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",6,0,3,50,392,0,5,88,93,29,3,1,94,93,31,6,3,2,2,1,0,1
tests/kafkatest/tests/core/zookeeper_tls_test.py,tests/kafkatest/tests/core/zookeeper_tls_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",9,0,3,84,700,0,8,153,156,76,2,1.5,156,156,78,3,3,2,1,0,0,0
tests/kafkatest/tests/streams/streams_broker_bounce_test.py,tests/kafkatest/tests/streams/streams_broker_bounce_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",32,2,2,170,1485,1,16,293,213,24,12,1.5,320,213,27,27,8,2,2,1,0,1
tests/kafkatest/tests/streams/utils/__init__.py,tests/kafkatest/tests/streams/utils/__init__.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",0,1,1,1,13,0,0,16,16,5,3,1,18,16,6,2,1,1,2,1,0,1
tests/kafkatest/tests/verifiable_consumer_test.py,tests/kafkatest/tests/verifiable_consumer_test.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",14,1,1,57,522,1,11,87,83,17,5,2,100,83,20,13,6,3,2,1,0,1
tests/kafkatest/utils/__init__.py,tests/kafkatest/utils/__init__.py,"KAFKA-10402: Upgrade system tests to python3 (#9196)

For now, Kafka system tests use python2 which is outdated and not supported.
This PR upgrades python to the third version.

Reviewers: Ivan Daschinskiy, Mickael Maison <mickael.maison@gmail.com>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",0,1,1,1,15,0,0,16,15,2,7,1,22,15,3,6,1,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TransactionAbortedException.java,clients/src/main/java/org/apache/kafka/common/errors/TransactionAbortedException.java,"KAFKA-10186; Abort transaction with pending data with TransactionAbortedException (#9280)

If a transaction is aborted with no underlying exception, throw a new kind of exception - `TransactionAbortedException` to
distinguish this from other fatal exceptions.

This API change is documented in KIP-654: https://cwiki.apache.org/confluence/display/KAFKA/KIP-654:+Aborted+transaction+with+non-flushed+data+should+throw+a+non-fatal+exception.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Jason Gustafson <jason@confluent.io>",3,38,0,13,68,3,3,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/network/CertStores.java,clients/src/test/java/org/apache/kafka/common/network/CertStores.java,"KAFKA-10338; Support PEM format for SSL key and trust stores (KIP-651) (#9345)

Adds support for SSL key and trust stores to be specified in PEM format either as files or directly as configuration values.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",23,71,4,117,835,9,16,157,67,22,7,3,181,71,26,24,9,3,2,1,0,1
clients/src/test/java/org/apache/kafka/test/TestSslUtils.java,clients/src/test/java/org/apache/kafka/test/TestSslUtils.java,"KAFKA-10338; Support PEM format for SSL key and trust stores (KIP-651) (#9345)

Adds support for SSL key and trust stores to be specified in PEM format either as files or directly as configuration values.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",87,253,49,513,4287,17,43,623,243,26,24,3.0,857,253,36,234,59,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/StateStoreContext.java,streams/src/main/java/org/apache/kafka/streams/processor/StateStoreContext.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",0,112,0,18,134,0,0,112,112,112,1,1,112,112,112,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/api/Processor.java,streams/src/main/java/org/apache/kafka/streams/processor/api/Processor.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",2,3,4,10,107,0,2,63,64,32,2,2.0,67,64,34,4,4,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/api/ProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/api/ProcessorContext.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",0,92,81,31,296,0,0,250,239,62,4,2.0,334,239,84,84,81,21,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/api/RecordMetadata.java,streams/src/main/java/org/apache/kafka/streams/processor/api/RecordMetadata.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",0,34,0,6,33,0,0,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorAdapter.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorAdapter.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",8,22,3,52,438,3,6,81,53,20,4,1.0,85,53,21,4,3,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ToInternal.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ToInternal.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",6,4,0,22,112,1,6,45,41,22,2,1.0,45,41,22,0,0,0,1,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorTest.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",23,2,1,346,3940,1,19,465,204,39,12,8.0,769,208,64,304,120,25,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockApiProcessor.java,streams/src/test/java/org/apache/kafka/test/MockApiProcessor.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",21,8,13,101,825,3,14,146,151,73,2,2.5,159,151,80,13,13,6,1,0,1,1
streams/src/test/java/org/apache/kafka/test/MockProcessor.java,streams/src/test/java/org/apache/kafka/test/MockProcessor.java,"KAFKA-10535: Split ProcessorContext into Processor/StateStore/Record Contexts (#9361)

Migrate different components of the old ProcessorContext interface
into separate interfaces that are more appropriate for their usages.
See KIP-478 for the details.

Reviewers: Guozhang Wang <guozhang@apache.org>, Paul Whalen <pgwhalen@gmail.com>",12,4,4,55,434,2,12,86,105,6,14,3.5,201,105,14,115,64,8,2,1,0,1
log4j-appender/src/main/java/org/apache/kafka/log4jappender/KafkaLog4jAppender.java,log4j-appender/src/main/java/org/apache/kafka/log4jappender/KafkaLog4jAppender.java,"KAFKA-9274: Revert deprecation of `retries` for producer and admin clients (#9333)

Reviewer: John Roesler <john@confluent.io>",82,0,1,291,1848,0,53,377,167,24,16,4.0,467,167,29,90,44,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsOptimizedTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsOptimizedTest.java,"KAFKA-9274: Revert deprecation of `retries` for producer and admin clients (#9333)

Reviewer: John Roesler <john@confluent.io>",6,0,1,107,1305,0,2,156,156,20,8,1.0,170,156,21,14,9,2,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/VerifiableProducer.java,tools/src/main/java/org/apache/kafka/tools/VerifiableProducer.java,"KAFKA-9274: Revert deprecation of `retries` for producer and admin clients (#9333)

Reviewer: John Roesler <john@confluent.io>",54,0,1,399,2495,0,36,564,307,18,31,2,806,307,26,242,132,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableJoinTest.java,"KAFKA-10277: Allow null keys with non-null mappedKey in KStreamKGlobalTable join (#9186)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",19,27,10,156,1418,8,12,249,211,19,13,7,356,211,27,107,19,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableLeftJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamGlobalKTableLeftJoinTest.java,"KAFKA-10277: Allow null keys with non-null mappedKey in KStreamKGlobalTable join (#9186)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",19,32,16,166,1586,9,12,259,211,20,13,8,375,211,29,116,22,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableAggregate.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableAggregate.java,"KAFKA-10077: Filter downstream of state-store results in spurious tombstones (#9156)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",13,3,1,87,693,2,6,129,118,6,20,2.0,215,118,11,86,32,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableAbstractJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableAbstractJoin.java,"KAFKA-10077: Filter downstream of state-store results in spurious tombstones (#9156)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",2,5,3,26,228,2,2,49,49,7,7,2,71,49,10,22,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableProcessorSupplier.java,"KAFKA-10077: Filter downstream of state-store results in spurious tombstones (#9156)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,15,1,6,65,0,0,40,26,7,6,1.5,49,26,8,9,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableReduce.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableReduce.java,"KAFKA-10077: Filter downstream of state-store results in spurious tombstones (#9156)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",12,3,1,78,605,2,6,119,120,7,17,2,195,120,11,76,31,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableRepartitionMap.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableRepartitionMap.java,"KAFKA-10077: Filter downstream of state-store results in spurious tombstones (#9156)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",23,1,1,77,723,2,9,132,110,7,20,2.0,205,110,10,73,20,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidUpdateVersionException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidUpdateVersionException.java,"KAFKA-8836; Add `AlterISR` RPC and use it for ISR modifications (#9100)

This patch implements [KIP-497](https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR), which introduces an asynchronous API for partition leaders to update ISR state.

Reviewers: Jason Gustafson <jason@confluent.io>",2,29,0,9,49,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/LeaderAndIsr.scala,core/src/main/scala/kafka/api/LeaderAndIsr.scala,"KAFKA-8836; Add `AlterISR` RPC and use it for ISR modifications (#9100)

This patch implements [KIP-497](https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR), which introduces an asynchronous API for partition leaders to update ISR state.

Reviewers: Jason Gustafson <jason@confluent.io>",10,10,0,35,271,1,6,62,93,5,13,2,149,93,11,87,43,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializerTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializerTest.java,"MINOR: clarify variables for skipping idempotent source updates (#9316)

Reviewers: Guozhang Wang <guozhang@apache.org>",5,2,2,60,566,2,5,98,74,24,4,2.0,108,74,27,10,8,2,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/FileBasedStateStoreTest.java,raft/src/test/java/org/apache/kafka/raft/FileBasedStateStoreTest.java,"MINOR: Use JUnit 5 in raft module (#9331)

I also removed a test class with no tests currently (Jason filed KAFKA-10519 for
filling the test gap).

Reviewers: Jason Gustafson <jason@confluent.io>",4,6,6,57,531,0,3,99,99,50,2,2.5,105,99,52,6,6,3,2,1,0,1
raft/src/test/java/org/apache/kafka/raft/MockFuturePurgatoryTest.java,raft/src/test/java/org/apache/kafka/raft/MockFuturePurgatoryTest.java,"MINOR: Use JUnit 5 in raft module (#9331)

I also removed a test class with no tests currently (Jason filed KAFKA-10519 for
filling the test gap).

Reviewers: Jason Gustafson <jason@confluent.io>",3,4,4,61,631,0,3,95,95,48,2,1.5,99,95,50,4,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformValues.java,"KAFKA-9450: Follow-up; Forbid process after closed (#9083)

A few cleanup and tighten screws:
* When a processor is closed (due to topology-closure), we should not allow processing more records.
* Let all built-in processors to extend from AbstractProcessor.
* Remove duplicated override functions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>",9,4,3,49,401,2,7,79,70,16,5,2,86,70,17,7,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamTransformValues.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamTransformValues.java,"KAFKA-9450: Follow-up; Forbid process after closed (#9083)

A few cleanup and tighten screws:
* When a processor is closed (due to topology-closure), we should not allow processing more records.
* Let all built-in processors to extend from AbstractProcessor.
* Remove duplicated override functions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>",7,3,3,43,355,1,7,72,107,3,21,2,249,113,12,177,118,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableValueGetter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableValueGetter.java,"KAFKA-9450: Follow-up; Forbid process after closed (#9083)

A few cleanup and tighten screws:
* When a processor is closed (due to topology-closure), we should not allow processing more records.
* Let all built-in processors to extend from AbstractProcessor.
* Remove duplicated override functions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>",1,1,1,8,73,1,1,29,28,4,7,1,37,28,5,8,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/AbstractProcessor.java,streams/src/main/java/org/apache/kafka/streams/processor/AbstractProcessor.java,"KAFKA-9450: Follow-up; Forbid process after closed (#9083)

A few cleanup and tighten screws:
* When a processor is closed (due to topology-closure), we should not allow processing more records.
* Let all built-in processors to extend from AbstractProcessor.
* Remove duplicated override functions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>",4,2,3,15,77,1,4,56,71,6,9,2,91,71,10,35,16,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplierTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplierTest.java,"KAFKA-9450: Follow-up; Forbid process after closed (#9083)

A few cleanup and tighten screws:
* When a processor is closed (due to topology-closure), we should not allow processing more records.
* Let all built-in processors to extend from AbstractProcessor.
* Remove duplicated override functions.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>",9,0,3,188,1640,1,9,226,223,56,4,3.5,235,223,59,9,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InconsistentVoterSetException.java,clients/src/main/java/org/apache/kafka/common/errors/InconsistentVoterSetException.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/ControlRecordType.java,clients/src/main/java/org/apache/kafka/common/record/ControlRecordType.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",13,7,0,64,430,1,5,111,87,18,6,1.5,121,87,20,10,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/ControlRecordUtils.java,clients/src/main/java/org/apache/kafka/common/record/ControlRecordUtils.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",3,44,0,19,157,2,2,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/SimpleRecord.java,clients/src/main/java/org/apache/kafka/common/record/SimpleRecord.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",26,4,0,82,615,1,16,122,109,24,5,1,125,109,25,3,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Time.java,clients/src/main/java/org/apache/kafka/common/utils/Time.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",3,2,2,20,140,2,3,89,23,10,9,3,102,23,11,13,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Timer.java,clients/src/main/java/org/apache/kafka/common/utils/Timer.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",16,26,0,63,350,3,13,206,180,103,2,2.5,206,180,103,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/MockScheduler.java,clients/src/test/java/org/apache/kafka/common/utils/MockScheduler.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",13,2,2,82,580,2,5,121,121,30,4,1.0,125,121,31,4,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/MockTime.java,clients/src/test/java/org/apache/kafka/common/utils/MockTime.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",17,7,7,81,487,4,11,126,28,10,13,2,175,30,13,49,16,4,2,1,0,1
core/src/main/scala/kafka/raft/KafkaFuturePurgatory.scala,core/src/main/scala/kafka/raft/KafkaFuturePurgatory.scala,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",15,122,0,78,555,8,8,122,122,122,1,1,122,122,122,0,0,0,0,0,0,0
core/src/main/scala/kafka/raft/SegmentPosition.scala,core/src/main/scala/kafka/raft/SegmentPosition.scala,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,23,0,5,38,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/raft/KafkaFuturePurgatoryTest.scala,core/src/test/scala/unit/kafka/raft/KafkaFuturePurgatoryTest.scala,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",3,122,0,78,616,3,3,122,122,122,1,1,122,122,122,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java,raft/src/main/java/org/apache/kafka/raft/FileBasedStateStore.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",16,170,0,108,919,8,8,170,170,170,1,1,170,170,170,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/FuturePurgatory.java,raft/src/main/java/org/apache/kafka/raft/FuturePurgatory.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,76,0,9,73,0,0,76,76,76,1,1,76,76,76,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/Isolation.java,raft/src/main/java/org/apache/kafka/raft/Isolation.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,22,0,5,17,0,0,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/LogAppendInfo.java,raft/src/main/java/org/apache/kafka/raft/LogAppendInfo.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",1,31,0,9,47,1,1,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/LogFetchInfo.java,raft/src/main/java/org/apache/kafka/raft/LogFetchInfo.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",1,33,0,10,60,1,1,33,33,33,1,1,33,33,33,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/LogOffsetMetadata.java,raft/src/main/java/org/apache/kafka/raft/LogOffsetMetadata.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",7,60,0,33,178,5,5,60,60,60,1,1,60,60,60,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/OffsetAndEpoch.java,raft/src/main/java/org/apache/kafka/raft/OffsetAndEpoch.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",10,60,0,36,220,5,5,60,60,60,1,1,60,60,60,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/OffsetMetadata.java,raft/src/main/java/org/apache/kafka/raft/OffsetMetadata.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,21,0,3,14,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/QuorumStateStore.java,raft/src/main/java/org/apache/kafka/raft/QuorumStateStore.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,52,0,9,56,0,0,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/RaftMessage.java,raft/src/main/java/org/apache/kafka/raft/RaftMessage.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",0,26,0,6,37,0,0,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/RaftResponse.java,raft/src/main/java/org/apache/kafka/raft/RaftResponse.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",8,75,0,48,211,8,8,75,75,75,1,1,75,75,75,0,0,0,0,0,0,0
raft/src/main/java/org/apache/kafka/raft/internals/LogOffset.java,raft/src/main/java/org/apache/kafka/raft/internals/LogOffset.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",6,48,0,24,162,4,4,48,48,48,1,1,48,48,48,0,0,0,0,0,0,0
raft/src/test/java/org/apache/kafka/raft/MockFuturePurgatory.java,raft/src/test/java/org/apache/kafka/raft/MockFuturePurgatory.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",17,117,0,87,577,9,9,117,117,117,1,1,117,117,117,0,0,0,0,0,0,0
raft/src/test/java/org/apache/kafka/raft/MockQuorumStateStore.java,raft/src/test/java/org/apache/kafka/raft/MockQuorumStateStore.java,"KAFKA-10492; Core Kafka Raft Implementation (KIP-595) (#9130)

This is the core Raft implementation specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. We have created a separate ""raft"" module where most of the logic resides. The new APIs introduced in this patch in order to support Raft election and such are disabled in the server until the integration with the controller is complete. Until then, there is a standalone server which can be used for testing the performance of the Raft implementation. See `raft/README.md` for details.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Boyang Chen <boyang@confluent.io>

Co-authored-by: Boyang Chen <boyang@confluent.io>
Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",3,36,0,16,62,3,3,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java,streams/src/test/java/org/apache/kafka/test/MockInternalTopicManager.java,"KAFKA-10068: add task assignment performance tests (#8892)

Add tests to bound the performance of the various Streams task assignors
when making assignments over large clusters/tasks.

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",8,1,2,49,460,1,3,76,55,5,16,1.5,124,55,8,48,13,3,2,1,0,1
core/src/main/scala/kafka/server/DelayedProduce.scala,core/src/main/scala/kafka/server/DelayedProduce.scala,"MINOR: Use `Map.forKeyValue` to avoid tuple allocation in Scala 2.13 (#9299)

`forKeyValue` invokes `foreachEntry` in Scala 2.13 and falls back to
`foreach` in Scala 2.12.

This change requires a newer version of scala-collection-compat, so
update it to the latest version (2.2.0).

Finally, included a minor clean-up in `GetOffsetShell` to use `toArray`
before `sortBy` since it's more efficient.

Reviewers: Jason Gustafson <jason@confluent.io>, David Jacot <djacot@confluent.io>, José Armando García Sancio <jsancio@users.noreply.github.com>, Chia-Ping Tsai <chia7712@gmail.com>",12,4,3,82,551,2,4,147,115,6,24,4.0,329,115,14,182,64,8,2,1,0,1
core/src/main/scala/kafka/utils/Implicits.scala,core/src/main/scala/kafka/utils/Implicits.scala,"MINOR: Use `Map.forKeyValue` to avoid tuple allocation in Scala 2.13 (#9299)

`forKeyValue` invokes `foreachEntry` in Scala 2.13 and falls back to
`foreach` in Scala 2.12.

This change requires a newer version of scala-collection-compat, so
update it to the latest version (2.2.0).

Finally, included a minor clean-up in `GetOffsetShell` to use `toArray`
before `sortBy` since it's more efficient.

Reviewers: Jason Gustafson <jason@confluent.io>, David Jacot <djacot@confluent.io>, José Armando García Sancio <jsancio@users.noreply.github.com>, Chia-Ping Tsai <chia7712@gmail.com>",0,18,0,20,186,0,0,67,49,22,3,1,68,49,23,1,1,0,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/UncompressedRecordBatchValidationBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/record/UncompressedRecordBatchValidationBenchmark.java,"KAFKA-10438: Lazy initialization of record header to reduce memory usage (#9223)

There are no checks on the header key so instantiating key (bytes to string) is unnecessary.
One implication is that conversion failures will be detected a bit later, but this is consistent
with how we handle the header value.

**JMH RESULT**

1. ops: +12%
1. The optimization of memory usage is very small as the cost of creating extra ```ByteBuffer``` is
almost same to byte array copy (used to construct ```String```). Using large key results in better
improvement but I don't think large key is common case.

**BEFORE**
```
Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (headerKeySize)  (maxBatchSize)  (maxHeaderSize)  (messageSize)  (messageVersion)   Mode  Cnt        Score      Error   Units
RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM               NONE               10             200                5           1000                 2  thrpt   15  2035938.174 ± 1653.566   ops/s
RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM               NONE               10             200                5           1000                 2  thrpt   15     2040.000 ±    0.001    B/op
```

```
Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (headerKeySize)  (maxBatchSize)  (maxHeaderSize)  (messageSize)  (messageVersion)   Mode  Cnt        Score      Error   Units
RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM               NONE               30             200                5           1000                 2  thrpt   15  1979193.376 ± 1239.286   ops/s
RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM               NONE               30             200                5           1000                 2  thrpt   15     2120.000 ±    0.001    B/op
```


**AFTER**

```
Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (headerKeySize)  (maxBatchSize)  (maxHeaderSize)  (messageSize)  (messageVersion)   Mode  Cnt        Score      Error   Units
RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM               NONE               10             200                5           1000                 2  thrpt   15  2289115.973 ± 2661.856   ops/s
RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM               NONE               10             200                5           1000                 2  thrpt   15     2032.000 ±    0.001    B/op
```

```
Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (headerKeySize)  (maxBatchSize)  (maxHeaderSize)  (messageSize)  (messageVersion)   Mode  Cnt        Score     Error   Units
RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM               NONE               30             200                5           1000                 2  thrpt   15  2222625.706 ± 908.358   ops/s
RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM               NONE               30             200                5           1000                 2  thrpt   15     2040.000 ±   0.001    B/op
```

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,53,0,33,283,2,2,53,53,53,1,1,53,53,53,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java,"MINOR: Generator config-specific HTML ids (#8878)

Currently the docs have HTML ids for each config key. That doesn't work
correctly for config keys like bootstrap.servers which occur across
producer, consumer, admin configs: We generate duplicate ids. So arrange
for each config to prefix the ids it generates with the HTML id of its
section heading.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",25,1,1,130,1168,1,14,186,146,19,10,1.5,207,149,21,21,8,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java,"MINOR: Generator config-specific HTML ids (#8878)

Currently the docs have HTML ids for each config key. That doesn't work
correctly for config keys like bootstrap.servers which occur across
producer, consumer, admin configs: We generate duplicate ids. So arrange
for each config to prefix the ids it generates with the HTML id of its
section heading.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",20,1,1,383,2403,1,12,494,192,16,30,2.0,805,278,27,311,159,10,2,1,0,1
generator/src/main/java/org/apache/kafka/message/StructRegistry.java,generator/src/main/java/org/apache/kafka/message/StructRegistry.java,"MINOR: Fix common struct `JsonConverter` and `Schema` generation (#9279)

This patch fixes a couple problems with the use of the `StructRegistry`. First, it fixes registration so that it is consistently based on the typename of the struct. Previously structs were registered under the field name which meant that fields which referred to common structs resulted in multiple entries. Second, the patch fixes `SchemaGenerator` so that common structs are considered first.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",27,13,13,128,886,2,11,192,144,38,5,8,238,144,48,46,21,9,2,1,0,1
core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala,core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala,"KAFKA-10435; Fetch protocol changes for KIP-595 (#9275)

This patch bumps the `Fetch` protocol as specified by KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. The main differences are the following:

- Truncation detection 
- Leader discovery through the response
- Flexible version support

The most notable change is truncation detection. This patch adds logic in the request handling path to detect truncation, but it does not change the replica fetchers to make use of this capability. This will be done separately.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",8,2,2,32,257,2,6,73,67,10,7,2,110,67,16,37,25,5,2,1,0,1
core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala,core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala,"KAFKA-10458; Updating controller quota does not work since Token Bucket (#9272)

This PR fixes two issues that have been introduced by #9114.
- When the metric was switched from Rate to TokenBucket in the ControllerMutationQuotaManager, the metrics were mixed up. That broke the quota update path.
- When a quota is updated, the ClientQuotaManager updates the MetricConfig of the KafkaMetric. That update was not reflected into the Sensor so the Sensor was still using the MetricConfig that it has been created with.

Reviewers: Anna Povzner <anna@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",8,1,1,54,436,3,6,90,54,6,14,2.0,154,54,11,64,17,5,2,1,0,1
core/src/main/scala/kafka/server/ControllerMutationQuotaManager.scala,core/src/main/scala/kafka/server/ControllerMutationQuotaManager.scala,"KAFKA-10458; Updating controller quota does not work since Token Bucket (#9272)

This PR fixes two issues that have been introduced by #9114.
- When the metric was switched from Rate to TokenBucket in the ControllerMutationQuotaManager, the metrics were mixed up. That broke the quota update path.
- When a quota is updated, the ClientQuotaManager updates the MetricConfig of the KafkaMetric. That update was not reflected into the Sensor so the Sensor was still using the MetricConfig that it has been created with.

Reviewers: Anna Povzner <anna@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",26,7,7,150,981,4,15,282,242,70,4,2.0,291,242,73,9,7,2,2,1,0,1
core/src/main/scala/kafka/log/LogCleanerManager.scala,core/src/main/scala/kafka/log/LogCleanerManager.scala,"KAFKA-8362: fix the old checkpoint won't be removed after alter log dir (#9178)

In KIP-113, we support replicas movement between log directories. But while the directory change, we forgot to remove the topicPartition offset data in old directory, which will cause there are more than 1 checkpoint copy stayed in the logs for the altered topicPartition. And it'll let the LogCleaner get stuck due to it's possible to always get the old topicPartition offset data from the old checkpoint file.

I added one more parameter topicPartitionToBeRemoved in updateCheckpoints() method. So, if the update parameter is None (as before), we'll do the remove action to remove the topicPartitionToBeRemoved data in dir, otherwise, update the data as before.

Reviewers: Jun Rao <junrao@gmail.com>",96,42,9,405,2766,7,24,637,188,12,53,2,1014,188,19,377,48,7,2,1,0,1
core/src/main/scala/kafka/server/checkpoints/OffsetCheckpointFile.scala,core/src/main/scala/kafka/server/checkpoints/OffsetCheckpointFile.scala,"KAFKA-8362: fix the old checkpoint won't be removed after alter log dir (#9178)

In KIP-113, we support replicas movement between log directories. But while the directory change, we forgot to remove the topicPartition offset data in old directory, which will cause there are more than 1 checkpoint copy stayed in the logs for the altered topicPartition. And it'll let the LogCleaner get stuck due to it's possible to always get the old topicPartition offset data from the old checkpoint file.

I added one more parameter topicPartitionToBeRemoved in updateCheckpoints() method. So, if the update parameter is None (as before), we'll do the remove action to remove the topicPartitionToBeRemoved data in dir, otherwise, update the data as before.

Reviewers: Jun Rao <junrao@gmail.com>",11,10,2,52,424,1,8,99,60,11,9,1,136,60,15,37,23,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java,"Adding reverse iterator usage for sliding windows processing (extending KIP-450) (#9239)

Add a backwardFetch call to the window store for sliding window
processing. While the implementation works with the forward call
to the window store, using backwardFetch allows for the iterator
to be closed earlier, making implementation more efficient.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",41,309,171,814,9894,15,20,1018,692,339,3,8,1197,692,399,179,171,60,1,0,1,1
tools/src/test/java/org/apache/kafka/tools/PushHttpMetricsReporterTest.java,tools/src/test/java/org/apache/kafka/tools/PushHttpMetricsReporterTest.java,"KAFKA-10447: Migrate tools module to JUnit 5 (#9231)

This change sets the groundwork for migrating other modules incrementally.

Main changes:
- Replace `junit` 4.13 with `junit-jupiter` and `junit-vintage` 5.7.0-RC1.
- All modules except for `tools` depend on `junit-vintage`.
- `tools` depends on `junit-jupiter`.
- Convert `tools` tests to JUnit 5.
- Update `PushHttpMetricsReporterTest` to use `mockito` instead of `powermock` and `easymock`
(powermock doesn't seem to work well with JUnit 5 and we don't need it since mockito can mock
static methods).
- Update `mockito` to 3.5.7.
- Update `TestUtils` to use JUnit 5 assertions since `tools` depends on it.

Unrelated clean-ups:
- Remove `unit` from package names in a few `core` tests.
- Replace `try/catch/fail` with `assertThrows` in a number of places.
- Tag `CoordinatorTest` as integration test.
- Remove unnecessary type parameters when invoking methods and constructors.

Tested with IntelliJ and gradle. Verified that the following commands work as expected:
* ./gradlew tools:unitTest
* ./gradlew tools:integrationTest
* ./gradlew tools:test
* ./gradlew core:unitTest
* ./gradlew core:integrationTest
* ./gradlew clients:test

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",22,113,120,271,2283,21,21,347,333,116,3,15,480,333,160,133,120,44,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/Heartbeat.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/Heartbeat.java,"KAFKA-10134: Enable heartbeat during PrepareRebalance and Depend On State For Poll Timeout (#8834)

1. Split the consumer coordinator's REBALANCING state into PREPARING_REBALANCE and COMPLETING_REBALANCE. The first is when the join group request is sent, and the second is after the join group response is received. During the first state we should still not send hb since it shares the same socket with the join group request and the group coordinator has disabled timeout, however when we transit to the second state we should start sending hb in case leader's assign takes long time. This is also for fixing KAFKA-10122.

2. When deciding coordinator#timeToNextPoll, do not count in timeToNextHeartbeat if the state is in UNJOINED or PREPARING_REBALANCE since we would disable hb and hence its timer would not be updated.

3. On the broker side, allow hb received during PREPARING_REBALANCE, return NONE error code instead of REBALANCE_IN_PROGRESS. However on client side, we still need to ignore REBALANCE_IN_PROGRESS if state is COMPLETING_REBALANCE in case it is talking to an old versioned broker.

4. Piggy-backing a log4j improvement on the broker coordinator for triggering rebalance reason, as I found it a bit blurred during the investigation. Also subsumed #9038 with log4j improvements.

The tricky part for allowing hb during COMPLETING_REBALANCE is in two parts: 1) before the sync-group response is received, a hb response may have reset the generation; also after the sync-group response but before the callback is triggered, a hb response can still reset the generation, we need to handle both cases by checking the generation / state. 2) with the hb thread enabled, the sync-group request may be sent by the hb thread even if the caller thread did not call poll yet.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>, John Roesler <john@confluent.io>",17,13,0,94,589,3,15,135,47,10,14,6.5,275,47,20,140,32,10,2,1,0,1
core/src/main/scala/kafka/server/ActionQueue.scala,core/src/main/scala/kafka/server/ActionQueue.scala,"KAFKA-8334 Make sure the thread which tries to complete delayed reque… (#8657)

The main changes of this PR are shown below.

1. replace tryLock by lock for DelayedOperation#maybeTryComplete
2. complete the delayed requests without holding group lock

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>",7,56,0,22,135,2,2,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/DelayedOperation.scala,core/src/main/scala/kafka/server/DelayedOperation.scala,"KAFKA-8334 Make sure the thread which tries to complete delayed reque… (#8657)

The main changes of this PR are shown below.

1. replace tryLock by lock for DelayedOperation#maybeTryComplete
2. complete the delayed requests without holding group lock

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>",41,51,67,208,1334,4,17,436,278,9,47,6,1379,278,29,943,172,20,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorParameters.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorParameters.java,"KAFKA-10436: Implement KIP-478 Topology changes (#9221)

Convert Topology#addProcessor and #addGlobalStore
Also, convert some of the internals in support of addProcessor

Reviewers: Bill Bejeck <bbejeck@apache.org>",11,44,5,55,426,9,9,95,46,16,6,3.5,115,46,19,20,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ConnectedStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/processor/ConnectedStoreProvider.java,"KAFKA-10436: Implement KIP-478 Topology changes (#9221)

Convert Topology#addProcessor and #addGlobalStore
Also, convert some of the internals in support of addProcessor

Reviewers: Bill Bejeck <bbejeck@apache.org>",1,1,1,14,126,0,1,117,117,58,2,1.0,118,117,59,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextAdapter.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextAdapter.java,"KAFKA-10436: Implement KIP-478 Topology changes (#9221)

Convert Topology#addProcessor and #addGlobalStore
Also, convert some of the internals in support of addProcessor

Reviewers: Bill Bejeck <bbejeck@apache.org>",39,3,2,177,1098,1,38,235,234,78,3,1,238,234,79,3,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterUserScramCredentialsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterUserScramCredentialsOptions.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",0,31,0,6,45,0,0,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AlterUserScramCredentialsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterUserScramCredentialsResult.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",3,60,0,20,168,3,3,60,60,60,1,1,60,60,60,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsOptions.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",0,31,0,6,45,0,0,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeUserScramCredentialsResult.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",12,150,0,83,785,5,5,150,150,150,1,1,150,150,150,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/ScramCredentialInfo.java,clients/src/main/java/org/apache/kafka/clients/admin/ScramCredentialInfo.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",10,78,0,35,182,6,6,78,78,78,1,1,78,78,78,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialAlteration.java,clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialAlteration.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,45,0,11,60,2,2,45,45,45,1,1,45,45,45,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialDeletion.java,clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialDeletion.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,46,0,12,67,2,2,46,46,46,1,1,46,46,46,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialUpsertion.java,clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialUpsertion.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",7,100,0,34,255,7,7,100,100,100,1,1,100,100,100,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialsDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/UserScramCredentialsDescription.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",10,82,0,38,231,6,6,82,82,82,1,1,82,82,82,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DuplicateResourceException.java,clients/src/main/java/org/apache/kafka/common/errors/DuplicateResourceException.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",5,77,0,22,127,5,5,77,77,77,1,1,77,77,77,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/ResourceNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/ResourceNotFoundException.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",5,76,0,22,127,5,5,76,76,76,1,1,76,76,76,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/UnacceptableCredentialException.java,clients/src/main/java/org/apache/kafka/common/errors/UnacceptableCredentialException.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,44,0,10,57,2,2,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramFormatter.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramFormatter.java,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",31,23,7,143,1355,12,25,193,169,32,6,2.5,222,169,37,29,10,5,1,0,1,1
core/src/test/scala/unit/kafka/utils/JaasTestUtils.scala,core/src/test/scala/unit/kafka/utils/JaasTestUtils.scala,"KAFKA-10259: KIP-554 Broker-side SCRAM Config API (#9032)

Implement the KIP-554 API to create, describe, and alter SCRAM user configurations via the AdminClient.  Add ducktape tests, and modify JUnit tests to test and use the new API where appropriate.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>",23,33,14,232,1339,4,10,295,48,12,24,4.0,565,97,24,270,59,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/MissingSourceTopicException.java,streams/src/main/java/org/apache/kafka/streams/errors/MissingSourceTopicException.java,"KAFKA-10355: Throw error when source topic was deleted (#9191)

Before this commit, Kafka Streams would gracefully shut down the whole application when a source topic is deleted. The graceful shutdown does not give the user the possibility to react on the deletion of the source topic in the uncaught exception handler.

This commit changes this behavior and throws an error when a source topic is deleted.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@apache.org>, John Roesler <vvcephei@apache.org>",1,26,0,7,40,1,1,26,26,26,1,1,26,26,26,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorder.java,"KAFKA-9924: Add remaining property-based RocksDB metrics as described in KIP-607 (#9232)

This commit adds the remaining property-based RocksDB metrics as described in KIP-607, except for num-entries-active-mem-table, which was added in PR #9177.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",50,208,17,422,2832,7,17,475,191,59,8,7.5,642,208,80,167,62,21,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderGaugesTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderGaugesTest.java,"KAFKA-9924: Add remaining property-based RocksDB metrics as described in KIP-607 (#9232)

This commit adds the remaining property-based RocksDB metrics as described in KIP-607, except for num-entries-active-mem-table, which was added in PR #9177.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",29,180,8,210,1709,28,28,267,172,134,2,5.0,275,180,138,8,8,4,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderTest.java,"KAFKA-9924: Add remaining property-based RocksDB metrics as described in KIP-607 (#9232)

This commit adds the remaining property-based RocksDB metrics as described in KIP-607, except for num-entries-active-mem-table, which was added in PR #9177.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",26,145,16,538,4495,12,26,638,213,80,8,14.5,827,264,103,189,56,24,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractMergedSortedCacheStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractMergedSortedCacheStoreIterator.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",37,0,1,135,929,0,10,194,166,19,10,1.0,248,166,25,54,18,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java,streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",33,22,7,187,1209,4,13,239,229,48,5,3,265,229,53,26,12,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWindowStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWindowStoreIterator.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",6,3,2,36,284,2,6,67,58,7,9,2,97,58,11,30,12,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWindowStoreKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWindowStoreKeyValueIterator.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",6,3,2,47,382,2,6,73,75,10,7,1,96,75,14,23,12,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentIterator.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",20,15,4,82,472,3,6,114,94,11,10,3.0,151,94,15,37,7,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java,streams/src/main/java/org/apache/kafka/streams/state/internals/Segments.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,2,2,15,146,0,0,44,179,2,19,3,410,179,22,366,207,19,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SessionKeySchema.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",25,3,2,123,1156,2,18,164,73,12,14,3.0,251,76,18,87,22,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",32,5,4,181,1559,2,24,239,133,15,16,3.0,327,140,20,88,29,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",31,222,42,370,3501,28,29,448,180,22,20,4.0,595,222,30,147,42,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIteratorTest.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,11,8,68,549,2,6,98,78,9,11,4,131,78,12,33,8,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreIteratorTest.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,100,6,179,1892,6,7,214,97,13,16,3.0,314,100,20,100,35,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreKeyValueIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreKeyValueIteratorTest.java,"KAFKA-9929: Support backward iterator on WindowStore (#9138)

Implements KIP-617 on WindowStore that depends on #9137.

Testing strategy: extend existing tests to validate reverse operations are supported.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",18,63,13,149,1420,16,18,191,132,27,7,7,250,132,36,59,17,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java,"KAFKA-5636: SlidingWindows (KIP-450) (#9039)

Add SlidingWindows API, implementation, and tests.
An edge case and an optimization are left to follow-on work.

Implements: KIP-450

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <mjsax@apache.org>, John Roesler <vvcephei@apache.org>",7,82,0,55,535,7,7,82,82,82,1,1,82,82,82,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java,"KAFKA-5636: SlidingWindows (KIP-450) (#9039)

Add SlidingWindows API, implementation, and tests.
An edge case and an optimization are left to follow-on work.

Implements: KIP-450

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <mjsax@apache.org>, John Roesler <vvcephei@apache.org>",20,439,0,387,4438,20,20,439,439,439,1,1,439,439,439,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/MessageClassGenerator.java,generator/src/main/java/org/apache/kafka/message/MessageClassGenerator.java,"KAFKA-10384: Separate converters from generated messages (#9194)

For the generated message code, put the JSON conversion functionality
in a separate JsonConverter class.

Make MessageDataGenerator simply another generator class, alongside the
new JsonConverterGenerator class.  Move some of the utility functions
from MessageDataGenerator into FieldSpec and other places, so that they
can be used by other generator classes.

Use argparse4j to support a better command-line for the generator.

Reviewers: David Arthur <mumrah@gmail.com>",0,36,0,6,40,0,0,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/StreamsNotStartedException.java,streams/src/main/java/org/apache/kafka/streams/errors/StreamsNotStartedException.java,"MINOR: fix JavaDoc (#9217)

Reviewer: John Roesler <john@confluent.io>",2,3,2,12,84,0,2,38,37,19,2,2.0,40,37,20,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/provider/DirectoryConfigProvider.java,clients/src/main/java/org/apache/kafka/common/config/provider/DirectoryConfigProvider.java,"KAFKA-10211: Add DirectoryConfigProvider (#9136)

See KIP-632: https://cwiki.apache.org/confluence/display/KAFKA/KIP-632%3A+Add+DirectoryConfigProvider

Reviewers: Mickael Maison <mickael.maison@gmail.com>, David Jacot <david.jacot@gmail.com>",12,106,0,59,457,6,6,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/KeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/KeyValueStore.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",0,2,2,10,103,0,0,70,86,5,14,2.0,165,86,12,95,37,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeKeyValueIterator.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",11,1,2,41,271,1,5,69,74,23,3,1,75,74,25,6,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",16,44,3,119,942,4,7,157,145,16,10,3.5,252,145,25,95,62,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryNavigableLRUCache.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MemoryNavigableLRUCache.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",13,25,3,81,630,3,11,117,103,11,11,3,190,103,17,73,28,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIterator.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",6,5,3,32,208,2,6,61,109,6,10,3.0,238,109,24,177,117,18,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ReadOnlyKeyValueStoreFacade.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ReadOnlyKeyValueStoreFacade.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",7,11,0,37,280,2,7,63,52,21,3,2,66,52,22,3,3,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbIterator.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",10,6,2,60,426,3,6,88,89,29,3,1,95,89,32,7,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheKeyValueBytesStoreIteratorTest.java,"KAFKA-9929: Support reverse iterator on KeyValueStore (#9137)

Add new methods to KeyValueStore interfaces to support reverse iteration.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, John Roesler <vvcephei@apache.org>",23,75,23,193,2392,12,12,235,91,15,16,3.5,344,99,22,109,23,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/internals/PartitionStates.java,clients/src/main/java/org/apache/kafka/common/internals/PartitionStates.java,"KAFKA-8806 Reduce calls to validateOffsetsIfNeeded (#7222)

Only check if positions need validation if there is new metadata. 

Also fix some inefficient java.util.stream code in the hot path of SubscriptionState.",30,8,3,113,759,3,22,191,174,16,12,1.0,222,174,18,31,11,3,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/consumer/SubscriptionStateBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/consumer/SubscriptionStateBenchmark.java,"KAFKA-8806 Reduce calls to validateOffsetsIfNeeded (#7222)

Only check if positions need validation if there is new metadata. 

Also fix some inefficient java.util.stream code in the hot path of SubscriptionState.",4,95,0,69,560,4,4,95,95,95,1,1,95,95,95,0,0,0,2,1,0,1
core/src/main/scala/kafka/utils/CommandLineUtils.scala,core/src/main/scala/kafka/utils/CommandLineUtils.scala,"KAFKA-10418: Improve kafka-topics help text for altering configs

Reviewers: Colin P. McCabe <cmccabe@apache.org>",25,3,2,71,633,3,9,145,23,7,22,2.0,236,34,11,91,22,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/GlobalKTableJoinsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/GlobalKTableJoinsTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,1,1,72,748,1,4,106,110,6,18,2.5,235,110,13,129,36,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFilterTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFilterTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",5,2,2,61,670,2,3,96,85,4,22,4.0,229,85,10,133,31,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,59,664,1,1,88,80,4,23,3,197,80,9,109,27,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatMapValuesTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,81,938,2,2,119,77,5,23,3,258,77,11,139,32,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamMapTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamMapTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,52,591,1,2,77,73,3,25,3,205,73,8,128,35,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamMapValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamMapValuesTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,60,715,2,2,92,71,4,24,3.0,235,71,10,143,41,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSelectKeyTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSelectKeyTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",4,2,2,56,599,1,2,84,83,5,18,3.5,198,83,11,114,32,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableMapKeysTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableMapKeysTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",3,2,2,50,601,1,1,78,88,4,20,3.5,173,88,9,95,24,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/PunctuationQueueTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/PunctuationQueueTest.java,"KAFKA-10379: Implement the KIP-478 StreamBuilder#addGlobalStore() (#9148)

From KIP-478, implement the new StreamBuilder#addGlobalStore() overload
that takes a stateUpdateSupplier fully typed Processor<KIn, VIn, Void, Void>.

Where necessary, use the adapters to make the old APIs defer to the new ones,
as well as limiting the scope of this change set.

Reviewers: Boyang Chen <boyang@apache.org>",7,20,20,102,1093,4,7,156,85,13,12,2.5,275,85,23,119,64,10,2,1,0,1
core/src/main/scala/kafka/utils/QuotaUtils.scala,core/src/main/scala/kafka/utils/QuotaUtils.scala,"KAFKA-10023: Enforce broker-wide and per-listener connection creation… (#8768)

Implements the part of KIP-612 that adds broker configurations for broker-wide and per-listener connection creation rate limits and enforces these limits.

Reviewers: David Jacot <djacot@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>",6,75,0,22,203,4,4,75,75,75,1,1,75,75,75,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicCreationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/TopicCreationTest.java,"KAFKA-10387: Fix inclusion of transformation configs when topic creation is enabled in Connect (#9172)

Addition of configs for custom topic creation with KIP-158 created a regression when transformation configs are also included in the configuration of a source connector. 

To experience the issue, just enabling topic creation at the worker is not sufficient. A user needs to supply a source connector configuration that contains both transformations and custom topic creation properties. 

The issue is that the enrichment of configs in `SourceConnectorConfig` happens on top of an `AbstractConfig` rather than a `ConnectorConfig`. Inheriting from the latter allows enrichment to be composable for both topic creation and transformations. 

Unit tests and integration tests are written to test these combinations. 

Reviewers: Randall Hauch <rhauch@gmail.com>",14,150,0,506,5959,2,14,638,488,319,2,3.0,638,488,319,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java,"KAFKA-9273: Extract testShouldAutoShutdownOnIncompleteMetadata from S… (#9108)

The main goal is to remove usage of embedded broker (EmbeddedKafkaCluster) in AbstractJoinIntegrationTest and its subclasses.
This is because the tests under this class are no longer using the embedded broker, except for two.
testShouldAutoShutdownOnIncompleteMetadata is one of such tests.
Furthermore, this test does not actually perfom stream-table join; it is testing an edge case of joining with a non-existent topic, so it should be in a separate test.

Testing strategy: run existing unit and integration test

Reviewers: Boyang Chen <boyang@confluent.io>, Bill Bejeck <bbejeck@apache.org>",4,0,32,78,534,3,4,111,112,11,10,1.5,200,112,20,89,36,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/BlockBasedTableConfigWithAccessibleCache.java,streams/src/main/java/org/apache/kafka/streams/state/internals/BlockBasedTableConfigWithAccessibleCache.java,"KAFKA-9924: Prepare RocksDB and metrics for RocksDB properties recording (#9098)

Refactor the RocksDB store and the metrics infrastructure in Streams
in preparation of the recordings of the RocksDB properties specified in KIP-607.

The refactoring includes:
* wrapper around BlockedBasedTableConfig to make the cache accessible to the
  RocksDB metrics recorder
* RocksDB metrics recorder now takes also the DB instance and the cache in addition
  to the statistics
* The value providers for the metrics are added to the RockDB metrics recorder also if
  the recording level is INFO.
* The creation of the RocksDB metrics recording trigger is moved to StreamsMetricsImpl

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",2,35,0,14,74,2,2,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/MockStreamsMetrics.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/MockStreamsMetrics.java,"KAFKA-9924: Prepare RocksDB and metrics for RocksDB properties recording (#9098)

Refactor the RocksDB store and the metrics infrastructure in Streams
in preparation of the recordings of the RocksDB properties specified in KIP-607.

The refactoring includes:
* wrapper around BlockedBasedTableConfig to make the cache accessible to the
  RocksDB metrics recorder
* RocksDB metrics recorder now takes also the DB instance and the cache in addition
  to the statistics
* The value providers for the metrics are added to the RockDB metrics recorder also if
  the recording level is INFO.
* The creation of the RocksDB metrics recording trigger is moved to StreamsMetricsImpl

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",1,2,1,10,99,1,1,29,28,5,6,2.0,43,28,7,14,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/BlockBasedTableConfigWithAccessibleCacheTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/BlockBasedTableConfigWithAccessibleCacheTest.java,"KAFKA-9924: Prepare RocksDB and metrics for RocksDB properties recording (#9098)

Refactor the RocksDB store and the metrics infrastructure in Streams
in preparation of the recordings of the RocksDB properties specified in KIP-607.

The refactoring includes:
* wrapper around BlockedBasedTableConfig to make the cache accessible to the
  RocksDB metrics recorder
* RocksDB metrics recorder now takes also the DB instance and the cache in addition
  to the statistics
* The value providers for the metrics are added to the RockDB metrics recorder also if
  the recording level is INFO.
* The creation of the RocksDB metrics recording trigger is moved to StreamsMetricsImpl

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",2,54,0,29,186,2,2,54,54,54,1,1,54,54,54,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/RetriableException.java,clients/src/main/java/org/apache/kafka/common/errors/RetriableException.java,"KAFKA-9911: Add new PRODUCER_FENCED error code (#8549)

Add a separate error code as PRODUCER_FENCED to differentiate INVALID_PRODUCER_EPOCH. On broker side, replace INVALID_PRODUCER_EPOCH with PRODUCER_FENCED when the request version is the latest, while still returning INVALID_PRODUCER_EPOCH to older clients. On client side, simply handling INVALID_PRODUCER_EPOCH the same as PRODUCER_FENCED if from txn coordinator APIs.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",4,1,1,15,77,0,4,41,37,14,3,1,53,37,18,12,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManager.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManager.java,"KAFKA-9450: Decouple flushing state from commiting (#8964)

In Kafka Streams the source-of-truth of a state store is in its changelog, therefore when committing a state store we only need to make sure its changelog records are all flushed and committed, but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.

On the other hand, flushing a state store too frequently may have side effects, e.g. rocksDB flushing would gets the memtable into an L0 sstable, leaving many small L0 files to be compacted later, which introduces larger overhead.

Therefore this PR decouples flushing from committing, such that we do not always flush the state store upon committing, but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled, then it is fine if the local persisted state is actually ahead of the checkpoint, b) if EOS is enabled, then we would never write a checkpoint file until close.

Here's a more detailed change list of this PR:

1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.

2. In post-commit, we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint, b) When the task is being closed, in which case we enforce checkpointing.

3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer, we still need to flush them in pre-commit to make sure all records sent via producers, while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.

I've also made some minor changes piggy-backed in this PR:

4. Do not delete checkpoint file upon loading it, and as a result simplify the checkpointNeeded logic, initializing the snapshotLastFlush to the loaded offsets.

5. In closing, also follow the commit -> suspend -> close ordering as in revocation / assignment.

6. If enforceCheckpoint == true during RUNNING, still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.

Reviewers: John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,3,1,22,192,0,0,57,41,5,11,2,82,41,7,25,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CachedStateStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CachedStateStore.java,"KAFKA-9450: Decouple flushing state from commiting (#8964)

In Kafka Streams the source-of-truth of a state store is in its changelog, therefore when committing a state store we only need to make sure its changelog records are all flushed and committed, but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.

On the other hand, flushing a state store too frequently may have side effects, e.g. rocksDB flushing would gets the memtable into an L0 sstable, leaving many small L0 files to be compacted later, which introduces larger overhead.

Therefore this PR decouples flushing from committing, such that we do not always flush the state store upon committing, but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled, then it is fine if the local persisted state is actually ahead of the checkpoint, b) if EOS is enabled, then we would never write a checkpoint file until close.

Here's a more detailed change list of this PR:

1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.

2. In post-commit, we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint, b) When the task is being closed, in which case we enforce checkpointing.

3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer, we still need to flush them in pre-commit to make sure all records sent via producers, while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.

I've also made some minor changes piggy-backed in this PR:

4. Do not delete checkpoint file upon loading it, and as a result simplify the checkpointNeeded logic, initializing the snapshotLastFlush to the loaded offsets.

5. In closing, also follow the commit -> suspend -> close ordering as in revocation / assignment.

6. If enforceCheckpoint == true during RUNNING, still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.

Reviewers: John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,7,0,6,45,0,0,35,28,6,6,1.0,46,28,8,11,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerStub.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateManagerStub.java,"KAFKA-9450: Decouple flushing state from commiting (#8964)

In Kafka Streams the source-of-truth of a state store is in its changelog, therefore when committing a state store we only need to make sure its changelog records are all flushed and committed, but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.

On the other hand, flushing a state store too frequently may have side effects, e.g. rocksDB flushing would gets the memtable into an L0 sstable, leaving many small L0 files to be compacted later, which introduces larger overhead.

Therefore this PR decouples flushing from committing, such that we do not always flush the state store upon committing, but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled, then it is fine if the local persisted state is actually ahead of the checkpoint, b) if EOS is enabled, then we would never write a checkpoint file until close.

Here's a more detailed change list of this PR:

1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.

2. In post-commit, we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint, b) When the task is being closed, in which case we enforce checkpointing.

3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer, we still need to flush them in pre-commit to make sure all records sent via producers, while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.

I've also made some minor changes piggy-backed in this PR:

4. Do not delete checkpoint file upon loading it, and as a result simplify the checkpointNeeded logic, initializing the snapshotLastFlush to the loaded offsets.

5. In closing, also follow the commit -> suspend -> close ordering as in revocation / assignment.

6. If enforceCheckpoint == true during RUNNING, still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.

Reviewers: John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",11,4,1,44,234,3,11,75,63,7,11,2,109,63,10,34,12,3,2,1,0,1
streams/src/test/java/org/apache/kafka/test/GlobalStateManagerStub.java,streams/src/test/java/org/apache/kafka/test/GlobalStateManagerStub.java,"KAFKA-9450: Decouple flushing state from commiting (#8964)

In Kafka Streams the source-of-truth of a state store is in its changelog, therefore when committing a state store we only need to make sure its changelog records are all flushed and committed, but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.

On the other hand, flushing a state store too frequently may have side effects, e.g. rocksDB flushing would gets the memtable into an L0 sstable, leaving many small L0 files to be compacted later, which introduces larger overhead.

Therefore this PR decouples flushing from committing, such that we do not always flush the state store upon committing, but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled, then it is fine if the local persisted state is actually ahead of the checkpoint, b) if EOS is enabled, then we would never write a checkpoint file until close.

Here's a more detailed change list of this PR:

1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.

2. In post-commit, we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint, b) When the task is being closed, in which case we enforce checkpointing.

3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer, we still need to flush them in pre-commit to make sure all records sent via producers, while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.

I've also made some minor changes piggy-backed in this PR:

4. Do not delete checkpoint file upon loading it, and as a result simplify the checkpointNeeded logic, initializing the snapshotLastFlush to the loaded offsets.

5. In closing, also follow the commit -> suspend -> close ordering as in revocation / assignment.

6. If enforceCheckpoint == true during RUNNING, still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.

Reviewers: John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",14,5,2,69,387,3,14,103,83,8,13,2,139,83,11,36,13,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalApiProcessorContext.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalApiProcessorContext.java,"KAFKA-10261: Introduce the KIP-478 apis with adapters (#9004)

Adds the new Processor and ProcessorContext interfaces
as proposed in KIP-478. To integrate in a staged fashion
with the code base, adapters are included to convert back
and forth between the new and old APIs.

ProcessorNode is converted to the new APIs.

Reviewers: Boyang Chen <boyang@confluent.io>",1,119,0,36,346,1,1,119,119,119,1,1,119,119,119,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextReverseAdapter.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextReverseAdapter.java,"KAFKA-10261: Introduce the KIP-478 apis with adapters (#9004)

Adds the new Processor and ProcessorContext interfaces
as proposed in KIP-478. To integrate in a staged fashion
with the code base, adapters are included to convert back
and forth between the new and old APIs.

ProcessorNode is converted to the new APIs.

Reviewers: Boyang Chen <boyang@confluent.io>",42,248,0,188,1188,41,41,248,248,248,1,1,248,248,248,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNodePunctuator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorNodePunctuator.java,"KAFKA-10261: Introduce the KIP-478 apis with adapters (#9004)

Adds the new Processor and ProcessorContext interfaces
as proposed in KIP-478. To integrate in a staged fashion
with the code base, adapters are included to convert back
and forth between the new and old APIs.

ProcessorNode is converted to the new APIs.

Reviewers: Boyang Chen <boyang@confluent.io>",0,1,1,6,69,0,0,26,26,6,4,1.0,29,26,7,3,1,1,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/TopologyTestDriverWrapper.java,streams/src/test/java/org/apache/kafka/streams/TopologyTestDriverWrapper.java,"KAFKA-10261: Introduce the KIP-478 apis with adapters (#9004)

Adds the new Processor and ProcessorContext interfaces
as proposed in KIP-478. To integrate in a staged fashion
with the code base, adapters are included to convert back
and forth between the new and old APIs.

ProcessorNode is converted to the new APIs.

Reviewers: Boyang Chen <boyang@confluent.io>",15,3,3,30,249,1,3,107,70,12,9,2,129,70,14,22,6,2,2,1,0,1
core/src/main/scala/kafka/controller/ControllerEventManager.scala,core/src/main/scala/kafka/controller/ControllerEventManager.scala,"KAFKA-10193: Add preemption for controller events that have callbacks

JIRA: https://issues.apache.org/jira/browse/KAFKA-10193
* add `preempt(): Unit` method for all `ControllerEvent` so that all events (and future events) must implement it
* for events that have callbacks, move the preemption from individual methods to `preempt()`
* add preemption for `ApiPartitionReassignment` and `ListPartitionReassignments`
* add integration tests:
1. test whether `preempt()` is called when controller shuts down
2. test whether the events with callbacks have the correct error response (`NOT_CONTROLLER`)
* explicit typing for `ControllerEvent` methods

Author: jeff kim <jeff.kim@confluent.io>

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>,Stanislav Kozlovski <stanislav@confluent.io>, David Arthur <mumrah@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #9050 from jeffkbkim/KAFKA-10193-controller-events-add-preemption",21,6,4,114,689,2,11,160,65,9,17,2,246,65,14,86,29,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/KeyQueryMetadata.java,streams/src/main/java/org/apache/kafka/streams/KeyQueryMetadata.java,"KAFKA-10316: Consider renaming getter method for Interactive Queries (#9120)

 - implements KIP-648
 - Deprecated the existing getters and added new getters without `get` prefix to `KeyQueryMetadata`

Co-authored-by: johnthotekat <Iabon1989*>

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",13,36,3,60,317,3,10,136,104,45,3,1,142,104,47,6,3,2,2,1,0,1
core/src/main/scala/kafka/server/ReplicationQuotaManager.scala,core/src/main/scala/kafka/server/ReplicationQuotaManager.scala,"KAFKA-10162; Use Token Bucket algorithm for controller mutation quota (KIP-599, Part III) (#9114)

Based on the discussion in #9072, I have put together an alternative way. This one does the following:

Instead of changing the implementation of the Rate to behave like a Token Bucket, it actually use two different metrics: the regular Rate and a new Token Bucket. The latter is used to enforce the quota.
The Token Bucket algorithm uses the rate of the quota as the refill rate for the credits and compute the burst based on the number of samples and their length (# samples * sample length * quota).
The Token Bucket algorithm used can go under zero in order to handle unlimited burst (e.g. create topic with a number of partitions higher than the burst). Throttling kicks in when the number of credits is under zero.
The throttle time is computed as credits under zero / refill rate (or quota).
Only the controller mutation uses it for now.
The remaining number of credits in the bucket is exposed with the tokens metrics per user/clientId.

Reviewers: Anna Povzner <anna@confluent.io>, Jun Rao <junrao@gmail.com>",12,1,3,99,618,1,9,200,202,15,13,1,233,202,18,33,7,3,2,1,0,1
core/src/main/scala/kafka/server/SensorAccess.scala,core/src/main/scala/kafka/server/SensorAccess.scala,"KAFKA-10162; Use Token Bucket algorithm for controller mutation quota (KIP-599, Part III) (#9114)

Based on the discussion in #9072, I have put together an alternative way. This one does the following:

Instead of changing the implementation of the Rate to behave like a Token Bucket, it actually use two different metrics: the regular Rate and a new Token Bucket. The latter is used to enforce the quota.
The Token Bucket algorithm uses the rate of the quota as the refill rate for the credits and compute the burst based on the number of samples and their length (# samples * sample length * quota).
The Token Bucket algorithm used can go under zero in order to handle unlimited burst (e.g. create topic with a number of partitions higher than the burst). Throttling kicks in when the number of credits is under zero.
The throttle time is computed as credits under zero / refill rate (or quota).
Only the controller mutation uses it for now.
The remaining number of credits in the bucket is exposed with the tokens metrics per user/clientId.

Reviewers: Anna Povzner <anna@confluent.io>, Jun Rao <junrao@gmail.com>",3,4,6,24,164,2,1,71,76,24,3,3,89,76,30,18,12,6,1,0,1,1
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Streams integration tests should not call exit (#9067)

- replace System.exit with Exit.exit in all relevant classes
- forbid use of System.exit in all relevant classes and add exceptions for others

Co-authored-by: John Roesler <vvcephei@apache.org>
Co-authored-by: Matthias J. Sax <matthias@confluent.io>

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@confluent.io>",9,4,3,60,487,1,1,100,99,50,2,2.5,103,99,52,3,3,2,1,0,1,1
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Streams integration tests should not call exit (#9067)

- replace System.exit with Exit.exit in all relevant classes
- forbid use of System.exit in all relevant classes and add exceptions for others

Co-authored-by: John Roesler <vvcephei@apache.org>
Co-authored-by: Matthias J. Sax <matthias@confluent.io>

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@confluent.io>",9,4,3,60,487,1,1,100,99,50,2,2.5,103,99,52,3,3,2,1,0,1,1
streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Streams integration tests should not call exit (#9067)

- replace System.exit with Exit.exit in all relevant classes
- forbid use of System.exit in all relevant classes and add exceptions for others

Co-authored-by: John Roesler <vvcephei@apache.org>
Co-authored-by: Matthias J. Sax <matthias@confluent.io>

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@confluent.io>",9,4,3,60,487,1,1,100,99,50,2,2.5,103,99,52,3,3,2,1,0,1,1
streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Streams integration tests should not call exit (#9067)

- replace System.exit with Exit.exit in all relevant classes
- forbid use of System.exit in all relevant classes and add exceptions for others

Co-authored-by: John Roesler <vvcephei@apache.org>
Co-authored-by: Matthias J. Sax <matthias@confluent.io>

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@confluent.io>",9,4,3,60,487,1,1,100,99,50,2,2.5,103,99,52,3,3,2,1,0,1,1
streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java,"MINOR: Streams integration tests should not call exit (#9067)

- replace System.exit with Exit.exit in all relevant classes
- forbid use of System.exit in all relevant classes and add exceptions for others

Co-authored-by: John Roesler <vvcephei@apache.org>
Co-authored-by: Matthias J. Sax <matthias@confluent.io>

Reviewers: Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@confluent.io>",9,4,3,60,487,1,1,100,99,50,2,2.5,103,99,52,3,3,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/utils/AppInfoParser.java,clients/src/main/java/org/apache/kafka/common/utils/AppInfoParser.java,"MINOR: add additional shutdown log info (#9124)

As title, additional logging added to detect the shutdown progress for Kafka server.

Reviewers: Jason Gustafson <jason@confluent.io>",20,2,0,111,798,1,15,153,97,13,12,2.0,188,97,16,35,9,3,2,1,0,1
streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-10341: Add 2.6.0 to system tests and streams upgrade tests (#9116)

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Matthias J. Sax <matthias@confluent.io>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-10341: Add 2.6.0 to system tests and streams upgrade tests (#9116)

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Matthias J. Sax <matthias@confluent.io>",87,622,0,507,4649,24,24,622,622,622,1,1,622,622,622,0,0,0,0,0,0,0
streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-26/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10341: Add 2.6.0 to system tests and streams upgrade tests (#9116)

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Matthias J. Sax <matthias@confluent.io>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/protocol/RecordsWritableTest.java,clients/src/test/java/org/apache/kafka/common/protocol/RecordsWritableTest.java,"KAFKA-9629 Use generated protocol for Fetch API (#9008)

Refactored FetchRequest and FetchResponse to use the generated message classes for serialization and deserialization. This allows us to bypass unnecessary Struct conversion in a few places. A new ""records"" type was added to the message protocol which uses BaseRecords as the field type. When sending, we can set a FileRecords instance on the message, and when receiving the message class will use MemoryRecords. 

Also included a few JMH benchmarks which indicate a small performance improvement for requests with high partition counts or small record sizes.

Reviewers: Jason Gustafson <jason@confluent.io>, Boyang Chen <boyang@confluent.io>, David Jacot <djacot@confluent.io>, Lucas Bradstreet <lucas@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Colin P. McCabe <cmccabe@apache.org>",3,63,0,38,351,1,1,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/DescribeLogDirsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeLogDirsResult.java,"KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values() (#9007)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, David Jacot <djacot@confluent.io>, Lee Dongjin <dongjin@apache.org>, Chia-Ping Tsai <chia7712@gmail.com>",9,66,18,68,656,7,6,118,70,24,5,1,140,70,28,22,18,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java,"KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values() (#9007)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, David Jacot <djacot@confluent.io>, Lee Dongjin <dongjin@apache.org>, Chia-Ping Tsai <chia7712@gmail.com>",4,64,0,26,150,4,4,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/ReplicaInfo.java,clients/src/main/java/org/apache/kafka/clients/admin/ReplicaInfo.java,"KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values() (#9007)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, David Jacot <djacot@confluent.io>, Lee Dongjin <dongjin@apache.org>, Chia-Ping Tsai <chia7712@gmail.com>",5,70,0,28,119,5,5,70,70,70,1,1,70,70,70,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java,clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java,"KAFKA-10270: A broker to controller channel manager (#9012)

Add a broker to controller channel manager for use cases such as redirection and AlterIsr.

Reviewers: David Arthur <mumrah@gmail.com>, Colin P. McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Viktor Somogyi <viktorsomogyi@gmail.com>
Co-authored-by: Boyang Chen <boyang@confluent.io>",10,1,1,45,271,1,10,87,76,10,9,3,126,76,14,39,16,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/NewTopic.java,clients/src/main/java/org/apache/kafka/clients/admin/NewTopic.java,"MINOR: remove NewTopic#NO_PARTITIONS and NewTopic#NO_REPLICATION_FACTOR as they are duplicate to CreateTopicsRequest#NO_NUM_PARTITIONS and CreateTopicsRequest#NO_REPLICATION_FACTOR (#9077)

Consolidate constant values of NO_PARTITIONS and NO_REPLICATION_FACTOR as stated in the title.

Reviewers: Boyang Chen <boyang@confluent.io>",24,5,7,102,803,3,13,176,85,18,10,2.0,210,85,21,34,11,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateMaintainer.java,"KAFKA-10306: GlobalThread should fail on InvalidOffsetException (#9075)

* KAFKA-10306: GlobalThread should fail on InvalidOffsetException

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>",0,1,1,11,96,0,0,37,37,12,3,1,44,37,15,7,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateConsumerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateConsumerTest.java,"KAFKA-10306: GlobalThread should fail on InvalidOffsetException (#9075)

* KAFKA-10306: GlobalThread should fail on InvalidOffsetException

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateUpdateTask.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>

* Update streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java

Co-authored-by: John Roesler <vvcephei@users.noreply.github.com>",16,10,3,125,1057,5,15,166,165,17,10,3.5,206,165,21,40,10,4,2,1,0,1
core/src/main/scala/kafka/utils/Pool.scala,core/src/main/scala/kafka/utils/Pool.scala,"KAFKA-10301: Do not clear Partition#remoteReplicasMap during partition assignment updates (#9065)

We would previously update the map by adding the new replicas to the map and then removing the old ones.
During a recent refactoring, we changed the logic to first clear the map and then add all the replicas to it.

While this is done in a write lock, not all callers that access the map structure use a lock. It is safer to revert to
the previous behavior of showing the intermediate state of the map with extra replicas, rather than an
intermediate state of the map with no replicas.

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,2,0,38,459,2,13,105,65,5,21,2,193,65,9,88,17,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/InvalidStateStoreException.java,streams/src/main/java/org/apache/kafka/streams/errors/InvalidStateStoreException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",3,2,7,13,74,0,3,39,27,6,6,2.5,65,27,11,26,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/InvalidStateStorePartitionException.java,streams/src/main/java/org/apache/kafka/streams/errors/InvalidStateStorePartitionException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",2,38,0,11,71,2,2,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/StateStoreMigratedException.java,streams/src/main/java/org/apache/kafka/streams/errors/StateStoreMigratedException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",2,37,0,10,60,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/StateStoreNotAvailableException.java,streams/src/main/java/org/apache/kafka/streams/errors/StateStoreNotAvailableException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",2,37,0,10,60,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/StreamsRebalancingException.java,streams/src/main/java/org/apache/kafka/streams/errors/StreamsRebalancingException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",2,35,0,10,60,2,2,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/UnknownStateStoreException.java,streams/src/main/java/org/apache/kafka/streams/errors/UnknownStateStoreException.java,"KAFKA-5876: Add new exception types for Interactive Queries (#8200)

- part of KIP-216
- adds new sub-classes of InvalidStateStoreException

Reviewers: Navinder Pal Singh Brar <navinder_brar@yahoo.com>, Matthias J. Sax <matthias@confluent.io>",2,35,0,10,60,2,2,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/NotLeaderForPartitionException.java,clients/src/main/java/org/apache/kafka/common/errors/NotLeaderForPartitionException.java,"MINOR: Fix deprecation version for NotLeaderForPartitionException (#9056)

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,1,1,17,82,0,4,44,23,6,7,1,73,23,10,29,15,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/CreatePartitionsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/CreatePartitionsOptions.java,"KAFKA-10164; Throttle Create Topic, Create Partition and Delete Topic Operations (KIP-599, Part II, Admin Changes) (#8968)

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",5,17,1,24,119,2,5,67,51,22,3,2,70,51,23,3,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateTopicsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateTopicsOptions.java,"KAFKA-10164; Throttle Create Topic, Create Partition and Delete Topic Operations (KIP-599, Part II, Admin Changes) (#8968)

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",5,16,0,26,131,2,5,76,47,10,8,1.5,101,47,13,25,19,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteTopicsOptions.java,"KAFKA-10164; Throttle Create Topic, Create Partition and Delete Topic Operations (KIP-599, Part II, Admin Changes) (#8968)

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",3,16,0,18,97,2,3,59,37,8,7,1,83,37,12,24,19,3,1,0,1,1
clients/src/main/java/org/apache/kafka/common/errors/ThrottlingQuotaExceededException.java,clients/src/main/java/org/apache/kafka/common/errors/ThrottlingQuotaExceededException.java,"KAFKA-10163; Throttle Create Topic, Create Partition and Delete Topic Operations (KIP-599, Part I, Broker Changes) (#8933)

This PR implements the broker side changes of KIP-599, except the changes of the Rate implementation which will be addressed separately. The PR changes/introduces the following:
  - It introduces the protocol changes.
  - It introduces a new quota manager ControllerMutationQuotaManager which is another specialization of the ClientQuotaManager.
  - It enforces the quota in the KafkaApis and in the AdminManager. This part handles new and old clients as described in the KIP.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",3,37,0,14,71,3,3,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaType.java,clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaType.java,"KAFKA-10163; Throttle Create Topic, Create Partition and Delete Topic Operations (KIP-599, Part I, Broker Changes) (#8933)

This PR implements the broker side changes of KIP-599, except the changes of the Rate implementation which will be addressed separately. The PR changes/introduces the following:
  - It introduces the protocol changes.
  - It introduces a new quota manager ControllerMutationQuotaManager which is another specialization of the ClientQuotaManager.
  - It enforces the quota in the KafkaApis and in the AdminManager. This part handles new and old clients as described in the KIP.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",0,2,1,7,23,0,0,27,26,14,2,1.0,28,26,14,1,1,0,1,0,1,1
tools/src/main/java/org/apache/kafka/tools/ClientCompatibilityTest.java,tools/src/main/java/org/apache/kafka/tools/ClientCompatibilityTest.java,"KAFKA-9432:(follow-up) Set `configKeys` to null in `describeConfigs()` to make it backward compatible with older Kafka versions.

- After #8312, older brokers are returning empty configs,  with latest `adminClient.describeConfigs`.  Old brokers  are receiving empty configNames in `AdminManageer.describeConfigs()` method. Older brokers does not handle empty configKeys. Due to this old brokers are filtering all the configs.
- Update ClientCompatibilityTest to verify describe configs
- Add test case to test describe configs with empty configuration Keys

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #9046 from omkreddy/KAFKA-9432",61,37,0,459,3305,4,19,529,359,31,17,3,752,359,44,223,93,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/JaasContext.java,clients/src/main/java/org/apache/kafka/common/security/JaasContext.java,"MINOR: Improved code quality for various files. (#9037)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",32,2,4,119,947,2,11,195,190,22,9,1,260,190,29,65,41,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java,clients/src/main/java/org/apache/kafka/common/security/ssl/SslPrincipalMapper.java,"MINOR: Improved code quality for various files. (#9037)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",38,1,2,156,1060,1,12,215,197,54,4,2.5,253,197,63,38,34,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotLeaderOrFollowerException.java,clients/src/main/java/org/apache/kafka/common/errors/NotLeaderOrFollowerException.java,"KAFKA-10223; Use NOT_LEADER_OR_FOLLOWER instead of non-retriable REPLICA_NOT_AVAILABLE for consumers (#8979)

Brokers currently return NOT_LEADER_FOR_PARTITION to producers and REPLICA_NOT_AVAILABLE to consumers if a replica is not available on the broker during reassignments. Non-Java clients treat REPLICA_NOT_AVAILABLE as a non-retriable exception, Java consumers handle this error by explicitly matching the error code even though it is not an InvalidMetadataException. This PR renames NOT_LEADER_FOR_PARTITION to NOT_LEADER_OR_FOLLOWER and uses the same error for producers and consumers. This is compatible with both Java and non-Java clients since all clients handle this error code (6) as retriable exception. The PR also makes ReplicaNotAvailableException a subclass of InvalidMetadataException.
    - ALTER_REPLICA_LOG_DIRS continues to return REPLICA_NOT_AVAILABLE. Retained this for compatibility since this request never returned NOT_LEADER_FOR_PARTITION earlier. 
   -  MetadataRequest version 0 also returns REPLICA_NOT_AVAILABLE as topic-level error code for compatibility. Newer versions filter these out and return Errors.NONE, so didn't change this.
   - Partition responses in MetadataRequest return REPLICA_NOT_AVAILABLE to indicate that one of the replicas is not available. Did not change this since NOT_LEADER_FOR_PARTITION is not suitable in this case.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>, Bob Barrett <bob.barrett@confluent.io>",4,48,0,17,85,4,4,48,48,48,1,1,48,48,48,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ReplicaNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/ReplicaNotAvailableException.java,"KAFKA-10223; Use NOT_LEADER_OR_FOLLOWER instead of non-retriable REPLICA_NOT_AVAILABLE for consumers (#8979)

Brokers currently return NOT_LEADER_FOR_PARTITION to producers and REPLICA_NOT_AVAILABLE to consumers if a replica is not available on the broker during reassignments. Non-Java clients treat REPLICA_NOT_AVAILABLE as a non-retriable exception, Java consumers handle this error by explicitly matching the error code even though it is not an InvalidMetadataException. This PR renames NOT_LEADER_FOR_PARTITION to NOT_LEADER_OR_FOLLOWER and uses the same error for producers and consumers. This is compatible with both Java and non-Java clients since all clients handle this error code (6) as retriable exception. The PR also makes ReplicaNotAvailableException a subclass of InvalidMetadataException.
    - ALTER_REPLICA_LOG_DIRS continues to return REPLICA_NOT_AVAILABLE. Retained this for compatibility since this request never returned NOT_LEADER_FOR_PARTITION earlier. 
   -  MetadataRequest version 0 also returns REPLICA_NOT_AVAILABLE as topic-level error code for compatibility. Newer versions filter these out and return Errors.NONE, so didn't change this.
   - Partition responses in MetadataRequest return REPLICA_NOT_AVAILABLE to indicate that one of the replicas is not available. Did not change this since NOT_LEADER_FOR_PARTITION is not suitable in this case.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>, Bob Barrett <bob.barrett@confluent.io>",3,7,1,13,70,0,3,41,36,14,3,1,46,36,15,5,4,2,2,1,0,1
generator/src/main/java/org/apache/kafka/message/TypeClassGenerator.java,generator/src/main/java/org/apache/kafka/message/TypeClassGenerator.java,"MINOR: Add ApiMessageTypeGenerator (#9002)

Previously, we had some code hard-coded to generate message type classes
for RPCs.  We might want to generate message type classes for other
things as well, so make it more generic.

Reviewers: Boyang Chen <boyang@confluent.io>",0,43,0,8,49,0,0,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/SuppressedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/SuppressedInternal.java,"MINOR: rename class `RecordTimeDefintion` to `RecordTimeDefinition` (#8939)

Fix a typo for class RecordTimeDefintion to RecordTimeDefinition

Reviewers: Boyang Chen <boyang@confluent.io>",21,1,1,75,499,1,10,118,76,12,10,1.5,166,76,17,48,18,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/TimeDefinitions.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/TimeDefinitions.java,"MINOR: rename class `RecordTimeDefintion` to `RecordTimeDefinition` (#8939)

Fix a typo for class RecordTimeDefintion to RecordTimeDefinition

Reviewers: Boyang Chen <boyang@confluent.io>",9,5,5,45,275,4,9,79,79,26,3,1,85,79,28,6,5,2,2,1,0,1
tests/kafkatest/tests/connect/connect_rest_test.py,tests/kafkatest/tests/connect/connect_rest_test.py,"KAFKA-10209: Fix connect_rest_test.py after the introduction of new connector configs (#8944)

There are two new configs introduced by 371f14c3c12d2e341ac96bd52393b43a10acfa84 and 1c4eb1a5757df611735cfac9b709e0d80d0da4b3 so we have to update the expected configs in the connect_rest_test.py system test too.

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",17,2,2,149,1292,0,6,218,163,11,20,2.5,286,163,14,68,18,3,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/OffsetSync.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/OffsetSync.java,"KAFKA-10232: MirrorMaker2 internal topics Formatters KIP-597 (#8604)

This PR includes 3 MessageFormatters for MirrorMaker2 internal topics:
- HeartbeatFormatter
- CheckpointFormatter
- OffsetSyncFormatter

This also introduces a new public interface org.apache.kafka.common.MessageFormatter that users can implement to build custom formatters.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, David Jacot <djacot@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",12,5,5,83,618,1,12,120,120,60,2,3.0,125,120,62,5,5,2,1,0,1,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/CheckpointFormatter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/CheckpointFormatter.java,"KAFKA-10232: MirrorMaker2 internal topics Formatters KIP-597 (#8604)

This PR includes 3 MessageFormatters for MirrorMaker2 internal topics:
- HeartbeatFormatter
- CheckpointFormatter
- OffsetSyncFormatter

This also introduces a new public interface org.apache.kafka.common.MessageFormatter that users can implement to build custom formatters.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, David Jacot <djacot@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",1,31,0,11,99,1,1,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/HeartbeatFormatter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/HeartbeatFormatter.java,"KAFKA-10232: MirrorMaker2 internal topics Formatters KIP-597 (#8604)

This PR includes 3 MessageFormatters for MirrorMaker2 internal topics:
- HeartbeatFormatter
- CheckpointFormatter
- OffsetSyncFormatter

This also introduces a new public interface org.apache.kafka.common.MessageFormatter that users can implement to build custom formatters.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, David Jacot <djacot@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",1,31,0,11,99,1,1,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/OffsetSyncFormatter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/formatters/OffsetSyncFormatter.java,"KAFKA-10232: MirrorMaker2 internal topics Formatters KIP-597 (#8604)

This PR includes 3 MessageFormatters for MirrorMaker2 internal topics:
- HeartbeatFormatter
- CheckpointFormatter
- OffsetSyncFormatter

This also introduces a new public interface org.apache.kafka.common.MessageFormatter that users can implement to build custom formatters.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, David Jacot <djacot@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",1,31,0,11,99,1,1,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/MessageFormatter.scala,core/src/main/scala/kafka/common/MessageFormatter.scala,"KAFKA-10232: MirrorMaker2 internal topics Formatters KIP-597 (#8604)

This PR includes 3 MessageFormatters for MirrorMaker2 internal topics:
- HeartbeatFormatter
- CheckpointFormatter
- OffsetSyncFormatter

This also introduces a new public interface org.apache.kafka.common.MessageFormatter that users can implement to build custom formatters.

Reviewers: Konstantine Karantasis <k.karantasis@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, David Jacot <djacot@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",0,2,13,4,25,2,0,28,39,9,3,2,43,39,14,15,13,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,1,1,96,794,1,8,134,184,7,20,1.5,317,184,16,183,75,9,2,1,0,1
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",89,632,0,513,4679,26,26,632,632,632,1,1,632,632,632,0,0,0,0,0,0,0
streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",87,622,0,507,4649,24,24,622,622,622,1,1,622,622,622,0,0,0,0,0,0,0
streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-23/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",87,622,0,507,4649,24,24,622,622,622,1,1,622,622,622,0,0,0,0,0,0,0
streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-24/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",25,298,0,231,2246,11,11,298,298,298,1,1,298,298,298,0,0,0,0,0,0,0
streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",87,622,0,507,4649,24,24,622,622,622,1,1,622,622,622,0,0,0,0,0,0,0
streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java,"KAFKA-10173: Use SmokeTest for upgrade system tests (#8938)

Replaces the previous upgrade test's trivial Streams app
with the commonly used SmokeTest, exercising many more
features. Also adjust the test matrix to test upgrading
from each released version since 2.2 to the current branch.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",14,134,0,96,794,8,8,134,134,134,1,1,134,134,134,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsOptions.java,"MINOR: Update AlterConfigsOptions Javadoc (#8958)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",3,1,1,18,97,0,3,59,45,7,8,1.0,89,45,11,30,19,4,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullChangeSerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullChangeSerde.java,"KAFKA-10173: Fix suppress changelog binary schema compatibility (#8905)

We inadvertently changed the binary schema of the suppress buffer changelog
in 2.4.0 without bumping the schema version number. As a result, it is impossible
to upgrade from 2.3.x to 2.4+ if you are using suppression.

* Refactor the schema compatibility test to use serialized data from older versions
as a more foolproof compatibility test.
* Refactor the upgrade system test to use the smoke test application so that we
actually exercise a significant portion of the Streams API during upgrade testing
* Add more recent versions to the upgrade system test matrix
* Fix the compatibility bug by bumping the schema version to 3

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",14,3,40,52,466,2,6,86,128,12,7,3,232,128,33,146,87,21,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/BufferValue.java,streams/src/main/java/org/apache/kafka/streams/state/internals/BufferValue.java,"KAFKA-10173: Fix suppress changelog binary schema compatibility (#8905)

We inadvertently changed the binary schema of the suppress buffer changelog
in 2.4.0 without bumping the schema version number. As a result, it is impossible
to upgrade from 2.3.x to 2.4+ if you are using suppression.

* Refactor the schema compatibility test to use serialized data from older versions
as a more foolproof compatibility test.
* Refactor the upgrade system test to use the smoke test application so that we
actually exercise a significant portion of the Streams API during upgrade testing
* Add more recent versions to the upgrade system test matrix
* Fix the compatibility bug by bumping the schema version to 3

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",31,8,19,120,824,3,12,169,98,42,4,4.0,218,108,54,49,29,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ContextualRecord.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ContextualRecord.java,"KAFKA-10173: Fix suppress changelog binary schema compatibility (#8905)

We inadvertently changed the binary schema of the suppress buffer changelog
in 2.4.0 without bumping the schema version number. As a result, it is impossible
to upgrade from 2.3.x to 2.4+ if you are using suppression.

* Refactor the schema compatibility test to use serialized data from older versions
as a more foolproof compatibility test.
* Refactor the upgrade system test to use the smoke test application so that we
actually exercise a significant portion of the Streams API during upgrade testing
* Add more recent versions to the upgrade system test matrix
* Fix the compatibility bug by bumping the schema version to 3

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",13,4,28,51,324,2,8,79,58,13,6,2.5,116,58,19,37,28,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java,"KAFKA-10173: Fix suppress changelog binary schema compatibility (#8905)

We inadvertently changed the binary schema of the suppress buffer changelog
in 2.4.0 without bumping the schema version number. As a result, it is impossible
to upgrade from 2.3.x to 2.4+ if you are using suppression.

* Refactor the schema compatibility test to use serialized data from older versions
as a more foolproof compatibility test.
* Refactor the upgrade system test to use the smoke test application so that we
actually exercise a significant portion of the Streams API during upgrade testing
* Add more recent versions to the upgrade system test matrix
* Fix the compatibility bug by bumping the schema version to 3

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",16,158,0,118,866,10,10,158,158,158,1,1,158,158,158,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/FullChangeSerdeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/FullChangeSerdeTest.java,"KAFKA-10173: Fix suppress changelog binary schema compatibility (#8905)

We inadvertently changed the binary schema of the suppress buffer changelog
in 2.4.0 without bumping the schema version number. As a result, it is impossible
to upgrade from 2.3.x to 2.4+ if you are using suppression.

* Refactor the schema compatibility test to use serialized data from older versions
as a more foolproof compatibility test.
* Refactor the upgrade system test to use the smoke test application so that we
actually exercise a significant portion of the Streams API during upgrade testing
* Add more recent versions to the upgrade system test matrix
* Fix the compatibility bug by bumping the schema version to 3

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",11,34,5,80,707,6,6,118,167,24,5,7,248,167,50,130,102,26,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java,clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java,"MINOR: Rename SslTransportLayer.State.""NOT_INITALIZED"" enum value to ""NOT_INITIALIZED""

The enum ```State``` is private so it is fine to fix typo without breaking compatibility.

Author: Chia-Ping Tsai <chia7712@gmail.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #8932 from chia7712/MINOR-8932",195,5,5,660,4386,4,46,1006,690,26,38,3.5,1371,690,36,365,109,10,2,1,0,1
core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala,core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",28,4,4,99,691,4,12,159,129,7,22,2.0,280,129,13,121,62,6,2,1,0,1
core/src/main/scala/kafka/tools/StateChangeLogMerger.scala,core/src/main/scala/kafka/tools/StateChangeLogMerger.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",32,2,2,129,1179,2,4,196,188,15,13,2,228,188,18,32,11,2,2,1,0,1
core/src/main/scala/kafka/utils/json/DecodeJson.scala,core/src/main/scala/kafka/utils/json/DecodeJson.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",11,1,1,61,691,0,6,111,122,16,7,1,146,122,21,35,17,5,2,1,0,1
core/src/main/scala/kafka/utils/timer/TimerTask.scala,core/src/main/scala/kafka/utils/timer/TimerTask.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",5,1,3,19,99,2,2,45,43,11,4,1.0,51,43,13,6,3,2,2,1,0,1
core/src/main/scala/org/apache/zookeeper/ZooKeeperMainWithTlsSupportForKafka.scala,core/src/main/scala/org/apache/zookeeper/ZooKeeperMainWithTlsSupportForKafka.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",15,2,2,62,455,1,5,100,100,33,3,2,104,100,35,4,2,1,2,1,0,1
core/src/test/scala/kafka/security/minikdc/MiniKdc.scala,core/src/test/scala/kafka/security/minikdc/MiniKdc.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",33,1,1,312,2332,1,14,442,433,32,14,2.0,485,433,35,43,10,3,2,1,0,1
core/src/test/scala/other/kafka/ReplicationQuotasTestRig.scala,core/src/test/scala/other/kafka/ReplicationQuotasTestRig.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",44,2,2,261,2214,1,24,344,334,16,21,2,403,334,19,59,8,3,2,1,0,1
core/src/test/scala/unit/kafka/utils/MockScheduler.scala,core/src/test/scala/unit/kafka/utils/MockScheduler.scala,"MINOR: Update Scala to 2.13.3 (#8931)

I had to fix several compiler errors due to deprecation of auto application of `()`. A related
Xlint config (`-Xlint:nullary-override`) is no longer valid in 2.13, so we now only enable it
for 2.12. The compiler flagged two new inliner warnings that required suppression and
the semantics of `&` in `@nowarn` annotations changed, requiring a small change in
one of the warning suppressions.

I also removed the deprecation of a number of methods in `KafkaZkClient` as
they should not have been deprecated in the first place since `KafkaZkClient` is an
internal class and we still use these methods in the Controller and so on. This
became visible because the Scala compiler now respects Java's `@Deprecated`
annotation.

Finally, I included a few minor clean-ups (eg using `toBuffer` instead `toList`) when fixing
the compilation warnings.

Noteworthy bug fixes in Scala 2.13.3:

* Fix 2.13-only bug in Java collection converters that caused some operations to perform an extra pass
* Fix 2.13.2 performance regression in Vector: restore special cases for small operands in appendedAll and prependedAll
* Increase laziness of #:: for LazyList
* Fixes related to annotation parsing of @Deprecated from Java sources in mixed compilation

Full release notes:
https://github.com/scala/scala/releases/tag/v2.13.3

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",22,1,1,89,559,1,13,149,83,10,15,2,232,83,15,83,26,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java,streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java,"KAFKA-10169: swallow non-fatal KafkaException and don't abort transaction during clean close (#8900)

If there's any pending data and we haven't flushed the producer when we abort a transaction, a KafkaException is returned for the previous send. This is a bit misleading, since the situation is not an unrecoverable error and so the Kafka Exception is really non-fatal. For now, we should just catch and swallow this in the RecordCollector (see also: KAFKA-10169)

The reason we ended up aborting an un-flushed transaction was due to the combination of
a. always aborting the ongoing transaction when any task is closed/revoked
b. only committing (and flushing) if at least one of the revoked tasks needs to be committed (regardless of whether any non-revoked tasks have data/transaction in flight)

Given the above, we can end up with an ongoing transaction that isn't committed since none of the revoked tasks have any data in the transaction. We then abort the transaction anyway, when those tasks are closed. So in addition to the above (swallowing this exception), we should avoid unnecessarily aborting data for tasks that haven't been revoked.

We can handle this by splitting the RecordCollector's close into a dirty and clean flavor: if dirty, we need to abort the transaction since it may be dirty due to the commit attempt failing. But if clean, we can skip aborting the transaction since we know that either we just committed and thus there is no ongoing transaction to abort, or else the transaction in flight contains no data from the tasks being closed

Note that this means we still abort the transaction any time a task is closed dirty, so we must close/reinitialize any active task with pending data (that was aborted).

In sum:

* hackily check the KafkaException message and swallow
* only abort the transaction during a dirty close
* refactor shutdown to make sure we don't closeClean a task whose data was actually aborted

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,4,0,10,64,1,2,36,52,5,7,3,116,52,17,80,46,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java,"KAFKA-10169: swallow non-fatal KafkaException and don't abort transaction during clean close (#8900)

If there's any pending data and we haven't flushed the producer when we abort a transaction, a KafkaException is returned for the previous send. This is a bit misleading, since the situation is not an unrecoverable error and so the Kafka Exception is really non-fatal. For now, we should just catch and swallow this in the RecordCollector (see also: KAFKA-10169)

The reason we ended up aborting an un-flushed transaction was due to the combination of
a. always aborting the ongoing transaction when any task is closed/revoked
b. only committing (and flushing) if at least one of the revoked tasks needs to be committed (regardless of whether any non-revoked tasks have data/transaction in flight)

Given the above, we can end up with an ongoing transaction that isn't committed since none of the revoked tasks have any data in the transaction. We then abort the transaction anyway, when those tasks are closed. So in addition to the above (swallowing this exception), we should avoid unnecessarily aborting data for tasks that haven't been revoked.

We can handle this by splitting the RecordCollector's close into a dirty and clean flavor: if dirty, we need to abort the transaction since it may be dirty due to the commit attempt failing. But if clean, we can skip aborting the transaction since we know that either we just committed and thus there is no ongoing transaction to abort, or else the transaction in flight contains no data from the tasks being closed

Note that this means we still abort the transaction any time a task is closed dirty, so we must close/reinitialize any active task with pending data (that was aborted).

In sum:

* hackily check the KafkaException message and swallow
* only abort the transaction during a dirty close
* refactor shutdown to make sure we don't closeClean a task whose data was actually aborted

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,7,2,33,230,0,0,88,80,3,31,2,271,80,9,183,100,6,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockRecordCollector.java,streams/src/test/java/org/apache/kafka/test/MockRecordCollector.java,"KAFKA-10169: swallow non-fatal KafkaException and don't abort transaction during clean close (#8900)

If there's any pending data and we haven't flushed the producer when we abort a transaction, a KafkaException is returned for the previous send. This is a bit misleading, since the situation is not an unrecoverable error and so the Kafka Exception is really non-fatal. For now, we should just catch and swallow this in the RecordCollector (see also: KAFKA-10169)

The reason we ended up aborting an un-flushed transaction was due to the combination of
a. always aborting the ongoing transaction when any task is closed/revoked
b. only committing (and flushing) if at least one of the revoked tasks needs to be committed (regardless of whether any non-revoked tasks have data/transaction in flight)

Given the above, we can end up with an ongoing transaction that isn't committed since none of the revoked tasks have any data in the transaction. We then abort the transaction anyway, when those tasks are closed. So in addition to the above (swallowing this exception), we should avoid unnecessarily aborting data for tasks that haven't been revoked.

We can handle this by splitting the RecordCollector's close into a dirty and clean flavor: if dirty, we need to abort the transaction since it may be dirty due to the commit attempt failing. But if clean, we can skip aborting the transaction since we know that either we just committed and thus there is no ongoing transaction to abort, or else the transaction in flight contains no data from the tasks being closed

Note that this means we still abort the transaction any time a task is closed dirty, so we must close/reinitialize any active task with pending data (that was aborted).

In sum:

* hackily check the KafkaException message and swallow
* only abort the transaction during a dirty close
* refactor shutdown to make sure we don't closeClean a task whose data was actually aborted

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",10,4,1,72,428,3,10,106,48,7,15,3,167,48,11,61,17,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/ConsumedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/ConsumedInternal.java,"MINOR: code cleanup for inconsistent naming (#8871)

Reviewer: Matthias J. Sax <matthias@confluent.io>",12,2,2,41,284,2,10,70,56,14,5,2,73,56,15,3,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/StoreQueryParameters.java,streams/src/main/java/org/apache/kafka/streams/StoreQueryParameters.java,"KAFKA-9891: add integration tests for EOS and StandbyTask (#8890)

Ports the test from #8886 to trunk -- this should be merged to 2.6 branch.

One open question. In 2.6 and trunk we rely on the active tasks to wipe out the store if it crashes. However, assume there is a hard JVM crash and we don't call closeDirty() the store would not be wiped out. Thus, I am wondering, if we would need to fix this (for both active and standby tasks) and do a check on startup if a local store must be wiped out?

The current test passes, as we do a proper cleanup after the exception is thrown.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",15,1,1,61,377,1,11,129,138,43,3,1,165,138,55,36,35,12,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/LogTruncationException.java,clients/src/main/java/org/apache/kafka/clients/consumer/LogTruncationException.java,"KAFKA-10113; Specify fetch offsets correctly in `LogTruncationException` (#8822)

This patch fixes a bug in the constructor of `LogTruncationException`. We were passing the divergent offsets to the super constructor as the fetch offsets. There is no way to fix this without breaking compatibility, but the harm is probably minimal since this exception was not getting raised properly until KAFKA-9840 anyway.

Note that I have also moved the check for unknown offset and epoch into `SubscriptionState`, which ensures that the partition is still awaiting validation and that the fetch offset hasn't changed. Finally, I made some minor improvements to the logging and exception messages to ensure that we always have the fetch offset and epoch as well as the divergent offset and epoch included.

Reviewers: Boyang Chen <boyang@confluent.io>, David Arthur <mumrah@gmail.com>",2,14,10,16,110,2,2,55,50,18,3,1,66,50,22,11,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetOutOfRangeException.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetOutOfRangeException.java,"KAFKA-10113; Specify fetch offsets correctly in `LogTruncationException` (#8822)

This patch fixes a bug in the constructor of `LogTruncationException`. We were passing the divergent offsets to the super constructor as the fetch offsets. There is no way to fix this without breaking compatibility, but the harm is probably minimal since this exception was not getting raised properly until KAFKA-9840 anyway.

Note that I have also moved the check for unknown offset and epoch into `SubscriptionState`, which ensures that the partition is still awaiting validation and that the fetch offset hasn't changed. Finally, I made some minor improvements to the logging and exception messages to ensure that we always have the fetch offset and epoch as well as the divergent offset and epoch included.

Reviewers: Boyang Chen <boyang@confluent.io>, David Arthur <mumrah@gmail.com>",4,3,0,23,144,0,4,54,22,6,9,1,102,22,11,48,20,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListOffsetsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListOffsetsResult.java,"KAFKA-10167: use the admin client to read end-offset (#8876)

Since admin client allows use to use flexible offset-spec, we can always set to use read-uncommitted regardless of the EOS config.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,2,2,64,444,2,8,107,107,54,2,1.5,109,107,54,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/OffsetSpec.java,clients/src/main/java/org/apache/kafka/clients/admin/OffsetSpec.java,"KAFKA-10167: use the admin client to read end-offset (#8876)

Since admin client allows use to use flexible offset-spec, we can always set to use read-uncommitted regardless of the EOS config.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,3,3,24,119,0,5,63,62,16,4,1.0,67,62,17,4,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogRegister.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ChangelogRegister.java,"KAFKA-10167: use the admin client to read end-offset (#8876)

Since admin client allows use to use flexible offset-spec, we can always set to use read-uncommitted regardless of the EOS config.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,1,1,7,59,0,0,39,27,6,6,1.5,54,27,9,15,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java,streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java,"KAFKA-10167: use the admin client to read end-offset (#8876)

Since admin client allows use to use flexible offset-spec, we can always set to use read-uncommitted regardless of the EOS config.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,3,3,62,595,2,7,91,52,6,15,3,130,52,9,39,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/PredicateDoc.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/PredicateDoc.java,"MINOR: Documentation for KIP-585 (#8839)

* Add documentation for using transformation predicates.
* Add `PredicateDoc` for generating predicate config docs, following the style of `TransformationDoc`.
* Fix the header depth mismatch.
* Avoid generating HTML ids based purely on the config name since there
are very likely to conflict (e.g. #name). Instead allow passing a function
which can be used to generate an id from a config key.

The docs have been generated and tested locally. 

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",5,85,0,55,462,4,4,85,85,85,1,1,85,85,85,0,0,0,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/HasHeaderKey.java,"MINOR: Documentation for KIP-585 (#8839)

* Add documentation for using transformation predicates.
* Add `PredicateDoc` for generating predicate config docs, following the style of `TransformationDoc`.
* Fix the header depth mismatch.
* Avoid generating HTML ids based purely on the config name since there
are very likely to conflict (e.g. #name). Instead allow passing a function
which can be used to generate an id from a config key.

The docs have been generated and tested locally. 

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",6,1,0,38,275,0,5,68,67,34,2,1.0,68,67,34,0,0,0,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/RecordIsTombstone.java,"MINOR: Documentation for KIP-585 (#8839)

* Add documentation for using transformation predicates.
* Add `PredicateDoc` for generating predicate config docs, following the style of `TransformationDoc`.
* Fix the header depth mismatch.
* Avoid generating HTML ids based purely on the config name since there
are very likely to conflict (e.g. #name). Instead allow passing a function
which can be used to generate an id from a config key.

The docs have been generated and tested locally. 

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",5,2,1,26,152,0,5,57,56,28,2,1.0,58,56,29,1,1,0,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/predicates/TopicNameMatches.java,"MINOR: Documentation for KIP-585 (#8839)

* Add documentation for using transformation predicates.
* Add `PredicateDoc` for generating predicate config docs, following the style of `TransformationDoc`.
* Fix the header depth mismatch.
* Avoid generating HTML ids based purely on the config name since there
are very likely to conflict (e.g. #name). Instead allow passing a function
which can be used to generate an id from a config key.

The docs have been generated and tested locally. 

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",7,3,0,48,356,0,5,80,77,40,2,1.0,80,77,40,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/ChangedDeserializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/ChangedDeserializer.java,"KAFKA-10049: Fixed FKJ bug where wrapped serdes are set incorrectly when using default StreamsConfig serdes (#8764)

Bug Details:
Mistakenly setting the value serde to the key serde for an internal wrapped serde in the FKJ workflow.

Testing:
Modified the existing test to reproduce the issue, then verified that the test passes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",8,3,3,39,335,2,6,69,59,8,9,3,108,59,12,39,8,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/ChangedSerializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/ChangedSerializer.java,"KAFKA-10049: Fixed FKJ bug where wrapped serdes are set incorrectly when using default StreamsConfig serdes (#8764)

Bug Details:
Mistakenly setting the value serde to the key serde for an internal wrapped serde in the FKJ workflow.

Testing:
Modified the existing test to reproduce the issue, then verified that the test passes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",11,3,3,50,394,2,6,87,57,7,12,3.0,129,57,11,42,7,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableDeserializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableDeserializer.java,"KAFKA-10049: Fixed FKJ bug where wrapped serdes are set incorrectly when using default StreamsConfig serdes (#8764)

Bug Details:
Mistakenly setting the value serde to the key serde for an internal wrapped serde in the FKJ workflow.

Testing:
Modified the existing test to reproduce the issue, then verified that the test passes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",0,3,3,5,61,0,0,23,23,12,2,1.0,26,23,13,3,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableSerializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WrappingNullableSerializer.java,"KAFKA-10049: Fixed FKJ bug where wrapped serdes are set incorrectly when using default StreamsConfig serdes (#8764)

Bug Details:
Mistakenly setting the value serde to the key serde for an internal wrapped serde in the FKJ workflow.

Testing:
Modified the existing test to reproduce the issue, then verified that the test passes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",0,2,2,5,61,0,0,23,23,12,2,1.0,25,23,12,2,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerde.java,"KAFKA-10049: Fixed FKJ bug where wrapped serdes are set incorrectly when using default StreamsConfig serdes (#8764)

Bug Details:
Mistakenly setting the value serde to the key serde for an internal wrapped serde in the FKJ workflow.

Testing:
Modified the existing test to reproduce the issue, then verified that the test passes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <vvcephei@apache.org>",21,6,6,99,845,4,9,147,124,37,4,5.5,164,124,41,17,6,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/feature/Features.java,clients/src/main/java/org/apache/kafka/common/feature/Features.java,"KAFKA-10027: Implement read path for feature versioning system (KIP-584) (#8680)

In this PR, I have implemented various classes and integration for the read path of the feature versioning system (KIP-584). The ultimate plan is that the cluster-wide finalized features information is going to be stored in ZK under the node /feature. The read path implemented in this PR is centered around reading this finalized features information from ZK, and, processing it inside the Broker.

Here is a summary of what's in this PR (a lot of it is new classes):

A facility is provided in the broker to declare its supported features, and advertise its supported features via its own BrokerIdZNode under a features key.
A facility is provided in the broker to listen to and propagate cluster-wide finalized feature changes from ZK.
When new finalized features are read from ZK, feature incompatibilities are detected by comparing against the broker's own supported features.
ApiVersionsResponse is now served containing supported and finalized feature information (using the newly added tagged fields).

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",17,184,0,83,604,15,15,184,184,184,1,1,184,184,184,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/SupportedFeatures.scala,core/src/main/scala/kafka/server/SupportedFeatures.scala,"KAFKA-10027: Implement read path for feature versioning system (KIP-584) (#8680)

In this PR, I have implemented various classes and integration for the read path of the feature versioning system (KIP-584). The ultimate plan is that the cluster-wide finalized features information is going to be stored in ZK under the node /feature. The read path implemented in this PR is centered around reading this finalized features information from ZK, and, processing it inside the Broker.

Here is a summary of what's in this PR (a lot of it is new classes):

A facility is provided in the broker to declare its supported features, and advertise its supported features via its own BrokerIdZNode under a features key.
A facility is provided in the broker to listen to and propagate cluster-wide finalized feature changes from ZK.
When new finalized features are read from ZK, feature incompatibilities are detected by comparing against the broker's own supported features.
ApiVersionsResponse is now served containing supported and finalized feature information (using the newly added tagged fields).

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",10,93,0,36,284,3,3,93,93,93,1,1,93,93,93,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/SupportedFeaturesTest.scala,core/src/test/scala/unit/kafka/server/SupportedFeaturesTest.scala,"KAFKA-10027: Implement read path for feature versioning system (KIP-584) (#8680)

In this PR, I have implemented various classes and integration for the read path of the feature versioning system (KIP-584). The ultimate plan is that the cluster-wide finalized features information is going to be stored in ZK under the node /feature. The read path implemented in this PR is centered around reading this finalized features information from ZK, and, processing it inside the Broker.

Here is a summary of what's in this PR (a lot of it is new classes):

A facility is provided in the broker to declare its supported features, and advertise its supported features via its own BrokerIdZNode under a features key.
A facility is provided in the broker to listen to and propagate cluster-wide finalized feature changes from ZK.
When new finalized features are read from ZK, feature incompatibilities are detected by comparing against the broker's own supported features.
ApiVersionsResponse is now served containing supported and finalized feature information (using the newly added tagged fields).

Reviewers: Boyang Chen <boyang@confluent.io>, Jun Rao <junrao@gmail.com>",3,56,0,32,230,3,3,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/ClientUtilsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ClientUtilsTest.java,"KAFKA-10085: correctly compute lag for optimized source changelogs (#8787)

Split out the optimized source changelogs and fetch the committed offsets rather than the end offset for task lag computation

Reviewers: John Roesler <vvcephei@apache.org>",7,51,20,91,987,8,7,125,96,42,3,13,157,96,52,32,20,11,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/CloseableConnectorContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/CloseableConnectorContext.java,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",0,32,0,7,58,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderConnectorContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderConnectorContext.java,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",6,26,2,38,206,4,4,69,42,10,7,3,92,42,13,23,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/ConnectorPluginsResource.java,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",14,17,1,92,779,1,5,126,49,11,11,4,157,49,14,31,6,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/ConnectUtils.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/ConnectUtils.java,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",13,11,0,66,546,2,6,93,34,10,9,3,107,34,12,14,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConnectorTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConnectorTest.java,"KAFKA-9374: Make connector interactions asynchronous (#8069)

These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.

The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.

If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).

Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.

Reviewers: Greg Harris <gregh@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",21,122,39,435,3257,11,20,597,336,75,8,15.5,698,336,87,101,39,13,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrorHandlingIntegrationTest.java,"KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter (#8829)

Make sure that the Errant Record Reporter recently added in KIP-610 adheres to the  `errors.tolerance` policy.

Author: Aakash Shah <ashah@confluent.io>
Reviewers: Arjun Satish <arjunconfluent.io>, Randall Hauch <rhauch@gmail.com>",25,68,0,216,2044,1,10,328,231,55,6,1.5,336,231,56,8,3,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java,"KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter (#8829)

Make sure that the Errant Record Reporter recently added in KIP-610 adheres to the  `errors.tolerance` policy.

Author: Aakash Shah <ashah@confluent.io>
Reviewers: Arjun Satish <arjunconfluent.io>, Randall Hauch <rhauch@gmail.com>",9,0,69,133,1233,1,5,246,137,22,11,3,338,137,31,92,69,8,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Connect.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Connect.java,"KAFKA-9216: Enforce that Connect’s internal topics use `compact` cleanup policy (#8828)

This change adds a check to the KafkaConfigBackingStore, KafkaOffsetBackingStore, and KafkaStatusBackingStore to use the admin client to verify that the internal topics are compacted and do not use the `delete` cleanup policy.

Connect already will create the internal topics with `cleanup.policy=compact` if the topics do not yet exist when the Connect workers are started; the new topics are created always as compacted, overwriting any user-specified `cleanup.policy`. However, if the topics already exist the worker did not previously verify the internal topics were compacted, such as when a user manually creates the internal topics before starting Connect or manually changes the topic settings after the fact.

The current change helps guard against users running Connect with topics that have delete cleanup policy enabled, which will remove all connector configurations, source offsets, and connector & task statuses that are older than the retention time. This means that, for example, the configuration for a long-running connector could be deleted by the broker, and this will cause restart issues upon a subsequent rebalance or restarting of Connect worker(s).

Connect behavior requires that its internal topics are compacted and not deleted after some retention time. Therefore, this additional check is simply enforcing the existing expectations, and therefore does not need a KIP.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>, Chris Egerton <chrise@confluent.io>",11,4,0,74,421,1,8,111,94,9,12,2.5,160,94,13,49,17,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/InternalTopicsIntegrationTest.java,"KAFKA-9216: Enforce that Connect’s internal topics use `compact` cleanup policy (#8828)

This change adds a check to the KafkaConfigBackingStore, KafkaOffsetBackingStore, and KafkaStatusBackingStore to use the admin client to verify that the internal topics are compacted and do not use the `delete` cleanup policy.

Connect already will create the internal topics with `cleanup.policy=compact` if the topics do not yet exist when the Connect workers are started; the new topics are created always as compacted, overwriting any user-specified `cleanup.policy`. However, if the topics already exist the worker did not previously verify the internal topics were compacted, such as when a user manually creates the internal topics before starting Connect or manually changes the topic settings after the fact.

The current change helps guard against users running Connect with topics that have delete cleanup policy enabled, which will remove all connector configurations, source offsets, and connector & task statuses that are older than the retention time. This means that, for example, the configuration for a long-running connector could be deleted by the broker, and this will cause restart issues upon a subsequent rebalance or restarting of Connect worker(s).

Connect behavior requires that its internal topics are compacted and not deleted after some retention time. Therefore, this additional check is simply enforcing the existing expectations, and therefore does not need a KIP.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>, Chris Egerton <chrise@confluent.io>",15,138,0,220,1813,6,15,315,176,79,4,2.5,321,176,80,6,4,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/WorkerHandle.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/WorkerHandle.java,"KAFKA-9216: Enforce that Connect’s internal topics use `compact` cleanup policy (#8828)

This change adds a check to the KafkaConfigBackingStore, KafkaOffsetBackingStore, and KafkaStatusBackingStore to use the admin client to verify that the internal topics are compacted and do not use the `delete` cleanup policy.

Connect already will create the internal topics with `cleanup.policy=compact` if the topics do not yet exist when the Connect workers are started; the new topics are created always as compacted, overwriting any user-specified `cleanup.policy`. However, if the topics already exist the worker did not previously verify the internal topics were compacted, such as when a user manually creates the internal topics before starting Connect or manually changes the topic settings after the fact.

The current change helps guard against users running Connect with topics that have delete cleanup policy enabled, which will remove all connector configurations, source offsets, and connector & task statuses that are older than the retention time. This means that, for example, the configuration for a long-running connector could be deleted by the broker, and this will cause restart issues upon a subsequent rebalance or restarting of Connect worker(s).

Connect behavior requires that its internal topics are compacted and not deleted after some retention time. Therefore, this additional check is simply enforcing the existing expectations, and therefore does not need a KIP.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>, Chris Egerton <chrise@confluent.io>",13,9,0,58,343,1,10,121,103,40,3,1,121,103,40,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginUtilsTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginUtilsTest.java,"KAFKA-9969: Exclude ConnectorClientConfigRequest from class loading isolation (#8630)

This fix excludes `ConnectorClientConfigRequest` and its inner class from class loading isolation in a similar way that KAFKA-8415 excluded `ConnectorClientConfigOverridePolicy`.

Reviewer: Konstantine Karantasis <konstantine@confluent.io>",32,257,107,437,2018,12,25,517,150,30,17,1,643,257,38,126,107,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java,"KAFKA-10079: improve thread-level stickiness (#8775)

Uses a similar (but slightly different) algorithm as in KAFKA-9987 to produce a maximally sticky -- and perfectly balanced -- assignment of tasks to threads within a single client. This is important for in-memory stores which get wiped out when transferred between threads.

Reviewers: John Roesler <vvcephei@apache.org>",54,1,1,351,2695,1,22,426,440,53,8,2.5,528,440,66,102,86,13,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ExtendedAssignment.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ExtendedAssignment.java,"KAFKA-9849: Fix issue with worker.unsync.backoff.ms creating zombie workers when incremental cooperative rebalancing is used (#8827)

When Incremental Cooperative Rebalancing is enabled and a worker fails to read to the end of the config topic, it needs to voluntarily revoke its locally running tasks on time, before these tasks get assigned to another worker, creating a situation where redundant tasks are running in the Connect cluster. 

Additionally, instead of using the delay `worker.unsync.backoff.ms` that was defined for the eager rebalancing protocol and has a long default value (which coincidentally is equal to the default value of the rebalance delay of the incremental cooperative protocol), the worker should quickly attempt to re-read the config topic and backoff for a fraction of the rebalance delay. After this fix, the worker will retry for a maximum time of 5 times before it revokes its running assignment and for a cumulative delay less than the configured `scheduled.rebalance.max.delay.ms`.

Unit tests are added to cover the backoff logic with incremental cooperative rebalancing. 

Reviewers: Randall Hauch <rhauch@gmail.com>",30,15,0,189,1514,1,14,284,269,142,2,1.5,284,269,142,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectIntegrationTestUtils.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectIntegrationTestUtils.java,"KAFKA-9849: Fix issue with worker.unsync.backoff.ms creating zombie workers when incremental cooperative rebalancing is used (#8827)

When Incremental Cooperative Rebalancing is enabled and a worker fails to read to the end of the config topic, it needs to voluntarily revoke its locally running tasks on time, before these tasks get assigned to another worker, creating a situation where redundant tasks are running in the Connect cluster. 

Additionally, instead of using the delay `worker.unsync.backoff.ms` that was defined for the eager rebalancing protocol and has a long default value (which coincidentally is equal to the default value of the rebalance delay of the incremental cooperative protocol), the worker should quickly attempt to re-read the config topic and backoff for a fraction of the rebalance delay. After this fix, the worker will retry for a maximum time of 5 times before it revokes its running assignment and for a cumulative delay less than the configured `scheduled.rebalance.max.delay.ms`.

Unit tests are added to cover the backoff logic with incremental cooperative rebalancing. 

Reviewers: Randall Hauch <rhauch@gmail.com>",1,1,0,21,130,0,1,43,42,22,2,1.0,43,42,22,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/SenderMetricsRegistry.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/SenderMetricsRegistry.java,"KAFKA-9716; Clarify meaning of compression rate metrics (#8664)

There is some confusion over the compression rate metrics, as the meaning of the value isn't clearly stated in the metric description. In this case, it was assumed that a higher compression rate value meant better compression. This PR clarifies the meaning of the value, to prevent misunderstandings.

Reviewers: Jason Gustafson <jason@confluent.io>",17,4,2,171,1156,1,17,223,125,37,6,5.0,346,188,58,123,116,20,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/BatchingStateRestoreCallback.java,streams/src/main/java/org/apache/kafka/streams/processor/BatchingStateRestoreCallback.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",1,4,0,10,79,1,1,45,41,22,2,1.0,45,41,22,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/StateRestoreListener.java,streams/src/main/java/org/apache/kafka/streams/processor/StateRestoreListener.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",0,6,3,15,83,0,0,87,84,29,3,1,92,84,31,5,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordBatchingStateRestoreCallback.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordBatchingStateRestoreCallback.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",1,0,5,12,114,1,1,32,46,8,4,1.0,54,46,14,22,16,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/BatchWritingStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/BatchWritingStore.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",0,1,2,9,76,0,0,27,28,14,2,1.0,29,28,14,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",2,0,7,18,75,1,2,38,171,1,30,3.5,770,171,26,732,476,24,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStoreTest.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",2,0,7,18,73,1,2,38,43,13,3,2,47,43,16,9,7,3,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockRestoreCallback.java,streams/src/test/java/org/apache/kafka/test/MockRestoreCallback.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",1,0,1,12,111,0,1,32,28,8,4,2.0,41,28,10,9,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockStateRestoreListener.java,streams/src/test/java/org/apache/kafka/test/MockStateRestoreListener.java,"KAFKA-10005: Decouple RestoreListener from RestoreCallback (#8676)

And remove bulk loading mechanism inside RocksDB.

Reviewers: John Roesler <vvcephei@apache.org>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,2,12,57,288,2,4,84,90,21,4,2.5,103,90,26,19,12,5,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedConnectClusterAssertions.java,"KAFKA-9851: Revoking Connect tasks due to connectivity issues should also clear the running assignment (#8804)

Until recently revocation of connectors and tasks was the result of a rebalance that contained a new assignment. Therefore the view of the running assignment was kept consistent outside the call to `RebalanceListener#onRevoke`. However, after KAFKA-9184 the need appeared for the worker to revoke tasks voluntarily and proactively without having received a new assignment. 

This commit will allow the worker to restart tasks that have been stopped as a result of voluntary revocation after a rebalance reassigns these tasks to the work. 

The fix is tested by extending an existing integration test.

Reviewers: Randall Hauch <rhauch@gmail.com>",50,2,2,294,2174,1,21,464,246,58,8,3.5,491,246,61,27,15,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/Metadata.java,clients/src/main/java/org/apache/kafka/clients/Metadata.java,"KAFKA-9840; Skip End Offset validation when the leader epoch is not reliable (#8486)

This PR provides two fixes:
1. Skip offset validation if the current leader epoch cannot be reliably determined.
2. Raise an out of range error if the leader returns an undefined offset in response to the OffsetsForLeaderEpoch request.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",84,4,2,369,2716,1,43,605,134,10,63,3,1314,150,21,709,221,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java,clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java,"KAFKA-9840; Skip End Offset validation when the leader epoch is not reliable (#8486)

This PR provides two fixes:
1. Skip offset validation if the current leader epoch cannot be reliably determined.
2. Raise an out of range error if the leader returns an undefined offset in response to the OffsetsForLeaderEpoch request.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",16,6,3,56,326,2,10,92,81,18,5,2,106,81,21,14,8,3,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/EpochEndOffsetTest.java,clients/src/test/java/org/apache/kafka/common/requests/EpochEndOffsetTest.java,"KAFKA-9840; Skip End Offset validation when the leader epoch is not reliable (#8486)

This PR provides two fixes:
1. Skip offset validation if the current leader epoch cannot be reliably determined.
2. Raise an out of range error if the leader returns an undefined offset in response to the OffsetsForLeaderEpoch request.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",3,64,0,39,333,3,3,64,64,64,1,1,64,64,64,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneConfigTest.java,"KAFKA-9570: Define SSL configs in all worker config classes, not just distributed (#8135)

Define SSL configs in all worker config classes, not just distributed

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Nigel Liang <nigel@nigelliang.com>, Randall Hauch <rhauch@gmail.com>",5,88,0,60,507,5,5,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/metrics/KafkaMetricsContext.java,clients/src/main/java/org/apache/kafka/common/metrics/KafkaMetricsContext.java,"KAFKA-10110: Corrected potential NPE when null label value added to KafkaMetricsContext (#8811)

Also added a new unit test to verify the functionality and expectations.

Author: Randall Hauch <rhauch@gmail.com>
Reviewer: Konstantine Karantasis <konstantine@confluent.io>",4,1,1,18,159,1,3,56,56,28,2,1.0,57,56,28,1,1,0,2,1,0,1
core/src/main/scala/kafka/controller/TopicDeletionManager.scala,core/src/main/scala/kafka/controller/TopicDeletionManager.scala,"KAFKA-10040; Make computing the PreferredReplicaImbalanceCount metric more efficient (#8724)

This PR changes the way `PreferredReplicaImbalanceCount` is computed. It moves from re-computing after the processing of each event in the controller, which requires a full pass over all partitions, to incrementally maintaining the count as assignments and leaders are changing.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",45,0,2,191,1282,1,21,358,373,10,37,4,960,373,26,602,153,16,2,1,0,1
core/src/test/scala/unit/kafka/controller/MockPartitionStateMachine.scala,core/src/test/scala/unit/kafka/controller/MockPartitionStateMachine.scala,"KAFKA-10040; Make computing the PreferredReplicaImbalanceCount metric more efficient (#8724)

This PR changes the way `PreferredReplicaImbalanceCount` is computed. It moves from re-computing after the processing of each event in the controller, which requires a full pass over all partitions, to incrementally maintaining the count as assignments and leaders are changing.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",19,2,2,96,653,1,4,131,110,22,6,2.0,177,110,30,46,22,8,2,1,0,1
tests/kafkatest/services/kafka/util.py,tests/kafkatest/services/kafka/util.py,"KAFKA-9320: Enable TLSv1.3 by default (KIP-573) (#8695)

1. Enables `TLSv1.3` by default with Java 11 or newer.
2. Add unit tests that cover the various TLSv1.2 and TLSv1.3 combinations.
3. Extend `benchmark_test.py` and `replication_test.py` to run with 'TLSv1.2'
or 'TLSv1.3'.

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,5,22,14,156,2,1,41,40,14,3,3,63,40,21,22,22,7,2,1,0,1
tests/kafkatest/services/kafka_log4j_appender.py,tests/kafkatest/services/kafka_log4j_appender.py,"KAFKA-9320: Enable TLSv1.3 by default (KIP-573) (#8695)

1. Enables `TLSv1.3` by default with Java 11 or newer.
2. Add unit tests that cover the various TLSv1.2 and TLSv1.3 combinations.
3. Extend `benchmark_test.py` and `replication_test.py` to run with 'TLSv1.2'
or 'TLSv1.3'.

Reviewers: Ismael Juma <ismael@juma.me.uk>",15,2,2,58,514,2,6,88,61,6,15,2,120,61,8,32,6,2,2,1,0,1
tests/kafkatest/services/log_compaction_tester.py,tests/kafkatest/services/log_compaction_tester.py,"KAFKA-9320: Enable TLSv1.3 by default (KIP-573) (#8695)

1. Enables `TLSv1.3` by default with Java 11 or newer.
2. Add unit tests that cover the various TLSv1.2 and TLSv1.3 combinations.
3. Extend `benchmark_test.py` and `replication_test.py` to run with 'TLSv1.2'
or 'TLSv1.3'.

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,3,2,57,483,2,7,89,88,44,2,1.5,91,88,46,2,2,1,2,1,0,1
tests/kafkatest/services/replica_verification_tool.py,tests/kafkatest/services/replica_verification_tool.py,"KAFKA-9320: Enable TLSv1.3 by default (KIP-573) (#8695)

1. Enables `TLSv1.3` by default with Java 11 or newer.
2. Add unit tests that cover the various TLSv1.2 and TLSv1.3 combinations.
3. Extend `benchmark_test.py` and `replication_test.py` to run with 'TLSv1.2'
or 'TLSv1.3'.

Reviewers: Ismael Juma <ismael@juma.me.uk>",9,4,2,56,484,2,7,95,81,12,8,3.0,111,81,14,16,4,2,2,1,0,1
tests/kafkatest/utils/remote_account.py,tests/kafkatest/utils/remote_account.py,"KAFKA-9320: Enable TLSv1.3 by default (KIP-573) (#8695)

1. Enables `TLSv1.3` by default with Java 11 or newer.
2. Add unit tests that cover the various TLSv1.2 and TLSv1.3 combinations.
3. Extend `benchmark_test.py` and `replication_test.py` to run with 'TLSv1.2'
or 'TLSv1.3'.

Reviewers: Ismael Juma <ismael@juma.me.uk>",12,19,0,30,232,2,5,58,32,14,4,1.0,59,32,15,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/CooperativeStickyAssignor.java,"KAFKA-9987: optimize sticky assignment algorithm for same-subscription case (#8668)

Motivation and pseudo code algorithm in the ticket.

Added a scale test with large number of topic partitions and consumers and 30s timeout.
With these changes, assignment with 2,000 consumers and 200 topics with 2,000 each completes within a few seconds.

Porting the same test to trunk, it took 2 minutes even with a 100x reduction in the number of topics (ie, 2 minutes for 2,000 consumers and 2 topics with 2,000 partitions)

Should be cherry-picked to 2.6, 2.5, and 2.4

Reviewers: Guozhang Wang <wangguoz@gmail.com>",13,25,21,62,527,4,6,109,105,54,2,5.0,130,105,65,21,21,10,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeConfigsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeConfigsOptions.java,"KAFKA-9494; Include additional metadata information in DescribeConfig response (KIP-569) (#8723)

Adds documentation and type of ConfigEntry in version 3 of DescribeConfigsResponse

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",5,15,0,26,131,2,5,75,37,9,8,2.0,98,37,12,23,18,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ConsumerGroupListing.java,clients/src/main/java/org/apache/kafka/clients/admin/ConsumerGroupListing.java,"KAFKA-9130; KIP-518 Allow listing consumer groups per state (#8238)

Implementation of KIP-518: https://cwiki.apache.org/confluence/display/KAFKA/KIP-518%3A+Allow+listing+consumer+groups+per+state. 

Reviewers: David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",18,56,0,61,341,6,8,115,59,58,2,4.0,115,59,58,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupsOptions.java,"KAFKA-9130; KIP-518 Allow listing consumer groups per state (#8238)

Implementation of KIP-518: https://cwiki.apache.org/confluence/display/KAFKA/KIP-518%3A+Allow+listing+consumer+groups+per+state. 

Reviewers: David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>

Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>",3,24,0,17,135,2,2,53,29,18,3,2,55,29,18,2,2,1,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorContextTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorContextTest.java,"KAFKA-9501: convert between active and standby without closing stores (#8248)

This PR has gone through several significant transitions of its own, but here's the latest:

* TaskManager just collects the tasks to transition and refers to the active/standby task creator to handle closing & recycling the old task and creating the new one. If we ever hit an exception during the close, we bail and close all the remaining tasks as dirty.

* The task creators tell the task to ""close but recycle state"". If this is successful, it tells the recycled processor context and state manager that they should transition to the new type.

* During ""close and recycle"" the task just does a normal clean close, but instead of closing the state manager it informs it to recycle itself: maintain all of its store information (most importantly the current store offsets) but unregister the changelogs from the changelog reader

* The new task will (re-)register its changelogs during initialization, but skip re-registering any stores. It will still read the checkpoint file, but only use the written offsets if the store offsets are not already initialized from pre-transition

* To ensure we don't end up with manual compaction disabled for standbys, we have to call the state restore listener's onRestoreEnd for any active restoring stores that are switching to standbys

Reviewers: John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",5,1,2,56,471,1,3,82,78,16,5,1,88,78,18,6,2,1,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/transforms/predicates/Predicate.java,connect/api/src/main/java/org/apache/kafka/connect/transforms/predicates/Predicate.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",0,53,0,10,86,0,0,53,53,53,1,1,53,53,53,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PredicatedTransformation.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PredicatedTransformation.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",7,81,0,50,328,6,6,81,81,81,1,1,81,81,81,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",12,8,0,76,553,2,10,106,55,15,7,6,117,55,17,11,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSinkConnector.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSinkConnector.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",16,1,1,119,911,1,12,161,115,27,6,1.5,169,115,28,8,4,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/TaskHandle.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/TaskHandle.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",29,13,3,127,777,4,19,256,111,64,4,9.0,283,111,71,27,12,7,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/PredicatedTransformationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/PredicatedTransformationTest.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",4,126,0,84,492,2,2,126,126,126,1,1,126,126,126,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Filter.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Filter.java,"KAFKA-9673: Filter and Conditional SMTs (#8699)

Implemented KIP-585 to support Filter and Conditional SMTs. Added unit tests and integration tests.

Author: Tom Bentley <tbentley@redhat.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",4,56,0,24,136,4,4,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java,connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",0,53,0,6,51,0,0,53,53,53,1,1,53,53,53,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",7,82,0,44,403,7,7,82,82,82,1,1,82,82,82,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",24,6,0,128,781,1,17,173,77,14,12,2.5,202,77,17,29,11,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",1,7,2,8,61,0,1,38,41,8,5,2,52,41,10,14,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",5,8,3,40,298,1,3,77,111,19,4,6.5,134,111,34,57,47,14,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",5,61,0,36,270,4,4,61,61,61,1,1,61,61,61,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporterTest.java,"KAFKA-9971: Error Reporting in Sink Connectors (KIP-610) (#8720)

Implementation for KIP-610: https://cwiki.apache.org/confluence/display/KAFKA/KIP-610%3A+Error+Reporting+in+Sink+Connectors based on which sink connectors can now report errors at the final stages of the stream that exports records to the sink system.
 
This PR adds the `ErrantRecordReporter` interface as well as its implementation - `WorkerErrantRecordReporter`. The `WorkerErrantRecordReporter` is created in `Worker` and brought up through `WorkerSinkTask` to `WorkerSinkTaskContext`. 

An integration test and unit tests have been added.

Reviewers: Lev Zemlyanov <lev@confluent.io>, Greg Harris <gregh@confluent.io>, Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",3,74,0,45,281,2,2,74,74,74,1,1,74,74,74,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java,"KAFKA-10052: Harden assertion of topic settings in Connect integration tests (#8735)

A recently added assertion in Connect integration tests uses `KafkaConsumer#partitionsFor` to verify that a topic was created with the expected number of partitions and replicas. However, probably because of metadata propagation delays, this call doesn't always return a valid `PartitionInfo` for the topic that has just been created and the test is terminated with a NPE. 

This commit changes the assertion to perform retries in order to verify the topic settings and uses the admin client to describe the topics instead.

Tests have been adjusted to use the new assertion. 

Reviewers: Randall Hauch <rhauch@gmail.com>",7,12,6,162,1617,3,7,239,233,120,2,3.5,245,233,122,6,6,3,1,0,1,1
tests/kafkatest/services/transactional_message_copier.py,tests/kafkatest/services/transactional_message_copier.py,"KAFKA-9802; Increase transaction timeout in system tests to reduce flakiness (#8736)

We have been seeing increased flakiness in transaction system tests. I believe the cause might be due to KIP-537, which increased the default zk session timeout from 6s to 18s and the default replica lag timeout from 10s to 30s. In the system test, we use the default transaction timeout of 10s. However, since the system test involves hard failures, the Produce request could be blocking for as long as the max of these two in order to wait for an ISR shrink. Hence this patch increases the timeout to 30s.

Note this patch also includes a minor logging fix in `Partition`. Previously we would see messages like the following:
```
[Broker id=3] Leader output-topic-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 3,2,1 addingReplicas  removingReplicas .Previous leader epoch was -1.
```
This patch fixes the log to print as the following:
```
[Broker id=3] Leader output-topic-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [3,2,1] addingReplicas []  removingReplicas []. Previous leader epoch was -1.
```

Reviewers: Bob Barrett <bob.barrett@confluent.io>, Ismael Juma <github@juma.me.uk>",30,2,1,158,1147,1,12,204,183,26,8,1.0,214,183,27,10,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupOptions.java,"KAFKA-9146: Add option to force delete active members in StreamsResetter (#8589)

Implements KIP-571

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,12,0,25,156,3,4,55,49,18,3,4,72,49,24,17,17,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupResult.java,clients/src/main/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupResult.java,"KAFKA-9146: Add option to force delete active members in StreamsResetter (#8589)

Implements KIP-571

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",16,23,3,77,524,3,5,116,96,58,2,2.5,119,96,60,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/MetricsContext.java,clients/src/main/java/org/apache/kafka/common/metrics/MetricsContext.java,"KAFKA-9960: implement KIP-606 to add metadata context to MetricsReporter (#8691)

Implemented KIP-606 to add metadata context to MetricsReporter.

Author: Xiaodong Du <xdu@confluent.io>
Reviewers: David Arthur <mumrah@gmail.com>, Randall Hauch <rhauch@gmail.com>, Xavier Léauté <xavier@confluent.io>, Ryan Pridgeon <ryan.n.pridgeon@gmail.com>",0,49,0,8,55,0,0,49,49,49,1,1,49,49,49,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/metrics/MetricsReporter.java,clients/src/main/java/org/apache/kafka/common/metrics/MetricsReporter.java,"KAFKA-9960: implement KIP-606 to add metadata context to MetricsReporter (#8691)

Implemented KIP-606 to add metadata context to MetricsReporter.

Author: Xiaodong Du <xdu@confluent.io>
Reviewers: David Arthur <mumrah@gmail.com>, Randall Hauch <rhauch@gmail.com>, Xavier Léauté <xavier@confluent.io>, Ryan Pridgeon <ryan.n.pridgeon@gmail.com>",4,9,0,24,174,1,4,77,27,7,11,1,112,27,10,35,16,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMemberTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMemberTest.java,"KAFKA-9960: implement KIP-606 to add metadata context to MetricsReporter (#8691)

Implemented KIP-606 to add metadata context to MetricsReporter.

Author: Xiaodong Du <xdu@confluent.io>
Reviewers: David Arthur <mumrah@gmail.com>, Randall Hauch <rhauch@gmail.com>, Xavier Léauté <xavier@confluent.io>, Ryan Pridgeon <ryan.n.pridgeon@gmail.com>",4,103,0,74,708,2,2,103,103,103,1,1,103,103,103,0,0,0,0,0,0,0
core/src/main/scala/kafka/cluster/Replica.scala,core/src/main/scala/kafka/cluster/Replica.scala,"KAFKA-9952; Remove immediate fetch completion logic on high watermark updates (#8709)

For KIP-392, we added logic to make sure that high watermark changes are propagated to followers without delay in order to improve end to end latency when fetching from followers. The downside of this change is that it can increase the rate of fetch requests from followers which can have a noticeable impact on performance (see KAFKA-9731). 

To fix that problem, we have previously modified the code so that we only propagate high watermark changes immediately when a replica selector is used (which is not the default). However, leaving this logic around means that it is risky to enable follower fetching since it changes the follower request rate, which can have a big impact on overall broker performance. 

This patch disables immediate propagation of the high watermark more generally. Instead, users can use the max wait time in order to control the worst-case latency. Note that this is typically only a problem anyway for low-throughput clusters since otherwise we will always have a steady rate of high watermark updates.

Reviewers: Ismael Juma <ismael@juma.me.uk>",8,4,25,54,372,3,3,108,117,2,44,3.0,604,119,14,496,163,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskSuite.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskSuite.java,"KAFKA-9983: KIP-613: add INFO level e2e latency metrics (#8697)

Add e2e latency metrics at the beginning and end of task topologies
as INFO-level processor-node-level metrics.

Implements: KIP-613
Reviewers: John Roesler <vvcephei@apache.org>, Andrew Choi <a24choi@edu.uwaterloo.ca>, Bruno Cadonna <cadonna@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,1,1,19,130,0,0,45,38,15,3,1,51,38,17,6,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovement.java,"KAFKA-6145: KIP-441: Fix assignor config passthough (#8716)

Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>",26,10,7,174,1245,4,10,238,101,30,8,8.0,420,101,52,182,97,23,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfigurationTest.java,"KAFKA-6145: KIP-441: Fix assignor config passthough (#8716)

Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>",1,37,0,16,121,1,1,37,37,37,1,1,37,37,37,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignorTest.java,"KAFKA-6145: KIP-441: Fix assignor config passthough (#8716)

Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>",3,1,1,48,486,1,3,74,74,25,3,1,76,74,25,2,1,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java,"KAFKA-6145: KIP-441: Fix assignor config passthough (#8716)

Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>",40,24,27,699,7244,16,34,834,796,76,11,5,1578,796,143,744,263,68,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskMovementTest.java,"KAFKA-6145: KIP-441: Fix assignor config passthough (#8716)

Also fixes a system test by configuring the HATA to perform a one-shot balanced assignment

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>",8,10,2,173,1575,1,6,224,141,32,7,20,694,164,99,470,218,67,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransform.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransform.java,"KAFKA-7523: Add ConnectedStoreProvider to Processor API (#6824)

Implements KIP-401:
 - Add ConnectedStoreProvider interface
 - let Processor/[*]Transformer[*]Suppliers extend ConnectedStoreProvider
 - allows to add and connect state stores to processors/transformers implicitly

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",9,8,0,48,444,1,7,77,71,5,15,2,131,71,9,54,13,4,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TransformerSupplierAdapter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TransformerSupplierAdapter.java,"KAFKA-7523: Add ConnectedStoreProvider to Processor API (#6824)

Implements KIP-401:
 - Add ConnectedStoreProvider interface
 - let Processor/[*]Transformer[*]Suppliers extend ConnectedStoreProvider
 - allows to add and connect state stores to processors/transformers implicitly

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",4,7,0,40,352,1,3,67,60,17,4,1.0,69,60,17,2,1,0,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamTransformIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamTransformIntegrationTest.java,"KAFKA-7523: Add ConnectedStoreProvider to Processor API (#6824)

Implements KIP-401:
 - Add ConnectedStoreProvider interface
 - let Processor/[*]Transformer[*]Suppliers extend ConnectedStoreProvider
 - allows to add and connect state stores to processors/transformers implicitly

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",36,335,121,478,3741,33,33,562,214,140,4,12.0,725,335,181,163,121,41,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/TransformerSupplierAdapterTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/TransformerSupplierAdapterTest.java,"KAFKA-7523: Add ConnectedStoreProvider to Processor API (#6824)

Implements KIP-401:
 - Add ConnectedStoreProvider interface
 - let Processor/[*]Transformer[*]Suppliers extend ConnectedStoreProvider
 - allows to add and connect state stores to processors/transformers implicitly

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>",7,15,0,114,1133,2,7,155,140,52,3,1,156,140,52,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java,"KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158) (#8722)

Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667 for details). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized. 

With the implementation of KIP-158 here, if `topic.creation.enable=true`, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations. 

This feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",8,142,0,112,815,4,4,142,142,142,1,1,142,142,142,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TransformationChain.java,"KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158) (#8722)

Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667 for details). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized. 

With the implementation of KIP-158 here, if `topic.creation.enable=true`, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations. 

This feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",14,2,1,54,438,0,6,85,69,17,5,4,102,69,20,17,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicCreation.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicCreation.java,"KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158) (#8722)

Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667 for details). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized. 

With the implementation of KIP-158 here, if `topic.creation.enable=true`, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations. 

This feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",13,148,0,63,423,9,9,148,148,148,1,1,148,148,148,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicCreationGroup.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicCreationGroup.java,"KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158) (#8722)

Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667 for details). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized. 

With the implementation of KIP-158 here, if `topic.creation.enable=true`, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations. 

This feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",18,151,0,87,644,8,8,151,151,151,1,1,151,151,151,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SourceConnectorConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SourceConnectorConfigTest.java,"KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158) (#8722)

Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667 for details). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized. 

With the implementation of KIP-158 here, if `topic.creation.enable=true`, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations. 

This feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",10,150,0,112,1183,8,8,150,150,150,1,1,150,150,150,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TaskStatus.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TaskStatus.java,"KAFKA-9472: Remove deleted Connect tasks from status store (#8118)

Although the statuses for tasks are removed from the status store when their connector is deleted, their statuses are not removed when only the task is deleted, which happens in the case that the number of tasks for a connector is reduced.

This commit adds logic for deleting the statuses for those tasks from the status store whenever a rebalance has completed and the leader of a distributed cluster has detected that there are recently-deleted tasks. Standalone is also updated to accomplish this.

Unit tests for the `DistributedHerder` and `StandaloneHerder` classes are updated and an integration test has been added.

Reviewers: Nigel Liang <nigel@nigelliang.com>, Konstantine Karantasis <konstantine@confluent.io>",2,7,0,18,147,0,2,72,53,18,4,1.0,79,53,20,7,7,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/connector/Connector.java,connect/api/src/main/java/org/apache/kafka/connect/connector/Connector.java,"KAFKA-4794: Add access to OffsetStorageReader from SourceConnector (#2604)

Added access to OffsetStorageReader from SourceConnector per KIP-131. 

Added two interfaces SinkConnectorContext/SourceConnectContext that extend ConnectorContext in order to expose an OffsetStorageReader instance.

Added unit tests for Connector, SinkConnector and SourceConnector default methods

Author: Florian Hussonnois <florian.hussonnois@gmail.com>, Randall Hauch <rhauch@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",6,9,0,39,300,1,5,153,117,13,12,2.0,186,117,16,33,8,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/sink/SinkConnector.java,connect/api/src/main/java/org/apache/kafka/connect/sink/SinkConnector.java,"KAFKA-4794: Add access to OffsetStorageReader from SourceConnector (#2604)

Added access to OffsetStorageReader from SourceConnector per KIP-131. 

Added two interfaces SinkConnectorContext/SourceConnectContext that extend ConnectorContext in order to expose an OffsetStorageReader instance.

Added unit tests for Connector, SinkConnector and SourceConnector default methods

Author: Florian Hussonnois <florian.hussonnois@gmail.com>, Randall Hauch <rhauch@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",1,5,0,9,55,1,1,42,40,8,5,2,52,40,10,10,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/sink/SinkConnectorContext.java,connect/api/src/main/java/org/apache/kafka/connect/sink/SinkConnectorContext.java,"KAFKA-4794: Add access to OffsetStorageReader from SourceConnector (#2604)

Added access to OffsetStorageReader from SourceConnector per KIP-131. 

Added two interfaces SinkConnectorContext/SourceConnectContext that extend ConnectorContext in order to expose an OffsetStorageReader instance.

Added unit tests for Connector, SinkConnector and SourceConnector default methods

Author: Florian Hussonnois <florian.hussonnois@gmail.com>, Randall Hauch <rhauch@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",0,25,0,4,31,0,0,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/source/SourceConnector.java,connect/api/src/main/java/org/apache/kafka/connect/source/SourceConnector.java,"KAFKA-4794: Add access to OffsetStorageReader from SourceConnector (#2604)

Added access to OffsetStorageReader from SourceConnector per KIP-131. 

Added two interfaces SinkConnectorContext/SourceConnectContext that extend ConnectorContext in order to expose an OffsetStorageReader instance.

Added unit tests for Connector, SinkConnector and SourceConnector default methods

Author: Florian Hussonnois <florian.hussonnois@gmail.com>, Randall Hauch <rhauch@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",1,4,0,8,47,1,1,31,29,6,5,2,39,29,8,8,4,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/source/SourceConnectorContext.java,connect/api/src/main/java/org/apache/kafka/connect/source/SourceConnectorContext.java,"KAFKA-4794: Add access to OffsetStorageReader from SourceConnector (#2604)

Added access to OffsetStorageReader from SourceConnector per KIP-131. 

Added two interfaces SinkConnectorContext/SourceConnectContext that extend ConnectorContext in order to expose an OffsetStorageReader instance.

Added unit tests for Connector, SinkConnector and SourceConnector default methods

Author: Florian Hussonnois <florian.hussonnois@gmail.com>, Randall Hauch <rhauch@gmail.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",0,32,0,6,49,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TransformationConfigTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TransformationConfigTest.java,"KAFKA-6755: Allow literal value for MaskField SMT (#6284)

Implemented KIP-437 by adding a new optional configuration property for the `MaskField` transformation that allows users to define a replacement literal for specific fields in matching records.

Author: Valeria Vasylieva <valeria.vasylieva@gmail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",12,1,1,148,1333,1,12,211,211,106,2,1.0,212,211,106,1,1,0,1,0,1,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/MaskField.java,"KAFKA-6755: Allow literal value for MaskField SMT (#6284)

Implemented KIP-437 by adding a new optional configuration property for the `MaskField` transformation that allows users to define a replacement literal for specific fields in matching records.

Author: Valeria Vasylieva <valeria.vasylieva@gmail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",27,43,5,162,1493,4,15,209,172,52,4,2.5,224,172,56,15,8,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImpl.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterStateImpl.java,"KAFKA-9888: Copy connector configs before passing to REST extensions (#8511)

The changes made in KIP-454 involved adding a `connectorConfig` method to the ConnectClusterState interface that REST extensions could use to query the worker for the configuration of a given connector. The implementation for this method returns the Java `Map` that's stored in the worker's view of the config topic (when running in distributed mode). No copying is performed, which causes mutations of that `Map` to persist across invocations of `connectorConfig` and, even worse, propagate to the worker when, e.g., starting a connector.

In this commit the map is copied before it's returned to REST extensions.

An existing unit test is modified to ensure that REST extensions receive a copy of the connector config, not the original.

Reviewers: Nigel Liang <nigel@nigelliang.com>, Konstantine Karantasis <konstantine@confluent.io>",9,1,1,86,622,1,6,115,86,16,7,5,144,86,21,29,13,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/RepartitionedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/RepartitionedInternal.java,"KAFKA-10003: Mark KStream.through() as deprecated and update Scala API (#8679)

 - part of KIP-221

Co-authored-by: John Roesler <john@confluent.io>",7,7,7,28,179,6,7,53,53,26,2,4.0,60,53,30,7,7,4,1,0,1,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Repartitioned.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Repartitioned.scala,"KAFKA-10003: Mark KStream.through() as deprecated and update Scala API (#8679)

 - part of KIP-221

Co-authored-by: John Roesler <john@confluent.io>",0,86,0,15,272,0,0,86,86,86,1,1,86,86,86,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/source/SourceTask.java,connect/api/src/main/java/org/apache/kafka/connect/source/SourceTask.java,"KAFKA-9780: Deprecate commit records without record metadata (#8379)

Author: Mario Molina <mmolimar@gmail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",4,3,2,25,156,1,4,140,62,9,15,2,164,62,11,24,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSourceTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSourceTask.java,"KAFKA-9780: Deprecate commit records without record metadata (#8379)

Author: Mario Molina <mmolimar@gmail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",10,2,1,103,825,2,5,146,128,21,7,2,156,128,22,10,5,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/Schema.java,clients/src/main/java/org/apache/kafka/common/protocol/types/Schema.java,"KAFKA-9855 - return cached Structs for Schemas with no fields (#8472)

At the time of this writing there are 6 schemas in kafka APIs with no fields - 3
versions each of LIST_GROUPS and API_VERSIONS.

When reading instances of these schemas off the wire there's little point in
returning a unique Struct object (or a unique values array inside that Struct)
since there is no payload.

Reviewers: Ismael Juma <ismael@juma.me.uk>",39,10,2,143,1097,2,15,235,134,14,17,2,322,134,19,87,23,5,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorTaskConfig.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorTaskConfig.java,"KAFKA-9950: Construct new ConfigDef for MirrorTaskConfig before defining new properties (#8608)

`MirrorTaskConfig` class mutates the `ConfigDef` by defining additional properties, which leads to a potential `ConcurrentModificationException` during worker configuration validation and unintended inclusion of those new properties in the `ConfigDef` for the connectors which in turn is then visible via the REST API's `/connectors/{name}/config/validate` endpoint.

The fix here is a one-liner that just creates a copy of the `ConfigDef` before defining new properties.

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>",8,1,1,50,319,0,4,75,75,38,2,1.0,76,75,38,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java,"KAFKA-9409: Supplement immutability of ClusterConfigState class in Connect (#7942)

The class claims to be immutable, but there are some mutable features of this class.
Increase the immutability of it and add a little cleanup:

* Pre-initialize size of ArrayList
* Remove superfluous syntax
* Use ArrayList instead of LinkedList since the list is created once

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>",38,12,11,172,1180,2,18,283,122,22,13,6,334,122,26,51,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/TopicConfig.java,clients/src/main/java/org/apache/kafka/common/config/TopicConfig.java,"MINOR: Fix redundant typos in comments and javadocs (#8693)

* MINOR: Fix typo in RecordAccumulator
* MINOR: Fix typo in several files

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>",0,1,1,133,594,0,0,184,163,11,17,1,219,163,13,35,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedKeyValueBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedKeyValueBytesStore.java,"MINOR: consolidate processor context for active/standby (#8669)

This is a prerequisite for KAFKA-9501 and will also be useful for KAFKA-9603

There should be no logical changes here: the main difference is the removal of StandbyContextImpl in preparation for contexts to transition between active and standby.

Also includes some minor cleanup, eg pulling the ReadOnly/ReadWrite decorators out into a separate file.

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>",3,2,2,19,170,1,2,40,39,13,3,1,42,39,14,2,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySet.java,"KAFKA-6145: KIP-441: Improve assignment balance (#8588)

Validate that the assignment is always balanced wrt:
* active assignment balance
* stateful assignment balance
* task-parallel balance

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",11,30,49,55,393,8,6,93,112,46,2,6.5,142,112,71,49,49,24,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java,"KAFKA-6145: KIP-441: Improve assignment balance (#8588)

Validate that the assignment is always balanced wrt:
* active assignment balance
* stateful assignment balance
* task-parallel balance

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",2,2,2,20,145,2,2,49,49,24,2,1.5,51,49,26,2,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignor.java,"KAFKA-6145: KIP-441: Improve assignment balance (#8588)

Validate that the assignment is always balanced wrt:
* active assignment balance
* stateful assignment balance
* task-parallel balance

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",70,2,2,265,1896,2,24,337,283,19,18,6.5,503,283,28,166,25,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignor.java,"KAFKA-6145: KIP-441: Improve assignment balance (#8588)

Validate that the assignment is always balanced wrt:
* active assignment balance
* stateful assignment balance
* task-parallel balance

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",0,1,1,11,83,0,0,33,195,3,12,2.0,274,195,23,241,203,20,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySetTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ConstrainedPrioritySetTest.java,"KAFKA-6145: KIP-441: Improve assignment balance (#8588)

Validate that the assignment is always balanced wrt:
* active assignment balance
* stateful assignment balance
* task-parallel balance

Reviewers: Bruno Cadonna <bruno@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",8,111,0,73,712,6,6,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamRepartitionTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamRepartitionTest.java,"KAFKA-9850 Move KStream#repartition operator validation during Topolo… (#8550)

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,56,0,124,1198,2,5,166,110,83,2,4.5,166,110,83,0,0,0,1,0,0,0
core/src/main/scala/kafka/admin/DeleteRecordsCommand.scala,core/src/main/scala/kafka/admin/DeleteRecordsCommand.scala,"MINOR: Use `forEach` and `ifPresent` to simplify Scala code (#8642)

* Use `forEach` instead of `asScala.foreach` for Java Iterables.
* Use `ifPresent` instead of `asScala.foreach` for Java Optionals.
* Use `forEach` instead of `entrySet.forEach` for Java maps.
* Keep `asScala.foreach` for `Properties` as the Scala implementation
has a better interface (keys and values are of type `String`).
* A few clean-ups: unnecessary `()`, `{}`, `new`, etc.

Reviewers: Manikumar Reddy <manikumar@confluent.io>",19,2,2,97,763,1,5,137,117,11,12,1.0,191,117,16,54,15,4,2,1,0,1
core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala,core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala,"MINOR: Use `forEach` and `ifPresent` to simplify Scala code (#8642)

* Use `forEach` instead of `asScala.foreach` for Java Iterables.
* Use `ifPresent` instead of `asScala.foreach` for Java Optionals.
* Use `forEach` instead of `entrySet.forEach` for Java maps.
* Keep `asScala.foreach` for `Properties` as the Scala implementation
has a better interface (keys and values are of type `String`).
* A few clean-ups: unnecessary `()`, `{}`, `new`, etc.

Reviewers: Manikumar Reddy <manikumar@confluent.io>",21,4,5,121,1138,3,16,175,284,3,55,3,905,284,16,730,316,13,2,1,0,1
core/src/main/scala/kafka/tools/EndToEndLatency.scala,core/src/main/scala/kafka/tools/EndToEndLatency.scala,"MINOR: Use `forEach` and `ifPresent` to simplify Scala code (#8642)

* Use `forEach` instead of `asScala.foreach` for Java Iterables.
* Use `ifPresent` instead of `asScala.foreach` for Java Optionals.
* Use `forEach` instead of `entrySet.forEach` for Java maps.
* Keep `asScala.foreach` for `Properties` as the Scala implementation
has a better interface (keys and values are of type `String`).
* A few clean-ups: unnecessary `()`, `{}`, `new`, etc.

Reviewers: Manikumar Reddy <manikumar@confluent.io>",12,1,1,117,1071,1,4,179,72,5,34,2.0,328,96,10,149,41,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/GlobalKTable.java,streams/src/main/java/org/apache/kafka/streams/kstream/GlobalKTable.java,"KAFKA-9290: Update IQ related JavaDocs (#8114)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,1,1,8,72,0,0,74,59,6,12,1.0,120,59,10,46,21,4,2,1,0,1
connect/json/src/main/java/org/apache/kafka/connect/json/JsonDeserializer.java,connect/json/src/main/java/org/apache/kafka/connect/json/JsonDeserializer.java,"KAFKA-9667: Connect JSON serde strip trailing zeros (#8230)

This change turns on exact decimal processing in JSON Converter for deserializing decimals, meaning trailing zeros are maintained. Serialization was already using the decimal scale to output the right value, so this change means a value of `1.2300` can now be serialized to JSON and deserialized back to Connect without any loss of information.

Author: Andy Coates <big-andy-coates@users.noreply.github.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Almog Gavra <almog@confluent.io>",5,9,3,34,227,3,3,71,87,10,7,4,121,87,17,50,28,7,2,1,0,1
connect/json/src/main/java/org/apache/kafka/connect/json/JsonSerializer.java,connect/json/src/main/java/org/apache/kafka/connect/json/JsonSerializer.java,"KAFKA-9667: Connect JSON serde strip trailing zeros (#8230)

This change turns on exact decimal processing in JSON Converter for deserializing decimals, meaning trailing zeros are maintained. Serialization was already using the decimal scale to output the right value, so this change means a value of `1.2300` can now be serialized to JSON and deserialized back to Connect without any loss of information.

Author: Andy Coates <big-andy-coates@users.noreply.github.com>
Reviewers: Randall Hauch <rhauch@gmail.com>, Almog Gavra <almog@confluent.io>",5,20,0,32,222,2,3,69,72,10,7,3,104,72,15,35,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/CircularIterator.java,clients/src/main/java/org/apache/kafka/common/utils/CircularIterator.java,"KAFKA-9419: Fix possible integer overflow in CircularIterator (#7950)

The CircularIterator class uses a wrapping index-based approach to iterate over a list. This can be a performance problem O(n^2) for a LinkedList. Also, the index counter itself is never reset, a modulo is applied to it for every list access. At some point, it may be possible that the index counter overflows to a negative value and therefore may cause a negative index read and an ArrayIndexOutOfBoundsException.

This fix changes the implementation to avoid these two scenarios. Uses the Collection Iterator classes to avoid using an index counter and it avoids having to seek to the correct index every time, this avoiding the LinkedList performance issue.

I have added unit tests to validate the new implementation.

* KAFKA-9419: Integer Overflow Possible with CircularIterator
* Added JavaDoc. Support null values in the underlying collection
* Always return true for hasNext(). Add more JavaDoc
* Use an advance method to load next value and always return true in hasNext()
* Simplify test suite
* Use assertThrows in tests and remove redundant 'this' identifier

Co-authored-by: David Mollitor <dmollitor@apache.org>
Co-authored-by: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>",8,62,9,41,223,5,6,106,54,35,3,4,120,62,40,14,9,5,2,1,0,1
core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala,core/src/test/scala/unit/kafka/utils/timer/MockTimer.scala,"KAFKA-9731: Disable immediate fetch response for hw propagation if replica selector is not defined (#8607)

In the case described in the JIRA, there was a 50%+ increase in the total fetch request rate in
2.4.0 due to this change.

I included a few additional clean-ups:
* Simplify `findPreferredReadReplica` and avoid unnecessary collection copies.
* Use `LongSupplier` instead of `Supplier<Long>` in `SubscriptionState` to avoid unnecessary boxing.

Added a unit test to ReplicaManagerTest and cleaned up the test class a bit including
consistent usage of Time in MockTimer and other components.

Reviewers: Gwen Shapira <gwen@confluent.io>, David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>",8,1,2,42,235,0,3,69,57,14,5,1,82,57,16,13,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/SimpleRate.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/SimpleRate.java,"MINOR: Use min/max function when possible (#8577)

Reviewers: Jason Gustafson <jason@confluent.io>",1,1,1,10,84,1,1,39,39,13,3,1,46,39,15,7,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecordingTrigger.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecordingTrigger.java,"MINOR: Improve Sensor recording efficiency (#8593)

1. Added a recordInternal function to let all other public functions trigger, so that shouldRecord would only be checked once.

2. In Streams, pass along the current wall-clock time inside InternalProcessorContext when process / punctuate which can be passed in to the record function to reduce the calling frequency of SystemTime.milliseconds().

Reviewers: John Roesler <vvcephei@apache.org>",8,9,1,39,284,2,5,64,56,32,2,2.5,65,56,32,1,1,0,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecordingTriggerTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecordingTriggerTest.java,"MINOR: Improve Sensor recording efficiency (#8593)

1. Added a recordInternal function to let all other public functions trigger, so that shouldRecord would only be checked once.

2. In Streams, pass along the current wall-clock time inside InternalProcessorContext when process / punctuate which can be passed in to the record function to reduce the calling frequency of SystemTime.milliseconds().

Reviewers: John Roesler <vvcephei@apache.org>",4,6,3,61,444,1,4,90,87,45,2,2.5,93,87,46,3,3,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/Transformer.java,streams/src/main/java/org/apache/kafka/streams/kstream/Transformer.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,1,1,13,131,0,0,99,57,4,22,2.0,184,62,8,85,21,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformer.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformer.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,1,1,14,138,0,0,99,56,5,20,2.0,181,68,9,82,22,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerWithKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueTransformerWithKey.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,1,1,14,144,0,0,101,101,17,6,1.0,115,101,19,14,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionTupleForwarder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionTupleForwarder.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",4,2,2,28,248,2,2,56,51,4,14,2.5,133,51,10,77,28,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionJoinForeignProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionJoinForeignProcessorSupplier.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",10,2,3,77,661,1,2,124,124,41,3,3,129,124,43,5,3,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/Processor.java,streams/src/main/java/org/apache/kafka/streams/processor/Processor.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",0,1,1,7,50,0,0,58,59,4,15,2,93,59,6,35,11,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyContextImpl.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",19,8,8,103,593,12,19,190,164,4,44,3.0,514,164,12,324,91,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformTest.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",8,1,1,91,664,0,7,137,142,34,4,1.0,146,142,36,9,7,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformValuesTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamFlatTransformValuesTest.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",8,1,1,89,621,0,7,135,135,45,3,1,137,135,46,2,1,1,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionTupleForwarderTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionTupleForwarderTest.java,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",6,2,2,63,578,2,5,97,80,19,5,2,119,80,24,22,13,4,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/FunctionsCompatConversions.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/FunctionsCompatConversions.scala,"KAFKA-8410: Revert Part 1: processor context bounds (#8414) (#8595)

This reverts commit 29e08fd2c2d3349ba5cbd8fe5a9d35a0cea02b85.
There turned out to be more than expected problems with adding the generic parameters.

Reviewers: Matthias J. Sax <matthias@confluent.io>",9,3,3,89,1207,5,9,128,117,16,8,2.5,223,117,28,95,76,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/BalancedAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/BalancedAssignor.java,"KAFKA-6145: KIP 441 remove balance factor (#8597)

Reviewers: John Roesler <vvcephei@apache.org>",0,1,2,11,96,0,0,30,31,10,3,1,39,31,13,9,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignor.java,"KAFKA-6145: KIP 441 remove balance factor (#8597)

Reviewers: John Roesler <vvcephei@apache.org>",11,4,6,64,486,4,3,86,88,29,3,4,108,88,36,22,16,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultBalancedAssignorTest.java,"KAFKA-6145: KIP 441 remove balance factor (#8597)

Reviewers: John Roesler <vvcephei@apache.org>",12,8,61,255,1639,9,12,295,348,98,3,17,492,348,164,197,136,66,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/QuotaViolationException.java,clients/src/main/java/org/apache/kafka/common/metrics/QuotaViolationException.java,"KAFKA-9652: Fix throttle metric in RequestChannel and request log due to KIP-219 (#8567)

After KIP-219, responses are sent immediately and we rely on a combination
of clients and muting of the channel to throttle. The result of this is that
we need to track `apiThrottleTimeMs` as an explicit value instead of
inferring it. On the other hand,  we no longer need
`apiRemoteCompleteTimeNanos`.

Extend `BaseQuotaTest` to verify that throttle time in the request channel
metrics are being set. Given the nature of the throttling numbers, the test
is not particularly precise.

I included a few clean-ups:
* Pass KafkaMetric to QuotaViolationException so that the caller doesn't
have to retrieve it from the metrics registry.
* Inline Supplier in SocketServer (use SAM).
* Reduce redundant `time.milliseconds` and `time.nanoseconds`calls.
* Use monotonic clock in ThrottledChannel and simplify `compareTo` method.
* Simplify `TimerTaskList.compareTo`.
* Consolidate the number of places where we update `apiLocalCompleteTimeNanos`
and `responseCompleteTimeNanos`.
* Added `toString` to ByteBufferSend` and `MultiRecordsSend`.
* Restrict access to methods in `QuotaTestClients` to expose only what we need
to.

Reviewers: Jun Rao <junrao@gmail.com>",6,6,7,36,160,5,6,65,23,8,8,2.0,85,25,11,20,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySchema.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySchema.java,"KAFKA-9925: decorate pseudo-topics with app id (#8574)

Reviewers: Boyang Chen <boyang@confluent.io>, Kin Siu",13,11,6,68,691,3,5,108,96,27,4,3.5,126,96,32,18,8,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java,"KAFKA-9925: decorate pseudo-topics with app id (#8574)

Reviewers: Boyang Chen <boyang@confluent.io>, Kin Siu",10,7,3,70,620,3,2,113,107,28,4,5.5,126,107,32,13,7,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/OperatingSystem.java,clients/src/main/java/org/apache/kafka/common/utils/OperatingSystem.java,"KAFKA-9704: Fix the issue z/OS won't let us resize file when mmap. (#8224)


Reviewers: Mickael Maison <mickael.maison@gmail.com>",1,3,0,14,85,0,1,37,23,7,5,2,50,23,10,13,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashMultiCollection.java,clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashMultiCollection.java,"MINOR: equals() should compare all fields for generated classes (#8539)

Reviewers: Jason Gustafson <jason@confluent.io>",20,8,8,74,461,2,6,142,142,36,4,3.0,158,142,40,16,8,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/ForeignKeyJoinSuite.java,streams/src/test/java/org/apache/kafka/streams/integration/ForeignKeyJoinSuite.java,"KAFKA-9388: Refactor integration tests to always use different application ids (#8530)

When debugging KAFKA-9388, I found the reason that the second test method test takes much longer (10s) than the previous one (~500ms) is because they used the same app.id. When the previous clients are shutdown, they would not send leave-group and hence we are still depending on the session timeout (10s) for the members to be removed out of the group.

When the second test is triggered, they will join the same group because of the same application id, and the prepare-rebalance phase would would for the full rebalance timeout before it kicks out the previous members.

Setting different application ids could resolve such issues for integration tests --- I did a quick search and found some other integration tests have the same issue. And after this PR my local unit test runtime reduced from about 14min to 7min.

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, John Roesler <john@confluent.io>",0,2,1,23,180,0,0,52,47,10,5,1,54,47,11,2,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentUtils.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentUtils.java,"KAFKA-6145: KIP-441: Build state constrained assignment from balanced one (#8497)

Implements: KIP-441
Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",3,16,10,15,119,2,2,41,66,8,5,2,92,66,18,51,34,10,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentUtilsTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssignmentUtilsTest.java,"KAFKA-6145: KIP-441: Build state constrained assignment from balanced one (#8497)

Implements: KIP-441
Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",3,57,0,33,321,3,3,57,57,57,1,1,57,57,57,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueueTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/ValidClientsByTaskLoadQueueTest.java,"KAFKA-6145: KIP-441: Build state constrained assignment from balanced one (#8497)

Implements: KIP-441
Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",6,126,0,83,866,6,6,126,126,126,1,1,126,126,126,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/StaticTopicNameExtractor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StaticTopicNameExtractor.java,"KAFKA-7885: TopologyDescription violates equals-hashCode contract. (#6210)

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>",12,19,0,32,212,2,5,60,41,30,2,1.5,60,41,30,0,0,0,1,0,0,0
tests/kafkatest/tests/streams/streams_eos_test.py,tests/kafkatest/tests/streams/streams_eos_test.py,"KAFKA-9832: extend Kafka Streams EOS system test (#8440)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",16,25,16,124,1179,8,16,174,105,14,12,4.5,291,105,24,117,51,10,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/BufferExhaustedException.java,clients/src/main/java/org/apache/kafka/clients/producer/BufferExhaustedException.java,"KAFKA-3720: Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms (#8399)

Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms

Refactored BufferExhaustedException to be a subclass of TimeoutException so existing code that catches TimeoutExceptions keeps working.

Added handling to count these Exceptions in the metric ""buffer-exhausted-records"".

Test Strategy
There were existing test cases to check this behavior, which I refactored.
I then added an extra case to check whether the expected Exception is actually thrown, which was not covered by current tests.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>",1,8,4,8,52,0,1,37,17,6,6,1.5,47,17,8,10,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/BufferPool.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/BufferPool.java,"KAFKA-3720: Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms (#8399)

Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms

Refactored BufferExhaustedException to be a subclass of TimeoutException so existing code that catches TimeoutExceptions keeps working.

Added handling to count these Exceptions in the metric ""buffer-exhausted-records"".

Test Strategy
There were existing test cases to check this behavior, which I refactored.
I then added an extra case to check whether the expected Exception is actually thrown, which was not covered by current tests.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>",39,9,2,216,1382,2,16,353,223,15,24,4.5,557,223,23,204,62,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/RelationalSmokeTestTest.java,streams/src/test/java/org/apache/kafka/streams/tests/RelationalSmokeTestTest.java,"KAFKA-9832: Extend Streams system tests for EOS-beta (#8443)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",3,3,0,72,622,1,1,105,102,52,2,2.0,105,102,52,0,0,0,1,0,0,0
tests/kafkatest/services/streams_property.py,tests/kafkatest/services/streams_property.py,"KAFKA-9832: Extend Streams system tests for EOS-beta (#8443)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,1,0,7,13,0,0,23,26,8,3,1,27,26,9,4,4,1,2,1,0,1
tests/kafkatest/tests/streams/streams_relational_smoke_test.py,tests/kafkatest/tests/streams/streams_relational_smoke_test.py,"KAFKA-9832: Extend Streams system tests for EOS-beta (#8443)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",8,12,9,86,644,5,6,129,126,64,2,4.5,138,126,69,9,9,4,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultStateConstrainedBalancedAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/DefaultStateConstrainedBalancedAssignorTest.java,"KAFKA-6145: KIP-441: avoid unnecessary movement of standbys (#8436)

Reviewers: John Roesler <vvcephei@apache.org>",34,1,0,850,5515,1,34,978,899,196,5,14,1324,899,265,346,218,69,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Repartitioned.java,streams/src/main/java/org/apache/kafka/streams/kstream/Repartitioned.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",11,171,0,62,581,11,11,171,171,171,1,1,171,171,171,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableForeignKeyJoinResolutionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/KTableKTableForeignKeyJoinResolutionNode.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",2,2,1,54,500,1,2,82,81,41,2,1.5,83,81,42,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicConfig.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",10,33,6,65,428,6,8,108,110,8,13,2,211,110,16,103,73,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicProperties.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicProperties.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",4,42,0,20,96,4,4,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopicConfig.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RepartitionTopicConfig.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",11,11,2,57,396,4,6,98,85,14,7,1,112,85,16,14,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/UnwindowedChangelogTopicConfig.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/UnwindowedChangelogTopicConfig.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",10,4,2,49,332,3,5,88,81,18,5,2,95,81,19,7,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/WindowedChangelogTopicConfig.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/WindowedChangelogTopicConfig.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",15,5,3,66,443,3,6,108,95,22,5,2,117,95,23,9,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/CopartitionedTopicsEnforcer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/CopartitionedTopicsEnforcer.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",20,78,6,135,966,5,7,182,110,91,2,5.0,188,110,94,6,6,3,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicConfigTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicConfigTest.java,"KAFKA-8611: Add KStream#repartition operation (#7170)

Implements KIP-221.

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>",2,58,0,32,193,2,2,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/IsNullConditional.java,generator/src/main/java/org/apache/kafka/message/IsNullConditional.java,"KAFKA-9309: Add the ability to translate Message classes to and from JSON (#7844)

Reviewers: David Arthur <mumrah@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",19,35,12,94,515,5,11,128,105,64,2,7.0,140,105,70,12,12,6,2,1,0,1
generator/src/main/java/org/apache/kafka/message/Target.java,generator/src/main/java/org/apache/kafka/message/Target.java,"KAFKA-9309: Add the ability to translate Message classes to and from JSON (#7844)

Reviewers: David Arthur <mumrah@gmail.com>, Ron Dagostino <rdagostino@confluent.io>",8,93,0,67,415,7,7,93,93,93,1,1,93,93,93,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultStateConstrainedBalancedAssignor.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/DefaultStateConstrainedBalancedAssignor.java,"KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away (#8425)

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",33,11,33,214,1588,19,12,304,326,76,4,13.5,426,326,106,122,67,30,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/RankedClient.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/RankedClient.java,"KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away (#8425)

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",23,23,0,96,727,1,8,143,120,72,2,1.5,143,120,72,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/RankedClientTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/RankedClientTest.java,"KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away (#8425)

Reviewers: Bruno Cadonna <bruno@confluent.io>, John Roesler <vvcephei@apache.org>",7,33,0,145,1345,2,7,196,163,98,2,3.0,196,163,98,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/EchoServer.java,clients/src/test/java/org/apache/kafka/common/network/EchoServer.java,KAFKA-8890: Make SSL context/engine configuration extensible (KIP-519) (#8338),17,2,1,105,687,1,5,136,119,14,10,1.5,195,119,20,59,30,6,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/processor/internals/TestDriverProducer.java,streams/test-utils/src/main/java/org/apache/kafka/streams/processor/internals/TestDriverProducer.java,"MINOR: Refactor StreamsProducer (#8380)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Andrew Choi <a24choi@edu.uwaterloo.ca>",2,7,4,23,206,2,2,44,39,11,4,3.0,54,39,14,10,4,2,1,0,1,1
core/src/main/scala/kafka/utils/ToolsUtils.scala,core/src/main/scala/kafka/utils/ToolsUtils.scala,"KAFKA-9775: Fix IllegalFormatConversionException in ToolsUtils

The runtime type of Metric.metricValue() needn't always be a Double,
for example, if it's a gauge from IntGaugeSuite.
Since it's impossible to format non-double values with 3 point precision
IllegalFormatConversionException resulted.

Author: Tom Bentley <tbentley@redhat.com>
Author: Tom Bentley <tombentley@users.noreply.github.com>

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #8373 from tombentley/KAFKA-9775-IllegalFormatConversionException",10,5,1,41,370,1,2,67,38,10,7,1,80,38,11,13,7,2,2,1,0,1
core/src/main/scala/kafka/admin/DelegationTokenCommand.scala,core/src/main/scala/kafka/admin/DelegationTokenCommand.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",22,1,1,162,1490,0,9,219,214,27,8,3.0,280,214,35,61,41,8,2,1,0,1
core/src/main/scala/kafka/log/OffsetIndex.scala,core/src/main/scala/kafka/log/OffsetIndex.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",21,1,1,107,686,1,11,207,277,4,52,3.0,930,277,18,723,262,14,2,1,0,1
core/src/main/scala/kafka/security/auth/Resource.scala,core/src/main/scala/kafka/security/auth/Resource.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",6,1,1,42,334,1,2,85,44,6,14,2.5,157,44,11,72,19,5,2,1,0,1
core/src/main/scala/kafka/security/authorizer/AclEntry.scala,core/src/main/scala/kafka/security/authorizer/AclEntry.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",22,1,1,89,853,0,7,146,146,73,2,1.0,147,146,74,1,1,0,2,1,0,1
core/src/main/scala/kafka/server/DelayedElectLeader.scala,core/src/main/scala/kafka/server/DelayedElectLeader.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",8,2,2,38,270,1,4,83,89,14,6,2.0,115,89,19,32,13,5,2,1,0,1
core/src/main/scala/kafka/utils/json/JsonArray.scala,core/src/main/scala/kafka/utils/json/JsonArray.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",0,2,2,7,70,0,0,27,27,14,2,1.0,29,27,14,2,2,1,2,1,0,1
core/src/main/scala/kafka/utils/json/JsonObject.scala,core/src/main/scala/kafka/utils/json/JsonObject.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",2,1,1,13,151,0,2,42,42,14,3,1,44,42,15,2,1,1,2,1,0,1
core/src/test/scala/kafka/tools/LogCompactionTester.scala,core/src/test/scala/kafka/tools/LogCompactionTester.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",37,2,2,275,2157,1,16,348,348,58,6,2.5,363,348,60,15,4,2,2,1,0,1
core/src/test/scala/other/kafka/TestPurgatoryPerformance.scala,core/src/test/scala/other/kafka/TestPurgatoryPerformance.scala,"MINOR: Fix Scala 2.13 compiler warnings (#8390)

Once Scala 2.13.2 is officially released, I will submit a follow up PR
that enables `-Xfatal-warnings` with the necessary warning
exclusions. Compiler warning exclusions were only introduced in 2.13.2
and hence why we have to wait for that. I used a snapshot build to
test it in the meantime.

Changes:
* Remove Deprecated annotation from internal request classes
* Class.newInstance is deprecated in favor of
Class.getConstructor().newInstance
* Replace deprecated JavaConversions with CollectionConverters
* Remove unused kafka.cluster.Cluster
* Don't use Map and Set methods deprecated in 2.13:
    - collection.Map +, ++, -, --, mapValues, filterKeys, retain
    - collection.Set +, ++, -, --
* Add scala-collection-compat dependency to streams-scala and
update version to 2.1.4.
* Replace usages of deprecated Either.get and Either.right
* Replace usage of deprecated Integer(String) constructor
* `import scala.language.implicitConversions` is not needed in Scala 2.13
* Replace usage of deprecated `toIterator`, `Traversable`, `seq`,
`reverseMap`, `hasDefiniteSize`
* Replace usage of deprecated alterConfigs with incrementalAlterConfigs
where possible
* Fix implicit widening conversions from Long/Int to Double/Float
* Avoid implicit conversions to String
* Eliminate usage of deprecated procedure syntax
* Remove `println`in `LogValidatorTest` instead of fixing the compiler
warning since tests should not `println`.
* Eliminate implicit conversion from Array to Seq
* Remove unnecessary usage of 3 argument assertEquals
* Replace `toStream` with `iterator`
* Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS
* Replace StringBuilder.newBuilder with new StringBuilder
* Rename AclBuffers to AclSeqs and remove usage of `filterKeys`
* More consistent usage of Set/Map in Controller classes: this also fixes
deprecated warnings with Scala 2.13
* Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",29,1,1,219,1775,0,17,292,275,29,10,1.0,317,275,32,25,9,2,2,1,0,1
core/src/main/scala/kafka/security/auth/Operation.scala,core/src/main/scala/kafka/security/auth/Operation.scala,"MINOR: reduce garbage in operation and resource java conversions (#8391)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",14,16,1,87,503,2,2,113,42,11,10,1.5,142,47,14,29,9,3,2,1,0,1
core/src/main/scala/kafka/security/auth/ResourceType.scala,core/src/main/scala/kafka/security/auth/ResourceType.scala,"MINOR: reduce garbage in operation and resource java conversions (#8391)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",9,11,2,64,463,3,3,93,51,5,17,2,143,51,8,50,13,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/ExceptionUtils.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ExceptionUtils.java,"KAFKA-9770: Close underlying state store also when flush throws (#8368)

When a caching state store is closed it calls its flush() method.
If flush() throws an exception the underlying state store is not closed.

This commit ensures that state stores underlying a wrapped state stores
are closed even when preceding operations in the close method throw.

Co-authored-by: John Roesler <vvcephei@apache.org>
Reviewers: John Roesler <vvcephei@apache.org>, Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <matthias@confluent.io>",7,46,0,26,166,3,3,46,46,46,1,1,46,46,46,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java,"KAFKA-9707: Fix InsertField.Key should apply to keys of tombstone records (#8280)

* KAFKA-9707: Fix InsertField.Key not applying to tombstone events

* Fix typo that hardcoded .value() instead of abstract operatingValue
* Add test for Key transform that was previously not tested

Signed-off-by: Greg Harris <gregh@confluent.io>

* Add null value assertion to tombstone test

* Remove mis-named function and add test for passing-through a null-keyed record.

Signed-off-by: Greg Harris <gregh@confluent.io>

* Simplify unchanged record assertion

Signed-off-by: Greg Harris <gregh@confluent.io>

* Replace assertEquals with assertSame

Signed-off-by: Greg Harris <gregh@confluent.io>

* Fix checkstyleTest indent issue

Signed-off-by: Greg Harris <gregh@confluent.io>",56,1,5,211,1893,2,15,277,296,40,7,2,389,296,56,112,95,16,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/MirrorClientConfig.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/MirrorClientConfig.java,"MINOR: Fix a number of warnings in mirror/mirror-client (#8074)


Reviewers: Ismael Juma <ismael@juma.me.uk>, Ryanne Dolan <ryannedolan@gmail.com>, Andrew Choi <a24choi@edu.uwaterloo.ca>",6,1,1,82,524,0,6,135,135,68,2,1.0,136,135,68,1,1,0,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/RemoteClusterUtils.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/RemoteClusterUtils.java,"MINOR: Fix a number of warnings in mirror/mirror-client (#8074)


Reviewers: Ismael Juma <ismael@juma.me.uk>, Ryanne Dolan <ryannedolan@gmail.com>, Andrew Choi <a24choi@edu.uwaterloo.ca>",6,1,4,41,304,0,6,94,97,47,2,1.5,98,97,49,4,4,2,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/Scheduler.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/Scheduler.java,"MINOR: Fix a number of warnings in mirror/mirror-client (#8074)


Reviewers: Ismael Juma <ismael@juma.me.uk>, Ryanne Dolan <ryannedolan@gmail.com>, Andrew Choi <a24choi@edu.uwaterloo.ca>",19,1,1,85,633,2,8,115,115,38,3,1,117,115,39,2,1,1,2,1,0,1
tests/kafkatest/services/verifiable_producer.py,tests/kafkatest/services/verifiable_producer.py,"KAFKA-9573: Fix JVM options to run early versions of Kafka on the latest JVMs (#8138)

Startup scripts for the early version of Kafka contain removed JVM options like `-XX:+PrintGCDateStamps` or `-XX:UseParNewGC`. 
When system tests run on JVM that doesn't support these options we should set up
environment variables with correct options.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Ron Dagostino <rdagostino@confluent.io>, Ismael Juma <ismael@juma.me.uk",46,2,0,231,1712,1,19,317,107,9,37,4,561,107,15,244,77,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/CompressionRatioEstimator.java,clients/src/main/java/org/apache/kafka/common/record/CompressionRatioEstimator.java,"KAFKA-9700: Fix negative estimatedCompressionRatio (#8285)

There are cases where `currentEstimation` is less than
`COMPRESSION_RATIO_IMPROVING_STEP` causing
`estimatedCompressionRatio` to be negative. This, in turn,
may result in `MESSAGE_TOO_LARGE`.

Reviewers: Ismael Juma <ismael@juma.me.uk>",13,2,2,58,432,1,7,111,111,56,2,2.0,113,111,56,2,2,1,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/SchemaSourceTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/SchemaSourceTask.java,"KAFKA-9407: Return an immutable list from poll in SchemaSourceTask (#7939)

Simple fix to return in all cases an immutable list from poll in SchemaSourceTask.

Co-authored-by: David Mollitor <dmollitor@apache.org>

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Ismael Juma <github@juma.me.uk>, Konstantine Karantasis <konstantine@confluent.io>",11,2,4,135,1103,1,4,169,174,28,6,2.5,184,174,31,15,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java,"KAFKA-9651: Fix ArithmeticException (÷ by 0) in DefaultStreamPartitioner (#8226)

In Streams `StreamsMetadataState.getMetadataWithKey`, we should use the inferred max topic partitions passed in directly from the caller than relying on cluster to contain its topic-partition information.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",6,19,7,23,240,2,5,83,55,5,16,3.0,204,55,13,121,33,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStreamPartitioner.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStreamPartitioner.java,"KAFKA-9651: Fix ArithmeticException (÷ by 0) in DefaultStreamPartitioner (#8226)

In Streams `StreamsMetadataState.getMetadataWithKey`, we should use the inferred max topic partitions passed in directly from the caller than relying on cluster to contain its topic-partition information.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,1,1,20,196,1,2,41,43,10,4,2.0,54,43,14,13,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/provider/ConfigProvider.java,clients/src/main/java/org/apache/kafka/common/config/provider/ConfigProvider.java,"KAFKA-9634: Add note about thread safety in the ConfigProvider interface (#8205)

In Kafka Connect, a ConfigProvider instance can be used concurrently (e.g. via a PUT request to the `/connector-plugins/{connectorType}/config/validate` REST endpoint), but there is no mention of concurrent usage in the Javadocs of the ConfigProvider interface. 

It's worth calling out that implementations need to be thread safe.

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",3,1,0,19,154,0,3,81,78,27,3,1,82,78,27,1,1,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AlterConsumerGroupOffsetsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterConsumerGroupOffsetsOptions.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",0,3,1,6,45,0,0,30,28,15,2,1.5,31,28,16,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersOptions.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",0,3,0,7,57,0,0,32,29,11,3,2,34,29,11,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ElectLeadersResult.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",6,3,0,40,304,0,3,79,76,26,3,1,81,76,27,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ElectPreferredLeadersOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ElectPreferredLeadersOptions.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",0,4,1,9,65,0,0,36,31,9,4,3.0,41,31,10,5,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ElectPreferredLeadersResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ElectPreferredLeadersResult.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",8,2,0,61,445,0,4,114,136,23,5,2,193,136,39,79,72,16,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListOffsetsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListOffsetsOptions.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",3,2,0,17,98,0,3,45,43,22,2,1.0,45,43,22,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFuture.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFuture.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",33,3,1,136,870,0,20,255,209,21,12,2.0,438,209,36,183,113,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/ClusterResourceListener.java,clients/src/main/java/org/apache/kafka/common/ClusterResourceListener.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",0,4,4,4,21,0,0,54,50,14,4,2.0,69,50,17,15,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/ElectionType.java,clients/src/main/java/org/apache/kafka/common/ElectionType.java,"MINOR: fix linking errors in javadoc (#8198)

This improvement fixes several linking errors to classes and methods from within javadocs. 

Related to #8291

Reviewers: Konstantine Karantasis <konstantine@confluent.io>",4,4,2,22,144,0,2,50,48,17,3,2,54,48,18,4,2,1,2,1,0,1
core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala,core/src/test/scala/unit/kafka/admin/ReassignPartitionsClusterTest.scala,"KAFKA-9654; Update epoch in `ReplicaAlterLogDirsThread` after new LeaderAndIsr  (#8223)

Currently when there is a leader change with a log dir reassignment in progress, we do not update the leader epoch in the partition state maintained by `ReplicaAlterLogDirsThread`. This can lead to a FENCED_LEADER_EPOCH error, which results in the partition being marked as failed, which is a permanent failure until the broker is restarted. This patch fixes the problem by updating the epoch in `ReplicaAlterLogDirsThread` after receiving a new LeaderAndIsr request from the controller.

Reviewers: Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",65,17,5,917,10245,7,55,1304,598,31,42,3.0,1804,662,43,500,74,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/GraphGraceSearchUtilTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/GraphGraceSearchUtilTest.java,"MINOR: Fix generic types in StreamsBuilder and Topology (#8273)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@confluent.io>, John Roesler <john@confluent.io>",9,8,9,181,1208,6,7,227,241,28,8,6.0,284,241,36,57,16,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBufferTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBufferTest.java,"MINOR: Fix generic types in StreamsBuilder and Topology (#8273)

Reviewers: Chia-Ping Tsai <chia7712@gmail.com>, Guozhang Wang <guozhang@confluent.io>, John Roesler <john@confluent.io>",4,6,4,38,319,2,4,64,32,16,4,2.0,70,32,18,6,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java,streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java,"KAFKA-9568: enforce rebalance if client endpoint has changed (#8299)

Since the assignment info includes a map with all member's host info, we can just check the received map to make sure our endpoint is contained. If not, we need to force the group to rebalance and get our updated endpoint info.

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",18,1,1,80,486,0,11,154,93,17,9,3,189,93,21,35,14,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/HostInfoTest.java,streams/src/test/java/org/apache/kafka/streams/state/HostInfoTest.java,"KAFKA-9568: enforce rebalance if client endpoint has changed (#8299)

Since the assignment info includes a map with all member's host info, we can just check the received map to make sure our endpoint is contained. If not, we need to force the group to rebalance and get our updated endpoint info.

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,52,0,28,194,4,4,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaFilterComponent.java,clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaFilterComponent.java,"MINOR: fix Scala 2.13 build error introduced in #8083 (#8301)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Brian Byrne <bbyrne@confluent.io>",13,1,1,41,284,1,9,109,109,54,2,1.0,110,109,55,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterClientQuotasOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterClientQuotasOptions.java,"KIP-546: Implement describeClientQuotas and alterClientQuotas. (#8083)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",2,46,0,13,74,2,2,46,46,46,1,1,46,46,46,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AlterClientQuotasResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterClientQuotasResult.java,"KIP-546: Implement describeClientQuotas and alterClientQuotas. (#8083)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",3,58,0,18,148,3,3,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaAlteration.java,clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaAlteration.java,"KIP-546: Implement describeClientQuotas and alterClientQuotas. (#8083)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",14,106,0,50,288,10,10,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaFilter.java,clients/src/main/java/org/apache/kafka/common/quota/ClientQuotaFilter.java,"KIP-546: Implement describeClientQuotas and alterClientQuotas. (#8083)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",13,101,0,42,277,9,9,101,101,101,1,1,101,101,101,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinPseudoTopicTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinPseudoTopicTest.java,"MINOR: reuse pseudo-topic in FKJoin (#8296)

Reuse the same pseudo-topic for serializing the LHS value in the foreign-key join resolver as
we originally used to serialize it before sending the subscription request.

Reviewers: Boyang Chen <boyang@confluent.io>",6,138,0,99,871,3,3,138,138,138,1,1,138,138,138,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/utils/UniqueTopicSerdeScope.java,streams/src/test/java/org/apache/kafka/streams/utils/UniqueTopicSerdeScope.java,"MINOR: reuse pseudo-topic in FKJoin (#8296)

Reuse the same pseudo-topic for serializing the LHS value in the foreign-key join resolver as
we originally used to serialize it before sending the subscription request.

Reviewers: Boyang Chen <boyang@confluent.io>",21,6,0,113,886,1,18,154,148,77,2,2.0,154,148,77,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigOp.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigOp.java,"KAFKA-9644: Handle non-existent configs in incrementalAlterConfigs APPEND/SUBTRACT

Problem
----
The `incrementalAlterConfigs` API supports OpType.APPEND and OpType.SUBTRACT for configuration properties of LIST type. If an APPEND or SUBTRACT OpType is submitted for a config property which currently has no value, then the operation fails with a NullPointerException on the broker side (conveyed as an ""unknown server error"" to the client).

This is because the alter code does a `getProperty` of the existing configuration value
with no concern as to whether or not the property actually exists.

This change handles the case of existing null properties.

Testing
-----
This change includes 2 test cases in the unit test that demonstrate the issue for OpType.SUBTRACT and OpType.APPEND.

Author: Steve Rodrigues <srodrigues@confluent.io>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Bob Barrett <bob.barrett@confluent.io>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #8216 from steverod/steverod.kafka-9644",13,21,1,61,370,0,9,116,96,39,3,1,118,96,39,2,1,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/QueryableStoreTypes.java,streams/src/main/java/org/apache/kafka/streams/state/QueryableStoreTypes.java,"MINOR: Update Streams IQ JavaDocs to not point to a deprecated method (#8271)

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",19,2,1,102,783,0,17,187,90,19,10,3.0,236,90,24,49,17,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadata.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerGroupMetadata.java,"KAFKA-9451: Update MockConsumer to support ConsumerGroupMetadata

Reviewers: Boyang Chan <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",15,17,0,59,343,2,9,92,50,23,4,2.0,102,50,26,10,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/StreamsMetadataTest.java,streams/src/test/java/org/apache/kafka/streams/state/StreamsMetadataTest.java,"KAFKA-9668: Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException (#8233)

`KafkaStreams.getAllMetadata()` returns `StreamsMetadataState.getAllMetadata()`. All the latter methods is `synchronized` it returns a reference to internal mutable state.  Not only does this break encapsulation, but it means any thread iterating over the returned collection when the metadata gets rebuilt will encounter a `ConcurrentModificationException`.

This change:
 * switches from clearing and rebuild `allMetadata` when `onChange` is called to building a new list and swapping this in. This is thread safe and has the benefit that the returned list is not empty during a rebuild: you either get the old or the new list.
 * removes synchronisation from `getAllMetadata` and `getLocalMetadata`. These are returning member variables. Synchronisation adds nothing.
 * changes `getAllMetadata` to wrap its return value in an unmodifiable wrapper to avoid breaking encapsulation.
 * changes the getters in `StreamsMetadata` to wrap their return values in unmodifiable wrapper to avoid breaking encapsulation.

Co-authored-by: Andy Coates <big-andy-coates@users.noreply.github.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>",4,64,0,38,253,3,3,64,64,64,1,1,64,64,64,0,0,0,2,1,0,1
tests/kafkatest/tests/produce_consume_validate.py,tests/kafkatest/tests/produce_consume_validate.py,KAFKA-9662: Wait for consumer offset reset in throttle test to avoid losing early messages (#8227),20,5,0,81,566,1,8,132,106,6,21,3,286,106,14,154,56,7,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/MockRebalanceListener.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/MockRebalanceListener.java,"KAFKA-9525: add enforceRebalance method to Consumer API (#8087)

As described in KIP-568.

Waiting on acceptance of the KIP to write the tests, on the off chance something changes. But rest assured unit tests are coming ⚡️

Will also kick off existing Streams system tests which leverage this new API (eg version probing, sometimes broker bounce)

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,48,0,27,159,3,3,48,48,48,1,1,48,48,48,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/consumer/internals/MockPartitionAssignor.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/MockPartitionAssignor.java,"KAFKA-9620: Do not throw in the middle of consumer user callbacks (#8187)

One way of fixing it forward.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",9,12,0,42,243,3,8,72,49,14,5,3,79,49,16,7,6,1,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/internals/ThrowOnAssignmentAssignor.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/ThrowOnAssignmentAssignor.java,"KAFKA-9620: Do not throw in the middle of consumer user callbacks (#8187)

One way of fixing it forward.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",3,49,0,23,124,3,3,49,49,49,1,1,49,49,49,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/BufferConfigInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/BufferConfigInternal.java,"KAFKA-8147: Add changelog topic configuration to KTable suppress (#8029)

Implements: KIP-446

Reviewers: John Roesler <vvcephei@apache.org>",3,7,1,28,199,0,3,55,54,11,5,1,66,54,13,11,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/EagerBufferConfigImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/EagerBufferConfigImpl.java,"KAFKA-8147: Add changelog topic configuration to KTable suppress (#8029)

Implements: KIP-446

Reviewers: John Roesler <vvcephei@apache.org>",19,34,2,78,446,8,14,112,76,19,6,2.0,130,76,22,18,10,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/StrictBufferConfigImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/StrictBufferConfigImpl.java,"KAFKA-8147: Add changelog topic configuration to KTable suppress (#8029)

Implements: KIP-446

Reviewers: John Roesler <vvcephei@apache.org>",21,35,0,94,543,7,15,130,91,32,4,3.0,143,91,36,13,11,3,1,0,1,1
core/src/test/scala/integration/kafka/api/GroupEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/GroupEndToEndAuthorizationTest.scala,"MINOR: Improve AuthorizerIntegrationTest (#7926)

This patch improves the authorizer integration tests in the following ways:

1. We use a separate principal for inter-broker communications. This ensures that ACLs set in the test cases do not interfere with inter-broker communication. We had two test cases (`testCreateTopicAuthorizationWithClusterCreate` and `testAuthorizationWithTopicExisting`) which depend on topic creation and were timing out because of inter-broker metadata propagation failures. The timeouts were treated as successfully satisfying the expectation of authorization. So the tests passed, but not because of the intended reason.
2. Previously `GroupAuthorizerIntegrationTest` was inheriting _all_ of the tests from `AuthorizerIntegrationTest`. This seemed like overkill since the ACL evaluation logic is essentially the same. 

Totally this should take about 5-10 minutes off the total build time and make the authorizer integration tests a little more resilient to problems with inter-broker communication.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",4,2,2,27,178,0,1,46,46,23,2,1.0,48,46,24,2,2,1,1,0,1,1
core/src/test/scala/integration/kafka/api/SaslOAuthBearerSslEndToEndAuthorizationTest.scala,core/src/test/scala/integration/kafka/api/SaslOAuthBearerSslEndToEndAuthorizationTest.scala,"MINOR: Improve AuthorizerIntegrationTest (#7926)

This patch improves the authorizer integration tests in the following ways:

1. We use a separate principal for inter-broker communications. This ensures that ACLs set in the test cases do not interfere with inter-broker communication. We had two test cases (`testCreateTopicAuthorizationWithClusterCreate` and `testAuthorizationWithTopicExisting`) which depend on topic creation and were timing out because of inter-broker metadata propagation failures. The timeouts were treated as successfully satisfying the expectation of authorization. So the tests passed, but not because of the intended reason.
2. Previously `GroupAuthorizerIntegrationTest` was inheriting _all_ of the tests from `AuthorizerIntegrationTest`. This seemed like overkill since the ACL evaluation logic is essentially the same. 

Totally this should take about 5-10 minutes off the total build time and make the authorizer integration tests a little more resilient to problems with inter-broker communication.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",0,3,2,9,75,0,0,27,26,14,2,1.5,29,26,14,2,2,1,1,0,1,1
streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java,streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java,"KAFKA-9441: Add internal TransactionManager (#8105)

Upfront refactoring for KIP-447.

Introduces `StreamsProducer` that allows to share a producer over multiple tasks and track the TX status.

Reviewers: Boyang Chen <boyang@confluent.io>, Guozhang Wang <guozhang@confluent.io>",2,2,2,16,153,1,2,39,24,6,7,3,65,24,9,26,10,4,1,0,1,1
clients/src/main/java/org/apache/kafka/common/utils/FixedOrderMap.java,clients/src/main/java/org/apache/kafka/common/utils/FixedOrderMap.java,"KAFKA-9481: Graceful handling TaskMigrated and TaskCorrupted (#8058)

1. Removed task field from TaskMigrated; the only caller that encodes a task id from StreamTask actually do not throw so we only log it. To handle it on StreamThread we just always enforce rebalance (and we would call onPartitionsLost to remove all tasks as dirty).

2. Added TaskCorruptedException with a set of task-ids. The first scenario of this is the restoreConsumer.poll which throws InvalidOffset indicating that the logs are truncated / compacted. To handle it on StreamThread we first close the corresponding tasks as dirty (if EOS is enabled we would also wipe out the state stores), and then revive them into the CREATED state.

3. Also fixed a bug while investigating KAFKA-9572: when suspending / closing a restoring task we should not commit the new offsets but only updating the checkpoint file.

4. Re-enabled the unit test.",4,0,6,25,142,1,4,57,63,28,2,1.0,63,63,32,6,6,3,2,1,0,1
examples/src/main/java/kafka/examples/Producer.java,examples/src/main/java/kafka/examples/Producer.java,"MINOR: Fix javadoc at org.apache.kafka.clients.producer.KafkaProducer.InterceptorCallback#onCompletion (#7337)

Reviewers: Guozhang Wang <wangguoz@gmail.com>",11,2,2,94,653,0,5,131,55,8,17,2,258,69,15,127,71,7,2,1,0,1
examples/src/main/java/kafka/examples/Consumer.java,examples/src/main/java/kafka/examples/Consumer.java,"MINOR: Improve EOS example exception handling (#8052)

The current EOS example mixes fatal and non-fatal error handling. This patch fixes this problem and simplifies the example.

Reviewers: Jason Gustafson <jason@confluent.io>",8,3,0,70,533,1,5,96,61,6,17,3,227,61,13,131,38,8,2,1,0,1
examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java,examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java,"MINOR: Improve EOS example exception handling (#8052)

The current EOS example mixes fatal and non-fatal error handling. This patch fixes this problem and simplifies the example.

Reviewers: Jason Gustafson <jason@confluent.io>",17,48,73,127,1075,4,7,187,209,62,3,2,261,209,87,74,73,25,2,1,0,1
examples/src/main/java/kafka/examples/KafkaConsumerProducerDemo.java,examples/src/main/java/kafka/examples/KafkaConsumerProducerDemo.java,"MINOR: Improve EOS example exception handling (#8052)

The current EOS example mixes fatal and non-fatal error handling. This patch fixes this problem and simplifies the example.

Reviewers: Jason Gustafson <jason@confluent.io>",3,2,1,20,194,1,1,42,29,4,11,2,74,29,7,32,12,3,2,1,0,1
examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java,examples/src/main/java/kafka/examples/KafkaExactlyOnceDemo.java,"MINOR: Improve EOS example exception handling (#8052)

The current EOS example mixes fatal and non-fatal error handling. This patch fixes this problem and simplifies the example.

Reviewers: Jason Gustafson <jason@confluent.io>",17,17,15,107,833,1,3,195,185,49,4,3.0,217,185,54,22,15,6,2,1,0,1
examples/src/main/java/kafka/examples/KafkaProperties.java,examples/src/main/java/kafka/examples/KafkaProperties.java,"MINOR: Improve EOS example exception handling (#8052)

The current EOS example mixes fatal and non-fatal error handling. This patch fixes this problem and simplifies the example.

Reviewers: Jason Gustafson <jason@confluent.io>",1,0,5,7,40,0,1,25,30,4,7,1,70,30,10,45,16,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/Table.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/Table.java,"KAFKA-9556; Fix two issues with KIP-558 and expand testing coverage (#8085)

Correct the Connect worker logic to properly disable the new topic status (KIP-558) feature when `topic.tracking.enable=false`, and fix automatic topic status reset after a connector is deleted.

Also adds new `ConnectorTopicsIntegrationTest` and expanded unit tests.

Reviewers: Randall Hauch <rhauch@gmail.com>",11,3,0,42,325,1,6,68,65,23,3,1,75,65,25,7,7,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorHandlingMetrics.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorHandlingMetrics.java,"KAFKA-9106 make metrics exposed via jmx configurable (#7674)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",10,0,9,68,566,1,10,141,150,47,3,2,158,150,53,17,9,6,2,1,0,1
core/src/main/java/kafka/metrics/KafkaYammerMetrics.java,core/src/main/java/kafka/metrics/KafkaYammerMetrics.java,"KAFKA-9106 make metrics exposed via jmx configurable (#7674)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",6,76,0,38,271,6,6,76,76,76,1,1,76,76,76,0,0,0,0,0,0,0
core/src/main/scala/kafka/metrics/KafkaCSVMetricsReporter.scala,core/src/main/scala/kafka/metrics/KafkaCSVMetricsReporter.scala,"KAFKA-9106 make metrics exposed via jmx configurable (#7674)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",9,2,3,51,294,2,3,87,85,7,13,2,135,85,10,48,22,4,2,1,0,1
core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala,core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala,"KAFKA-9106 make metrics exposed via jmx configurable (#7674)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",16,5,6,58,716,4,8,107,116,4,25,3,362,118,14,255,102,10,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorClientPolicyIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorClientPolicyIntegrationTest.java,"MINOR: Small Connect integration test fixes (#8100)

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",11,9,6,108,905,4,9,149,146,50,3,1,156,146,52,7,6,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/RestExtensionIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/RestExtensionIntegrationTest.java,"MINOR: Small Connect integration test fixes (#8100)

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",11,8,3,153,1106,1,9,217,119,54,4,5.5,232,119,58,15,9,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/SessionedProtocolIntegrationTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/SessionedProtocolIntegrationTest.java,"MINOR: Small Connect integration test fixes (#8100)

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",3,1,2,109,817,1,3,167,168,56,3,2,173,168,58,6,4,2,2,1,0,1
core/src/test/scala/unit/kafka/controller/MockReplicaStateMachine.scala,core/src/test/scala/unit/kafka/controller/MockReplicaStateMachine.scala,"KAFKA-9499; Improve deletion process by batching more aggressively (#8053)

This PR speeds up the deletion process by doing the following:
- Batch whenever possible to minimize the number of requests sent out to other brokers;
- Refactor `onPartitionDeletion` to remove the usage of `allLiveReplicas`.

Reviewers: Jason Gustafson <jason@confluent.io>",5,12,0,27,204,3,3,50,36,17,3,1,50,36,17,0,0,0,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java,"KAFKA-7052 Avoiding NPE in ExtractField SMT in case of non-existent fields (#8059)

Author: Gunnar Morling <gunnar.morling@googlemail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",14,8,1,80,693,1,10,121,114,24,5,2,139,114,28,18,8,4,2,1,0,1
streams/src/test/java/org/apache/kafka/common/metrics/SensorAccessor.java,streams/src/test/java/org/apache/kafka/common/metrics/SensorAccessor.java,"KAFKA-9480: Fix bug that prevented to measure task-level process-rate (#8018)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",2,35,0,11,60,2,2,35,35,35,1,1,35,35,35,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsResult.java,"KAFKA-9505: Only loop over topics-to-validate in retries (#8039)

Found this bug from the repeated flaky runs of system tests, it seems to be long lurking but also would only happen if there are frequent rebalances / topic creation within a short time, which is exactly the case in some of our smoke system tests.

Also added a unit test.

Reviewers: Boyang Chen <boyang@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,1,1,31,257,1,3,68,68,8,9,1,91,68,10,23,13,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/MetadataCache.java,clients/src/main/java/org/apache/kafka/clients/MetadataCache.java,"KAFKA-8904: Improve producer's topic metadata fetching. (#7781)

When the producer encouteres new topic(s), it now only fetches the metadata for the new topics. For cases where a producer interacts with a lot of topics, this reduces the cost for the topic being evicted from the cache, and during startup when populating the topic cache.

Additionally adds a new producer configuration variable 'metadata.max.idle.ms', which controls how long topic metadata may be idle (i.e. not produced to) before it's finally discarded from the metadata cache.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, dengziming <dengziming1993@gmail.com>",21,71,5,144,1119,7,12,210,192,35,6,3.0,308,192,51,98,70,16,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java,"KAFKA-8904: Improve producer's topic metadata fetching. (#7781)

When the producer encouteres new topic(s), it now only fetches the metadata for the new topics. For cases where a producer interacts with a lot of topics, this reduces the cost for the topic being evicted from the cache, and during startup when populating the topic cache.

Additionally adds a new producer configuration variable 'metadata.max.idle.ms', which controls how long topic metadata may be idle (i.e. not produced to) before it's finally discarded from the metadata cache.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, dengziming <dengziming1993@gmail.com>",23,39,4,109,752,8,13,159,129,26,6,1.5,178,129,30,19,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/JaasUtils.java,clients/src/main/java/org/apache/kafka/common/security/JaasUtils.java,"KAFKA-8843: KIP-515: Zookeeper TLS support

Signed-off-by: Ron Dagostino <rdagostinoconfluent.io>

Author: Ron Dagostino <rdagostino@confluent.io>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #8003 from rondagostino/KAFKA-8843",6,4,1,48,325,2,3,81,64,5,16,2.0,258,64,16,177,111,11,2,1,0,1
core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala,core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala,"MINOR: Add missing quote for malformed line content (#8070)


Reviewers: Mickael Maison <mickael.maison@gmail.com>",20,1,1,110,668,2,5,136,103,6,22,2.0,357,103,16,221,56,10,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsResult.java,"KAFKA-9507; AdminClient should check for missing committed offsets (#8057)

Addresses exception being thrown by `AdminClient` when `listConsumerGroupOffsets` returns a negative offset. A negative offset indicates the absence of a committed offset for a requested partition, and should result in a null in the returned offset map.

Reviewers: Anna Povzner <anna@confluent.io>, Jason Gustafson <jason@confluent.io>",2,1,0,16,126,0,2,49,48,16,3,1,51,48,17,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsResult.java,"KAFKA-9447: Add new customized EOS model example (#8031)

With the improvement of 447, we are now offering developers a better experience on writing their customized EOS apps with group subscription, instead of manual assignments. With the demo, user should be able to get started more quickly on writing their own EOS app, and understand the processing logic much better.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",4,2,12,22,170,2,4,60,67,9,7,2,86,67,12,26,12,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndMetadata.java,"KAFKA-9261; Client should handle unavailable leader metadata (#7770)

The client caches metadata fetched from Metadata requests. Previously, each metadata response overwrote all of the metadata from the previous one, so we could rely on the expectation that the broker only returned the leaderId for a partition if it had connection information available. This behavior changed with KIP-320 since having the leader epoch allows the client to filter out partition metadata which is known to be stale. However, because of this, we can no longer rely on the request-level guarantee of leader availability. There is no mechanism similar to the leader epoch to track the staleness of broker metadata, so we still overwrite all of the broker metadata from each response, which means that the partition metadata can get out of sync with the broker metadata in the client's cache. Hence it is no longer safe to validate inside the `Cluster` constructor that each leader has an associated `Node`

Fixing this issue was unfortunately not straightforward because the cache was built to maintain references to broker metadata through the `Node` object at the partition level. In order to keep the state consistent, each `Node` reference would need to be updated based on the new broker metadata. Instead of doing that, this patch changes the cache so that it is structured more closely with the Metadata response schema. Broker node information is maintained at the top level in a single collection and cached partition metadata only references the id of the broker. To accommodate this, we have removed `PartitionInfoAndEpoch` and we have altered `MetadataResponse.PartitionMetadata` to eliminate its `Node` references.

Note that one of the side benefits of the refactor here is that we virtually eliminate one of the hotspots in Metadata request handling in `MetadataCache.getEndpoints` (which was renamed to `maybeFilterAliveReplicas`). The only reason this was expensive was because we had to build a new collection for the `Node` representations of each of the replica lists. This information was doomed to just get discarded on serialization, so the whole effort was wasteful. Now, we work with the lower level id lists and no copy of the replicas is needed (at least for all versions other than 0).

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>",18,3,1,59,362,1,9,126,80,18,7,2,152,80,22,26,9,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java,"KAFKA-9113: Clean up task management and state management (#7997)

This PR is collaborated by Guozhang Wang and John Roesler. It is a significant tech debt cleanup on task management and state management, and is broken down by several sub-tasks listed below:

Extract embedded clients (producer and consumer) into RecordCollector from StreamTask.
guozhangwang#2
guozhangwang#5

Consolidate the standby updating and active restoring logic into ChangelogReader and extract out of StreamThread.
guozhangwang#3
guozhangwang#4

Introduce Task state life cycle (created, restoring, running, suspended, closing), and refactor the task operations based on the current state.
guozhangwang#6
guozhangwang#7

Consolidate AssignedTasks into TaskManager and simplify the logic of changelog management and task management (since they are already moved in step 2) and 3)).
guozhangwang#8
guozhangwang#9

Also simplified the StreamThread logic a bit as the embedded clients / changelog restoration logic has been moved into step 1) and 2).
guozhangwang#10

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Boyang Chen <boyang@confluent.io>",26,2,2,478,4306,1,14,557,535,80,7,1,1079,535,154,522,307,75,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockBatchingStateRestoreListener.java,streams/src/test/java/org/apache/kafka/test/MockBatchingStateRestoreListener.java,"KAFKA-9113: Clean up task management and state management (#7997)

This PR is collaborated by Guozhang Wang and John Roesler. It is a significant tech debt cleanup on task management and state management, and is broken down by several sub-tasks listed below:

Extract embedded clients (producer and consumer) into RecordCollector from StreamTask.
guozhangwang#2
guozhangwang#5

Consolidate the standby updating and active restoring logic into ChangelogReader and extract out of StreamThread.
guozhangwang#3
guozhangwang#4

Introduce Task state life cycle (created, restoring, running, suspended, closing), and refactor the task operations based on the current state.
guozhangwang#6
guozhangwang#7

Consolidate AssignedTasks into TaskManager and simplify the logic of changelog management and task management (since they are already moved in step 2) and 3)).
guozhangwang#8
guozhangwang#9

Also simplified the StreamThread logic a bit as the embedded clients / changelog restoration logic has been moved into step 1) and 2).
guozhangwang#10

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Boyang Chen <boyang@confluent.io>",3,0,4,19,160,1,3,44,48,22,2,1.0,48,48,24,4,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Grouped.java,streams/src/main/java/org/apache/kafka/streams/kstream/Grouped.java,"KAFKA-9490: Fix generics for Grouped (#8028)

Reviewers: Andrew Choi <andchoi@linkedin.com>, John Roesler <john@confluent.io>",10,2,5,45,427,2,10,156,157,39,4,4.5,185,157,46,29,21,7,2,1,0,1
core/src/main/scala/kafka/server/DelayedFuture.scala,core/src/main/scala/kafka/server/DelayedFuture.scala,"KAFKA-9027, KAFKA-9028: Convert create/delete acls requests/response to use generated protocol (#7725)

Also add support for flexible versions to both protocol types.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Colin Patrick McCabe <cmccabe@apache.org>

Co-authored-by: Rajini Sivaram <rajinisivaram@googlemail.com>
Co-authored-by: Jason Gustafson <jason@confluent.io>",7,2,2,57,432,0,6,100,100,33,3,1,103,100,34,3,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/StoreChangeLogger.java,streams/src/main/java/org/apache/kafka/streams/state/internals/StoreChangeLogger.java,"MINOR: updated documentation where RocksDBStore was being used as the sample class for byte[] versus Bytes examples (#5884)

Co-authored-by: Guozhang Wang <wangguoz@gmail.com>",4,1,1,39,315,0,4,71,87,4,20,2.0,204,87,10,133,68,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java,clients/src/main/java/org/apache/kafka/common/utils/ThreadUtils.java,"KAFKA-9375: Add names to all Connect threads (#7901)


Reviewers: Mickael Maison <mickael.maison@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>, gcsaba2",2,1,1,23,139,0,1,55,55,28,2,1.0,56,55,28,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/AbstractOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AbstractOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",2,2,2,12,64,0,2,47,46,12,4,1.0,50,46,12,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateAclsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateAclsOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",1,1,1,10,63,0,1,43,34,7,6,1.0,66,34,11,23,19,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteAclsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteAclsOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",1,1,1,10,63,0,1,43,34,7,6,1.0,66,34,11,23,19,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeAclsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeAclsOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",1,1,1,10,69,0,1,42,34,7,6,1.0,65,34,11,23,19,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeClusterOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeClusterOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",3,1,1,17,88,0,3,55,37,7,8,1.0,79,37,10,24,19,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeTopicsOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",3,1,1,18,95,0,3,54,37,8,7,1,78,37,11,24,19,3,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListTopicsOptions.java,"KAFKA-8503; Add default api timeout to AdminClient (KIP-533) (#8011)

This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.

Reviewers: David Arthur <mumrah@gmail.com>

Co-authored-by: huxi <huxi_2b@hotmail.com>",3,1,1,17,90,0,3,61,54,9,7,1,86,54,12,25,19,4,1,0,1,1
core/src/main/scala/kafka/message/CompressionCodec.scala,core/src/main/scala/kafka/message/CompressionCodec.scala,"MINOR: Add explicit result type in public defs/vals (#7993)

Reviewers: Ismael Juma <ismael@juma.me.uk>",20,3,3,74,439,0,5,108,35,8,13,3,146,35,11,38,7,3,2,1,0,1
core/src/main/scala/kafka/metrics/KafkaMetricsConfig.scala,core/src/main/scala/kafka/metrics/KafkaMetricsConfig.scala,"MINOR: Add explicit result type in public defs/vals (#7993)

Reviewers: Ismael Juma <ismael@juma.me.uk>",0,3,2,10,81,0,0,41,38,7,6,2.5,54,38,9,13,5,2,2,1,0,1
core/src/main/scala/kafka/api/ApiUtils.scala,core/src/main/scala/kafka/api/ApiUtils.scala,"KAFKA-9408: Use StandardCharsets.UTF-8 instead of ""UTF-8"" (#7940)

Reviewers: Ron Dagostino <rdagostino@confluent.io>, Ismael Juma <ismael@juma.me.uk>",8,4,5,39,246,3,3,78,92,8,10,1.0,133,92,13,55,22,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ByteUtils.java,clients/src/main/java/org/apache/kafka/common/utils/ByteUtils.java,"KAFKA-9474: Adds 'float64' to the RPC protocol types (#8012)

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",44,40,0,186,1470,4,30,436,324,87,5,1,459,324,92,23,23,5,2,1,0,1
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorHeartbeatConnector.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorHeartbeatConnector.java,"KAFKA-9360: Allow disabling MM2 heartbeat and checkpoint emissions (#7887)

`emit.heartbeats.enabled` and `emit.checkpoints.enabled` are supposed to
be the knobs to control if the heartbeat message or checkpoint message
will be sent or not to the topics respectively. In our experiments,
setting them to false will not suspend the activity in their SourceTasks,
e.g. MirrorHeartbeatTask, MirrorCheckpointTask.

The observations are, when setting those knobs to false, huge volume of
`SourceRecord` are being sent without interval, causing significantly high
CPU usage and GC time  of MirrorMaker 2 instance and congesting the single
partition of the heartbeat topic and checkpoint topic.

The proposed fix in the following PR is to (1) explicitly check if `interval`
is set to negative (e.g. -1), when the `emit.heartbeats.enabled` or
`emit.checkpoints.enabled` is off. (2) if `interval` is indeed set to negative,
no task is created.

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>",10,14,0,50,314,3,9,85,71,42,2,1.5,85,71,42,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicStatus.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicStatus.java,"KAFKA-9422: Track the set of topics a connector is using (KIP-558) (#8017)

This feature corresponds to KIP-558 and extends how the internal status topic (set via `status.storage.topic` distributed worker config) is used to include information that allows Kafka Connect to keep track which topics a connector is using.

The set of topics a connector is actively using, is exposed via a new endpoint that is added to the REST API of Connect workers.
* A `GET /connectors/{name}/topics` request will return the set of topics that have been recorded as active since a connector started or since the set of topics was reset for this connector.

An additional endpoints allows users to reset the set of active topics for a connector via the second endpoint that this feature is adding:
* A `PUT /connectors/{name}/topics/reset` request clears the set of active topics. An operator may enable or disable this feature by setting `topic.tracking.enable` (true by default).

The `topic.tracking.enable` worker config property (true by default) allows an operator to enable/disable the entire feature. Or if the feature is enabled, the `topic.tracking.allow.reset` worker config property (true by default) allows an operator to control whether reset requests submitted to the Connect REST API are allowed.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",14,110,0,57,312,9,9,110,110,110,1,1,110,110,110,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ActiveTopicsInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ActiveTopicsInfo.java,"KAFKA-9422: Track the set of topics a connector is using (KIP-558) (#8017)

This feature corresponds to KIP-558 and extends how the internal status topic (set via `status.storage.topic` distributed worker config) is used to include information that allows Kafka Connect to keep track which topics a connector is using.

The set of topics a connector is actively using, is exposed via a new endpoint that is added to the REST API of Connect workers.
* A `GET /connectors/{name}/topics` request will return the set of topics that have been recorded as active since a connector started or since the set of topics was reset for this connector.

An additional endpoints allows users to reset the set of active topics for a connector via the second endpoint that this feature is adding:
* A `PUT /connectors/{name}/topics/reset` request clears the set of active topics. An operator may enable or disable this feature by setting `topic.tracking.enable` (true by default).

The `topic.tracking.enable` worker config property (true by default) allows an operator to enable/disable the entire feature. Or if the feature is enabled, the `topic.tracking.allow.reset` worker config property (true by default) allows an operator to control whether reset requests submitted to the Connect REST API are allowed.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",3,43,0,20,120,3,3,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryStatusBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryStatusBackingStore.java,"KAFKA-9422: Track the set of topics a connector is using (KIP-558) (#8017)

This feature corresponds to KIP-558 and extends how the internal status topic (set via `status.storage.topic` distributed worker config) is used to include information that allows Kafka Connect to keep track which topics a connector is using.

The set of topics a connector is actively using, is exposed via a new endpoint that is added to the REST API of Connect workers.
* A `GET /connectors/{name}/topics` request will return the set of topics that have been recorded as active since a connector started or since the set of topics was reset for this connector.

An additional endpoints allows users to reset the set of active topics for a connector via the second endpoint that this feature is adding:
* A `PUT /connectors/{name}/topics/reset` request clears the set of active topics. An operator may enable or disable this feature by setting `topic.tracking.enable` (true by default).

The `topic.tracking.enable` worker config property (true by default) allows an operator to enable/disable the entire feature. Or if the feature is enabled, the `topic.tracking.allow.reset` worker config property (true by default) allows an operator to control whether reset requests submitted to the Connect REST API are allowed.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",22,35,0,100,756,5,17,141,105,35,4,3.0,149,105,37,8,7,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/StatusBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/StatusBackingStore.java,"KAFKA-9422: Track the set of topics a connector is using (KIP-558) (#8017)

This feature corresponds to KIP-558 and extends how the internal status topic (set via `status.storage.topic` distributed worker config) is used to include information that allows Kafka Connect to keep track which topics a connector is using.

The set of topics a connector is actively using, is exposed via a new endpoint that is added to the REST API of Connect workers.
* A `GET /connectors/{name}/topics` request will return the set of topics that have been recorded as active since a connector started or since the set of topics was reset for this connector.

An additional endpoints allows users to reset the set of active topics for a connector via the second endpoint that this feature is adding:
* A `PUT /connectors/{name}/topics/reset` request clears the set of active topics. An operator may enable or disable this feature by setting `topic.tracking.enable` (true by default).

The `topic.tracking.enable` worker config property (true by default) allows an operator to enable/disable the entire feature. Or if the feature is enabled, the `topic.tracking.allow.reset` worker config property (true by default) allows an operator to control whether reset requests submitted to the Connect REST API are allowed.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",0,29,0,26,214,0,0,135,100,27,5,3,145,100,29,10,7,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocol.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocol.java,"KAFKA-9405: Use Map.computeIfAbsent where applicable (#7937)

Reviewers: Ismael Juma <ismael@juma.me.uk>",27,2,12,209,1618,1,20,407,246,58,7,4,472,246,67,65,19,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Exit.java,clients/src/main/java/org/apache/kafka/common/utils/Exit.java,"MINOR: MiniKdc JVM shutdown hook fix (#7946)

Also made all shutdown hooks consistent and added tests

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",15,26,0,66,389,4,14,104,79,35,3,4,108,79,36,4,4,1,2,1,0,1
core/src/main/scala/kafka/utils/Exit.scala,core/src/main/scala/kafka/utils/Exit.scala,"MINOR: MiniKdc JVM shutdown hook fix (#7946)

Also made all shutdown hooks consistent and added tests

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",11,11,1,31,303,5,11,63,53,16,4,1.0,66,53,16,3,1,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsNamedRepartitionTest.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsNamedRepartitionTest.java,"MINOR: MiniKdc JVM shutdown hook fix (#7946)

Also made all shutdown hooks consistent and added tests

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",6,3,2,80,978,1,1,122,120,24,5,2,128,120,26,6,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerMetadata.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerMetadata.java,"KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState (#7941)

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",12,2,2,56,379,2,6,83,77,28,3,2,87,77,29,4,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ProducerIdAndEpoch.java,clients/src/main/java/org/apache/kafka/common/utils/ProducerIdAndEpoch.java,"KAFKA-7737; Use single path in producer for initializing the producerId (#7920)

Previously the idempotent producer and transactional producer use separate logic when initializing the producerId. This patch consolidates the two paths. We also do some cleanup in `TransactionManagerTest` to eliminate brittle expectations on `Sender`.

Reviewers: Bob Barrett <bob.barrett@confluent.io>, Viktor Somogyi <viktorsomogyi@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",9,19,0,32,220,2,5,59,36,10,6,1.0,70,36,12,11,7,2,1,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/storage/Converter.java,connect/api/src/main/java/org/apache/kafka/connect/storage/Converter.java,"KAFKA-7273 Clarification on mutability of headers passed to Converter#fromConnectData() (#7489)

Author: Gunnar Morling <gunnar.morling@googlemail.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",2,2,1,16,159,0,2,89,45,10,9,3,118,45,13,29,8,3,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ValueToKey.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ValueToKey.java,"KAFKA-9024: Better error message when field specified does not exist (#7819)

Author: Nigel Liang <nigel@nigelliang.com>
Reviewer: Randall Hauch <rhauch@gmail.com>",12,7,2,79,754,1,6,115,111,29,4,2.0,126,111,32,11,8,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/SchemaBuilder.java,connect/api/src/main/java/org/apache/kafka/connect/data/SchemaBuilder.java,"KAFKA-9083: Various fixes/improvements for Connect's Values class (#7593)

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Greg Harris <gregh@confluent.io>, Randall Hauch <rhauch@gmail.com>",65,20,0,228,1371,4,42,444,371,26,17,2,494,371,29,50,12,3,2,1,0,1
core/src/main/scala/kafka/security/auth/Acl.scala,core/src/main/scala/kafka/security/auth/Acl.scala,"KAFKA-8847; Deprecate and remove usage of supporting classes in kafka.security.auth (#7966)

Removes references to the old scala Acl classes from kafka.security.auth (Acl, Operation, ResourceType, Resource etc.) and replaces these with the Java API. Only the old SimpleAclAuthorizer, AuthorizerWrapper used to wrap legacy authorizer instances and tests using SimpleAclAuthorizer continue to use the old API. Deprecates the old scala API.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",3,21,45,42,346,2,3,86,111,10,9,2,175,111,19,89,45,10,2,1,0,1
core/src/main/scala/kafka/security/auth/PermissionType.scala,core/src/main/scala/kafka/security/auth/PermissionType.scala,"KAFKA-8847; Deprecate and remove usage of supporting classes in kafka.security.auth (#7966)

Removes references to the old scala Acl classes from kafka.security.auth (Acl, Operation, ResourceType, Resource etc.) and replaces these with the Java API. Only the old SimpleAclAuthorizer, AuthorizerWrapper used to wrap legacy authorizer instances and tests using SimpleAclAuthorizer continue to use the old API. Deprecates the old scala API.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",2,4,0,26,185,0,2,50,46,6,8,1.0,70,46,9,20,7,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/MetricConfig.java,clients/src/main/java/org/apache/kafka/common/metrics/MetricConfig.java,"MINOR: Remove unnecessary call to `super` in `MetricConfig` constructor (#7975)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Jason Gustafson <jason@confluent.io>",14,0,1,64,349,1,13,101,71,11,9,3,122,71,14,21,11,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java,"KAFKA-9431: Expose API in KafkaStreams to fetch all local offset lags (#7961)

Add a new method to KafkaStreams to return an estimate of the lags for
all partitions of all local stores.

Implements: KIP-535
Co-authored-by: Navinder Pal Singh Brar <navinder_brar@yahoo.com>
Reviewed-by: John Roesler <vvcephei@apache.org>",94,3,2,435,3174,0,35,557,255,29,19,3,799,274,42,242,59,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java,"KAFKA-9431: Expose API in KafkaStreams to fetch all local offset lags (#7961)

Add a new method to KafkaStreams to return an estimate of the lags for
all partitions of all local stores.

Implements: KIP-535
Co-authored-by: Navinder Pal Singh Brar <navinder_brar@yahoo.com>
Reviewed-by: John Roesler <vvcephei@apache.org>",46,1,1,220,1557,0,23,297,440,8,38,4.0,887,440,23,590,133,16,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/StoreQuerySuite.java,streams/src/test/java/org/apache/kafka/streams/integration/StoreQuerySuite.java,"KAFKA-6144: IQ option to query standbys (#7962)

Add a new overload of KafkaStreams#store that allows users
to query standby and restoring stores in addition to active ones.

Closes: #7962
Implements: KIP-535
Co-authored-by: Navinder Pal Singh Brar <navinder_brar@yahoo.com>
Reviewed-by: John Roesler <vvcephei@apache.org>",0,49,0,21,167,0,0,49,49,49,1,1,49,49,49,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetCommitCallback.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetCommitCallback.java,"KAFKA-9365: Add server side change  to include consumer group information within transaction commit (#7897)

To be able to correctly fence zombie producer txn commit, we propose to add (member.id, group.instance.id, generation) into the transaction commit protocol to raise the same level of correctness guarantee as consumer commit.

Major changes involve:

1. Upgrade transaction commit protocol with (member.id, group.instance.id, generation). The client will fail if the broker is not supporting the new protocol.
2. Refactor group coordinator logic to handle new txn commit errors such as FENCED_INSTANCE_ID, UNKNOWN_MEMBER_ID and ILLEGAL_GENERATION. We loose the check on transaction commit when the member.id is set to empty. This is because the member.id check is an add-on safety for producer commit, and we also need to consider backward compatibility for old producer clients without member.id information. And if producer equips with group.instance.id, then it must provide a valid member.id (not empty definitely), the same as a consumer commit.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,1,1,8,63,0,0,54,33,5,10,1.0,70,33,7,16,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/SchemaException.java,clients/src/main/java/org/apache/kafka/common/protocol/types/SchemaException.java,"KAFKA-9420; Add flexible version support for converted protocols (#7931)

This patch bumps the following APIs in order to enable flexible version support:

- SaslAuthenticate
- SaslHandshake
- CreatePartitions
- DescribeDelegationToken
- ExpireDelegationToken
- RenewDelegationToken

This change was documented and approved in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>",2,3,0,11,70,1,2,35,16,6,6,1.0,40,16,7,5,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnstableOffsetCommitException.java,clients/src/main/java/org/apache/kafka/common/errors/UnstableOffsetCommitException.java,"KAFKA-9346: Consumer back-off logic when fetching pending offsets (#7878)

Let consumer back-off and retry offset fetch when the specific offset topic has pending commits.
The major change lies in the broker side offset fetch logic, where a request configured with flag WaitTransaction to true will be required to back-off when some pending transactional commit is ongoing. This prevents any ongoing transaction being modified by third party, thus guaranteeing the correctness with input partition writer shuffling.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",1,29,0,7,39,1,1,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HoistField.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/HoistField.java,"MINOR: Cleanup generic & throw syntax in connect (#7892)

Reviewers: Jason Gustafson <jason@confluent.io>",12,1,1,85,715,1,10,126,127,25,5,1,160,127,32,34,23,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/CommitFailedException.java,clients/src/main/java/org/apache/kafka/clients/consumer/CommitFailedException.java,"KAFKA-8421: Still return data during rebalance (#7312)

Not wait until updateAssignmentMetadataIfNeeded returns true, but only call it once with 0 timeout. Also do not return empty if in rebalance.

Trim the pre-fetched records after long polling since assignment may have been changed.

Also need to update SubscriptionState to retain the state in assignFromSubscribed if it already exists (similar to assignFromUser), so that we do not need the transition of INITIALIZING to FETCHING.

Unit test: this actually took me the most time :)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Sophie Blee-Goldman <sophie@confluent.io>, Jason Gustafson <jason@confluent.io>, Richard Yu <yohan.richard.yu@gmail.com>, dengziming <dengziming1993@gmail.com>",2,4,0,16,72,1,2,43,35,7,6,1.0,50,35,8,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/RetriableCommitFailedException.java,clients/src/main/java/org/apache/kafka/clients/consumer/RetriableCommitFailedException.java,"KAFKA-8421: Still return data during rebalance (#7312)

Not wait until updateAssignmentMetadataIfNeeded returns true, but only call it once with 0 timeout. Also do not return empty if in rebalance.

Trim the pre-fetched records after long polling since assignment may have been changed.

Also need to update SubscriptionState to retain the state in assignFromSubscribed if it already exists (similar to assignFromUser), so that we do not need the transition of INITIALIZING to FETCHING.

Unit test: this actually took me the most time :)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Sophie Blee-Goldman <sophie@confluent.io>, Jason Gustafson <jason@confluent.io>, Richard Yu <yohan.richard.yu@gmail.com>, dengziming <dengziming1993@gmail.com>",3,0,6,15,87,1,3,37,32,5,7,1,50,32,7,13,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/BranchedMultiLevelRepartitionConnectedTopologyTest.java,streams/src/test/java/org/apache/kafka/streams/integration/BranchedMultiLevelRepartitionConnectedTopologyTest.java,"KAFKA-9335: Fix StreamPartitionAssignor regression in repartition topics counts (#7904)

This PR fixes the regression introduced in 2.4 from 2 refactoring PRs:
#7249
#7419

The bug was introduced by having a logical path leading numPartitionsCandidate to be 0, which is assigned to numPartitions and later being checked by setNumPartitions. In the subsequent check we will throw illegal argument if the numPartitions is 0.

This bug is both impacting new 2.4 application and upgrades to 2.4 in certain types of topology. The example in original JIRA was imported as a new integration test to guard against such regression. We also verify that without the bug fix application will still fail by running this integration test.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,151,0,96,923,2,2,151,151,151,1,1,151,151,151,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/KafkaThread.java,clients/src/main/java/org/apache/kafka/common/utils/KafkaThread.java,"KAFKA-9324: Drop support for Scala 2.11 (KIP-531) (#7859)

* Adjust build and documentation.
* Use lambda syntax for SAM types in `core`, `streams-scala` and
`connect-runtime` modules.
* Remove `runnable` and `newThread` from `CoreUtils` as lambda
syntax for SAM types make them unnecessary.
* Remove stale comment in `FunctionsCompatConversions`,
`KGroupedStream`, `KGroupedTable' and `KStream` about Scala 2.11,
the conversions are needed for Scala 2.12 too.
* Deprecate `org.apache.kafka.streams.scala.kstream.Suppressed`
and use `org.apache.kafka.streams.kstream.Suppressed` instead.
* Use `Admin.create` instead of `AdminClient.create`. Static methods
in Java interfaces can be invoked since Scala 2.12. I noticed that
MirrorMaker 2 uses `AdminClient.create`, but I did not change them
as Connectors have restrictions on newer client APIs.
* Improve efficiency in a few `Gauge` implementations by avoiding
unnecessary intermediate collections.
* Remove pointless `Option.apply` in `ZookeeperClient`
`SessionState` metric.
* Fix unused import/variable and other compiler warnings.
* Reduce visibility of some vals/defs.

Reviewers: Manikumar Reddy <manikumar@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Gwen Shapira <gwen@confluent.io>",5,1,5,24,187,1,5,52,18,6,8,1.0,87,18,11,35,16,4,2,1,0,1
core/src/main/scala/kafka/admin/AdminUtils.scala,core/src/main/scala/kafka/admin/AdminUtils.scala,"KAFKA-9324: Drop support for Scala 2.11 (KIP-531) (#7859)

* Adjust build and documentation.
* Use lambda syntax for SAM types in `core`, `streams-scala` and
`connect-runtime` modules.
* Remove `runnable` and `newThread` from `CoreUtils` as lambda
syntax for SAM types make them unnecessary.
* Remove stale comment in `FunctionsCompatConversions`,
`KGroupedStream`, `KGroupedTable' and `KStream` about Scala 2.11,
the conversions are needed for Scala 2.12 too.
* Deprecate `org.apache.kafka.streams.scala.kstream.Suppressed`
and use `org.apache.kafka.streams.kstream.Suppressed` instead.
* Use `Admin.create` instead of `AdminClient.create`. Static methods
in Java interfaces can be invoked since Scala 2.12. I noticed that
MirrorMaker 2 uses `AdminClient.create`, but I did not change them
as Connectors have restrictions on newer client APIs.
* Improve efficiency in a few `Gauge` implementations by avoiding
unnecessary intermediate collections.
* Remove pointless `Option.apply` in `ZookeeperClient`
`SessionState` metric.
* Fix unused import/variable and other compiler warnings.
* Reduce visibility of some vals/defs.

Reviewers: Manikumar Reddy <manikumar@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Gwen Shapira <gwen@confluent.io>",36,1,1,120,1019,1,6,208,159,2,99,2,1654,197,17,1446,427,15,2,1,0,1
core/src/main/scala/kafka/utils/KafkaScheduler.scala,core/src/main/scala/kafka/utils/KafkaScheduler.scala,"KAFKA-9324: Drop support for Scala 2.11 (KIP-531) (#7859)

* Adjust build and documentation.
* Use lambda syntax for SAM types in `core`, `streams-scala` and
`connect-runtime` modules.
* Remove `runnable` and `newThread` from `CoreUtils` as lambda
syntax for SAM types make them unnecessary.
* Remove stale comment in `FunctionsCompatConversions`,
`KGroupedStream`, `KGroupedTable' and `KStream` about Scala 2.11,
the conversions are needed for Scala 2.12 too.
* Deprecate `org.apache.kafka.streams.scala.kstream.Suppressed`
and use `org.apache.kafka.streams.kstream.Suppressed` instead.
* Use `Admin.create` instead of `AdminClient.create`. Static methods
in Java interfaces can be invoked since Scala 2.12. I noticed that
MirrorMaker 2 uses `AdminClient.create`, but I did not change them
as Connectors have restrictions on newer client APIs.
* Improve efficiency in a few `Gauge` implementations by avoiding
unnecessary intermediate collections.
* Remove pointless `Option.apply` in `ZookeeperClient`
`SessionState` metric.
* Fix unused import/variable and other compiler warnings.
* Reduce visibility of some vals/defs.

Reviewers: Manikumar Reddy <manikumar@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Gwen Shapira <gwen@confluent.io>",14,4,6,80,535,3,8,148,46,5,29,2,303,85,10,155,39,5,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Suppressed.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Suppressed.scala,"KAFKA-9324: Drop support for Scala 2.11 (KIP-531) (#7859)

* Adjust build and documentation.
* Use lambda syntax for SAM types in `core`, `streams-scala` and
`connect-runtime` modules.
* Remove `runnable` and `newThread` from `CoreUtils` as lambda
syntax for SAM types make them unnecessary.
* Remove stale comment in `FunctionsCompatConversions`,
`KGroupedStream`, `KGroupedTable' and `KStream` about Scala 2.11,
the conversions are needed for Scala 2.12 too.
* Deprecate `org.apache.kafka.streams.scala.kstream.Suppressed`
and use `org.apache.kafka.streams.kstream.Suppressed` instead.
* Use `Admin.create` instead of `AdminClient.create`. Static methods
in Java interfaces can be invoked since Scala 2.12. I noticed that
MirrorMaker 2 uses `AdminClient.create`, but I did not change them
as Connectors have restrictions on newer client APIs.
* Improve efficiency in a few `Gauge` implementations by avoiding
unnecessary intermediate collections.
* Remove pointless `Option.apply` in `ZookeeperClient`
`SessionState` metric.
* Fix unused import/variable and other compiler warnings.
* Reduce visibility of some vals/defs.

Reviewers: Manikumar Reddy <manikumar@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Gwen Shapira <gwen@confluent.io>",3,3,3,28,221,0,3,128,128,43,3,2,133,128,44,5,3,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/MaterializedTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/MaterializedTest.java,"KAFKA-9334: Added more unit tests for Materialized class (#7871)

Reviewer: Matthias J. Sax <matthias@confluent.io>",8,53,8,63,479,6,7,99,54,33,3,2,109,54,36,10,8,3,1,0,1,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Grouped.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Grouped.scala,"MINOR: Kafka Streams Scala API cleanup (#7852)

Reviewers: Bill Bejeck <bill@confluent.io>",0,0,1,9,137,0,0,51,36,17,3,1,60,36,20,9,8,3,1,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/StreamJoined.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/StreamJoined.scala,"MINOR: Kafka Streams Scala API cleanup (#7852)

Reviewers: Bill Bejeck <bill@confluent.io>",0,4,5,23,258,0,0,89,90,44,2,3.0,94,90,47,5,5,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/utils/Java.java,clients/src/main/java/org/apache/kafka/common/utils/Java.java,"KAFKA-7251; Add support for TLS 1.3 (#7804)

Adds support for TLSv1.3 in SslTransportLayer. Note that TLSv1.3 is only enabled from Java 11 onwards, so we test the code only when running with Java11 and above.

Tests run on this PR:
  - SslTransportLayerTest: This covers testing of our SslTransportLayer and all tests are run with TLSv1.3 when running with Java 11. These tests are also run with TLSv1.2 for all Java versions.
  - SslFactoryTest: Also run with TLSv1.3 on Java 11 onwards in addition to TLSv1.2 for all Java versions.
  - SslEndToEndAuthorizationTest - Run only with TLSv1.3 on Java 11 onwards and only with TLSv1.2 on earlier Java versions. We have other versions of this test which use SSL that continue to be with TLSv1.2 on Java 11 to avoid reducing test coverage for TLSv1.2

Additional testing for done for TLSv1.3:
  - Most tests that use SSL use TestSslUtils.DEFAULT_TLS_PROTOCOL_FOR_TESTS which is set to TLSv1.2. I have run all clients and core tests with DEFAULT_TLS_PROTOCOL_FOR_TESTS=TLSv1.3 with Java 11.
  - Ran a few system tests locally with TKSv1.3

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>",8,5,0,40,244,1,7,74,44,12,6,1.5,97,44,16,23,18,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/network/SslSender.java,clients/src/test/java/org/apache/kafka/common/network/SslSender.java,"KAFKA-7251; Add support for TLS 1.3 (#7804)

Adds support for TLSv1.3 in SslTransportLayer. Note that TLSv1.3 is only enabled from Java 11 onwards, so we test the code only when running with Java11 and above.

Tests run on this PR:
  - SslTransportLayerTest: This covers testing of our SslTransportLayer and all tests are run with TLSv1.3 when running with Java 11. These tests are also run with TLSv1.2 for all Java versions.
  - SslFactoryTest: Also run with TLSv1.3 on Java 11 onwards in addition to TLSv1.2 for all Java versions.
  - SslEndToEndAuthorizationTest - Run only with TLSv1.3 on Java 11 onwards and only with TLSv1.2 on earlier Java versions. We have other versions of this test which use SSL that continue to be with TLSv1.2 on Java 11 to avoid reducing test coverage for TLSv1.2

Additional testing for done for TLSv1.3:
  - Most tests that use SSL use TestSslUtils.DEFAULT_TLS_PROTOCOL_FOR_TESTS which is set to TLSv1.2. I have run all clients and core tests with DEFAULT_TLS_PROTOCOL_FOR_TESTS=TLSv1.3 with Java 11.
  - Ran a few system tests locally with TKSv1.3

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>",7,4,2,55,399,3,6,85,83,42,2,2.0,87,83,44,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/MetricName.java,clients/src/main/java/org/apache/kafka/common/MetricName.java,"MINOR: trivial cleanups

- Reformat header: `CustomDeserializerTest`, `ReplicaVerificationToolTest`
- Remove unused constructor: `ConsumerGroupDescription`
- Remove unused variables in `TimeOrderedKeyValueBufferTest#shouldRestoreV2Format`
- Remove deprecated `Number` consturctor calls; use `Number#valueOf` instread.

Author: Lee Dongjin <dongjin@apache.org>

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #7202 from dongjinleekr/cleanup/201908",14,2,2,56,360,0,8,132,179,12,11,2,254,179,23,122,64,11,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/StoreChangeLoggerTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/StoreChangeLoggerTest.java,"KAFKA-9113: Extract clients from tasks to record collectors (#7833)

This is part1 of a series of PRs for task management cleanup:

1. Primarily cleanup MockRecordCollectors: remove unnecessary anonymous inheritance but just consolidate on the NoOpRecordCollector -> renamed to MockRecordCollector. Most relevant changes are unit tests that would be relying on this MockRecordCollector.

2. Let StandbyContextImpl#recordCollector() to return null instead of returning a no-op collector, since in standby tasks we should ALWAYS bypass the logging logic and only use the inner store for restoreBatch. Returning null helps us to realize this assertion failed as NPE as early as possible whereas a no-op collector just hides the bug.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,30,56,56,792,2,2,84,146,3,29,2,330,146,11,246,56,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ChannelBuilder.java,clients/src/main/java/org/apache/kafka/common/network/ChannelBuilder.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",0,2,1,11,92,0,0,48,44,6,8,2.0,70,44,9,22,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ChannelMetadataRegistry.java,clients/src/main/java/org/apache/kafka/common/network/ChannelMetadataRegistry.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",0,55,0,9,54,0,0,55,55,55,1,1,55,55,55,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/CipherInformation.java,clients/src/main/java/org/apache/kafka/common/network/CipherInformation.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",13,2,2,37,202,1,6,61,61,30,2,1.0,63,61,32,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/network/ClientInformation.java,clients/src/main/java/org/apache/kafka/common/network/ClientInformation.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",11,65,0,39,220,6,6,65,65,65,1,1,65,65,65,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/DefaultChannelMetadataRegistry.java,clients/src/main/java/org/apache/kafka/common/network/DefaultChannelMetadataRegistry.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",6,50,0,28,121,5,5,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java,clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java,"KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2) (#7749)

Collect and expose the KIP-511 client name and version information the clients now provide to the server as part of ApiVersionsRequests.  Also refactor how we handle selector metrics by creating a ChannelMetadataRegistry class.  This will make it easier for various parts of the networking code to modify channel metrics.

Reviewers: Colin P. McCabe <cmccabe@apache.org>",25,0,6,107,593,1,23,217,217,17,13,2,266,217,20,49,14,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreIterator.java,"KAFKA-9230: Refactor user-customizable Streams metrics (#7762)

As proposed in KIP-444, the user customizable metrics shall be refactored. The refactoring consists of:

* adding methods addLatencyRateTotalSensor and addRateTotalSensor to interface StreamMetrics
* implement the newly added methods in StreamsMetricsImpl
* deprecate methods recordThroughput() and recordLatency() in StreamsMetrics

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,1,1,48,329,1,5,72,77,24,3,1,78,77,26,6,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowedKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredWindowedKeyValueIterator.java,"KAFKA-9230: Refactor user-customizable Streams metrics (#7762)

As proposed in KIP-444, the user customizable metrics shall be refactored. The refactoring consists of:

* adding methods addLatencyRateTotalSensor and addRateTotalSensor to interface StreamMetrics
* implement the newly added methods in StreamsMetricsImpl
* deprecate methods recordThroughput() and recordLatency() in StreamsMetrics

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",6,1,1,54,437,1,6,79,84,26,3,1,85,84,28,6,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ExtractRecordMetadataTimestamp.java,streams/src/main/java/org/apache/kafka/streams/processor/ExtractRecordMetadataTimestamp.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",2,1,1,15,106,0,1,76,77,13,6,2.0,88,77,15,12,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.java,streams/src/main/java/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",1,1,1,20,130,0,1,74,68,11,7,2,87,68,12,13,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestamp.java,streams/src/main/java/org/apache/kafka/streams/processor/LogAndSkipOnInvalidTimestamp.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",1,1,1,14,97,0,1,68,69,11,6,2.0,77,69,13,9,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestamp.java,streams/src/main/java/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestamp.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",2,67,0,16,93,1,1,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/UsePreviousTimeOnInvalidTimestamp.java,streams/src/main/java/org/apache/kafka/streams/processor/UsePreviousTimeOnInvalidTimestamp.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",2,4,2,18,100,0,1,71,70,10,7,2,86,70,12,15,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/WallclockTimestampExtractor.java,streams/src/main/java/org/apache/kafka/streams/processor/WallclockTimestampExtractor.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",1,1,1,8,59,0,1,46,28,4,11,2,65,28,6,19,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestampTest.java,streams/src/test/java/org/apache/kafka/streams/processor/UsePartitionTimeOnInvalidTimestampTest.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",4,56,0,32,228,3,3,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/UsePreviousTimeOnInvalidTimestampTest.java,streams/src/test/java/org/apache/kafka/streams/processor/UsePreviousTimeOnInvalidTimestampTest.java,"KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp (#7633)

Implements KIP-530

Reviewer: Matthias J. Sax <matthias@confluent.io>",4,1,1,33,233,0,3,56,45,11,5,2,64,45,13,8,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/internals/IntGaugeSuite.java,clients/src/main/java/org/apache/kafka/common/metrics/internals/IntGaugeSuite.java,"KAFKA-9091: Add a metric tracking the number of open connections with a given SSL cipher type (#7588)

Reviewers: Tom Bentley <tbentley@redhat.com>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",29,295,0,184,1184,14,14,295,295,295,1,1,295,295,295,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java,clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java,"KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure (#7682)

This patch fixes an NPE in `DefaultMetadataUpdater` due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",0,15,14,20,182,0,0,93,72,8,11,2,133,72,12,40,14,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RecoverableClientException.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RecoverableClientException.java,"KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled (#7748)

Fix three independent causes of threads dying:

1. `ProducerFencedException` isn't properly handed while suspending a task, and leads to the thread dying.
2. `IllegalStateException`: an internal assertion is violated because a store can get orphaned when an exception is thrown during initialization, again leading to the thread dying.
3. `UnknownProducerIdException`: This exception isn't expected by the Streams code, so when we get it, the relevant thread dies. It's not clear whether we always need to catch this, and in the future, we won't expect it at all, but we are catching it now to be sure we're resilient if/when it happens. Important note: this might actually harm performance if the errors turn out to be ignorable, and we will now rebalance instead of ignoring them.

Also, add missing test coverage for the exception handling code.

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <mjsax@apache.org>, Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bill@confluent.io>",1,33,0,7,51,1,1,33,33,33,1,1,33,33,33,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java,"KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled (#7748)

Fix three independent causes of threads dying:

1. `ProducerFencedException` isn't properly handed while suspending a task, and leads to the thread dying.
2. `IllegalStateException`: an internal assertion is violated because a store can get orphaned when an exception is thrown during initialization, again leading to the thread dying.
3. `UnknownProducerIdException`: This exception isn't expected by the Streams code, so when we get it, the relevant thread dies. It's not clear whether we always need to catch this, and in the future, we won't expect it at all, but we are catching it now to be sure we're resilient if/when it happens. Important note: this might actually harm performance if the errors turn out to be ignorable, and we will now rebalance instead of ignoring them.

Also, add missing test coverage for the exception handling code.

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <mjsax@apache.org>, Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bill@confluent.io>",41,35,3,552,4495,4,38,705,412,29,24,6.0,1025,412,43,320,54,13,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java,"MINoR: Replace '>' in RoundRobinAssignor.java into '&gt;' (#7073)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",9,1,1,49,460,0,3,144,114,12,12,2.0,231,114,19,87,34,7,2,1,0,1
tests/kafkatest/services/trogdor/trogdor.py,tests/kafkatest/services/trogdor/trogdor.py,"KAFKA-9123 Test a large number of replicas (#7621)

Two tests using 50k replicas on 8 brokers:
* Do a rolling restart with clean shutdown, delete topics
* Run produce bench and consumer bench on a subset of topics

Reviewed-By: David Jacot <djacot@confluent.io>, Vikas Singh <vikas@confluent.io>, Jason Gustafson <jason@confluent.io>",39,1,1,221,1530,1,27,354,255,71,5,2,383,255,77,29,19,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/CloseableOffsetStorageReader.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/CloseableOffsetStorageReader.java,"KAFKA-9051: Prematurely complete source offset read requests for stopped tasks (#7532)

Prematurely complete source offset read requests for stopped tasks, and added unit tests.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Arjun Satish <arjun@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Jinxin Liu <liukrimhim@gmail.com>, Randall Hauch <rhauch@gmail.com>",0,33,0,8,55,0,0,33,33,33,1,1,33,33,33,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetBackingStore.java,"KAFKA-9051: Prematurely complete source offset read requests for stopped tasks (#7532)

Prematurely complete source offset read requests for stopped tasks, and added unit tests.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Arjun Satish <arjun@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Jinxin Liu <liukrimhim@gmail.com>, Randall Hauch <rhauch@gmail.com>",0,2,6,14,128,0,0,73,74,10,7,3,102,74,15,29,8,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageReaderImpl.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageReaderImpl.java,"KAFKA-9051: Prematurely complete source offset read requests for stopped tasks (#7532)

Prematurely complete source offset read requests for stopped tasks, and added unit tests.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Arjun Satish <arjun@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Jinxin Liu <liukrimhim@gmail.com>, Randall Hauch <rhauch@gmail.com>",17,49,2,119,874,3,4,158,114,16,10,3.0,209,114,21,51,22,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/ConvertingFutureCallback.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/ConvertingFutureCallback.java,"KAFKA-9051: Prematurely complete source offset read requests for stopped tasks (#7532)

Prematurely complete source offset read requests for stopped tasks, and added unit tests.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Arjun Satish <arjun@confluent.io>, Nigel Liang <nigel@nigelliang.com>, Jinxin Liu <liukrimhim@gmail.com>, Randall Hauch <rhauch@gmail.com>",18,47,11,87,473,6,9,120,84,24,5,1,138,84,28,18,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.java,clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerRebalanceListener.java,"HOTFIX: safely clear all active state in onPartitionsLost (#7691)

After a number of last minute bugs were found stemming from the incremental closing of lost tasks in StreamsRebalanceListener#onPartitionsLost, a safer approach to this edge case seems warranted. We initially wanted to be as ""future-proof"" as possible, and avoid baking further protocol assumptions into the code that may be broken as the protocol evolves. This meant that rather than simply closing all active tasks and clearing all associated state in #onPartitionsLost(lostPartitions) we would loop through the lostPartitions/lost tasks and remove them one by one from the various data structures/assignments, then verify that everything was empty in the end. This verification in particular has caused us significant trouble, as it turns out to be nontrivial to determine what should in fact be empty, and if so whether it is also being correctly updated.

Therefore, before worrying about it being ""future-proof"" it seems we should make sure it is ""present-day-proof"" and implement this callback in the safest possible way, by blindly clearing and closing all active task state. We log all the relevant state (at debug level) before clearing it, so we can at least tell from the logs whether/which emptiness checks were being violated.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Andrew Choi <andchoi@linkedin.com>",1,2,1,11,78,0,1,200,72,11,19,3,313,85,16,113,23,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java,"HOTFIX: safely clear all active state in onPartitionsLost (#7691)

After a number of last minute bugs were found stemming from the incremental closing of lost tasks in StreamsRebalanceListener#onPartitionsLost, a safer approach to this edge case seems warranted. We initially wanted to be as ""future-proof"" as possible, and avoid baking further protocol assumptions into the code that may be broken as the protocol evolves. This meant that rather than simply closing all active tasks and clearing all associated state in #onPartitionsLost(lostPartitions) we would loop through the lostPartitions/lost tasks and remove them one by one from the various data structures/assignments, then verify that everything was empty in the end. This verification in particular has caused us significant trouble, as it turns out to be nontrivial to determine what should in fact be empty, and if so whether it is also being correctly updated.

Therefore, before worrying about it being ""future-proof"" it seems we should make sure it is ""present-day-proof"" and implement this callback in the safest possible way, by blindly clearing and closing all active task state. We log all the relevant state (at debug level) before clearing it, so we can at least tell from the logs whether/which emptiness checks were being violated.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Andrew Choi <andchoi@linkedin.com>",9,2,1,54,404,1,4,90,42,13,7,2,98,42,14,8,5,1,2,1,0,1
tests/kafkatest/services/trogdor/degraded_network_fault_spec.py,tests/kafkatest/services/trogdor/degraded_network_fault_spec.py,"KAFKA-8981 Add rate limiting to NetworkDegradeSpec (#7446)

* Add rate limiting to tc

* Feedback from PR

* Add a sanity test for tc

* Add iperf to vagrant scripts

* Dynamically determine the network interface

* Add some temp code for testing on AWS

* Temp: use hostname instead of external IP

* Temp: more AWS debugging

* More AWS WIP

* More AWS temp

* Lower latency some

* AWS wip

* Trying this again now that ping should work

* Add cluster decorator to tests

* Fix broken import

* Fix device name

* Fix decorator arg

* Remove errant import

* Increase timeouts

* Fix tbf command, relax assertion on latency test

* Fix log line

* Final bit of cleanup

* Newline

* Revert Trogdor retry count

* PR feedback

* More PR feedback

* Feedback from PR

* Remove unused argument",2,15,4,15,100,3,2,48,37,24,2,2.0,52,37,26,4,4,2,2,1,0,1
tests/kafkatest/services/trogdor/kibosh.py,tests/kafkatest/services/trogdor/kibosh.py,"KAFKA-8746: Kibosh must handle an empty JSON string from Trogdor (#7155)

When Trogdor wants to clear all the faults injected to Kibosh, it sends the empty JSON object {}. However, Kibosh expects {""faults"":[]} instead.  Kibosh should handle the empty JSON object, since that's consistent with how Trogdor handles empty JSON fields in general (if they're empty, they can be omitted). We should also have a test for this.

Reviewers: David Arthur <mumrah@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",19,6,3,95,762,1,11,156,152,39,4,1.0,160,152,40,4,3,1,2,1,0,1
tests/kafkatest/tests/tools/kibosh_test.py,tests/kafkatest/tests/tools/kibosh_test.py,"KAFKA-8746: Kibosh must handle an empty JSON string from Trogdor (#7155)

When Trogdor wants to clear all the faults injected to Kibosh, it sends the empty JSON object {}. However, Kibosh expects {""faults"":[]} instead.  Kibosh should handle the empty JSON object, since that's consistent with how Trogdor handles empty JSON fields in general (if they're empty, they can be omitted). We should also have a test for this.

Reviewers: David Arthur <mumrah@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",11,7,3,60,506,3,6,85,81,28,3,1,89,81,30,4,3,1,1,0,1,1
tools/src/main/java/org/apache/kafka/trogdor/agent/AgentResourceBinder.java,tools/src/main/java/org/apache/kafka/trogdor/agent/AgentResourceBinder.java,"KAFKA-9165: Fix jersey warnings in Trogdor (#7669)

Reviewers: David Arthur <mumrah@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,44,0,15,86,2,2,44,44,44,1,1,44,44,44,0,0,0,2,1,0,1
tools/src/main/java/org/apache/kafka/trogdor/coordinator/CoordinatorResourceBinder.java,tools/src/main/java/org/apache/kafka/trogdor/coordinator/CoordinatorResourceBinder.java,"KAFKA-9165: Fix jersey warnings in Trogdor (#7669)

Reviewers: David Arthur <mumrah@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,44,0,15,86,2,2,44,44,44,1,1,44,44,44,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala,core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala,"MINOR: refactor replica last sent HW updates due to performance regression (#7671)

This change fixes a performance regression due to follower last seen highwatermark
handling introduced in 23beeea. maybeUpdateHwAndSendResponse is expensive for
brokers with high partition counts, as it requires a partition and a replica lookup for every
partition being fetched. This refactor moves the last seen watermark update into the follower
fetch state update where we have already looked up the partition and replica.

I've seen cases where maybeUpdateHwAndSendResponse is responsible 8% of CPU usage, not including the responseCallback call that is part of it.

I have benchmarked this change with `UpdateFollowerFetchStateBenchmark` and it adds 5ns
of overhead to Partition.updateFollowerFetchState, which is a rounding error compared to the
current overhead of maybeUpdateHwAndSendResponse.

Reviewers: David Arthur <mumrah@gmail.com>, Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",3,2,1,139,1176,1,3,206,210,3,74,2.0,730,210,10,524,190,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressTopologyTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressTopologyTest.java,"KAFKA-9098: When users name repartition topic, use the name for the repartition filter, source and sink node. (#7598)

When users specify a name for a repartition topic, we should use the same name for the repartition filter, source, and sink nodes. With the addition of KIP-307 if users go to the effort of naming every node in the topology having processor nodes with generated names is inconsistent behavior.

Updated tests in the streams test suite.

Reviewers: John Roesler <john@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>",4,14,14,174,1111,0,4,211,210,70,3,3,227,210,76,16,14,5,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/NewPartitionReassignment.java,clients/src/main/java/org/apache/kafka/clients/admin/NewPartitionReassignment.java,"MINOR: Rework NewPartitionReassignment public API (#7638)

This patch removes the NewPartitionReassignment#of() method in favor of a simple constructor. Said method was confusing due to breaking two conventions - always returning a non-empty Optional and thus not being used as a static factory method.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Colin P. McCabe <cmccabe@apache.org>",4,2,7,16,114,2,2,43,44,11,4,2.5,62,44,16,19,11,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ByteBufferUnmapper.java,clients/src/main/java/org/apache/kafka/common/utils/ByteBufferUnmapper.java,"KAFKA-9110: Improve efficiency of disk reads when TLS is enabled (#7604)

1. Avoid a buffer allocation and a buffer copy per file read.
2. Ensure we flush `netWriteBuffer` successfully before reading from
disk to avoid wasted disk reads.
3. 32k reads instead of 8k reads to reduce the number of disk reads
(improves efficiency for magnetic drives and reduces the number of
system calls).
4. Update SslTransportLayer.write(ByteBuffer) to loop until the socket
buffer is full or the src buffer has no remaining bytes.
5. Renamed `MappedByteBuffers` to `ByteBufferUnmapper` since it's also
applicable for direct byte buffers.
6. Skip empty `RecordsSend`
7. Some minor clean-ups for readability.

I ran a simple consumer perf benchmark on a 6 partition topic (large
enough not to fit into page cache) starting from the beginning of the
log with TLS enabled on my 6 core MacBook Pro as a sanity check.
This laptop has fast SSDs so it benefits less from the larger reads
than the case where magnetic disks are used. Consumer throughput
was ~260 MB/s before the changes and ~300 MB/s after
(~15% improvement).

Credit to @junrao  for pointing out that this code could be more efficient.

Reviewers: Jun Rao <junrao@confluent.io>, Colin P. McCabe <cmccabe@apache.org>",13,16,14,83,631,6,8,138,136,69,2,4.5,152,136,76,14,14,7,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/ValueJoiner.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueJoiner.java,"MINOR: Fix Kafka Streams JavaDocs with regard to new StreamJoined class (#7627)

Reviewers: Bruno Cadonna <bruno@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",0,3,3,4,35,0,0,53,23,5,11,2,82,28,7,29,10,3,2,1,0,1
tests/kafkatest/services/performance/streams_performance.py,tests/kafkatest/services/performance/streams_performance.py,"KAFKA-9077: Fix reading of metrics of Streams' SimpleBenchmark (#7610)

With KIP-444 the metrics definitions are refactored. Thus, Streams' SimpleBenchmark needs to be updated to correctly access the refactored metrics.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  Bill Bejeck <bbejeck@gmail.com>",14,1,1,72,493,1,6,108,132,10,11,2,250,132,23,142,105,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/metrics/Sensors.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/metrics/Sensors.java,"KAFKA-8968: Refactor task-level metrics (#7566)

Introduces TaskMetrics class
Introduces dropped-records
Replaces skipped-records with dropped-records with latest built-in
metrics version
Does not add standby-process-ratio and active-process-ratio
Does not refactor parent sensors for processor node metrics

Reviewers: Guozhang Wang <wangguoz@gmail.com>, John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",3,4,35,65,463,3,3,93,43,9,10,4.0,163,45,16,70,35,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsResult.java,"KAFKA-8992; Redefine RemoveMembersFromGroup interface on AdminClient  (#7478)

This PR fixes the inconsistency involved in the `removeMembersFromGroup` admin API calls:

1. Fail the `all()` request when there is sub level error (either partition or member)
2. Change getMembers() to members()
3. Hide the actual Errors from user
4. Do not expose generated MemberIdentity type
5. Use more consistent naming for Options and Result types

Reviewers: Guozhang Wang <wangguoz@gmail.com>, David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>",11,39,26,59,412,5,4,97,84,48,2,5.5,123,84,62,26,26,13,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/MemberToRemove.java,clients/src/main/java/org/apache/kafka/clients/admin/MemberToRemove.java,"KAFKA-8992; Redefine RemoveMembersFromGroup interface on AdminClient  (#7478)

This PR fixes the inconsistency involved in the `removeMembersFromGroup` admin API calls:

1. Fail the `all()` request when there is sub level error (either partition or member)
2. Change getMembers() to members()
3. Hide the actual Errors from user
4. Do not expose generated MemberIdentity type
5. Use more consistent naming for Options and Result types

Reviewers: Guozhang Wang <wangguoz@gmail.com>, David Jacot <djacot@confluent.io>, Jason Gustafson <jason@confluent.io>",6,58,0,31,166,5,5,58,58,58,1,1,58,58,58,0,0,0,2,1,0,1
core/src/main/scala/kafka/controller/Election.scala,core/src/main/scala/kafka/controller/Election.scala,"KAFKA-9089; Reassignment should be resilient to unexpected errors (#7562)

The purpose of this patch is to make the reassignment algorithm simpler and more resilient to unexpected errors. Specifically, it has the following improvements:

1. Remove `ReassignedPartitionContext`. We no longer need to track the previous reassignment through the context and we now use the assignment state as the single source of truth for the target replicas in a reassignment.
2. Remove the intermediate assignment state when overriding a previous reassignment. Instead, an overriding reassignment directly updates the assignment state and shuts down any unneeded replicas. Reassignments are _always_ persisted in Zookeeper before being updated in the controller context.
3. To fix race conditions with concurrent submissions, reassignment completion for a partition always checks for a zk partition reassignment to be removed. This means the controller no longer needs to track the source of the reassignment.
4. Api reassignments explicitly remove reassignment state from zk prior to beginning the new reassignment. This fixes an inconsistency in precedence. Upon controller failover, zookeeper reassignments always take precedence over any active reassignment. So if we do not have the logic to remove the zk reassignment when an api reassignment is triggered, then we can revert to the older zk reassignment.

Reviewers: Viktor Somogyi <viktorsomogyi@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jun Rao <junrao@gmail.com>",15,4,4,90,727,1,8,157,152,31,5,2,196,152,39,39,29,8,2,1,0,1
core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandTest.scala,core/src/test/scala/unit/kafka/admin/ReassignPartitionsCommandTest.scala,"KAFKA-9089; Reassignment should be resilient to unexpected errors (#7562)

The purpose of this patch is to make the reassignment algorithm simpler and more resilient to unexpected errors. Specifically, it has the following improvements:

1. Remove `ReassignedPartitionContext`. We no longer need to track the previous reassignment through the context and we now use the assignment state as the single source of truth for the target replicas in a reassignment.
2. Remove the intermediate assignment state when overriding a previous reassignment. Instead, an overriding reassignment directly updates the assignment state and shuts down any unneeded replicas. Reassignments are _always_ persisted in Zookeeper before being updated in the controller context.
3. To fix race conditions with concurrent submissions, reassignment completion for a partition always checks for a zk partition reassignment to be removed. This means the controller no longer needs to track the source of the reassignment.
4. Api reassignments explicitly remove reassignment state from zk prior to beginning the new reassignment. This fixes an inconsistency in precedence. Upon controller failover, zookeeper reassignments always take precedence over any active reassignment. So if we do not have the logic to remove the zk reassignment when an api reassignment is triggered, then we can revert to the older zk reassignment.

Reviewers: Viktor Somogyi <viktorsomogyi@gmail.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jun Rao <junrao@gmail.com>",47,2,2,426,4595,1,36,582,163,24,24,3.5,870,195,36,288,126,12,2,1,0,1
clients/src/test/java/org/apache/kafka/test/NoRetryException.java,clients/src/test/java/org/apache/kafka/test/NoRetryException.java,"KAFKA-8700: Flaky Test QueryableStateIntegrationTest#queryOnRebalance (#7548)

This is not guaranteed to actually fix queryOnRebalance, since the
failure could never be reproduced locally. I did not bump timeouts
because it looks like that has been done in the past for this test
without success. Instead this change makes the following improvements:

It waits for the application to be in a RUNNING state before
proceeding with the test.

It waits for the remaining instance to return to RUNNING state
within a timeout after rebalance. I observed once that we were able to
do the KV queries but the instance was still in REBALANCING, so this
should reduce some opportunity for flakiness.

The meat of this change: we now iterate over all keys in one shot
(vs. one at a time with a timeout) and collect various failures, all of
which are reported at the end. This should help us to narrow down the
cause of flakiness if it shows up again.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,35,0,11,49,2,2,35,35,35,1,1,35,35,35,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/NetworkReceive.java,clients/src/main/java/org/apache/kafka/common/network/NetworkReceive.java,"KAFKA-9056; Inbound/outbound byte metrics should reflect incomplete sends/receives (#7551)

Currently we only record completed sends and receives in the selector metrics. If there is a disconnect in the middle of the respective operation, then it is not counted. The metrics will be more accurate if we take into account partial sends and receives.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com",31,10,3,118,729,1,14,164,74,13,13,1,248,74,19,84,28,6,2,1,0,1
clients/src/test/java/org/apache/kafka/test/TestCondition.java,clients/src/test/java/org/apache/kafka/test/TestCondition.java,"KAFKA-9056; Inbound/outbound byte metrics should reflect incomplete sends/receives (#7551)

Currently we only record completed sends and receives in the selector metrics. If there is a disconnect in the middle of the respective operation, then it is not counted. The metrics will be more accurate if we take into account partial sends and receives.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com",0,1,1,5,23,0,0,27,26,5,5,1,44,26,9,17,15,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListPartitionReassignmentsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListPartitionReassignmentsResult.java,"MINOR: Add toString to PartitionReassignment (#7579)

This patch adds a `toString()` implementation to `PartitionReassignment`. It also makes the `ListPartitionReassignmentsResult` constructor use default access, which is the standard for the admin client *Result classes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,1,1,13,97,1,2,43,43,22,2,1.0,44,43,22,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/PartitionReassignment.java,clients/src/main/java/org/apache/kafka/clients/admin/PartitionReassignment.java,"MINOR: Add toString to PartitionReassignment (#7579)

This patch adds a `toString()` implementation to `PartitionReassignment`. It also makes the `ListPartitionReassignmentsResult` constructor use default access, which is the standard for the admin client *Result classes.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",5,10,1,30,175,1,5,69,60,34,2,1.5,70,60,35,1,1,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java,"KAFKA-8940: Tighten up SmokeTestDriver (#7565)

After many runs of reproducing the failure (on my local MP5 it takes about 100 - 200 run to get one) I think it is more likely a flaky one and not exposing a real bug in rebalance protocol.

What I've observed is that, when the verifying consumer is trying to fetch from the output topics (there are 11 of them), it poll(1sec) each time, and retries 30 times if there's no more data to fetch and stop. It means that if there are no data fetched from the output topics for 30 * 1 = 30 seconds then the verification would stop (potentially too early). And for the failure cases, we observe consistent rebalancing among the closing / newly created clients since the closing is async, i.e. while new clients are added it is possible that closing clients triggered rebalance are not completed yet (note that each instance is configured with 3 threads, and in the worst case there are 6 instances running / pending shutdown at the same time, so a group fo 3 * 6 = 18 members is possible).

However, there's still a possible bug that in KIP-429, somehow the rebalance can never stabilize and members keep re-rejoining and hence cause it to fail. We have another unit test that have bumped up to 3 rebalance by @ableegoldman and if that failed again then it may be a better confirmation such bug may exist.

So what I've done so far for this test:

1. When closing a client, wait for it to complete closure before moving on to the next iteration and starting a new instance to reduce the rebalance churns.

2. Poll for 5 seconds instead of 1 to wait for longer time: 5 * 30 = 150 seconds, and locally my laptop finished this test in about 50 seconds.

3. Minor debug logging improvement; in fact some of them is to reduce redundant debug logging since it is too long and sometimes hides the key information.

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>",87,52,31,507,4649,9,24,622,495,19,32,2.0,1422,495,44,800,344,25,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/SecurityConfig.java,clients/src/main/java/org/apache/kafka/common/config/SecurityConfig.java,"KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package (#7564)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",0,3,2,7,36,0,0,29,28,14,2,1.0,31,28,16,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProviderCreator.java,clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProviderCreator.java,"KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package (#7564)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",1,3,1,11,81,0,1,43,41,22,2,2.0,44,41,22,1,1,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestPlainSaslServerProviderCreator.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestPlainSaslServerProviderCreator.java,"KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package (#7564)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,1,1,13,75,0,1,34,34,17,2,1.0,35,34,18,1,1,0,1,0,1,1
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestProviderCreator.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestProviderCreator.java,"KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package (#7564)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,1,1,13,75,0,1,34,34,17,2,1.0,35,34,18,1,1,0,1,0,1,1
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestScramSaslServerProviderCreator.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestScramSaslServerProviderCreator.java,"KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package (#7564)

Reviewers: Mickael Maison <mickael.maison@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,1,1,13,75,0,1,34,34,17,2,1.0,35,34,18,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterReplicaLogDirsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterReplicaLogDirsResult.java,"KAFKA-8482: Improve documentation on AdminClient#alterReplicaLogDirs, AlterReplicaLogDirsResult (#7083)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",3,32,10,26,241,0,3,79,57,20,4,2.0,93,57,23,14,10,4,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterConsumerGroupOffsetsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterConsumerGroupOffsetsResult.java,"KAFKA-7689; Add AlterConsumerGroup/List Offsets to AdminClient [KIP-396] (#7296)

This patch implements new AdminClient APIs to list offsets and alter consumer group offsets as documented in KIP-396: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97551484.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",8,96,0,59,464,3,3,96,96,96,1,1,96,96,96,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/IsolationLevel.java,clients/src/main/java/org/apache/kafka/common/IsolationLevel.java,"KAFKA-7689; Add AlterConsumerGroup/List Offsets to AdminClient [KIP-396] (#7296)

This patch implements new AdminClient APIs to list offsets and alter consumer group offsets as documented in KIP-396: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97551484.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",5,1,1,21,97,0,3,42,42,21,2,1.0,43,42,22,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/VoidDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/VoidDeserializer.java,"KAFKA-8455: Add VoidSerde to Serdes (#7485)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",2,27,0,9,53,1,1,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/VoidSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/VoidSerializer.java,"KAFKA-8455: Add VoidSerde to Serdes (#7485)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",1,24,0,7,40,1,1,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
streams/test-utils/src/main/java/org/apache/kafka/streams/TestInputTopic.java,streams/test-utils/src/main/java/org/apache/kafka/streams/TestInputTopic.java,"MINOR: fix typo in TestInputTopic.getTimestampAndAdvance (#7553)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",23,2,2,115,819,3,15,263,256,88,3,2,280,256,93,17,15,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/utils/CompositeStateListener.java,streams/src/test/java/org/apache/kafka/streams/integration/utils/CompositeStateListener.java,"MINOR: Add ability to wait for all instances in an application to be RUNNING (#7500)

Reviewers: Matthias J. Sax <matthias@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>",4,50,0,23,173,3,3,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/SamplingTestPlugin.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/SamplingTestPlugin.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",12,122,0,67,412,7,7,122,122,122,1,1,122,122,122,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/TestPlugins.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/TestPlugins.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",20,267,0,167,1241,10,10,267,267,267,1,1,267,267,267,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/aliased-static-field/test/plugins/AliasedStaticField.java,connect/runtime/src/test/resources/test-plugins/aliased-static-field/test/plugins/AliasedStaticField.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",6,75,0,42,254,6,6,75,75,75,1,1,75,75,75,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/always-throw-exception/test/plugins/AlwaysThrowException.java,connect/runtime/src/test/resources/test-plugins/always-throw-exception/test/plugins/AlwaysThrowException.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",4,53,0,25,166,4,4,53,53,53,1,1,53,53,53,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/sampling-config-provider/test/plugins/SamplingConfigProvider.java,connect/runtime/src/test/resources/test-plugins/sampling-config-provider/test/plugins/SamplingConfigProvider.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",10,101,0,66,386,10,10,101,101,101,1,1,101,101,101,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/sampling-configurable/test/plugins/SamplingConfigurable.java,connect/runtime/src/test/resources/test-plugins/sampling-configurable/test/plugins/SamplingConfigurable.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",7,79,0,47,287,7,7,79,79,79,1,1,79,79,79,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/sampling-converter/test/plugins/SamplingConverter.java,connect/runtime/src/test/resources/test-plugins/sampling-converter/test/plugins/SamplingConverter.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",6,76,0,45,267,6,6,76,76,76,1,1,76,76,76,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/sampling-header-converter/test/plugins/SamplingHeaderConverter.java,connect/runtime/src/test/resources/test-plugins/sampling-header-converter/test/plugins/SamplingHeaderConverter.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",8,89,0,56,321,8,8,89,89,89,1,1,89,89,89,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoadedClass.java,connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoadedClass.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,48,0,20,91,2,2,48,48,48,1,1,48,48,48,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoadedSubclass.java,connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoadedSubclass.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,46,0,19,76,2,2,46,46,46,1,1,46,46,46,0,0,0,2,1,0,1
connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoaderPlugin.java,connect/runtime/src/test/resources/test-plugins/service-loader/test/plugins/ServiceLoaderPlugin.java,"KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins (#7315)

Added plugin isolation unit tests for various scenarios, with a `TestPlugins` class that compiles and builds multiple test plugins without them being on the classpath and verifies that the Plugins and DelegatingClassLoader behave properly. These initially failed for several cases, but now pass since the issues have been fixed.

KAFKA-8340 and KAFKA-8819 are closely related, and this fix corrects the problems reported in both issues.

Author: Greg Harris <gregh@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Magesh Nandakumar <mageshn@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",8,85,0,54,382,7,8,85,85,85,1,1,85,85,85,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/test/ValuelessCallable.java,clients/src/test/java/org/apache/kafka/test/ValuelessCallable.java,"MINOR: Provide better messages when waiting for a condition in test (#7488)

Reviewers: Boyang Chen <boyang@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>",0,25,0,4,21,0,0,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorState.java,connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorState.java,"KAFKA-8945/KAFKA-8947: Fix bugs in Connect REST extension API (#7392)

Fix bug in Connect REST extension API caused by invalid constructor parameter validation, and update integration test to play nicely with Jenkins

Fix instantiation of TaskState objects by Connect framework.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Magesh Nandakumar <mageshn@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,9,0,14,79,1,2,44,35,22,2,1.0,44,35,22,0,0,0,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/health/TaskState.java,connect/api/src/main/java/org/apache/kafka/connect/health/TaskState.java,"KAFKA-8945/KAFKA-8947: Fix bugs in Connect REST extension API (#7392)

Fix bug in Connect REST extension API caused by invalid constructor parameter validation, and update integration test to play nicely with Jenkins

Fix instantiation of TaskState objects by Connect framework.

Author: Chris Egerton <chrise@confluent.io>
Reviewers: Magesh Nandakumar <mageshn@confluent.io>, Randall Hauch <rhauch@gmail.com>",9,15,7,36,206,3,5,77,69,38,2,3.0,84,69,42,7,7,4,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/TestOutputTopic.java,streams/test-utils/src/main/java/org/apache/kafka/streams/TestOutputTopic.java,"MINOR: code and JavaDoc cleanup (#7462)

Reviewers: Jukka Karvanen <jukka.karvanen@jukinimi.com>, Bill Bejeck <bill@confluent.io>",16,19,16,91,670,0,11,201,198,67,3,1,218,198,73,17,16,6,1,0,1,1
streams/test-utils/src/main/java/org/apache/kafka/streams/test/TestRecord.java,streams/test-utils/src/main/java/org/apache/kafka/streams/test/TestRecord.java,"MINOR: code and JavaDoc cleanup (#7462)

Reviewers: Jukka Karvanen <jukka.karvanen@jukinimi.com>, Bill Bejeck <bill@confluent.io>",32,16,29,119,826,3,19,237,250,118,2,8.0,266,250,133,29,29,14,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/AbstractTaskTest.java,"MINOR: unify calls to get committed offsets and metadata (#7463)

Reviewers: Chris Pettitt <cpettitt@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@confluent.io>",8,18,17,188,1632,1,6,242,119,7,35,2,453,133,13,211,54,6,2,1,0,1
core/src/main/scala/kafka/utils/ShutdownableThread.scala,core/src/main/scala/kafka/utils/ShutdownableThread.scala,"KAFKA-7981; Add fetcher and log cleaner thread count metrics (#6514)

This patch adds metrics for failed threads as documented in KIP-434: https://cwiki.apache.org/confluence/display/KAFKA/KIP-434%3A+Add+Replica+Fetcher+and+Log+Cleaner+Count+Metrics.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",15,12,7,65,344,3,5,113,60,9,12,2.0,169,60,14,56,19,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/Sensors.java,streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/Sensors.java,"KAFKA-8964: Rename tag client-id for thread-level metrics and below (#7429)

* Renamed tag client-id to thread-id for thread-level metrics and below
* Corrected metrics tag keys for state store that had suffix ""-id"" instead of ""state-id""

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,1,7,83,602,1,5,117,67,23,5,3,136,68,27,19,7,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/perf/SimpleBenchmark.java,streams/src/test/java/org/apache/kafka/streams/perf/SimpleBenchmark.java,"KAFKA-7245: Deprecate WindowStore#put(key, value) (#7105)

Implements KIP-474.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Matthias J. Sax <matthias@confluent.io>",71,1,1,523,4337,1,30,752,413,16,46,4.0,1772,413,39,1020,523,22,2,1,0,1
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/DefaultReplicationPolicy.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/DefaultReplicationPolicy.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",7,73,0,43,295,4,4,73,73,73,1,1,73,73,73,0,0,0,0,0,0,0
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/MirrorClient.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/MirrorClient.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",32,243,0,168,1264,19,19,243,243,243,1,1,243,243,243,0,0,0,0,0,0,0
connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/ReplicationPolicy.java,connect/mirror-client/src/main/java/org/apache/kafka/connect/mirror/ReplicationPolicy.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",6,60,0,20,132,2,2,60,60,60,1,1,60,60,60,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/ConfigPropertyFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/ConfigPropertyFilter.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",2,37,0,12,83,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/GroupFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/GroupFilter.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",2,37,0,12,83,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorUtils.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",16,116,0,78,684,13,13,116,116,116,1,1,116,116,116,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/OffsetSyncStore.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/OffsetSyncStore.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",9,84,0,53,430,7,7,84,84,84,1,1,84,84,84,0,0,0,0,0,0,0
connect/mirror/src/main/java/org/apache/kafka/connect/mirror/TopicFilter.java,connect/mirror/src/main/java/org/apache/kafka/connect/mirror/TopicFilter.java,"KAFKA-7500: MirrorMaker 2.0 (KIP-382)

Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0)

Author: Ryanne Dolan <ryannedolan@gmail.com>
Author: Arun Mathew <arunmathew88@gmail.com>
Author: In Park <inpark@cloudera.com>
Author: Andre Price <obsoleted@users.noreply.github.com>
Author: christian.hagel@rio.cloud <christian.hagel@rio.cloud>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, William Hammond <william.t.hammond@gmail.com>, Viktor Somogyi <viktorsomogyi@gmail.com>, Jakub Korzeniowski, Tim Carey-Smith, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Arun Mathew, Jeremy-l-ford, vpernin, Oleg Kasian <oleg.kasian@gmail.com>, Mickael Maison <mickael.maison@gmail.com>, Qihong Chen, Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>, Randall Hauch <rhauch@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #6295 from ryannedolan/KIP-382",2,37,0,12,83,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamForeachTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamForeachTest.java,"KAFKA-8233: TopologyTestDriver test input and output usability improvements (#7378)

Implements KIP-470

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",4,3,3,63,657,1,2,95,85,6,16,2.5,188,85,12,93,32,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKTableLeftJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKTableLeftJoinTest.java,"KAFKA-8233: TopologyTestDriver test input and output usability improvements (#7378)

Implements KIP-470

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",13,11,8,140,1314,4,10,201,171,7,30,3.5,501,171,17,300,79,10,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamPeekTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamPeekTest.java,"KAFKA-8233: TopologyTestDriver test input and output usability improvements (#7378)

Implements KIP-470

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,3,3,53,577,1,3,80,85,6,14,3.0,167,85,12,87,38,6,2,1,0,1
streams/test-utils/src/main/java/org/apache/kafka/streams/test/OutputVerifier.java,streams/test-utils/src/main/java/org/apache/kafka/streams/test/OutputVerifier.java,"KAFKA-8233: TopologyTestDriver test input and output usability improvements (#7378)

Implements KIP-470

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",59,4,0,235,1780,0,14,458,241,114,4,2.5,458,241,114,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/types/CompactArrayOf.java,clients/src/main/java/org/apache/kafka/common/protocol/types/CompactArrayOf.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",24,139,0,99,571,12,12,139,139,139,1,1,139,139,139,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/types/RawTaggedField.java,clients/src/main/java/org/apache/kafka/common/protocol/types/RawTaggedField.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",9,56,0,31,181,6,6,56,56,56,1,1,56,56,56,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/types/RawTaggedFieldWriter.java,clients/src/main/java/org/apache/kafka/common/protocol/types/RawTaggedFieldWriter.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",9,80,0,48,310,4,4,80,80,80,1,1,80,80,80,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/types/TaggedFields.java,clients/src/main/java/org/apache/kafka/common/protocol/types/TaggedFields.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",25,181,0,141,1069,10,10,181,181,181,1,1,181,181,181,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/StructSpec.java,generator/src/main/java/org/apache/kafka/message/StructSpec.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",13,27,2,67,452,1,6,99,74,33,3,1,101,74,34,2,2,1,2,1,0,1
generator/src/main/java/org/apache/kafka/message/VersionConditional.java,generator/src/main/java/org/apache/kafka/message/VersionConditional.java,"KAFKA-8885; The Kafka Protocol should Support Optional Tagged Fields (#7325)

This patch implements support for optional (tagged) fields in the Kafka protocol as documented in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TypeClasses.

Reviewers: David Jacot <djacot@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",35,6,6,159,950,3,12,220,220,110,2,3.5,226,220,113,6,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Bytes.java,clients/src/main/java/org/apache/kafka/common/utils/Bytes.java,"KAFKA-3705 Added a foreignKeyJoin implementation for KTable. (#5527)

https://issues.apache.org/jira/browse/KAFKA-3705

Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Jan Filipiak <Jan.Filipiak@trivago.com>, pgwhalen, Alexei Daniline",34,27,1,116,855,1,11,210,178,23,9,1,220,178,24,10,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKey.java,"KAFKA-3705 Added a foreignKeyJoin implementation for KTable. (#5527)

https://issues.apache.org/jira/browse/KAFKA-3705

Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Jan Filipiak <Jan.Filipiak@trivago.com>, pgwhalen, Alexei Daniline",7,55,0,31,172,5,5,55,55,55,1,1,55,55,55,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapper.java,"KAFKA-3705 Added a foreignKeyJoin implementation for KTable. (#5527)

https://issues.apache.org/jira/browse/KAFKA-3705

Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Jan Filipiak <Jan.Filipiak@trivago.com>, pgwhalen, Alexei Daniline",7,62,0,37,208,6,6,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapper.java,"KAFKA-3705 Added a foreignKeyJoin implementation for KTable. (#5527)

https://issues.apache.org/jira/browse/KAFKA-3705

Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Jan Filipiak <Jan.Filipiak@trivago.com>, pgwhalen, Alexei Daniline",13,111,0,67,379,10,10,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java,"KAFKA-3705 Added a foreignKeyJoin implementation for KTable. (#5527)

https://issues.apache.org/jira/browse/KAFKA-3705

Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Christopher Pettitt <cpettitt@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Jan Filipiak <Jan.Filipiak@trivago.com>, pgwhalen, Alexei Daniline",6,54,0,32,214,2,2,54,54,54,1,1,54,54,54,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SessionKey.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SessionKey.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",9,73,0,31,172,5,5,73,73,73,1,1,73,73,73,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocolCompatibility.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocolCompatibility.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",13,61,3,70,362,3,10,148,90,74,2,4.5,151,90,76,3,3,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeConnectProtocol.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeConnectProtocol.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",12,38,11,124,1224,4,6,274,247,137,2,6.5,285,247,142,11,11,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/InternalRequestSignature.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/InternalRequestSignature.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",23,148,0,99,703,9,9,148,148,148,1,1,148,148,148,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/storage/ConfigBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/ConfigBackingStore.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",1,9,0,31,256,1,1,133,126,27,5,3,145,126,29,12,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTestUtils.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTestUtils.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",15,1,0,149,1202,1,15,193,192,96,2,1.0,193,192,96,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocolCompatibilityTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocolCompatibilityTest.java,"KAFKA-8804: Secure internal Connect REST endpoints (#7310)

Implemented KIP-507 to secure the internal Connect REST endpoints that are only for intra-cluster communication. A new V2 of the Connect subprotocol enables this feature, where the leader generates a new session key, shares it with the other workers via the configuration topic, and workers send and validate requests to these internal endpoints using the shared key.

Currently the internal `POST /connectors/<connector>/tasks` endpoint is the only one that is secured.

This change adds unit tests and makes some small alterations to system tests to target the new `sessioned` Connect subprotocol. A new integration test ensures that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a 400 BAD RESPONSE status).

Author: Chris Egerton <chrise@confluent.io>
Reviewed: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",12,25,2,211,2094,5,12,259,236,130,2,2.5,261,236,130,2,2,1,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/LoggingResource.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/LoggingResource.java,"KAFKA-7772: Dynamically Adjust Log Levels in Connect (#7403)

Implemented KIP-495 to expose a new `admin/loggers` endpoint for the Connect REST API that lists the current log levels and allows the caller to change log levels. 

Author: Arjun Satish <arjun@confluent.io>
Reviewer: Randall Hauch <rhauch@gmail.com>",22,206,0,128,952,8,8,206,206,206,1,1,206,206,206,0,0,0,0,0,0,0
connect/json/src/main/java/org/apache/kafka/connect/json/DecimalFormat.java,connect/json/src/main/java/org/apache/kafka/connect/json/DecimalFormat.java,"KAFKA-8595: Support deserialization of JSON decimals encoded in NUMERIC  (#7354)

Implemented KIP-481 by adding support for deserializing Connect DECIMAL values encoded in JSON as numbers, in addition to raw byte array (base64) format used previously.

Author: Almog Gavra <almog@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",0,36,0,5,19,0,0,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverterConfig.java,connect/json/src/main/java/org/apache/kafka/connect/json/JsonConverterConfig.java,"KAFKA-8595: Support deserialization of JSON decimals encoded in NUMERIC  (#7354)

Implemented KIP-481 by adding support for deserializing Connect DECIMAL values encoded in JSON as numbers, in addition to raw byte array (base64) format used previously.

Author: Almog Gavra <almog@confluent.io>
Reviewers: Chris Egerton <chrise@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",5,37,2,63,470,4,5,114,79,57,2,4.5,116,79,58,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/InvalidRecordException.java,clients/src/main/java/org/apache/kafka/common/InvalidRecordException.java,"KAFKA-8729, pt 2: Add error_records and error_message to PartitionResponse (#7150)

As noted in the KIP-467, the updated ProduceResponse is

```
Produce Response (Version: 8) => [responses] throttle_time_ms
  responses => topic [partition_responses]
    topic => STRING
    partition_responses => partition error_code base_offset log_append_time log_start_offset
      partition => INT32
      error_code => INT16
      base_offset => INT64
      log_append_time => INT64
      log_start_offset => INT64
      error_records => [INT32]         // new field, encodes the relative offset of the records that caused error
      error_message => STRING          // new field, encodes the error message that client can use to log itself
    throttle_time_ms => INT32
with a new error code:
```

INVALID_RECORD(86, ""Some record has failed the validation on broker and hence be rejected."", InvalidRecordException::new);

Reviewers: Jason Gustafson <jason@confluent.io>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",2,3,3,11,68,0,2,33,16,4,8,1.0,43,16,5,10,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/EndTransactionMarker.java,clients/src/main/java/org/apache/kafka/common/record/EndTransactionMarker.java,"KAFKA-8729, pt 2: Add error_records and error_message to PartitionResponse (#7150)

As noted in the KIP-467, the updated ProduceResponse is

```
Produce Response (Version: 8) => [responses] throttle_time_ms
  responses => topic [partition_responses]
    topic => STRING
    partition_responses => partition error_code base_offset log_append_time log_start_offset
      partition => INT32
      error_code => INT16
      base_offset => INT64
      log_append_time => INT64
      log_start_offset => INT64
      error_records => [INT32]         // new field, encodes the relative offset of the records that caused error
      error_message => STRING          // new field, encodes the error message that client can use to log itself
    throttle_time_ms => INT32
with a new error code:
```

INVALID_RECORD(86, ""Some record has failed the validation on broker and hence be rejected."", InvalidRecordException::new);

Reviewers: Jason Gustafson <jason@confluent.io>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",20,1,0,83,606,0,10,125,124,62,2,1.0,125,124,62,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/LegacyRecord.java,clients/src/main/java/org/apache/kafka/common/record/LegacyRecord.java,"KAFKA-8729, pt 2: Add error_records and error_message to PartitionResponse (#7150)

As noted in the KIP-467, the updated ProduceResponse is

```
Produce Response (Version: 8) => [responses] throttle_time_ms
  responses => topic [partition_responses]
    topic => STRING
    partition_responses => partition error_code base_offset log_append_time log_start_offset
      partition => INT32
      error_code => INT16
      base_offset => INT64
      log_append_time => INT64
      log_start_offset => INT64
      error_records => [INT32]         // new field, encodes the relative offset of the records that caused error
      error_message => STRING          // new field, encodes the error message that client can use to log itself
    throttle_time_ms => INT32
with a new error code:
```

INVALID_RECORD(86, ""Some record has failed the validation on broker and hence be rejected."", InvalidRecordException::new);

Reviewers: Jason Gustafson <jason@confluent.io>, Magnus Edenhill <magnus@edenhill.se>, Guozhang Wang <wangguoz@gmail.com>",87,3,2,338,2364,1,42,577,570,115,5,3,607,570,121,30,19,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/PartitionGrouper.java,streams/src/main/java/org/apache/kafka/streams/processor/PartitionGrouper.java,"KAFKA-8927: Deprecate PartitionGrouper interface (#7376)

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,3,0,9,80,0,0,55,55,4,13,1,102,55,8,47,26,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/SingleGroupPartitionGrouperStub.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/SingleGroupPartitionGrouperStub.java,"KAFKA-8927: Deprecate PartitionGrouper interface (#7376)

Reviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,5,6,21,224,1,1,45,47,11,4,3.0,62,47,16,17,7,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/AbstractControlRequest.java,clients/src/main/java/org/apache/kafka/common/requests/AbstractControlRequest.java,"KAFKA-8471: Replace control requests/responses with automated protocol (#7353)

Replaced UpdateMetadata{Request, Response}, LeaderAndIsr{Request, Response}
and StopReplica{Request, Response} with the automated protocol classes.

Updated the JSON schema for the 3 request types to be more consistent and
less strict (if needed to avoid duplication).

The general approach is to avoid generating new collections in the request
classes. Normalization happens in the constructor to make this possible. Builders
still have to group by topic to maintain the external ungrouped view.

Introduced new tests for LeaderAndIsrRequest and UpdateMetadataRequest to
verify that the new logic is correct.

A few other clean-ups/fixes in code that was touched due to these changes:
* KAFKA-8956: Refactor DelayedCreatePartitions#updateWaiting to avoid modifying
collection in foreach.
* Avoid unnecessary allocation for state change trace logging if trace logging is not enabled
* Use `toBuffer` instead of `toList`, `toIndexedSeq` or `toSeq` as it generally performs
better and it matches the performance characteristics of `java.util.ArrayList`. This is
particularly important when passing such instances to Java code.
* Minor refactoring for clarity and readability.
* Removed usage of deprecated `/:`, unused imports and unnecessary `var`s.
* Include exception in `AdminClientIntegrationTest` failure message.
* Move StopReplicaRequest verification in `AuthorizerIntegrationTest` to the end
to match the comment.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>",2,6,36,22,159,7,2,50,80,25,2,4.0,86,80,43,36,36,18,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/FlattenedIterator.java,clients/src/main/java/org/apache/kafka/common/utils/FlattenedIterator.java,"KAFKA-8471: Replace control requests/responses with automated protocol (#7353)

Replaced UpdateMetadata{Request, Response}, LeaderAndIsr{Request, Response}
and StopReplica{Request, Response} with the automated protocol classes.

Updated the JSON schema for the 3 request types to be more consistent and
less strict (if needed to avoid duplication).

The general approach is to avoid generating new collections in the request
classes. Normalization happens in the constructor to make this possible. Builders
still have to group by topic to maintain the external ungrouped view.

Introduced new tests for LeaderAndIsrRequest and UpdateMetadataRequest to
verify that the new logic is correct.

A few other clean-ups/fixes in code that was touched due to these changes:
* KAFKA-8956: Refactor DelayedCreatePartitions#updateWaiting to avoid modifying
collection in foreach.
* Avoid unnecessary allocation for state change trace logging if trace logging is not enabled
* Use `toBuffer` instead of `toList`, `toIndexedSeq` or `toSeq` as it generally performs
better and it matches the performance characteristics of `java.util.ArrayList`. This is
particularly important when passing such instances to Java code.
* Minor refactoring for clarity and readability.
* Removed usage of deprecated `/:`, unused imports and unnecessary `var`s.
* Include exception in `AdminClientIntegrationTest` failure message.
* Move StopReplicaRequest verification in `AuthorizerIntegrationTest` to the end
to match the comment.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>",5,45,0,22,163,2,2,45,45,45,1,1,45,45,45,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/MappedIterator.java,clients/src/main/java/org/apache/kafka/common/utils/MappedIterator.java,"KAFKA-8471: Replace control requests/responses with automated protocol (#7353)

Replaced UpdateMetadata{Request, Response}, LeaderAndIsr{Request, Response}
and StopReplica{Request, Response} with the automated protocol classes.

Updated the JSON schema for the 3 request types to be more consistent and
less strict (if needed to avoid duplication).

The general approach is to avoid generating new collections in the request
classes. Normalization happens in the constructor to make this possible. Builders
still have to group by topic to maintain the external ungrouped view.

Introduced new tests for LeaderAndIsrRequest and UpdateMetadataRequest to
verify that the new logic is correct.

A few other clean-ups/fixes in code that was touched due to these changes:
* KAFKA-8956: Refactor DelayedCreatePartitions#updateWaiting to avoid modifying
collection in foreach.
* Avoid unnecessary allocation for state change trace logging if trace logging is not enabled
* Use `toBuffer` instead of `toList`, `toIndexedSeq` or `toSeq` as it generally performs
better and it matches the performance characteristics of `java.util.ArrayList`. This is
particularly important when passing such instances to Java code.
* Minor refactoring for clarity and readability.
* Removed usage of deprecated `/:`, unused imports and unnecessary `var`s.
* Include exception in `AdminClientIntegrationTest` failure message.
* Move StopReplicaRequest verification in `AuthorizerIntegrationTest` to the end
to match the comment.

Reviewers: Colin Patrick McCabe <cmccabe@apache.org>",3,44,0,19,135,3,3,44,44,44,1,1,44,44,44,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java,"KAFKA-6883: Add toUpperCase support to sasl.kerberos.principal.to.local rule (KIP-309)

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #7375 from omkreddy/KAFKA-6883-KerberosShortNamer",33,10,1,141,897,5,6,206,189,34,6,3.5,225,189,38,19,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosShortNamer.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosShortNamer.java,"KAFKA-6883: Add toUpperCase support to sasl.kerberos.principal.to.local rule (KIP-309)

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #7375 from omkreddy/KAFKA-6883-KerberosShortNamer",14,3,2,63,526,1,5,106,103,15,7,3,161,103,23,55,31,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/Endpoint.java,clients/src/main/java/org/apache/kafka/common/Endpoint.java,"MINOR: Address review comments for KIP-504 authorizer changes (#7379)

Reviewers: Manikumar Reddy <manikumar@confluent.io>",13,12,9,57,318,7,8,104,101,52,2,4.5,113,101,56,9,9,4,1,0,1,1
core/src/main/scala/kafka/cluster/EndPoint.scala,core/src/main/scala/kafka/cluster/EndPoint.scala,"MINOR: Address review comments for KIP-504 authorizer changes (#7379)

Reviewers: Manikumar Reddy <manikumar@confluent.io>",5,4,5,39,328,2,2,78,78,6,12,1.0,126,78,10,48,32,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestConverterWithHeaders.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestConverterWithHeaders.java,"KAFKA-7273: Extend Connect Converter to support headers (#6362)

Implemented KIP-440 to allow Connect converters to use record headers when serializing or deserializing keys and values. This change is backward compatible in that the new methods default to calling the older existing methods, so existing Converter implementations need not be changed. This changes the WorkerSinkTask and WorkerSourceTask to use the new converter methods, but Connect's existing Converter implementations and the use of converters for internal topics are intentionally not modified. Added unit tests.

Author: Yaroslav Tkachenko <sapiensy@gmail.com>
Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Randall Hauch <rhauch@gmail.com>",9,80,0,48,364,6,6,80,80,80,1,1,80,80,80,0,0,0,0,0,0,0
generator/src/main/java/org/apache/kafka/message/ClauseGenerator.java,generator/src/main/java/org/apache/kafka/message/ClauseGenerator.java,"MINOR: improve the Kafka RPC code generator (#7340)

Move the generator checkstyle suppressions to a special section, rather
than mixing them in with the other sections.  For generated code, do not
complain about variable names or cyclic complexity.

FieldType.java: remove isInteger since it isn't used anywhere.  This way, we
don't have to decide whether a UUID is an integer or not (there are arguments
for both choices).  Add FieldType#serializationIsDifferentInFlexibleVersions
and FieldType#isVariableLength.

HeaderGenerator: add the ability to generate static imports.  Add
IsNullConditional, VersionConditional, and ClauseGenerator as easier ways of
generating ""if"" statements.",0,25,0,4,21,0,0,25,25,25,1,1,25,25,25,0,0,0,2,1,0,1
generator/src/main/java/org/apache/kafka/message/HeaderGenerator.java,generator/src/main/java/org/apache/kafka/message/HeaderGenerator.java,"MINOR: improve the Kafka RPC code generator (#7340)

Move the generator checkstyle suppressions to a special section, rather
than mixing them in with the other sections.  For generated code, do not
complain about variable names or cyclic complexity.

FieldType.java: remove isInteger since it isn't used anywhere.  This way, we
don't have to decide whether a UUID is an integer or not (there are arguments
for both choices).  Add FieldType#serializationIsDifferentInFlexibleVersions
and FieldType#isVariableLength.

HeaderGenerator: add the ability to generate static imports.  Add
IsNullConditional, VersionConditional, and ClauseGenerator as easier ways of
generating ""if"" statements.",9,13,0,63,319,3,5,94,78,24,4,2.5,97,78,24,3,3,1,2,1,0,1
generator/src/main/java/org/apache/kafka/message/Versions.java,generator/src/main/java/org/apache/kafka/message/Versions.java,"MINOR: improve the Kafka RPC code generator (#7340)

Move the generator checkstyle suppressions to a special section, rather
than mixing them in with the other sections.  For generated code, do not
complain about variable names or cyclic complexity.

FieldType.java: remove isInteger since it isn't used anywhere.  This way, we
don't have to decide whether a UUID is an integer or not (there are arguments
for both choices).  Add FieldType#serializationIsDifferentInFlexibleVersions
and FieldType#isVariableLength.

HeaderGenerator: add the ability to generate static imports.  Add
IsNullConditional, VersionConditional, and ClauseGenerator as easier ways of
generating ""if"" statements.",37,53,0,120,744,1,13,199,139,50,4,1.0,199,139,50,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/ArrayOf.java,clients/src/main/java/org/apache/kafka/common/protocol/types/ArrayOf.java,"MINOR: Improve the org.apache.kafka.common.protocol code (#7344)

Add UUID to the list of types documented in Type#toHtml.

Type, Protocol, ArrayOf: use Type#isArray and Type#arrayElementType rather than typecasting to handle arrays.  This is cleaner.  It will also make it easier for us to add compact arrays (as specified by KIP-482) as a new array type distinct from the old array type.

Add MessageUtil#byteBufferToArray, as well as a test for it.  We will need this for handling tagged fields of type ""bytes"".

Schema#Visitor: we don't need a separate function overload for visiting arrays. We can just call ""visit(Type field)"".

TestUUID.json: reformat the JSON file to match the others.

ProtocolSerializationTest: improve the error messages on failure.  Check that each type has the name we expect it to have.

Reviewers: David Arthur <mumrah@gmail.com>, José Armando García Sancio <jsancio@gmail.com>, Vikas Singh <soondenana@users.noreply.github.com>",25,4,2,92,538,2,12,133,63,13,10,1.5,149,63,15,16,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/Murmur3.java,streams/src/main/java/org/apache/kafka/streams/state/internals/Murmur3.java,MINOR: Move Murmur3 to Streams,62,1,1,367,3298,0,23,548,548,274,2,1.0,549,548,274,1,1,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/Murmur3Test.java,streams/src/test/java/org/apache/kafka/streams/state/internals/Murmur3Test.java,MINOR: Move Murmur3 to Streams,4,1,1,37,459,0,2,70,70,35,2,1.0,71,70,36,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupOffsetsOptions.java,"KAFKA-8730; Add API to delete consumer offsets (KIP-496) (#7276)

This adds an administrative API to delete consumer offsets of a group as well as extends the mechanism to expire offsets of consumer groups.

It makes the group coordinator aware of the set of topics a consumer group (protocol type == 'consumer') is actively subscribed to, allowing offsets of topics which are not actively subscribed to by the group to be either expired or administratively deleted. The expiration rules remain the same.

For the other groups (non-consumer), the API allows to delete offsets when the group is empty and the expiration remains the same.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",0,30,0,6,45,0,0,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/GroupSubscribedToTopicException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupSubscribedToTopicException.java,"KAFKA-8730; Add API to delete consumer offsets (KIP-496) (#7276)

This adds an administrative API to delete consumer offsets of a group as well as extends the mechanism to expire offsets of consumer groups.

It makes the group coordinator aware of the set of topics a consumer group (protocol type == 'consumer') is actively subscribed to, allowing offsets of topics which are not actively subscribed to by the group to be either expired or administratively deleted. The expiration rules remain the same.

For the other groups (non-consumer), the API allows to delete offsets when the group is empty and the expiration remains the same.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/message/TestUUIDDataTest.java,clients/src/test/java/org/apache/kafka/common/message/TestUUIDDataTest.java,"MINOR: Add UUID type to Kafka API code generation (#7291)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",5,86,0,54,447,5,5,86,86,86,1,1,86,86,86,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/RemoveMemberFromConsumerGroupOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/RemoveMemberFromConsumerGroupOptions.java,"KAFKA-8222 & KIP-345 part 5: admin request to batch remove members (#7122)

This PR adds supporting features for static membership. It could batch remove consumers from the group with provided group.instance.id list.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,50,0,21,159,2,2,50,50,50,1,1,50,50,50,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/RemoveMemberFromGroupResult.java,clients/src/main/java/org/apache/kafka/clients/admin/RemoveMemberFromGroupResult.java,"KAFKA-8222 & KIP-345 part 5: admin request to batch remove members (#7122)

This PR adds supporting features for static membership. It could batch remove consumers from the group with provided group.instance.id list.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",8,85,0,51,384,4,4,85,85,85,1,1,85,85,85,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/MembershipChangeResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/MembershipChangeResultTest.java,"KAFKA-8222 & KIP-345 part 5: admin request to batch remove members (#7122)

This PR adds supporting features for static membership. It could batch remove consumers from the group with provided group.instance.id list.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",2,50,0,27,201,1,1,50,50,50,1,1,50,50,50,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/admin/RemoveMemberFromGroupResultTest.java,clients/src/test/java/org/apache/kafka/clients/admin/RemoveMemberFromGroupResultTest.java,"KAFKA-8222 & KIP-345 part 5: admin request to batch remove members (#7122)

This PR adds supporting features for static membership. It could batch remove consumers from the group with provided group.instance.id list.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",12,154,0,122,922,4,4,154,154,154,1,1,154,154,154,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/server/authorizer/AclCreateResult.java,clients/src/main/java/org/apache/kafka/server/authorizer/AclCreateResult.java,"KAFKA-8866; Return exceptions as Optional<ApiException> in authorizer API (#7294)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>",4,6,11,18,122,3,3,45,50,22,2,2.5,56,50,28,11,11,6,1,0,1,1
clients/src/main/java/org/apache/kafka/server/authorizer/AclDeleteResult.java,clients/src/main/java/org/apache/kafka/server/authorizer/AclDeleteResult.java,"KAFKA-8866; Return exceptions as Optional<ApiException> in authorizer API (#7294)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>",11,12,14,45,289,3,9,95,97,48,2,3.5,109,97,54,14,14,7,1,0,1,1
clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizationResult.java,clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizationResult.java,"KAFKA-8760; New Java Authorizer API (KIP-504) (#7268)

New Java Authorizer API and a new out-of-the-box authorizer (AclAuthorizer) that implements the new interface.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",0,26,0,7,36,0,0,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizerServerInfo.java,clients/src/main/java/org/apache/kafka/server/authorizer/AuthorizerServerInfo.java,"KAFKA-8760; New Java Authorizer API (KIP-504) (#7268)

New Java Authorizer API and a new out-of-the-box authorizer (AclAuthorizer) that implements the new interface.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",0,51,0,12,85,0,0,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
core/src/main/scala/kafka/security/auth/Authorizer.scala,core/src/main/scala/kafka/security/auth/Authorizer.scala,"KAFKA-8760; New Java Authorizer API (KIP-504) (#7268)

New Java Authorizer API and a new out-of-the-box authorizer (AclAuthorizer) that implements the new interface.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",0,1,0,15,158,0,0,149,81,15,10,1.5,190,81,19,41,13,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionBytesStoreSupplier.java,"MINOR: Refactor tag key for store level metrics (#7257)

The tag key for store level metrics specified in StreamsMetricsImpl
is unified with the tag keys on thread and task level.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",6,1,1,33,171,1,6,59,59,30,2,1.0,60,59,30,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbKeyValueBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbKeyValueBytesStoreSupplier.java,"MINOR: Refactor tag key for store level metrics (#7257)

The tag key for store level metrics specified in StreamsMetricsImpl
is unified with the tag keys on thread and task level.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",5,1,1,27,157,1,4,50,48,10,5,1,58,48,12,8,4,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDbSessionBytesStoreSupplier.java,"MINOR: Refactor tag key for store level metrics (#7257)

The tag key for store level metrics specified in StreamsMetricsImpl
is unified with the tag keys on thread and task level.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",6,1,1,39,199,1,6,64,59,6,10,1.0,85,59,8,21,7,2,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/cache/LRUCacheBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/cache/LRUCacheBenchmark.java,"MINOR: Fix integer overflow in LRUCacheBenchmark (#7270)

The jmh LRUCacheBenchmark will exhibit an int overflow when run on a fast machine:

```
java.lang.ArrayIndexOutOfBoundsException: Index -3648 out of bounds for length 10000
	at org.apache.kafka.jmh.cache.LRUCacheBenchmark.testCachePerformance(LRUCacheBenchmark.java:70)
	at org.apache.kafka.jmh.cache.generated.LRUCacheBenchmark_testCachePerformance_jmhTest.testCachePerformance_thrpt_jmhStub(LRUCacheBenchmark_testCachePerformance_jmhTest.java:119)
	at org.apache.kafka.jmh.cache.generated.LRUCacheBenchmark_testCachePerformance_jmhTest.testCachePerformance_Throughput(LRUCacheBenchmark_testCachePerformance_jmhTest.java:83)
```

Reviewers: Jason Gustafson <jason@confluent.io>",4,2,2,47,393,1,3,84,68,28,3,2,92,68,31,8,6,3,2,1,0,1
core/src/main/scala/kafka/server/KafkaServerStartable.scala,core/src/main/scala/kafka/server/KafkaServerStartable.scala,"KAFKA-8837: KafkaMetricReporterClusterIdTest may not shutdown ZooKeeperTestHarness (#7255)

- Call `assertNoNonDaemonThreads` in test method instead of tear down method
to avoid situation where parent's class tear down is not invoked.
- Pass the thread prefix in tests that call `assertNoNonDaemonThreads` so that it
works correctly.
- Rename `verifyNonDaemonThreadsStatus` to `assertNoNonDaemonThreads` to
make it clear that it may throw.

Reviewers: Anna Povzner <anna@confluent.io>, Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>",11,8,4,38,263,2,7,-91,96,-3,33,2,406,108,12,497,168,15,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java,"KAFKA-8179: Part 4, add CooperativeStickyAssignor (#7130)

Splits the existing StickyAssignor logic into an AbstractStickyAssignor class, which is extended by the existing (eager) StickyAssignor and by the new CooperativeStickyAssignor which supports incremental cooperative rebalancing.

There is no actual change to the logic -- most methods from StickyAssignor were moved to AbstractStickyAssignor to be shared with CooperativeStickyAssignor, and the abstract MemberData memberData(Subscription) method converts the Subscription to the embedded list of owned partitions for each assignor.

The ""generation"" logic is left in, however this is always Optional.empty() for the CooperativeStickyAssignor as onPartitionsLost should always be called when a generation is missed.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",16,35,850,96,829,44,6,272,933,23,12,4.5,1286,933,107,1014,850,84,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignor.java,"KAFKA-8179: Part 4, add CooperativeStickyAssignor (#7130)

Splits the existing StickyAssignor logic into an AbstractStickyAssignor class, which is extended by the existing (eager) StickyAssignor and by the new CooperativeStickyAssignor which supports incremental cooperative rebalancing.

There is no actual change to the logic -- most methods from StickyAssignor were moved to AbstractStickyAssignor to be shared with CooperativeStickyAssignor, and the abstract MemberData memberData(Subscription) method converts the Subscription to the embedded list of owned partitions for each assignor.

The ""generation"" logic is left in, however this is always Optional.empty() for the CooperativeStickyAssignor as onPartitionsLost should always be called when a generation is missed.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,4,1,63,351,0,11,147,117,11,13,2,326,117,25,179,134,14,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapter.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/PartitionAssignorAdapter.java,"KAFKA-8179: Part 4, add CooperativeStickyAssignor (#7130)

Splits the existing StickyAssignor logic into an AbstractStickyAssignor class, which is extended by the existing (eager) StickyAssignor and by the new CooperativeStickyAssignor which supports incremental cooperative rebalancing.

There is no actual change to the logic -- most methods from StickyAssignor were moved to AbstractStickyAssignor to be shared with CooperativeStickyAssignor, and the abstract MemberData memberData(Subscription) method converts the Subscription to the embedded list of owned partitions for each assignor.

The ""generation"" logic is left in, however this is always Optional.empty() for the CooperativeStickyAssignor as onPartitionsLost should always be called when a generation is missed.

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",19,5,1,93,777,0,9,140,136,70,2,1.0,141,136,70,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStore.java,"KAFKA-8579: Expose RocksDB metrics (#7209)

RocksDB metrics are added to the Kafka metrics. For each segmented state store only
one set of metrics is exposed rather than one set of metrics for each segment.

The metrics are not computed yet.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",1,2,2,10,68,2,1,28,134,1,19,2,386,134,20,358,264,19,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStore.java,"KAFKA-8579: Expose RocksDB metrics (#7209)

RocksDB metrics are added to the Kafka metrics. For each segmented state store only
one set of metrics is exposed rather than one set of metrics for each segment.

The metrics are not computed yet.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",1,2,2,10,68,2,1,28,28,14,2,1.5,30,28,15,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/LoginManager.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/LoginManager.java,"KAFKA-8669: Add security providers in kafka security config (#7090)

* Adds custom provider class to security config 
* Implementation of KIP-492
Reviewers: Sriharsha Chintalapani <sriharsha@apache.org> , Jeff Huang",36,2,0,156,1268,1,13,225,118,12,19,2,373,118,20,148,31,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestPlainSaslServerProvider.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestPlainSaslServerProvider.java,"KAFKA-8669: Add security providers in kafka security config (#7090)

* Adds custom provider class to security config 
* Implementation of KIP-492
Reviewers: Sriharsha Chintalapani <sriharsha@apache.org> , Jeff Huang",2,31,0,10,69,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestScramSaslServerProvider.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestScramSaslServerProvider.java,"KAFKA-8669: Add security providers in kafka security config (#7090)

* Adds custom provider class to security config 
* Implementation of KIP-492
Reviewers: Sriharsha Chintalapani <sriharsha@apache.org> , Jeff Huang",2,31,0,10,69,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
tests/kafkatest/tests/streams/streams_multiple_rolling_upgrade_test.py,tests/kafkatest/tests/streams/streams_multiple_rolling_upgrade_test.py,"KAFKA-8594: Add version 2.3 to Streams system tests (#7131)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Boyang Chen <boyang@confluent.io>, Bill Bejeck <bill@confluent.io>",11,4,2,83,604,0,7,119,116,40,3,2,122,116,41,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/UniformStickyPartitioner.java,clients/src/main/java/org/apache/kafka/clients/producer/UniformStickyPartitioner.java,"KAFKA-8601: Add UniformStickyPartitioner and tests (#7199)

Reviewers: Colin P. McCabe <cmccabe@apache.org>",4,65,0,15,146,4,4,65,65,65,1,1,65,65,65,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/Config.scala,core/src/main/scala/kafka/common/Config.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",4,1,1,16,116,1,1,41,40,8,5,2,50,40,10,9,3,2,2,1,0,1
core/src/main/scala/kafka/common/MessageReader.scala,core/src/main/scala/kafka/common/MessageReader.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,2,2,9,74,2,2,39,39,20,2,1.5,41,39,20,2,2,1,2,1,0,1
core/src/main/scala/kafka/log/OffsetMap.scala,core/src/main/scala/kafka/log/OffsetMap.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",14,6,6,99,703,5,8,201,136,20,10,2.0,246,136,25,45,30,4,2,1,0,1
core/src/main/scala/kafka/security/CredentialProvider.scala,core/src/main/scala/kafka/security/CredentialProvider.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",5,1,1,30,274,1,1,54,52,8,7,1,63,52,9,9,3,1,2,1,0,1
core/src/main/scala/kafka/server/BrokerStates.scala,core/src/main/scala/kafka/server/BrokerStates.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,2,2,17,143,2,2,78,80,26,3,2,89,80,30,11,9,4,2,1,0,1
core/src/main/scala/kafka/server/ReplicaFetcherManager.scala,core/src/main/scala/kafka/server/ReplicaFetcherManager.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,1,1,26,174,2,2,47,29,2,22,1.5,95,29,4,48,9,2,2,1,0,1
core/src/main/scala/kafka/utils/FileLock.scala,core/src/main/scala/kafka/utils/FileLock.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",7,2,2,39,188,3,4,82,64,10,8,2.0,143,64,18,61,47,8,2,1,0,1
core/src/main/scala/kafka/utils/Throttler.scala,core/src/main/scala/kafka/utils/Throttler.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",8,2,2,65,430,2,2,105,81,10,11,3,166,81,15,61,16,6,2,1,0,1
core/src/main/scala/kafka/utils/VersionInfo.scala,core/src/main/scala/kafka/utils/VersionInfo.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",1,1,1,17,78,1,1,40,30,13,3,1,44,30,15,4,3,1,2,1,0,1
core/src/test/scala/unit/kafka/utils/LogCaptureAppender.scala,core/src/test/scala/unit/kafka/utils/LogCaptureAppender.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",5,1,1,39,219,1,5,66,66,33,2,1.0,67,66,34,1,1,0,2,1,0,1
core/src/test/scala/unit/kafka/utils/MockTime.scala,core/src/test/scala/unit/kafka/utils/MockTime.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",2,1,1,10,93,1,2,40,33,6,7,1,83,33,12,43,32,6,2,1,0,1
core/src/test/scala/unit/kafka/zk/ZkFourLetterWords.scala,core/src/test/scala/unit/kafka/zk/ZkFourLetterWords.scala,"MINOR: Remove Deprecated Scala Procedure Syntax (#7214)

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",4,1,1,21,136,1,1,47,47,16,3,1,49,47,16,2,1,1,2,1,0,1
core/src/main/scala/kafka/common/InconsistentBrokerMetadataException.scala,core/src/main/scala/kafka/common/InconsistentBrokerMetadataException.scala,"KAFKA-7335; Store clusterId locally to ensure broker joins the right cluster (#7189)

This patch stores `clusterId` in the `meta.properties` file. During startup, the broker checks that it joins the correct cluster and fails fast otherwise.

The `meta.properties' is versioned. I have decided to not bump the version because 1) the clusterId is null anyway if not present in the file; and 2) bumping it means that rolling back to a previous version won't work.

I have refactored the way the metadata is read and written as it was strongly coupled with the brokerId bits. Now, the metadata is read independently during the startup and used to 1) check the clusterId and 2) get or generate the brokerId (as before).

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",3,27,0,6,63,3,3,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/InconsistentClusterIdException.scala,core/src/main/scala/kafka/common/InconsistentClusterIdException.scala,"KAFKA-7335; Store clusterId locally to ensure broker joins the right cluster (#7189)

This patch stores `clusterId` in the `meta.properties` file. During startup, the broker checks that it joins the correct cluster and fails fast otherwise.

The `meta.properties' is versioned. I have decided to not bump the version because 1) the clusterId is null anyway if not present in the file; and 2) bumping it means that rolling back to a previous version won't work.

I have refactored the way the metadata is read and written as it was strongly coupled with the brokerId bits. Now, the metadata is read independently during the startup and used to 1) check the clusterId and 2) get or generate the brokerId (as before).

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",3,27,0,6,63,3,3,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterPartitionReassignmentsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterPartitionReassignmentsOptions.java,"KAFKA-8345: KIP-455: Admin API changes (Part 2) (#7120)

Add the AlterPartitionReassignments and ListPartitionReassignments APIs.  Also remove an unused methodlength suppression for KafkaAdminClient.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Viktor Somogyi <viktorsomogyi@gmail.com>",0,31,0,6,45,0,0,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterPartitionReassignmentsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterPartitionReassignmentsResult.java,"KAFKA-8345: KIP-455: Admin API changes (Part 2) (#7120)

Add the AlterPartitionReassignments and ListPartitionReassignments APIs.  Also remove an unused methodlength suppression for KafkaAdminClient.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Viktor Somogyi <viktorsomogyi@gmail.com>",3,59,0,18,145,3,3,59,59,59,1,1,59,59,59,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ListPartitionReassignmentsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListPartitionReassignmentsOptions.java,"KAFKA-8345: KIP-455: Admin API changes (Part 2) (#7120)

Add the AlterPartitionReassignments and ListPartitionReassignments APIs.  Also remove an unused methodlength suppression for KafkaAdminClient.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Viktor Somogyi <viktorsomogyi@gmail.com>",0,29,0,5,38,0,0,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindow.java,"KAFKA-8765: Remove interface annotations in Streams API (#7174)

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,0,2,19,135,0,2,68,37,8,8,2.5,91,37,11,23,8,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/UnlimitedWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/UnlimitedWindow.java,"KAFKA-8765: Remove interface annotations in Streams API (#7174)

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",3,0,2,15,95,0,2,63,37,8,8,2.0,88,37,11,25,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/StateRestoreCallback.java,streams/src/main/java/org/apache/kafka/streams/processor/StateRestoreCallback.java,"KAFKA-8765: Remove interface annotations in Streams API (#7174)

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,0,3,4,30,0,0,26,27,5,5,2,35,27,7,9,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TimestampExtractor.java,streams/src/main/java/org/apache/kafka/streams/processor/TimestampExtractor.java,"KAFKA-8765: Remove interface annotations in Streams API (#7174)

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,0,2,6,57,0,0,51,34,4,12,2.0,72,34,6,21,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TopicNameExtractor.java,streams/src/main/java/org/apache/kafka/streams/processor/TopicNameExtractor.java,"KAFKA-8765: Remove interface annotations in Streams API (#7174)

Reviewers: Bruno Cadonna <cadonna@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,0,3,4,37,0,0,34,37,17,2,1.5,37,37,18,3,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/KeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/KeyValueIterator.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,2,1,9,70,0,0,44,29,5,9,1,66,29,7,22,18,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/DelegatingPeekingKeyValueIterator.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",12,0,5,57,339,1,6,88,73,11,8,2.0,112,73,14,24,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/FilteredCacheIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/FilteredCacheIterator.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",9,0,9,68,397,2,6,101,73,25,4,1.5,113,73,28,12,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueIterators.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueIterators.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",6,0,3,37,238,1,6,68,71,34,2,1.0,71,71,36,3,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreIteratorWrapper.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreIteratorWrapper.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",13,0,10,73,530,2,13,107,195,18,6,1.5,227,195,38,120,74,20,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedSessionStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedSessionStoreIterator.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,0,5,28,222,1,5,52,89,6,8,1.5,122,89,15,70,27,9,2,1,0,1
streams/src/test/java/org/apache/kafka/test/KeyValueIteratorStub.java,streams/src/test/java/org/apache/kafka/test/KeyValueIteratorStub.java,"MINOR: remove unnecessary #remove overrides (#7178)

Iterator#remove has a default implementation that throws UnsupportedOperatorException so there's no need to override it with the same thing.

Should be cherry-picked back to whenever we switched to Java 8

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,0,4,25,151,1,5,52,56,17,3,1,62,56,21,10,6,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopCounter.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopCounter.java,"KAFKA-8391; Improved the Connect integration tests to make them less flaky

Added the ability for the connector handles and task handles, which are used by the monitorable source and sink connectors used to verify the functionality of the Connect framework, to record the number of times the connector and tasks have each been started, and to allow a test to obtain a `RestartLatch` that can be used to block until the connectors and/or tasks have been restarted a specified number of types.

Typically, a test will get the `ConnectorHandle` for a connector, and call the `ConnectorHandle.expectedRestarts(int)` method with the expected number of times that the connector and/or tasks will be restarted, and will hold onto the resulting `RestartLatch`. The test will then change the connector (or otherwise cause the connector to restart) one or more times as desired, and then call `RestartLatch.await(long, TimeUnit)` to block the test up to a specified duration for the connector and all tasks to be started the specified number of times.

This commit also increases several of the maximum wait times used in other integration tests. It doesn’t hurt to potentially wait longer, since most test runs will not need to wait the maximum amount of time anyway. However, in the rare cases that do need that extra time, waiting a bit more is fine if we can reduce the flakiness and minimize test failures that happened to time out too early.

Unit tests were added for the new `RestartLatch` and `StopAndStartCounter` utility classes. This PR only affects the tests and does not affect any runtime code or API.

**This should be merged on `trunk` and backported to the `2.3.x` branch.**

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis, Arjun Satish

Closes #7019 from rhauch/kafka-8391",16,179,0,60,416,15,15,179,179,179,1,1,179,179,179,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopCounterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopCounterTest.java,"KAFKA-8391; Improved the Connect integration tests to make them less flaky

Added the ability for the connector handles and task handles, which are used by the monitorable source and sink connectors used to verify the functionality of the Connect framework, to record the number of times the connector and tasks have each been started, and to allow a test to obtain a `RestartLatch` that can be used to block until the connectors and/or tasks have been restarted a specified number of types.

Typically, a test will get the `ConnectorHandle` for a connector, and call the `ConnectorHandle.expectedRestarts(int)` method with the expected number of times that the connector and/or tasks will be restarted, and will hold onto the resulting `RestartLatch`. The test will then change the connector (or otherwise cause the connector to restart) one or more times as desired, and then call `RestartLatch.await(long, TimeUnit)` to block the test up to a specified duration for the connector and all tasks to be started the specified number of times.

This commit also increases several of the maximum wait times used in other integration tests. It doesn’t hurt to potentially wait longer, since most test runs will not need to wait the maximum amount of time anyway. However, in the rare cases that do need that extra time, waiting a bit more is fine if we can reduce the flakiness and minimize test failures that happened to time out too early.

Unit tests were added for the new `RestartLatch` and `StopAndStartCounter` utility classes. This PR only affects the tests and does not affect any runtime code or API.

**This should be merged on `trunk` and backported to the `2.3.x` branch.**

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis, Arjun Satish

Closes #7019 from rhauch/kafka-8391",9,116,0,83,552,7,7,116,116,116,1,1,116,116,116,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopLatch.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopLatch.java,"KAFKA-8391; Improved the Connect integration tests to make them less flaky

Added the ability for the connector handles and task handles, which are used by the monitorable source and sink connectors used to verify the functionality of the Connect framework, to record the number of times the connector and tasks have each been started, and to allow a test to obtain a `RestartLatch` that can be used to block until the connectors and/or tasks have been restarted a specified number of types.

Typically, a test will get the `ConnectorHandle` for a connector, and call the `ConnectorHandle.expectedRestarts(int)` method with the expected number of times that the connector and/or tasks will be restarted, and will hold onto the resulting `RestartLatch`. The test will then change the connector (or otherwise cause the connector to restart) one or more times as desired, and then call `RestartLatch.await(long, TimeUnit)` to block the test up to a specified duration for the connector and all tasks to be started the specified number of times.

This commit also increases several of the maximum wait times used in other integration tests. It doesn’t hurt to potentially wait longer, since most test runs will not need to wait the maximum amount of time anyway. However, in the rare cases that do need that extra time, waiting a bit more is fine if we can reduce the flakiness and minimize test failures that happened to time out too early.

Unit tests were added for the new `RestartLatch` and `StopAndStartCounter` utility classes. This PR only affects the tests and does not affect any runtime code or API.

**This should be merged on `trunk` and backported to the `2.3.x` branch.**

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis, Arjun Satish

Closes #7019 from rhauch/kafka-8391",12,118,0,48,338,4,4,118,118,118,1,1,118,118,118,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopLatchTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/StartAndStopLatchTest.java,"KAFKA-8391; Improved the Connect integration tests to make them less flaky

Added the ability for the connector handles and task handles, which are used by the monitorable source and sink connectors used to verify the functionality of the Connect framework, to record the number of times the connector and tasks have each been started, and to allow a test to obtain a `RestartLatch` that can be used to block until the connectors and/or tasks have been restarted a specified number of types.

Typically, a test will get the `ConnectorHandle` for a connector, and call the `ConnectorHandle.expectedRestarts(int)` method with the expected number of times that the connector and/or tasks will be restarted, and will hold onto the resulting `RestartLatch`. The test will then change the connector (or otherwise cause the connector to restart) one or more times as desired, and then call `RestartLatch.await(long, TimeUnit)` to block the test up to a specified duration for the connector and all tasks to be started the specified number of times.

This commit also increases several of the maximum wait times used in other integration tests. It doesn’t hurt to potentially wait longer, since most test runs will not need to wait the maximum amount of time anyway. However, in the rare cases that do need that extra time, waiting a bit more is fine if we can reduce the flakiness and minimize test failures that happened to time out too early.

Unit tests were added for the new `RestartLatch` and `StopAndStartCounter` utility classes. This PR only affects the tests and does not affect any runtime code or API.

**This should be merged on `trunk` and backported to the `2.3.x` branch.**

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis, Arjun Satish

Closes #7019 from rhauch/kafka-8391",12,137,0,103,760,10,10,137,137,137,1,1,137,137,137,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/MetricNameTemplate.java,clients/src/main/java/org/apache/kafka/common/MetricNameTemplate.java,"MINOR: Remove Utils.notNull, use Objects.requireNonNull instead (#7194)",15,4,6,55,379,1,10,129,72,26,5,2,149,72,30,20,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/TopicPartitionReplica.java,clients/src/main/java/org/apache/kafka/common/TopicPartitionReplica.java,"MINOR: Remove Utils.notNull, use Objects.requireNonNull instead (#7194)",13,2,3,51,284,1,7,81,91,27,3,3,98,91,33,17,14,6,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/RoundRobinPartitioner.java,clients/src/main/java/org/apache/kafka/clients/producer/RoundRobinPartitioner.java,"MINOR: some small style fixes to RoundRobinPartitioner

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Gwen Shapira

Closes #7058 from cmccabe/rr-style-fixes",5,1,1,33,297,0,4,76,76,38,2,1.5,77,76,38,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigResource.java,clients/src/main/java/org/apache/kafka/common/config/ConfigResource.java,"KAFKA-7800; Dynamic log levels admin API (KIP-412)

<!--
Is there any breaking changes?  If so this is a major release, make sure '#major' is in at least one
commit message to get CI to bump the major.  This will prevent automatic down stream dependency
bumping / consuming.  For more information about semantic versioning see: https://semver.org/

Suggested PR template: Fill/delete/add sections as needed. Optionally delete any commented block.
-->
What
----
<!--
Briefly describe **what** you have changed and **why**.
Optionally include implementation strategy.
-->

References
----------
[**KIP-412**](https://cwiki.apache.org/confluence/display/KAFKA/KIP-412%3A+Extend+Admin+API+to+support+dynamic+application+log+levels)
[**KAFKA-7800**](https://issues.apache.org/jira/browse/KAFKA-7800)
[**Discussion Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201901.mbox/%3CCANZZNGyeVw8q%3Dx9uOQS-18wL3FEmnOwpBnpJ9x3iMLdXY3gEug%40mail.gmail.com%3E)
[**Vote Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201902.mbox/%3CCANZZNGzpTJg5YX1Gpe5S%3DHSr%3DXGvmxvYLTdA3jWq_qwH-UvorQ%40mail.gmail.com%3E)

<!--
Copy&paste links: to Jira ticket, other PRs, issues, Slack conversations...
For code bumps: link to PR, tag or GitHub `/compare/master...master`
-->

Test&Review
------------
Test cases covered:
* DescribeConfigs
* Alter the log level with and without validateOnly, validate the results with DescribeConfigs

Open questions / Follow ups
--------------------------
If you're a reviewer, I'd appreciate your thoughts on these questions I have open:
1. Should we add synchronization to the Log4jController methods? - Seems like we don't get much value from it
2. Should we instantiate a new Log4jController instead of it having static methods? - All operations are stateless, so I thought static methods would do well
3. A logger which does not have a set value returns ""null"" (as seen in the unit tests). Should we just return the Root logger's level?

Author: Stanislav Kozlovski <familyguyuser192@windowslive.com>

Reviewers: Gwen Shapira

Closes #6903 from stanislavkozlovski/KAFKA-7800-dynamic-log-levels-admin-ap",14,1,1,61,394,0,10,118,65,15,8,1.0,123,65,15,5,1,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/LogLevelConfig.java,clients/src/main/java/org/apache/kafka/common/config/LogLevelConfig.java,"KAFKA-7800; Dynamic log levels admin API (KIP-412)

<!--
Is there any breaking changes?  If so this is a major release, make sure '#major' is in at least one
commit message to get CI to bump the major.  This will prevent automatic down stream dependency
bumping / consuming.  For more information about semantic versioning see: https://semver.org/

Suggested PR template: Fill/delete/add sections as needed. Optionally delete any commented block.
-->
What
----
<!--
Briefly describe **what** you have changed and **why**.
Optionally include implementation strategy.
-->

References
----------
[**KIP-412**](https://cwiki.apache.org/confluence/display/KAFKA/KIP-412%3A+Extend+Admin+API+to+support+dynamic+application+log+levels)
[**KAFKA-7800**](https://issues.apache.org/jira/browse/KAFKA-7800)
[**Discussion Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201901.mbox/%3CCANZZNGyeVw8q%3Dx9uOQS-18wL3FEmnOwpBnpJ9x3iMLdXY3gEug%40mail.gmail.com%3E)
[**Vote Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201902.mbox/%3CCANZZNGzpTJg5YX1Gpe5S%3DHSr%3DXGvmxvYLTdA3jWq_qwH-UvorQ%40mail.gmail.com%3E)

<!--
Copy&paste links: to Jira ticket, other PRs, issues, Slack conversations...
For code bumps: link to PR, tag or GitHub `/compare/master...master`
-->

Test&Review
------------
Test cases covered:
* DescribeConfigs
* Alter the log level with and without validateOnly, validate the results with DescribeConfigs

Open questions / Follow ups
--------------------------
If you're a reviewer, I'd appreciate your thoughts on these questions I have open:
1. Should we add synchronization to the Log4jController methods? - Seems like we don't get much value from it
2. Should we instantiate a new Log4jController instead of it having static methods? - All operations are stateless, so I thought static methods would do well
3. A logger which does not have a set value returns ""null"" (as seen in the unit tests). Should we just return the Root logger's level?

Author: Stanislav Kozlovski <familyguyuser192@windowslive.com>

Reviewers: Gwen Shapira

Closes #6903 from stanislavkozlovski/KAFKA-7800-dynamic-log-levels-admin-ap",0,71,0,16,117,0,0,71,71,71,1,1,71,71,71,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java,"KAFKA-8179: PartitionAssignorAdapter (#7110)

Follow up to new PartitionAssignor interface merged in 7108 is merged

Adds a PartitionAssignorAdapter class to maintain backwards compatibility

Reviewers: Boyang Chen <boyang@confluent.io>, Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",19,2,2,84,723,2,8,132,90,15,9,3,182,90,20,50,14,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NoReassignmentInProgressException.java,clients/src/main/java/org/apache/kafka/common/errors/NoReassignmentInProgressException.java,"KAFKA-8345: KIP-455 Protocol changes (part 1) (#7114)

Add a new exception, NoReassignmentInProgressException.  Modify LeaderAndIsrRequest to include the AddingRepicas and RemovingReplicas fields.  Add the ListPartitionReassignments and AlterPartitionReassignments RPCs.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Viktor Somogyi <viktorsomogyi@gmail.com>",2,31,0,9,49,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/zk/EmbeddedZookeeper.scala,core/src/test/scala/unit/kafka/zk/EmbeddedZookeeper.scala,"MINOR: Close ZKDatabase in EmbeddedZookeeper (#6237)

And remove redundant call. Closing ZKDatabase is necessary to allow the data
directory to be deleted when running on Windows.

Reviewers: Ismael Juma <ismael@juma.me.uk>",4,2,1,31,236,1,2,66,42,3,23,2,123,42,5,57,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/MeasurableStat.java,clients/src/main/java/org/apache/kafka/common/metrics/MeasurableStat.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,1,1,3,20,0,0,26,16,5,5,1,33,16,7,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Count.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Count.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,7,23,4,22,3,0,29,29,4,8,1.5,86,29,11,57,23,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/CumulativeCount.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/CumulativeCount.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",1,9,13,8,64,2,1,34,38,17,2,3.0,47,38,24,13,13,6,0,0,0,0
clients/src/main/java/org/apache/kafka/common/metrics/stats/Sum.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Sum.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,7,23,4,22,3,0,29,45,14,2,2.0,52,45,26,23,23,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Total.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Total.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,9,27,4,22,4,0,29,31,4,7,2,88,31,13,59,27,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/WindowedCount.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/WindowedCount.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",1,35,0,8,68,1,1,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/metrics/stats/WindowedSum.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/WindowedSum.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,48,0,19,120,3,3,48,48,48,1,1,48,48,48,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/test/MetricsBench.java,clients/src/test/java/org/apache/kafka/test/MetricsBench.java,"KAFKA-8696: clean up Sum/Count/Total metrics (#7057)

* Clean up one redundant and one misplaced metric
* Clarify the relationship among these metrics to avoid future confusion

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,2,2,38,428,1,1,58,38,6,9,2,124,38,14,66,17,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AdminClient.java,clients/src/main/java/org/apache/kafka/clients/admin/AdminClient.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,9,899,11,80,31,2,52,186,2,30,3.0,1123,186,37,1071,899,36,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterConfigsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,2,18,147,0,3,54,42,11,5,2,60,42,12,6,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/AlterReplicaLogDirsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/AlterReplicaLogDirsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,1,1,6,45,0,0,29,29,10,3,1,32,29,11,3,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/Config.java,clients/src/main/java/org/apache/kafka/clients/admin/Config.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",10,1,1,38,245,0,6,81,63,20,4,4.0,93,63,23,12,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateAclsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateAclsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,2,19,154,0,3,54,48,11,5,2,63,48,13,9,4,2,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateDelegationTokenOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateDelegationTokenOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",4,2,2,24,150,0,4,53,53,26,2,1.5,55,53,28,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/CreateDelegationTokenResult.java,clients/src/main/java/org/apache/kafka/clients/admin/CreateDelegationTokenResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,1,1,14,98,0,2,43,43,22,2,1.0,44,43,22,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/CreatePartitionsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/CreatePartitionsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,2,17,134,0,3,53,53,18,3,2,56,53,19,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteAclsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteAclsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",13,2,2,64,442,0,9,126,107,18,7,2,175,107,25,49,24,7,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,2,2,6,45,0,0,31,31,16,2,1.5,33,31,16,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteConsumerGroupsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,2,18,142,0,3,52,41,17,3,2,57,41,19,5,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteRecordsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteRecordsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,2,2,6,45,0,0,32,32,16,2,1.5,34,32,17,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DeleteRecordsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DeleteRecordsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,2,18,146,0,3,54,54,18,3,1,57,54,19,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeAclsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeAclsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,1,1,16,123,0,2,46,37,9,5,1,51,37,10,5,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeClusterResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeClusterResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",5,1,1,35,236,0,5,80,43,11,7,2,95,43,14,15,6,2,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeConsumerGroupsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeConsumerGroupsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,2,2,14,77,0,2,41,31,14,3,1,43,31,14,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeConsumerGroupsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeConsumerGroupsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",5,1,1,34,288,0,3,72,47,18,4,3.0,81,47,20,9,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeDelegationTokenOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeDelegationTokenOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,2,2,15,101,0,2,48,48,24,2,1.5,50,48,25,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeDelegationTokenResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeDelegationTokenResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,1,1,15,114,0,2,45,45,22,2,1.0,46,45,23,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeLogDirsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeLogDirsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,2,2,6,45,0,0,33,33,11,3,1,36,33,12,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeReplicaLogDirsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeReplicaLogDirsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,2,2,6,45,0,0,31,31,8,4,1.5,36,31,9,5,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DescribeReplicaLogDirsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/DescribeReplicaLogDirsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",13,2,2,82,520,0,10,132,132,33,4,1.5,138,132,34,6,3,2,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ExpireDelegationTokenOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ExpireDelegationTokenOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,2,2,13,73,0,2,39,39,20,2,1.5,41,39,20,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ExpireDelegationTokenResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ExpireDelegationTokenResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,1,1,13,81,0,2,42,42,21,2,1.0,43,42,22,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupOffsetsOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,2,2,15,99,0,2,53,53,26,2,1.5,55,53,28,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ListConsumerGroupsResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",7,2,2,48,350,0,4,102,59,20,5,3,168,65,34,66,55,13,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/NewPartitions.java,clients/src/main/java/org/apache/kafka/clients/admin/NewPartitions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",6,2,2,29,187,0,6,99,100,33,3,2,106,100,35,7,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/RecordsToDelete.java,clients/src/main/java/org/apache/kafka/clients/admin/RecordsToDelete.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",9,2,2,31,172,0,6,73,58,24,3,1,75,58,25,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/RenewDelegationTokenOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/RenewDelegationTokenOptions.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,2,2,13,73,0,2,39,39,20,2,1.5,41,39,20,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/RenewDelegationTokenResult.java,clients/src/main/java/org/apache/kafka/clients/admin/RenewDelegationTokenResult.java,"KAFKA-8454; Add Java AdminClient Interface (KIP-476) (#7087)

Adds an `Admin` interface as specified in [KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface).

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,1,1,13,81,0,2,42,42,21,2,1.0,43,42,22,1,1,0,1,0,1,1
tests/kafkatest/services/security/listener_security_config.py,tests/kafkatest/services/security/listener_security_config.py,MINOR: kafkatest - adding whitelist for interbroker sasl configs (#7093),2,8,1,11,66,1,2,43,36,22,2,2.0,44,36,22,1,1,0,1,0,1,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonTimestampExtractor.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonTimestampExtractor.java,"KAFKA-8615: Change to track partition time breaks TimestampExtractor (#7054)

The timestamp extractor takes a previousTimestamp parameter which should be the partition time. This PR adds back in partition time tracking for the extractor, and renames previousTimestamp --> partitionTime

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Matthias J. Sax <mjsax@apache.org>",4,1,1,19,184,2,1,45,46,8,6,1.0,55,46,9,10,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockTimestampExtractor.java,streams/src/test/java/org/apache/kafka/test/MockTimestampExtractor.java,"KAFKA-8615: Change to track partition time breaks TimestampExtractor (#7054)

The timestamp extractor takes a previousTimestamp parameter which should be the partition time. This PR adds back in partition time tracking for the extractor, and renames previousTimestamp --> partitionTime

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Matthias J. Sax <mjsax@apache.org>",1,1,1,9,70,2,1,29,30,7,4,1.0,35,30,9,6,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupAuthorizationException.java,"MINOR: Fix api exception single argument constructor usage (#6956)

Api exception types usually have a single argument constructor which accepts the exception message. However, some types actually use this constructor to initialize a field. This inconsistency has led to some cases where exception messages were being incorrectly passed to these constructors and interpreted incorrectly. For example, this leads to confusing messages like the following in the log when we hit a GROUP_MAX_SIZE_REACHED error:
```
Attempt to join group failed due to fatal error: Consumer group The consumer group has reached its max size. already has the configured ...
```
This patch fixes the problem by changing these constructors so that the exception message is provided consistently. This affected `GroupAuthorizationException`, `TopicAuthorizationException`, and `GroupMaxSizeReachedException`. 

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Ismael Juma <ismael@juma.me.uk>",4,16,2,17,91,4,4,45,27,15,3,3,56,27,19,11,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupMaxSizeReachedException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupMaxSizeReachedException.java,"MINOR: Fix api exception single argument constructor usage (#6956)

Api exception types usually have a single argument constructor which accepts the exception message. However, some types actually use this constructor to initialize a field. This inconsistency has led to some cases where exception messages were being incorrectly passed to these constructors and interpreted incorrectly. For example, this leads to confusing messages like the following in the log when we hit a GROUP_MAX_SIZE_REACHED error:
```
Attempt to join group failed due to fatal error: Consumer group The consumer group has reached its max size. already has the configured ...
```
This patch fixes the problem by changing these constructors so that the exception message is provided consistently. This affected `GroupAuthorizationException`, `TopicAuthorizationException`, and `GroupMaxSizeReachedException`. 

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Ismael Juma <ismael@juma.me.uk>",1,2,2,7,39,2,1,28,28,14,2,1.0,30,28,15,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TopicAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/TopicAuthorizationException.java,"MINOR: Fix api exception single argument constructor usage (#6956)

Api exception types usually have a single argument constructor which accepts the exception message. However, some types actually use this constructor to initialize a field. This inconsistency has led to some cases where exception messages were being incorrectly passed to these constructors and interpreted incorrectly. For example, this leads to confusing messages like the following in the log when we hit a GROUP_MAX_SIZE_REACHED error:
```
Attempt to join group failed due to fatal error: Consumer group The consumer group has reached its max size. already has the configured ...
```
This patch fixes the problem by changing these constructors so that the exception message is provided consistently. This affected `GroupAuthorizationException`, `TopicAuthorizationException`, and `GroupMaxSizeReachedException`. 

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Ismael Juma <ismael@juma.me.uk>",4,14,4,19,117,4,4,47,33,16,3,3,60,33,20,13,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/MemberDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/MemberDescription.java,"KAFKA-8643; Bring back public MemberDescription constructor (#7060)

This patch fixes a compatibility breaking `MemberDescription` constructor change in #6957. It also updates `equals` and `hashCode` for the new `groupInstanceId` field that was added in the same patch.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",21,14,6,67,392,5,10,113,98,23,5,5,150,98,30,37,25,7,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java,"KAFKA-8591; WorkerConfigTransformer NPE on connector configuration reloading (#6991)

A bug in `WorkerConfigTransformer` prevents the connector configuration reload when the ConfigData TTL expires. 

The issue boils down to the fact that `worker.herder().restartConnector` is receiving a null callback. 

```
[2019-06-17 14:34:12,320] INFO Scheduling a restart of connector workshop-incremental in 60000 ms (org.apache.kafka.connect.runtime.WorkerConfigTransformer:88)
[2019-06-17 14:34:12,321] ERROR Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:227)
java.lang.NullPointerException
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1187)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1183)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:273)
        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:219)
```
This patch adds a callback which just logs the error.

Reviewers: Robert Yokota <rayokota@gmail.com>, Jason Gustafson <jason@confluent.io>",16,5,4,112,1018,2,12,154,146,26,6,3.0,169,146,28,15,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetcherMetricsRegistry.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/FetcherMetricsRegistry.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",4,6,2,143,821,2,4,182,140,26,7,4,227,140,32,45,34,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/replica/ClientMetadata.java,clients/src/main/java/org/apache/kafka/common/replica/ClientMetadata.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",16,124,0,71,379,9,9,124,124,124,1,1,124,124,124,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/replica/PartitionView.java,clients/src/main/java/org/apache/kafka/common/replica/PartitionView.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",10,72,0,43,238,6,6,72,72,72,1,1,72,72,72,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/replica/RackAwareReplicaSelector.java,clients/src/main/java/org/apache/kafka/common/replica/RackAwareReplicaSelector.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",5,54,0,28,230,1,1,54,54,54,1,1,54,54,54,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/replica/ReplicaSelector.java,clients/src/main/java/org/apache/kafka/common/replica/ReplicaSelector.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",2,49,0,18,113,2,2,49,49,49,1,1,49,49,49,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/replica/ReplicaView.java,clients/src/main/java/org/apache/kafka/common/replica/ReplicaView.java,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",13,105,0,57,319,8,8,105,105,105,1,1,105,105,105,0,0,0,2,1,0,1
tests/kafkatest/tests/client/truncation_test.py,tests/kafkatest/tests/client/truncation_test.py,"KAFKA-8443; Broker support for fetch from followers (#6832)

Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers). 

Changes:
* All brokers will handle FetchRequest regardless of leadership
* Leaders can compute a preferred replica to return to the client
* New ReplicaSelector interface for determining the preferred replica
* Incremental fetches will include partitions with no records if the preferred replica has been computed
* Adds new JMX to expose the current preferred read replica of a partition in the consumer

Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout:
* For regular fetches, if the high watermark changes within a single fetch request 
* For incremental fetch sessions, if the follower's high watermark is lower than the leader

A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",6,0,1,88,691,0,5,149,150,74,2,1.0,150,150,75,1,1,0,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pipe/PipeDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pipe/PipeDemo.java,"MINOR: Fix a wrong description in PipeDemo's javadoc (#6901)

This PR fixes a wrong input stream name in PipeDemo's javadoc.

Reviewers: Kamal Chandraprakash <kamal.chandraprakash@gmail.com>, Jason Gustafson <jason@confluent.io>",2,1,1,36,308,0,1,75,50,4,17,2,118,50,7,43,6,3,2,1,0,1
core/src/test/scala/integration/kafka/server/MultipleListenersWithAdditionalJaasContextTest.scala,core/src/test/scala/integration/kafka/server/MultipleListenersWithAdditionalJaasContextTest.scala,"MINOR: Make the build compile with Scala 2.13 (#6989)

Scala 2.13 support was added to build via #5454. This PR adjusts the code so that
it compiles with 2.11, 2.12 and 2.13.

Changes:
* Add `scala-collection-compat` dependency.
* Import `scala.collection.Seq` in a number of places for consistent behavior between
Scala 2.11, 2.12 and 2.13.
* Remove wildcard imports that were causing the Java classes to have priority over the
Scala ones, related Scala issue: https://github.com/scala/scala/pull/6589.
* Replace parallel collection usage with `Future`. The former is no longer included by
default in the standard library.
* Replace val _: Unit workaround with one that is more concise and works with Scala 2.13
* Replace `filterKeys` with `filter` when we expect a `Map`. `filterKeys` returns a view
that doesn't implement the `Map` trait in Scala 2.13.
* Replace `mapValues` with `map` or add a `toMap` as an additional transformation
when we expect a `Map`. `mapValues` returns a view that doesn't implement the
`Map` trait in Scala 2.13.
* Replace `breakOut` with `iterator` and `to`, `breakOut` was removed in Scala
2.13.
* Replace to() with toMap, toIndexedSeq and toSet
* Replace `mutable.Buffer.--` with `filterNot`.
* ControlException is an abstract class in Scala 2.13.
* Variable arguments can only receive arrays or immutable.Seq in Scala 2.13.
* Use `Factory` instead of `CanBuildFrom` in DecodeJson. `CanBuildFrom` behaves
a bit differently in Scala 2.13 and it's been deprecated. `Factory` has the behavior
we need and it's available via the compat library.
* Fix failing tests due to behavior change in Scala 2.13,
""Map.values.map is not strict in Scala 2.13"" (https://github.com/scala/bug/issues/11589).
* Use Java collections instead of Scala ones in StreamResetter (a Java class).
* Adjust CheckpointFile.write to take an `Iterable` instead of `Seq` to avoid
unnecessary collection copies.
* Fix DelayedElectLeader to use a Map instead of Set and avoid `to` call that
doesn't work in Scala 2.13.
* Use unordered map for mapping in SimpleAclAuthorizer, mapping of ordered
maps require an `Ordering` in Scala 2.13 for safety reasons.
* Adapt `ConsumerGroupCommand` to compile with Scala 2.13.
* CoreUtils.min takes an `Iterable` instead of `TraversableOnce`, the latter does
not exist in Scala 2.13.
* Replace `Unit` with `()` in a couple places. Scala 2.13 is stricter when it expects
a value instead of a type.
* Fix bug in CustomQuotaCallbackTest where we did not necessarily set `partitionRatio`
correctly, `forall` can terminate early.
* Add a couple of spotbugs exclusions that are needed by code generated by Scala 2.13
* Remove unused variables, simplify some code and remove procedure syntax in a few
places.
* Remove unused `CoreUtils.JSONEscapeString`.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, José Armando García Sancio <jsancio@users.noreply.github.com>",0,2,0,21,129,0,0,45,47,11,4,2.0,73,47,18,28,15,7,2,1,0,1
core/src/test/scala/integration/kafka/server/MultipleListenersWithDefaultJaasContextTest.scala,core/src/test/scala/integration/kafka/server/MultipleListenersWithDefaultJaasContextTest.scala,"MINOR: Make the build compile with Scala 2.13 (#6989)

Scala 2.13 support was added to build via #5454. This PR adjusts the code so that
it compiles with 2.11, 2.12 and 2.13.

Changes:
* Add `scala-collection-compat` dependency.
* Import `scala.collection.Seq` in a number of places for consistent behavior between
Scala 2.11, 2.12 and 2.13.
* Remove wildcard imports that were causing the Java classes to have priority over the
Scala ones, related Scala issue: https://github.com/scala/scala/pull/6589.
* Replace parallel collection usage with `Future`. The former is no longer included by
default in the standard library.
* Replace val _: Unit workaround with one that is more concise and works with Scala 2.13
* Replace `filterKeys` with `filter` when we expect a `Map`. `filterKeys` returns a view
that doesn't implement the `Map` trait in Scala 2.13.
* Replace `mapValues` with `map` or add a `toMap` as an additional transformation
when we expect a `Map`. `mapValues` returns a view that doesn't implement the
`Map` trait in Scala 2.13.
* Replace `breakOut` with `iterator` and `to`, `breakOut` was removed in Scala
2.13.
* Replace to() with toMap, toIndexedSeq and toSet
* Replace `mutable.Buffer.--` with `filterNot`.
* ControlException is an abstract class in Scala 2.13.
* Variable arguments can only receive arrays or immutable.Seq in Scala 2.13.
* Use `Factory` instead of `CanBuildFrom` in DecodeJson. `CanBuildFrom` behaves
a bit differently in Scala 2.13 and it's been deprecated. `Factory` has the behavior
we need and it's available via the compat library.
* Fix failing tests due to behavior change in Scala 2.13,
""Map.values.map is not strict in Scala 2.13"" (https://github.com/scala/bug/issues/11589).
* Use Java collections instead of Scala ones in StreamResetter (a Java class).
* Adjust CheckpointFile.write to take an `Iterable` instead of `Seq` to avoid
unnecessary collection copies.
* Fix DelayedElectLeader to use a Map instead of Set and avoid `to` call that
doesn't work in Scala 2.13.
* Use unordered map for mapping in SimpleAclAuthorizer, mapping of ordered
maps require an `Ordering` in Scala 2.13 for safety reasons.
* Adapt `ConsumerGroupCommand` to compile with Scala 2.13.
* CoreUtils.min takes an `Iterable` instead of `TraversableOnce`, the latter does
not exist in Scala 2.13.
* Replace `Unit` with `()` in a couple places. Scala 2.13 is stricter when it expects
a value instead of a type.
* Fix bug in CustomQuotaCallbackTest where we did not necessarily set `partitionRatio`
correctly, `forall` can terminate early.
* Add a couple of spotbugs exclusions that are needed by code generated by Scala 2.13
* Remove unused variables, simplify some code and remove procedure syntax in a few
places.
* Remove unused `CoreUtils.JSONEscapeString`.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, José Armando García Sancio <jsancio@users.noreply.github.com>",0,2,0,10,74,0,0,34,37,7,5,1,49,37,10,15,7,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ConsumerGroupDescription.java,clients/src/main/java/org/apache/kafka/clients/admin/ConsumerGroupDescription.java,"KAFKA-8538 (part of KIP-345): add group.instance.id to DescribeGroup (#6957)

Include group.instance.id in the describe group result for better visibility.

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",24,6,6,92,562,1,12,148,104,16,9,1,200,104,22,52,29,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/Maybe.java,streams/src/main/java/org/apache/kafka/streams/state/internals/Maybe.java,"Minor: code enhancment (#6999)


Reviewers: Bill Bejeck <bbejeck@gmail.com>",18,1,1,51,291,1,9,89,89,44,2,1.0,90,89,45,1,1,0,2,1,0,1
core/src/main/scala/kafka/common/KafkaException.scala,core/src/main/scala/kafka/common/KafkaException.scala,"KAFKA-8545: Remove legacy ZkUtils (#6948)

`ZkUtils` is not used by the broker, has been deprecated since
2.0.0 and it was never intended as a public API. We should
remove it along with `AdminUtils` methods that rely on it.

Reviewers: David Arthur <mumrah@gmail.com>",2,2,3,5,52,0,2,27,25,9,3,1,31,25,10,4,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/AbstractRecordBatch.java,clients/src/main/java/org/apache/kafka/common/record/AbstractRecordBatch.java,"KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator (#6785)

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* github comments

* use batch.skipKeyValueIterator

* cleanups

* no need to skip kv for uncompressed iterator

* checkstyle fixes

* fix findbugs

* adding unit tests

* reuse decompression buffer; and using streaming iterator

* checkstyle

* add unit tests

* remove reusing buffer supplier

* fix unit tests

* add unit tests

* use streaming iterator

* minor refactoring

* rename

* github comments

* github comments

* reuse buffer at DefaultRecord caller

* some further optimization

* major refactoring

* further refactoring

* update comment

* github comments

* minor fix

* add jmh benchmarks

* update jmh

* github comments

* minor fix

* github comments",3,1,1,15,70,0,3,36,31,12,3,1,37,31,12,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/PartialDefaultRecord.java,clients/src/main/java/org/apache/kafka/common/record/PartialDefaultRecord.java,"KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator (#6785)

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* github comments

* use batch.skipKeyValueIterator

* cleanups

* no need to skip kv for uncompressed iterator

* checkstyle fixes

* fix findbugs

* adding unit tests

* reuse decompression buffer; and using streaming iterator

* checkstyle

* add unit tests

* remove reusing buffer supplier

* fix unit tests

* add unit tests

* use streaming iterator

* minor refactoring

* rename

* github comments

* github comments

* reuse buffer at DefaultRecord caller

* some further optimization

* major refactoring

* further refactoring

* update comment

* github comments

* minor fix

* add jmh benchmarks

* update jmh

* github comments

* minor fix

* github comments",13,99,0,67,317,11,11,99,99,99,1,1,99,99,99,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/CloseableIterator.java,clients/src/main/java/org/apache/kafka/common/utils/CloseableIterator.java,"KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator (#6785)

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* github comments

* use batch.skipKeyValueIterator

* cleanups

* no need to skip kv for uncompressed iterator

* checkstyle fixes

* fix findbugs

* adding unit tests

* reuse decompression buffer; and using streaming iterator

* checkstyle

* add unit tests

* remove reusing buffer supplier

* fix unit tests

* add unit tests

* use streaming iterator

* minor refactoring

* rename

* github comments

* github comments

* reuse buffer at DefaultRecord caller

* some further optimization

* major refactoring

* further refactoring

* update comment

* github comments

* minor fix

* add jmh benchmarks

* update jmh

* github comments

* minor fix

* github comments",1,22,0,24,130,1,1,52,30,26,2,1.0,52,30,26,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/PrimitiveRef.java,clients/src/main/java/org/apache/kafka/common/utils/PrimitiveRef.java,"KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator (#6785)

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation.

* github comments

* use batch.skipKeyValueIterator

* cleanups

* no need to skip kv for uncompressed iterator

* checkstyle fixes

* fix findbugs

* adding unit tests

* reuse decompression buffer; and using streaming iterator

* checkstyle

* add unit tests

* remove reusing buffer supplier

* fix unit tests

* add unit tests

* use streaming iterator

* minor refactoring

* rename

* github comments

* github comments

* reuse buffer at DefaultRecord caller

* some further optimization

* major refactoring

* further refactoring

* update comment

* github comments

* minor fix

* add jmh benchmarks

* update jmh

* github comments

* minor fix

* github comments",2,36,0,12,56,2,2,36,36,36,1,1,36,36,36,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordsUtil.java,clients/src/main/java/org/apache/kafka/common/record/RecordsUtil.java,"KAFKA-8570; Grow buffer to hold down converted records if it was insufficiently sized (#6974)

When the log contains out of order message formats (for example v2 message followed by v1 message) and consists of compressed batches typically greater than 1kB in size, it is possible for down-conversion to fail. With compressed batches, we estimate the size of down-converted batches using:

```
    private static int estimateCompressedSizeInBytes(int size, CompressionType compressionType) {
        return compressionType == CompressionType.NONE ? size : Math.min(Math.max(size / 2, 1024), 1 << 16);
    }
```

This almost always underestimates size of down-converted records if the batch is between 1kB-64kB in size. In general, this means we may under estimate the total size required for compressed batches.

Because of an implicit assumption in the code that messages with a lower message format appear before any with a higher message format, we do not grow the buffer we copy the down converted records into when we see a message <= the target message format. This assumption becomes incorrect when the log contains out of order message formats, for example because of leaders flapping while upgrading the message format.

Reviewers: Jason Gustafson <jason@confluent.io>",20,3,0,87,724,1,3,140,130,47,3,2,142,130,47,2,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/Change.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/Change.java,"KAFKA-8452: Compressed BufferValue review follow-up (#6940)

Belatedly address a few code review comments from #6848

Reviewers: Bill Bejeck <bbejeck@gmail.com>",8,1,1,30,180,1,4,53,34,8,7,1,62,34,9,9,4,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/BufferValueTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/BufferValueTest.java,"KAFKA-8452: Compressed BufferValue review follow-up (#6940)

Belatedly address a few code review comments from #6848

Reviewers: Bill Bejeck <bbejeck@gmail.com>",16,4,0,161,2028,3,16,207,203,104,2,2.5,207,203,104,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/clients/GroupRebalanceConfig.java,clients/src/main/java/org/apache/kafka/clients/GroupRebalanceConfig.java,"KAFKA-7853: Refactor coordinator config (#6854)

An attempt to refactor current coordinator logic.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Konstantine Karantasis <konstantine@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",7,100,0,64,404,3,3,100,100,100,1,1,100,100,100,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/LogOffsetMetadata.scala,core/src/main/scala/kafka/server/LogOffsetMetadata.scala,"KAFKA-8457; Move `Log' reference from `Replica` into `Partition` (#6841)

A `Partition` object contain one or many `Replica` objects. These replica
objects in turn can have the ""log"" if the replica corresponds to the
local node. All the code in Partition or ReplicaManager peek into
replica object to fetch the log if they need to operate on that. As
replica object can represent a local replica or a remote one, this
lead to a bunch of ""if-else"" code in log fetch and offset update code.

NOTE: In addition to a ""log"" that is in use during normal operation, if
an alter log directory command is issued, we also create a future log
object. This object catches up with local log and then we switch the log
directory. So temporarily a Partition can have two local logs. Before
this change both logs are inside replica objects.

This change is an attempt to untangle this relationship. In particular
it moves `Log` from `Replica` into `Partition`. So a partition contains
a local log to which all writes go and possibly a ""future log"" if the 
partition is being moved between directories. Additionally, it maintains
a list of remote replicas for offset and ""caught up time"" data that it uses
for replication protocol. 

Reviewers: José Armando García Sancio <jsancio@users.noreply.github.com>, Jason Gustafson <jason@confluent.io>",9,1,1,40,249,0,5,84,87,9,9,1,117,87,13,33,16,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/LRUCacheEntry.java,streams/src/main/java/org/apache/kafka/streams/state/internals/LRUCacheEntry.java,"KAFKA-8452: Compressed BufferValue (#6848)

De-duplicate the common case in which the prior value is the same as the old value.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",14,1,1,60,339,1,9,94,92,9,10,2.0,182,92,18,88,45,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/SuppressSuite.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/SuppressSuite.java,"KAFKA-8452: Compressed BufferValue (#6848)

De-duplicate the common case in which the prior value is the same as the old value.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",0,10,4,28,225,0,0,56,50,28,2,4.0,60,50,30,4,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java,"KAFKA-8452: Compressed BufferValue (#6848)

De-duplicate the common case in which the prior value is the same as the old value.

Reviewers: Sophie Blee-Goldman <sophie@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",5,5,5,68,321,5,5,98,98,49,2,3.0,103,98,52,5,5,2,2,1,0,1
tests/kafkatest/services/consumer_property.py,tests/kafkatest/services/consumer_property.py,"KAFKA-8331: stream static membership system test (#6877)

As title suggested, we boost 3 stream instances stream job with one minute session timeout, and once the group is stable, doing couple of rolling bounces for the entire cluster. Every rejoin based on restart should have no generation bump on the client side.

Reviewers: Guozhang Wang <wangguoz@gmail.com>,  Bill Bejeck <bbejeck@gmail.com>",0,21,0,5,7,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
tests/bin/external_trogdor_command_example.py,tests/bin/external_trogdor_command_example.py,"MINOR: Improve Trogdor external command worker docs (#6438)

Reviewers: Colin McCabe <cmccabe@apache.org>, Xi Yang <xi@confluent.io>",0,8,5,15,112,0,0,41,38,20,2,2.5,46,38,23,5,5,2,1,0,1,1
clients/src/test/java/org/apache/kafka/common/config/provider/MockVaultConfigProvider.java,clients/src/test/java/org/apache/kafka/common/config/provider/MockVaultConfigProvider.java,"KAFKA-8426; Fix for keeping the ConfigProvider configs consistent with KIP-297 (#6750)

According to KIP-297 a parameter is passed to ConfigProvider with syntax ""config.providers.{name}.param.{param-name}"". Currently AbstractConfig allows parameters of the format ""config.providers.{name}.{param-name}"". With this fix AbstractConfig will be consistent with KIP-297 syntax.

Reviewers: Robert Yokota <rayokota@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",3,17,1,23,145,3,3,45,29,22,2,2.0,46,29,23,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/NamedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/NamedInternal.java,"KAFKA-6958: Overload KStream methods to allow to name operation name using the new Named class (#6411)

Sub-task required to allow to define custom processor names with KStreams DSL(KIP-307) :
 - overload methods for stateless operations to accept a Named parameter (filter, filterNot, map, mapValues, foreach, peek, branch, transform, transformValue, flatTransform)
 - overload process method to accept a Named parameter
 - overload join/leftJoin/outerJoin methods

Reviewers: John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>,
Bill Bejeck <bbejeck@gmail.com>",12,18,1,48,259,3,9,96,81,32,3,3,122,81,41,26,25,9,1,0,1,1
clients/src/main/java/org/apache/kafka/common/utils/SystemTime.java,clients/src/main/java/org/apache/kafka/common/utils/SystemTime.java,"MINOR:Replace duplicated code with common function in utils (#6819)

Reviewers: Ivan Yurchenko <ivanyu@aiven.io>, Matthias J. Sax <matthias@confluent.io>",7,1,6,30,163,1,4,60,26,5,11,1,79,26,7,19,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/BufferKey.java,streams/src/main/java/org/apache/kafka/streams/state/internals/BufferKey.java,"KAFKA-8199: Implement ValueGetter for Suppress (#6781)

See also #6684

KTable processors must be supplied with a KTableProcessorSupplier, which in turn requires implementing a ValueGetter, for use with joins and groupings.

For suppression, a correct view only includes the previously emitted values (not the currently buffered ones), so this change also involves pushing the Change value type into the suppression buffer's interface, so that it can get the prior value upon first buffering (which is also the previously emitted value).

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>",12,72,0,45,251,7,7,72,72,72,1,1,72,72,72,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBuffer.java,streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBuffer.java,"KAFKA-8199: Implement ValueGetter for Suppress (#6781)

See also #6684

KTable processors must be supplied with a KTableProcessorSupplier, which in turn requires implementing a ValueGetter, for use with joins and groupings.

For suppression, a correct view only includes the previously emitted values (not the currently buffered ones), so this change also involves pushing the Change value type into the suppression buffer's interface, so that it can get the prior value upon first buffering (which is also the previously emitted value).

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>",16,9,4,54,443,3,7,88,47,22,4,4.0,100,52,25,12,5,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MaybeTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MaybeTest.java,"KAFKA-8199: Implement ValueGetter for Suppress (#6781)

See also #6684

KTable processors must be supplied with a KTableProcessorSupplier, which in turn requires implementing a ValueGetter, for use with joins and groupings.

For suppression, a correct view only includes the previously emitted values (not the currently buffered ones), so this change also involves pushing the Change value type into the suppression buffer's interface, so that it can get the prior value upon first buffering (which is also the previously emitted value).

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>",6,79,0,48,549,5,5,79,79,79,1,1,79,79,79,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ElectionNotNeededException.java,clients/src/main/java/org/apache/kafka/common/errors/ElectionNotNeededException.java,"KAFKA-8286; Generalized Leader Election Admin RPC (KIP-460) (#6686)

Implements KIP-460: https://cwiki.apache.org/confluence/display/KAFKA/KIP-460%3A+Admin+Leader+Election+RPC.

Reviewers: Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",2,28,0,9,49,2,2,28,28,28,1,1,28,28,28,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/EligibleLeadersNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/EligibleLeadersNotAvailableException.java,"KAFKA-8286; Generalized Leader Election Admin RPC (KIP-460) (#6686)

Implements KIP-460: https://cwiki.apache.org/confluence/display/KAFKA/KIP-460%3A+Admin+Leader+Election+RPC.

Reviewers: Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",2,28,0,9,49,2,2,28,28,28,1,1,28,28,28,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/DelayedOperationKey.scala,core/src/main/scala/kafka/server/DelayedOperationKey.scala,"KAFKA-8286; Generalized Leader Election Admin RPC (KIP-460) (#6686)

Implements KIP-460: https://cwiki.apache.org/confluence/display/KAFKA/KIP-460%3A+Admin+Leader+Election+RPC.

Reviewers: Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",1,6,1,25,146,2,1,62,38,7,9,1,74,38,8,12,6,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java,streams/src/main/java/org/apache/kafka/streams/state/StateSerdes.java,"MINOR: improve error message for Serde type miss match (#6801)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Boyang Chen <boyang@confluent.io>",20,13,4,92,662,2,13,212,108,19,11,3,275,108,25,63,25,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java,clients/src/main/java/org/apache/kafka/common/acl/AclOperation.java,"MINOR: Auth operations must be null when talking to a pre-KIP-430 broker (#6812)

Authorized operations must be null when talking to a pre-KIP-430 broker.
If we present this as the empty set instead, it is impossible for clients
to know if they have no permissions, or are talking to an old broker.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",8,3,0,50,310,0,6,167,122,19,9,1,181,122,20,14,10,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java,"MINOR: Fix `toString` NPE in tableProcessorNode (#6807)

This gets hit when debug logging is enabled.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,60,0,36,200,4,4,60,60,60,1,1,60,60,60,0,0,0,2,1,0,1
core/src/main/scala/kafka/utils/ReplicationUtils.scala,core/src/main/scala/kafka/utils/ReplicationUtils.scala,"KAFKA-8371: Remove dependence on ReplicaManager from Partition (#6705)

This patch attempts to simplify the interaction between Partition and the various components from `ReplicaManager`. This is primarily to make unit testing easier. I have also tried to eliminate the OfflinePartition sentinel which has always been unsafe.

Reviewers: Boyang Chen <bchen11@outlook.com>, David Arthur <mumrah@gmail.com>",8,2,2,33,261,1,2,56,83,3,17,2,216,83,13,160,56,9,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/AllConnectorClientConfigOverridePolicy.java,connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/AllConnectorClientConfigOverridePolicy.java,"KAFKA-8265: Fix config name to match KIP-458. (#6755)

Return a copy of the ConfigDef in Client Configs. Related to KIP-458.

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com
Reviewer: Randall Hauch <rhauch@gmail.com>",3,1,1,20,118,0,3,46,46,23,2,1.0,47,46,24,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/NoneConnectorClientConfigOverridePolicy.java,connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/NoneConnectorClientConfigOverridePolicy.java,"KAFKA-8265: Fix config name to match KIP-458. (#6755)

Return a copy of the ConfigDef in Client Configs. Related to KIP-458.

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com
Reviewer: Randall Hauch <rhauch@gmail.com>",3,1,1,20,118,0,3,47,47,24,2,1.0,48,47,24,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/PrincipalConnectorClientConfigOverridePolicy.java,connect/runtime/src/main/java/org/apache/kafka/connect/connector/policy/PrincipalConnectorClientConfigOverridePolicy.java,"KAFKA-8265: Fix config name to match KIP-458. (#6755)

Return a copy of the ConfigDef in Client Configs. Related to KIP-458.

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com
Reviewer: Randall Hauch <rhauch@gmail.com>",3,1,1,29,213,0,3,57,57,28,2,1.0,58,57,29,1,1,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/util/SSLUtilsTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/util/SSLUtilsTest.java,"KAFKA-8316; Remove deprecated usage of Slf4jRequestLog, SslContextFactory (#6668)

* Remove deprecated class Slf4jRequestLog: use Slf4jRequestLogWriter, CustomRequestLog instread.

1. Remove '@SuppressWarnings(""deprecation"")' from RestServer#initializeResources, JsonRestServer#start.
2. Remove unused JsonRestServer#httpRequest.

* Fix deprecated class usage: SslContextFactory -> SslContextFactory.[Server, Client]

1. Split SSLUtils#createSslContextFactory into SSLUtils#create[Server, Client]SideSslContextFactory: each method instantiates SslContextFactory.[Server, Client], respectively.
2. SSLUtils#configureSslContextFactoryAuthentication is called from SSLUtils#createServerSideSslContextFactory only.
3. Update SSLUtilsTest following splittion; for client-side SSL Context Factory, SslContextFactory#get[Need, Want]ClientAuth is always false. (SSLUtilsTest#testCreateClientSideSslContextFactory)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",5,73,4,162,1685,6,5,196,126,49,4,2.0,204,126,51,8,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueIteratorFacade.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueIteratorFacade.java,"KAFKA-6455: Improve DSL operator timestamp semantics (#6725)

Basic idea:
KTable-KTable join: set max(left-ts,right-ts) for result
#agg(...) (stream/table windowed/non-windowed): set max(ts1, ts2, ts3,...) of all input records that contribute to the aggregation result
for all stateless transformation: input-ts -> output-ts

Reviewers: Guozhang Wang <wangguoz@gmail.com>,  John Roesler <john@confluent.io>, Andy Coates <andy@confluent.io>,  Bill Bejeck <bbejeck@gmail.com",5,3,1,28,213,1,5,52,50,26,2,1.5,53,50,26,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/Named.java,streams/src/main/java/org/apache/kafka/streams/kstream/Named.java,"MINOR: Add select changes from 3rd KIP-307 PR for incrementing name index counter (#6754)

When users provide a name for operation via the Streams DSL, we need to increment the counter used for auto-generated names to make sure any operators downstream of a named operator still produce a compatible name.

This PR is a subset of #6411 by @fhussonnois. We need to merge this PR now because it covers cases when users name repartition topics or state stores.

Updated tests to reflect the counter produces expected number even when the user provides a name.

Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>",22,5,1,47,350,2,6,87,83,44,2,1.5,88,83,44,1,1,0,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/NamedInternalTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/NamedInternalTest.java,"MINOR: Add select changes from 3rd KIP-307 PR for incrementing name index counter (#6754)

When users provide a name for operation via the Streams DSL, we need to increment the counter used for auto-generated names to make sure any operators downstream of a named operator still produce a compatible name.

This PR is a subset of #6411 by @fhussonnois. We need to merge this PR now because it covers cases when users name repartition topics or state stores.

Updated tests to reflect the counter produces expected number even when the user provides a name.

Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>",5,29,19,46,256,5,5,76,66,38,2,4.5,95,66,48,19,19,10,1,0,1,1
connect/api/src/main/java/org/apache/kafka/connect/connector/policy/ConnectorClientConfigOverridePolicy.java,connect/api/src/main/java/org/apache/kafka/connect/connector/policy/ConnectorClientConfigOverridePolicy.java,"KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458) (#6624)

Implementation to enable policy for Connector Client config overrides. This is
implemented per the KIP-458.

Reviewers: Randall Hauch <rhauch@gmail.com>",0,47,0,7,63,0,0,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java,"KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458) (#6624)

Implementation to enable policy for Connector Client config overrides. This is
implemented per the KIP-458.

Reviewers: Randall Hauch <rhauch@gmail.com>",6,2,0,40,295,0,4,64,58,13,5,2,65,58,13,1,1,0,1,0,1,1
connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/BaseConnectorClientConfigOverridePolicyTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/BaseConnectorClientConfigOverridePolicyTest.java,"KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458) (#6624)

Implementation to enable policy for Connector Client config overrides. This is
implemented per the KIP-458.

Reviewers: Randall Hauch <rhauch@gmail.com>",5,59,0,33,280,5,5,59,59,59,1,1,59,59,59,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/NoneConnectorClientConfigOverridePolicyTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/NoneConnectorClientConfigOverridePolicyTest.java,"KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458) (#6624)

Implementation to enable policy for Connector Client config overrides. This is
implemented per the KIP-458.

Reviewers: Randall Hauch <rhauch@gmail.com>",3,49,0,25,163,3,3,49,49,49,1,1,49,49,49,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/PrincipalConnectorClientConfigOverridePolicyTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/connector/policy/PrincipalConnectorClientConfigOverridePolicyTest.java,"KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458) (#6624)

Implementation to enable policy for Connector Client config overrides. This is
implemented per the KIP-458.

Reviewers: Randall Hauch <rhauch@gmail.com>",3,50,0,26,178,3,3,50,50,50,1,1,50,50,50,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectAssignor.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectAssignor.java,"KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415) (#6363)

Added the incremental cooperative rebalancing in Connect to avoid global rebalances on all connectors and tasks with each new/changed/removed connector. This new protocol is backward compatible and will work with heterogeneous clusters that exist during a rolling upgrade, but once the clusters consist of new workers only some affected connectors and tasks will be rebalanced: connectors and tasks on existing nodes still in the cluster and not added/changed/removed will continue running while the affected connectors and tasks are rebalanced.

This commit attempted to minimize the changes to the existing V0 protocol logic, though that was not entirely possible.

This commit adds extensive unit and integration tests for both the old V0 protocol and the new v1 protocol. Soak testing has been performed multiple times to verify behavior while connectors and added, changed, and removed and while workers are added and removed from the cluster.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Robert Yokota <rayokota@gmail.com>, David Arthur <mumrah@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>",0,43,0,10,78,0,0,43,43,43,1,1,43,43,43,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/EagerAssignor.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/EagerAssignor.java,"KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415) (#6363)

Added the incremental cooperative rebalancing in Connect to avoid global rebalances on all connectors and tasks with each new/changed/removed connector. This new protocol is backward compatible and will work with heterogeneous clusters that exist during a rolling upgrade, but once the clusters consist of new workers only some affected connectors and tasks will be rebalanced: connectors and tasks on existing nodes still in the cluster and not added/changed/removed will continue running while the affected connectors and tasks are rebalanced.

This commit attempted to minimize the changes to the existing V0 protocol logic, though that was not entirely possible.

This commit adds extensive unit and integration tests for both the old V0 protocol and the new v1 protocol. Soak testing has been performed multiple times to verify behavior while connectors and added, changed, and removed and while workers are added and removed from the cluster.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Robert Yokota <rayokota@gmail.com>, David Arthur <mumrah@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>",21,182,0,127,1096,7,7,182,182,182,1,1,182,182,182,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ExtendedWorkerState.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ExtendedWorkerState.java,"KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415) (#6363)

Added the incremental cooperative rebalancing in Connect to avoid global rebalances on all connectors and tasks with each new/changed/removed connector. This new protocol is backward compatible and will work with heterogeneous clusters that exist during a rolling upgrade, but once the clusters consist of new workers only some affected connectors and tasks will be rebalanced: connectors and tasks on existing nodes still in the cluster and not added/changed/removed will continue running while the affected connectors and tasks are rebalanced.

This commit attempted to minimize the changes to the existing V0 protocol logic, though that was not entirely possible.

This commit adds extensive unit and integration tests for both the old V0 protocol and the new v1 protocol. Soak testing has been performed multiple times to verify behavior while connectors and added, changed, and removed and while workers are added and removed from the cluster.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Robert Yokota <rayokota@gmail.com>, David Arthur <mumrah@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>",4,48,0,19,106,3,3,48,48,48,1,1,48,48,48,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerRebalanceListener.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerRebalanceListener.java,"KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415) (#6363)

Added the incremental cooperative rebalancing in Connect to avoid global rebalances on all connectors and tasks with each new/changed/removed connector. This new protocol is backward compatible and will work with heterogeneous clusters that exist during a rolling upgrade, but once the clusters consist of new workers only some affected connectors and tasks will be rebalanced: connectors and tasks on existing nodes still in the cluster and not added/changed/removed will continue running while the affected connectors and tasks are rebalanced.

This commit attempted to minimize the changes to the existing V0 protocol logic, though that was not entirely possible.

This commit adds extensive unit and integration tests for both the old V0 protocol and the new v1 protocol. Soak testing has been performed multiple times to verify behavior while connectors and added, changed, and removed and while workers are added and removed from the cluster.

Author: Konstantine Karantasis <konstantine@confluent.io>
Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Robert Yokota <rayokota@gmail.com>, David Arthur <mumrah@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>",0,6,4,7,67,0,0,39,38,8,5,3,53,38,11,14,5,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/LoggingContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/LoggingContext.java,"KAFKA-3816: Add MDC logging to Connect runtime (#5743)

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-449%3A+Add+connector+contexts+to+Connect+worker+logs

Added LoggingContext as a simple mechanism to set and unset Mapped Diagnostic Contexts (MDC) in the loggers to provide for each thread useful parameters that can be used within the logging configuration. MDC avoids having to modify lots of log statements, since the parameters are available to all log statements issued by the thread, no matter what class makes those calls.

The design intentionally minimizes the number of changes to any existing classes, and does not use Java 8 features so it can be easily backported if desired, although per this KIP it will be applied initially only in AK 2.3 and later and must be enabled via the Log4J configuration.

Reviewers: Jason Gustafson <jason@conflent.io>, Guozhang Wang <wangguoz@gmail.com>",15,227,0,81,547,10,10,227,227,227,1,1,227,227,227,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SourceTaskOffsetCommitterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/SourceTaskOffsetCommitterTest.java,"KAFKA-3816: Add MDC logging to Connect runtime (#5743)

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-449%3A+Add+connector+contexts+to+Connect+worker+logs

Added LoggingContext as a simple mechanism to set and unset Mapped Diagnostic Contexts (MDC) in the loggers to provide for each thread useful parameters that can be used within the logging configuration. MDC avoids having to modify lots of log statements, since the parameters are available to all log statements issued by the thread, no matter what class makes those calls.

The design intentionally minimizes the number of changes to any existing classes, and does not use Java 8 features so it can be easily backported if desired, although per this KIP it will be applied initially only in AK 2.3 and later and must be enabled via the Log4J configuration.

Reviewers: Jason Gustafson <jason@conflent.io>, Guozhang Wang <wangguoz@gmail.com>",5,6,0,138,1232,1,4,197,194,28,7,4,245,194,35,48,19,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/SslClientAuth.java,clients/src/main/java/org/apache/kafka/common/config/SslClientAuth.java,"MINOR: Refactor SslFactory (#6674)

SslFactory: split the part of SslFactory that creates SSLEngine instances into SslEngineBuilder.  When (re)configuring, we simply create a new SslEngineBuilder.  This allows us to make all the builder fields immutable.  It also simplifies the logic for reconfiguring.  Because we sometimes need to test old SslEngine instances against new ones, being able to use both the old and the new builder at once is useful.

Create an enum named SslClientAuth which encodes the possible values for ssl.client.auth.  This will simplify the handling of this configuration.

SslTransportLayer#maybeProcessHandshakeFailure should treat an SSLHandshakeException with a ""Received fatal alert"" message as a handshake error (and therefore an authentication error.)

SslFactoryTest: add some line breaks for very long lines.

ConfigCommand#main: when terminating the command due to an uncaught exception, log the exception using debug level in slf4j, in addition to printing it to stderr.  This makes it easier to debug failing junit tests, where stderr may not be kept, or may be reordered with respect to other slf4j messages.  The use of debug level is consistent with how we handle other types of exceptions in ConfigCommand#main.

StateChangeLogMerger#main: spell out the full name of scala.io.Source rather than abbreviating it as io.Source.  This makes it clearer that it is part of the Scala standard library.  It also avoids compiler errors when other libraries whose groupId starts with ""io"" are used in the broker.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",4,48,0,24,140,1,1,48,48,48,1,1,48,48,48,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java,clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java,"KAFKA-8363: Fix parsing bug for config providers (#6726)

Author: Chris Egerton <cegerton@oberlin.edu>
Reviewers: Robert Yokota <rayokota@gmail.com>, Randall Hauch <rhauch@gmail.com>",23,1,1,107,1022,0,6,177,169,30,6,1.0,186,169,31,9,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractStoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractStoreBuilder.java,"KAFKA-6455: Session Aggregation should use window-end-time as record timestamp (#6645)

For session-windows, the result record should have the window-end timestamp as record timestamp.

Rebased to resolve merge conflicts. Removed unused classes TupleForwarder and ForwardingCacheFlushListener (replace with TimestampedTupleForwarder, SessionTupleForwarder, TimestampedCacheFlushListerner, and SessionCacheFlushListener)

Reviewers: John Roesler <john@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Boyang Chen <boyang@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,2,2,63,366,1,8,90,84,22,4,1.0,98,84,24,8,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedKeyValueStoreMaterializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimestampedKeyValueStoreMaterializer.java,"KAFKA-6521: Use timestamped stores for KTables (#6667)

Reviewers: John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,6,5,33,283,4,2,58,52,14,4,1.5,67,52,17,9,5,2,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/ValueAndTimestamp.java,streams/src/main/java/org/apache/kafka/streams/state/ValueAndTimestamp.java,"KAFKA-6521: Use timestamped stores for KTables (#6667)

Reviewers: John Roesler <john@confluent.io>, Boyang Chen <boyang@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",14,13,1,46,280,1,8,95,74,32,3,2,97,74,32,2,1,1,1,0,1,1
connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterDetails.java,connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterDetails.java,"KAFKA-8231: Expansion of ConnectClusterState interface (#6584)

Expand ConnectClusterState interface and implementation with methods that provide the immutable cluster details and the connector configuration. This includes unit tests for the new methods.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviews: Arjun Satish <arjun@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",0,32,0,4,21,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterState.java,connect/api/src/main/java/org/apache/kafka/connect/health/ConnectClusterState.java,"KAFKA-8231: Expansion of ConnectClusterState interface (#6584)

Expand ConnectClusterState interface and implementation with methods that provide the immutable cluster details and the connector configuration. This includes unit tests for the new methods.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviews: Arjun Satish <arjun@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,28,2,13,78,2,2,72,46,36,2,2.0,74,46,37,2,2,1,1,0,1,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterDetailsImpl.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/health/ConnectClusterDetailsImpl.java,"KAFKA-8231: Expansion of ConnectClusterState interface (#6584)

Expand ConnectClusterState interface and implementation with methods that provide the immutable cluster details and the connector configuration. This includes unit tests for the new methods.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviews: Arjun Satish <arjun@confluent.io>, Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,34,0,12,64,2,2,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/admin/MemberAssignment.java,clients/src/main/java/org/apache/kafka/clients/admin/MemberAssignment.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",10,2,1,32,234,1,5,69,65,17,4,1.5,76,65,19,7,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/ProducerRecord.java,clients/src/main/java/org/apache/kafka/clients/producer/ProducerRecord.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",41,8,14,95,803,1,15,225,84,12,18,2.0,336,84,19,111,23,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/Node.java,clients/src/main/java/org/apache/kafka/common/Node.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",24,5,3,75,446,1,13,138,76,11,13,2,198,76,15,60,26,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java,clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",17,5,4,51,381,1,8,108,58,15,7,2,118,58,17,10,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/token/delegation/DelegationToken.java,clients/src/main/java/org/apache/kafka/common/security/token/delegation/DelegationToken.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",12,2,4,47,268,1,7,79,80,20,4,2.5,90,80,22,11,5,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/token/delegation/TokenInformation.java,clients/src/main/java/org/apache/kafka/common/security/token/delegation/TokenInformation.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",26,8,15,93,554,2,14,133,136,33,4,1.5,151,136,38,18,15,4,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java,connect/api/src/main/java/org/apache/kafka/connect/sink/SinkRecord.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",15,1,1,64,545,1,10,103,71,11,9,3,127,71,14,24,8,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/source/SourceRecord.java,connect/api/src/main/java/org/apache/kafka/connect/source/SourceRecord.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",20,3,6,84,771,1,13,136,103,12,11,2,203,103,18,67,30,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/ConnectorTaskId.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/ConnectorTaskId.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",13,2,3,50,293,1,7,83,71,10,8,2.0,101,71,13,18,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/KeyValue.java,streams/src/main/java/org/apache/kafka/streams/KeyValue.java,"MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode (#6680)

* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",8,1,2,32,205,1,5,83,34,8,10,2.0,114,34,11,31,15,3,2,1,0,1
generator/src/main/java/org/apache/kafka/message/EntityType.java,generator/src/main/java/org/apache/kafka/message/EntityType.java,"KAFKA-8158: Add EntityType for Kafka RPC fields (#6503)

Reviewers: Jason Gustafson <jason@confluent.io>",5,62,0,35,228,2,2,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/RootResource.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/resources/RootResource.java,"KAFKA-8304: Fix registration of Connect REST extensions (#6651)

Fix registration of Connect REST extensions to prevent deadlocks when extensions get the list of connectors before the herder is available. Added integration test to check the behavior.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviewers: Arjun Satish <arjun@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,4,4,20,144,3,2,42,36,7,6,3.5,58,36,10,16,5,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/RootResourceTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/resources/RootResourceTest.java,"KAFKA-8304: Fix registration of Connect REST extensions (#6651)

Fix registration of Connect REST extensions to prevent deadlocks when extensions get the list of connectors before the herder is available. Added integration test to check the behavior.

Author: Chris Egerton <cegerton@oberlin.edu>
Reviewers: Arjun Satish <arjun@confluent.io>, Randall Hauch <rhauch@gmail.com>",2,1,2,33,260,1,2,58,58,19,3,2,61,58,20,3,2,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/GlobalStateStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/state/internals/GlobalStateStoreProvider.java,"KAFKA-3522: Interactive Queries must return timestamped stores (#6661)

Reviewers: John Roesler <john@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",9,8,0,33,330,1,2,54,46,14,4,1.5,61,46,15,7,6,2,2,1,0,1
core/src/main/scala/kafka/utils/CommandDefaultOptions.scala,core/src/main/scala/kafka/utils/CommandDefaultOptions.scala,"KAFKA-8131; Move --version implementation into CommandLineUtils (#6481)

This patch refactors the implementation of the --version option and moves it into the default command options. This has the benefit of automatically including it in the usage output of the command line tools. Several tools had to be manually updated because they did not use the common options.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",0,2,1,8,76,0,0,27,26,14,2,1.0,28,26,14,1,1,0,1,0,1,1
core/src/main/scala/kafka/admin/AdminClient.scala,core/src/main/scala/kafka/admin/AdminClient.scala,"KAFKA-8056; Use automatic RPC generation for FindCoordinator (#6408)

Reviewers: Jason Gustafson <jason@confluent.io>",76,7,2,396,2841,0,29,493,242,8,61,2,871,242,14,378,70,6,2,1,0,1
core/src/test/scala/unit/kafka/admin/AdminTest.scala,core/src/test/scala/unit/kafka/admin/AdminTest.scala,"MINOR: Upgrade dependencies for Kafka 2.3 (#6665)

Many patch and minor updates.

Scalatest and Jetty deprecated classes that we
use. I removed usages for the former and filed KAFKA-8316 for the latter (I
suppressed the relevant deprecation warnings until the JIRA is fixed). As
part of the scalatest fixes, I also removed `TestUtils.fail` since it duplicates
`Assertions.fail`.

I also fixed a few compiler warnings that have crept in since my last sweep.

Updates of note:
- Jetty: 9.4.14 -> 9.4.18
  * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.15.v20190215
  * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.16.v20190411
  * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.17.v20190418
  * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.17.v20190418
  * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.18.v20190429
- zstd: 1.3.8-1 -> 1.4.0-1
  * https://github.com/facebook/zstd/releases/tag/v1.4.0
  * zstd's fastest strategy, 6-8% faster in most scenarios
- zookeeper: 3.4.13 -> 3.4.14
  * https://zookeeper.apache.org/doc/r3.4.14/releasenotes.html

### Committer Checklist (excluded from commit message)
- [ ] Verify design and implementation 
- [ ] Verify test coverage and CI build status
- [ ] Verify documentation (including upgrade notes)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",10,1,0,152,1373,0,8,210,180,3,83,4,1655,181,20,1445,379,17,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestKeyManagerFactory.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestKeyManagerFactory.java,"KAFKA-8191: Add pluggability of KeyManager to generate the broker Private Keys and Certificates

Reviewers: Sriharsha Chintalapani <sriharsha@apache.org>, Ismael Juma <ismael@juma.me.uk>",11,113,0,78,547,10,10,113,113,113,1,1,113,113,113,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestProvider.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestProvider.java,"KAFKA-8191: Add pluggability of KeyManager to generate the broker Private Keys and Certificates

Reviewers: Sriharsha Chintalapani <sriharsha@apache.org>, Ismael Juma <ismael@juma.me.uk>",2,36,0,14,133,2,2,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestTrustManagerFactory.java,clients/src/test/java/org/apache/kafka/common/security/ssl/mock/TestTrustManagerFactory.java,"KAFKA-8191: Add pluggability of KeyManager to generate the broker Private Keys and Certificates

Reviewers: Sriharsha Chintalapani <sriharsha@apache.org>, Ismael Juma <ismael@juma.me.uk>",10,88,0,48,306,10,10,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/TimestampedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/TimestampedBytesStore.java,"KAFAK-3522: add API to create timestamped stores (#6601)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@confluent.io>",2,3,0,15,90,1,1,34,29,11,3,1,42,29,14,8,8,3,1,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueToTimestampedKeyValueIteratorAdapter.java,streams/src/main/java/org/apache/kafka/streams/state/internals/KeyValueToTimestampedKeyValueIteratorAdapter.java,"KAFAK-3522: add API to create timestamped stores (#6601)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,58,0,27,199,5,5,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedWindowStore.java,"KAFAK-3522: add API to create timestamped stores (#6601)

Reviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <guozhang@confluent.io>",1,6,13,9,59,2,1,29,36,14,2,2.0,42,36,21,13,13,6,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/FencedInstanceIdException.java,clients/src/main/java/org/apache/kafka/common/errors/FencedInstanceIdException.java,"KAFKA-7862 & KIP-345 part-one: Add static membership logic to JoinGroup protocol (#6177)

This is the first diff for the implementation of JoinGroup logic for static membership. The goal of this diff contains:

* Add group.instance.id to be unique identifier for consumer instances, provided by end user;
Modify group coordinator to accept JoinGroupRequest with/without static membership, refactor the logic for readability and code reusability.
* Add client side support for incorporating static membership changes, including new config for group.instance.id, apply stream thread client id by default, and new join group exception handling.
* Increase max session timeout to 30 min for more user flexibility if they are inclined to tolerate partial unavailability than burdening rebalance.
* Unit tests for each module changes, especially on the group coordinator logic. Crossing the possibilities like:
6.1 Dynamic/Static member
6.2 Known/Unknown member id
6.3 Group stable/unstable
6.4 Leader/Follower

The rest of the 345 change will be broken down to 4 separate diffs:

* Avoid kicking out members through rebalance.timeout, only do the kick out through session timeout.
* Changes around LeaveGroup logic, including version bumping, broker logic, client logic, etc.
* Admin client changes to add ability to batch remove static members
* Deprecate group.initial.rebalance.delay

Reviewers: Liquan Pei <liquanpei@gmail.com>, Stanislav Kozlovski <familyguyuser192@windowslive.com>, Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,29,0,10,57,2,2,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessor.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessor.java,"KAFKA-8254: Pass Changelog as Topic in Suppress Serdes (#6602)

Reviewers:  Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",15,14,28,101,846,7,9,133,66,13,10,6.0,217,79,22,84,32,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/LoggingSignalHandler.java,clients/src/main/java/org/apache/kafka/common/utils/LoggingSignalHandler.java,"MINOR: log which signals are handled on startup (#6620)

Reviewers: Gwen Shapira <cshapi@gmail.com>",7,9,3,64,516,1,4,107,101,54,2,2.0,110,101,55,3,3,2,1,0,1,1
release_notes.py,release_notes.py,"KAFKA-3851; Automate release notes and include links to upgrade notes for release and most recent docs to forward users of older releases to newest docs.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1670 from ewencp/kafka-3851-automate-release-notes##HOTFIX: Add license information to release_notes.py##MINOR: Make release notes script check resolutions to avoid spurious inclusion of non-fix 'fixes' in release notes.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2174 from ewencp/release-notes-resolution-filters##MINOR: Change version format in release notes python code

ijuma ewencp

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #4171 from guozhangwang/KMinor-update-releasepy##MINOR: Use https instead of http in links (#6477)

Verified that the https links work.

I didn't update the license header in this PR since that touches
so many files. Will file a separate one for that.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",7,123,7,81,453,3,3,116,84,23,5,1,123,84,25,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncClient.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncClient.java,"KAFKA-7747; Check for truncation after leader changes [KIP-320] (#6371)

After the client detects a leader change we need to check the offset of the current leader for truncation. These changes were part of KIP-320: https://cwiki.apache.org/confluence/display/KAFKA/KIP-320%3A+Allow+fetchers+to+detect+and+handle+log+truncation.

Reviewers: Jason Gustafson <jason@confluent.io>",6,75,0,49,387,3,3,75,75,75,1,1,75,75,75,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/KeyValueTimestamp.java,streams/src/test/java/org/apache/kafka/streams/KeyValueTimestamp.java,"KAFKA-7895: fix Suppress changelog restore (#6536)

Several issues have come to light since the 2.2.0 release:
upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata
restoring a tombstone incorrectly didn't update the buffer size and min-timestamp

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>,  Bruno Cadonna <bruno@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",16,17,0,38,239,2,7,63,46,32,2,1.5,63,46,32,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Consumed.java,streams/src/main/java/org/apache/kafka/streams/kstream/Consumed.java,"KAFKA-6958: Allow to name operation using parameter classes (#6410)

This is the 2nd PR for the KIP-307
Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",24,39,7,90,692,9,14,230,158,29,8,2.0,249,158,31,19,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalNameProvider.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalNameProvider.java,"KAFKA-6958: Allow to name operation using parameter classes (#6410)

This is the 2nd PR for the KIP-307
Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",0,2,2,5,34,0,0,23,23,12,2,1.5,25,23,12,2,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/ProducedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/ProducedInternal.java,"KAFKA-6958: Allow to name operation using parameter classes (#6410)

This is the 2nd PR for the KIP-307
Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",5,6,0,21,143,1,5,45,39,11,4,1.5,50,39,12,5,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentedCacheFunction.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SegmentedCacheFunction.java,"KAFKA-7652: Restrict range of fetch/findSessions in cache (#6448)

Reduce the total key space cache iterators have to search for segmented byte stores by wrapping several single-segment iterators.

Summary of Benchmarking Results (# records processed as primary indicator)

Session Store:
Only single-key findSessions seems to benefit (~4x improvement) due to conservative scanning of potentially variable-sized keys in key-range findSessions. Could get improvement from key-range findSessions as well if we can tell when/if keys are a fixed size, or pending an efficient custom comparator API from RocksDB

Window Store:
Both single and multi-key fetch saw some improvement; this depended on the size of the time-range in the fetch (in the DSL this would be window size) relative to the retention period. Performance benefits from this patch when the fetch spans multiple segments; hence the larger the time range being searched, the better this will do.

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>",10,15,3,56,426,5,9,88,76,22,4,2.5,104,76,26,16,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/CompoundStat.java,clients/src/main/java/org/apache/kafka/common/metrics/CompoundStat.java,"MINOR: Remove redundant access specifiers from metrics interfaces (#6527)

Reviewers: Sönke Liebau <soenke.liebau@opencore.com>, Ismael Juma <ismael@juma.me.uk>",3,2,2,21,105,0,3,52,40,9,6,1.5,67,40,11,15,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Stat.java,clients/src/main/java/org/apache/kafka/common/metrics/Stat.java,"MINOR: Remove redundant access specifiers from metrics interfaces (#6527)

Reviewers: Sönke Liebau <soenke.liebau@opencore.com>, Ismael Juma <ismael@juma.me.uk>",0,1,1,4,29,0,0,32,16,5,7,1,40,16,6,8,3,1,2,1,0,1
core/src/test/scala/integration/kafka/api/LegacyAdminClientTest.scala,core/src/test/scala/integration/kafka/api/LegacyAdminClientTest.scala,"KAFKA-7893; Refactor ConsumerBounceTest to reuse functionality from BaseConsumerTest (#6238)

This PR should help address the flakiness in the ConsumerBounceTest#testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup test (https://issues.apache.org/jira/browse/KAFKA-7965). I tested this locally and have verified it significantly reduces flakiness - 25/25 tests now pass. Running the test 25 times in trunk, I'd get `18/25` passes.

It does so by reusing the less-flaky consumer integration testing functionality inside `BaseConsumerTest`. Most notably, the test now makes use of the `ConsumerAssignmentPoller` class  - each consumer now polls non-stop rather than the more batch-oriented polling we had in `ConsumerBounceTest#waitForRebalance()`.

Reviewers: Jason Gustafson <jason@confluent.io>",10,2,2,112,807,1,9,156,129,5,30,1.0,377,154,13,221,116,7,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/header/Headers.java,connect/api/src/main/java/org/apache/kafka/connect/header/Headers.java,"MINOR: Fixed a few warning in core and connects (#6545)

- var -> val
- unused imports
- Javadoc fix

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",0,1,1,44,401,0,0,308,308,154,2,1.0,309,308,154,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ServerInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ServerInfo.java,"KAFKA-8014: Extend Connect integration tests to add and remove workers dynamically (#6342)

Extend Connect's integration test framework to add or remove workers to EmbeddedConnectCluster, and choosing whether to fail the test on ungraceful service shutdown. Also added more JavaDoc and other minor improvements. 

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Arjun Satish <arjun@confluent.io>, Randall Hauch <rhauch@gmail.com>

Closes #6342 from kkonstantine/KAFKA-8014",5,11,3,32,183,2,5,55,41,11,5,3,69,41,14,14,5,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/UngracefulShutdownException.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/UngracefulShutdownException.java,"KAFKA-8014: Extend Connect integration tests to add and remove workers dynamically (#6342)

Extend Connect's integration test framework to add or remove workers to EmbeddedConnectCluster, and choosing whether to fail the test on ungraceful service shutdown. Also added more JavaDoc and other minor improvements. 

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Arjun Satish <arjun@confluent.io>, Randall Hauch <rhauch@gmail.com>

Closes #6342 from kkonstantine/KAFKA-8014",3,38,0,13,75,3,3,38,38,38,1,1,38,38,38,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/NamedOperation.java,streams/src/main/java/org/apache/kafka/streams/kstream/NamedOperation.java,"KAFKA-6958: Add new NamedOperation interface to enforce consistency in naming operations (#6409)

Sub-task required to allow to define custom processor names with KStreams DSL(KIP-307) :
  - add new public interface NamedOperation
  - deprecate methods Joined.as() and Joined.name()
  - update Suppredded interface to extend NamedOperation

Reviewers: Matthias J. Sax <mjsax@apache.org>,  John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>",0,32,0,4,31,0,0,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/LineSplit.java,streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/LineSplit.java,"KAFKA-7855: Kafka Streams Maven Archetype quickstart fails to compile out of the box (#6194)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",2,3,3,40,335,1,1,72,86,24,3,1,89,86,30,17,14,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerExtensionsValidatorCallback.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerExtensionsValidatorCallback.java,"MINOR: Fix JavaDocs warnings (#6435)

Fix JavaDocs Warning
Reviewers: uozhang Wang <wangguoz@gmail.com>,  Bill Bejeck <bbejeck@gmail.com>",10,1,1,43,359,0,8,114,114,57,2,1.0,115,114,58,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionWindow.java,"MINOR: cleanup deprectaion annotations (#6290)

If deprecated interface methods are inherited, the @Deprication tag should be used (instead on suppressing the deprecation warning).

Reviewers:  Guozhang Wang <wangguoz@gmail.com>,  John Roesler <john@confluent.io>,  Bill Bejeck <bbejeck@gmail.com>",4,0,2,15,122,0,2,60,58,10,6,1.5,89,58,15,29,17,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/SystemTimeTest.java,clients/src/test/java/org/apache/kafka/common/utils/SystemTimeTest.java,"KAFKA-7831; Do not modify subscription state from background thread (#6221)

Metadata may be updated from the background thread, so we need to protect access to SubscriptionState. This patch restructures the metadata handling so that we only check pattern subscriptions in the foreground. Additionally, it improves the following:

1. SubscriptionState is now the source of truth for the topics that will be fetched. We had a lot of messy logic previously to try and keep the the topic set in Metadata consistent with the subscription, so this simplifies the logic.
2. The metadata needs for the producer and consumer are quite different, so it made sense to separate the custom logic into separate extensions of Metadata. For example, only the producer requires topic expiration.
3. We've always had an edge case in which a metadata change with an inflight request may cause us to effectively miss an expected update. This patch implements a separate version inside Metadata which is bumped when the needed topics changes.
4. This patch removes the MetadataListener, which was the cause of https://issues.apache.org/jira/browse/KAFKA-7764. 

Reviewers: David Arthur <mumrah@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",1,25,0,7,32,1,1,25,25,25,1,1,25,25,25,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/TimestampedWindowStore.java,streams/src/main/java/org/apache/kafka/streams/state/TimestampedWindowStore.java,"KAFKA-3522: Add public interfaces for timestamped stores (#6175)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,34,0,3,44,0,0,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueIteratorFacadeTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueIteratorFacadeTest.java,"KAFKA-3522: Add public interfaces for timestamped stores (#6175)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,87,0,58,428,5,5,87,87,87,1,1,87,87,87,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyKeyValueStoreFacadeTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyKeyValueStoreFacadeTest.java,"KAFKA-3522: Add public interfaces for timestamped stores (#6175)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,99,0,70,641,5,5,99,99,99,1,1,99,99,99,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/TimestampedKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/TimestampedKeyValueStore.java,"KAFKA-3522: Add TimestampedKeyValueStore builder/runtime classes (#6152)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,25,0,2,31,0,0,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/admin/PreferredReplicaElectionCommandTest.scala,core/src/test/scala/unit/kafka/admin/PreferredReplicaElectionCommandTest.scala,"KAFKA-7864; validate partitions are 0-based (#6246)

Reviewers: Sriharsha Chintalapani <sriharsha@apache.org>, Jun Rao <junrao@gmail.com>",3,2,2,41,338,1,3,70,68,23,3,2,73,68,24,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ByteArrayDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ByteArrayDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",1,0,12,7,44,2,1,25,34,6,4,3.0,50,34,12,25,12,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ByteArraySerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ByteArraySerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",1,0,13,7,44,2,1,24,34,6,4,3.0,50,34,12,26,13,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ByteBufferDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ByteBufferDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,10,9,59,2,1,28,34,9,3,3,47,34,16,19,10,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ByteBufferSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ByteBufferSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,0,10,19,140,2,1,40,46,13,3,3,59,46,20,19,10,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/BytesDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/BytesDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,11,9,64,2,1,28,35,9,3,3,48,35,16,20,11,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/BytesSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/BytesSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,11,9,64,2,1,29,36,10,3,3,49,36,16,20,11,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/Deserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/Deserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,11,2,15,111,2,3,72,38,6,12,2.5,109,38,9,37,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/DoubleDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/DoubleDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,12,18,108,2,1,38,46,13,3,3,59,46,20,21,12,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/DoubleSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/DoubleSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,13,19,145,2,1,37,46,12,3,3,59,46,20,22,13,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/FloatDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/FloatDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,14,18,110,2,1,37,47,12,3,3,60,47,20,23,14,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/FloatSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/FloatSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,13,15,103,2,1,33,42,11,3,3,55,42,18,22,13,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/IntegerDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/IntegerDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,11,17,101,2,1,36,44,9,4,2.0,58,44,14,22,11,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/IntegerSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/IntegerSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,11,13,90,2,1,31,38,10,3,3,51,38,17,20,11,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/LongDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/LongDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,11,17,101,2,1,36,44,9,4,2.0,58,44,14,22,11,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/LongSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/LongSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,11,17,134,2,1,35,42,12,3,3,55,42,18,20,11,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/Serde.java,clients/src/main/java/org/apache/kafka/common/serialization/Serde.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,7,2,12,77,2,2,54,26,14,4,3.0,68,26,17,14,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/Serializer.java,clients/src/main/java/org/apache/kafka/common/serialization/Serializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,7,3,15,111,2,3,74,38,7,11,2,109,38,10,35,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/serialization/ShortDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ShortDeserializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,10,17,101,2,1,37,47,18,2,2.0,47,47,24,10,10,5,1,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/ShortSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ShortSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,0,11,11,68,2,1,29,40,14,2,2.0,40,40,20,11,11,6,1,0,0,0
clients/src/test/java/org/apache/kafka/test/MockSerializer.java,clients/src/test/java/org/apache/kafka/test/MockSerializer.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,0,5,29,228,1,4,54,46,11,5,2,64,46,13,10,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/perf/YahooBenchmark.java,streams/src/test/java/org/apache/kafka/streams/perf/YahooBenchmark.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",19,0,12,214,1915,3,10,306,361,17,18,3.0,519,361,29,213,73,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SerdeThatDoesntHandleNull.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SerdeThatDoesntHandleNull.java,"KAFKA-6161 Add default implementation to close() and configure() for Serdes (#5348)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",3,0,12,24,167,2,2,43,55,22,2,1.5,55,55,28,12,12,6,1,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshingLogin.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshingLogin.java,"KAFKA-7945: Calc refresh time correctly when token created in the past (#6288)

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",51,9,2,307,1895,2,19,444,429,89,5,1,455,429,91,11,7,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/FinalResultsSuppressionBuilder.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/FinalResultsSuppressionBuilder.java,"MINOR: add log indicating the suppression time (#6260)

Reviewer: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",11,6,1,53,307,1,7,79,58,13,6,1.5,91,58,15,12,4,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/NamedSuppressed.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/NamedSuppressed.java,"MINOR: add log indicating the suppression time (#6260)

Reviewer: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,28,0,5,46,0,0,28,28,28,1,1,28,28,28,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverter.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverter.java,"KAFKA-7916: Unify store wrapping code for clarity (#6255)

Refactor internal store wrapping for improved maintainability.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,0,28,5,57,1,0,23,35,8,3,2,64,35,21,41,28,14,1,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StateRestorerTest.java,"KAFKA-7916: Unify store wrapping code for clarity (#6255)

Refactor internal store wrapping for improved maintainability.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",9,3,2,81,554,1,9,111,70,9,12,2.5,146,70,12,35,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainSaslServer.java,clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainSaslServer.java,"MINOR: Improve PlainSaslServer error message for empty tokens (#6249)

Empty username or password would result in the ""expected 3 tokens""
error instead of ""username not specified"" or ""password not specified"".

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",28,31,15,122,859,3,12,201,170,17,12,3.0,271,170,23,70,17,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/internals/ApiUtilsTest.java,streams/src/test/java/org/apache/kafka/streams/internals/ApiUtilsTest.java,"MINOR: Update JUnit to 4.13 and annotate log cleaner integration test (#6248)

JUnit 4.13 fixes the issue where `Category` and `Parameterized` annotations
could not be used together. It also deprecates `ExpectedException` and
`assertThat`. Given this, we:

- Replace `ExpectedException` with the newly introduced `assertThrows`.
- Replace `Assert.assertThat` with `MatcherAssert.assertThat`.
- Annotate `AbstractLogCleanerIntegrationTest` with `IntegrationTest` category.

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>, David Arthur <mumrah@gmail.com>",11,2,2,72,495,0,7,110,110,55,2,2.5,112,110,56,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/CompositeRestoreListenerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/CompositeRestoreListenerTest.java,"MINOR: Update JUnit to 4.13 and annotate log cleaner integration test (#6248)

JUnit 4.13 fixes the issue where `Category` and `Parameterized` annotations
could not be used together. It also deprecates `ExpectedException` and
`assertThat`. Given this, we:

- Replace `ExpectedException` with the newly introduced `assertThrows`.
- Replace `Assert.assertThat` with `MatcherAssert.assertThat`.
- Annotate `AbstractLogCleanerIntegrationTest` with `IntegrationTest` category.

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>, David Arthur <mumrah@gmail.com>",19,2,2,166,1244,0,19,222,232,32,7,4,263,232,38,41,22,6,2,1,0,1
log4j-appender/src/test/java/org/apache/kafka/log4jappender/MockKafkaLog4jAppender.java,log4j-appender/src/test/java/org/apache/kafka/log4jappender/MockKafkaLog4jAppender.java,"KAFKA-7896; Add sasl.jaas.config/sasl.mechanism props to the log4j kafka appender

This patch adds 2 props to the log4j kafka appender that get put directly
into the sasl properties passed to the producer:
    - ClientJaasConf: This property sets sasl.jaas.config
    - SaslMechanim: This property sets sasl.mechanism

Author: Rohan Desai <desai.p.rohan@gmail.com>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #6216 from rodesai/add-kafka-appender-security-props",6,7,0,34,245,2,5,59,47,12,5,3,65,47,13,6,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordVersion.java,clients/src/main/java/org/apache/kafka/common/record/RecordVersion.java,"KAFKA-7897; Disable leader epoch cache when older message formats are used (#6232)

When an older message format is in use, we should disable the leader epoch cache so that we resort to truncation by high watermark. Previously we updated the cache for all versions when a broker became leader for a partition. This can cause large and unnecessary truncations after leader changes because we relied on the presence of _any_ cached epoch in order to tell whether to use the improved truncation logic possible with the OffsetsForLeaderEpoch API.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Viktor Somogyi-Vass <viktorsomogyi@gmail.com>, Jun Rao <junrao@gmail.com>",6,9,0,20,130,1,4,56,41,19,3,1,66,41,22,10,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientCallbackHandler.java,"KAFKA-7902: Replace original loginContext if SASL/OAUTHBEARER refresh login fails (#6233)

Replaces original loginContext if login fails in the refresh thread to ensure that the refresh thread is left in a clean state when there are exceptions while connecting to an OAuth server. Also makes client callback handler more robust by using the token with the longest remaining time for expiry instead of throwing an exception if multiple tokens are found.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",17,36,5,86,708,1,6,143,96,29,5,2,153,96,31,10,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ThreadMetadata.java,streams/src/main/java/org/apache/kafka/streams/processor/ThreadMetadata.java,"KAFKA-7798: Expose embedded clientIds (#6107)

Reviewers: Damian Guy <damian@confluent.io>, John Roesler <john@confluent.io>, Boyang Chen <bchen11@outlook.com>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>",22,52,2,99,503,8,12,143,93,48,3,2,147,93,49,4,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/requests/ElectPreferredLeadersResponse.java,clients/src/main/java/org/apache/kafka/common/requests/ElectPreferredLeadersResponse.java,"KAFKA-7859: Use automatic RPC generation in LeaveGroups (#6188)

Reviewed-by: Colin P. McCabe <cmccabe@apache.org>",11,5,0,53,386,1,9,83,78,42,2,1.0,83,78,42,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SessionKeySchemaTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SessionKeySchemaTest.java,"KAFKA-7652: Part II; Add single-point query for SessionStore and use for flushing / getter (#6161)

#2972 tried to fix a bug about flushing operation, but it was not complete, since findSessions(key, earliestEnd, latestStart) does not guarantee to only return a single entry since its semantics are to return any sessions whose end > earliestEnd and whose start < latestStart.

I've tried various ways to fix it completely and I ended up having to add a single-point query to the public ReadOnlySessionStore API for the exact needed semantics. It is used for flushing to read the old values (otherwise the wrong old values will be sent downstreams, hence it is a correctness issue) and also for getting the value for value-getters (it is for perf only).",22,33,39,188,2033,7,21,243,86,27,9,5,326,109,36,83,39,9,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestorer.java,"KAFKA-3522: Replace RecordConverter with TimestampedBytesStore (#6204)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",24,1,1,96,588,0,20,139,78,11,13,3,173,78,13,34,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/PreferredLeaderNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/PreferredLeaderNotAvailableException.java,"KAFKA-5692: Change PreferredReplicaLeaderElectionCommand to use Admin… (#3848)

See also KIP-183.

This implements the following algorithm:

AdminClient sends ElectPreferredLeadersRequest.
KafakApis receives ElectPreferredLeadersRequest and delegates to
ReplicaManager.electPreferredLeaders()
ReplicaManager delegates to KafkaController.electPreferredLeaders()
KafkaController adds a PreferredReplicaLeaderElection to the EventManager,
ReplicaManager.electPreferredLeaders()'s callback uses the
delayedElectPreferredReplicasPurgatory to wait for the results of the
election to appear in the metadata cache. If there are no results
because of errors, or because the preferred leaders are already leading
the partitions then a response is returned immediately.
In the EventManager work thread the preferred leader is elected as follows:

The EventManager runs PreferredReplicaLeaderElection.process()
process() calls KafkaController.onPreferredReplicaElectionWithResults()
KafkaController.onPreferredReplicaElectionWithResults()
calls the PartitionStateMachine.handleStateChangesWithResults() to
perform the election (asynchronously the PSM will send LeaderAndIsrRequest
to the new and old leaders and UpdateMetadataRequest to all brokers)
then invokes the callback.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jun Rao <junrao@gmail.com>",2,28,0,9,49,2,2,28,28,28,1,1,28,28,28,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ElectPreferredLeadersRequest.java,clients/src/main/java/org/apache/kafka/common/requests/ElectPreferredLeadersRequest.java,"KAFKA-5692: Change PreferredReplicaLeaderElectionCommand to use Admin… (#3848)

See also KIP-183.

This implements the following algorithm:

AdminClient sends ElectPreferredLeadersRequest.
KafakApis receives ElectPreferredLeadersRequest and delegates to
ReplicaManager.electPreferredLeaders()
ReplicaManager delegates to KafkaController.electPreferredLeaders()
KafkaController adds a PreferredReplicaLeaderElection to the EventManager,
ReplicaManager.electPreferredLeaders()'s callback uses the
delayedElectPreferredReplicasPurgatory to wait for the results of the
election to appear in the metadata cache. If there are no results
because of errors, or because the preferred leaders are already leading
the partitions then a response is returned immediately.
In the EventManager work thread the preferred leader is elected as follows:

The EventManager runs PreferredReplicaLeaderElection.process()
process() calls KafkaController.onPreferredReplicaElectionWithResults()
KafkaController.onPreferredReplicaElectionWithResults()
calls the PartitionStateMachine.handleStateChangesWithResults() to
perform the election (asynchronously the PSM will send LeaderAndIsrRequest
to the new and old leaders and UpdateMetadataRequest to all brokers)
then invokes the callback.

Reviewers: Colin P. McCabe <cmccabe@apache.org>, Jun Rao <junrao@gmail.com>",17,129,0,94,747,11,11,129,129,129,1,1,129,129,129,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/ClusterResource.java,clients/src/main/java/org/apache/kafka/common/ClusterResource.java,"KAFKA-7738; Track leader epochs in client Metadata (#6045)

Track the last seen partition epoch in the Metadata class. When handling metadata updates, check that the partition info being received is for the last seen epoch or a newer one. This prevents stale metadata from being loaded into the client.

Reviewers: Jason Gustafson <jason@confluent.io>",8,15,0,26,142,2,5,64,50,21,3,2,68,50,23,4,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/MemberIdRequiredException.java,clients/src/main/java/org/apache/kafka/common/errors/MemberIdRequiredException.java,"KAFKA-7824; Require member.id for initial join group request [KIP-394] (#6058)

This patch implements KIP-394 as documented in https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>",2,30,0,10,57,2,2,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderProvider.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderProvider.java,"KAFKA-7503: Connect integration test harness

Expose a programmatic way to bring up a Kafka and Zk cluster through Java API to facilitate integration tests for framework level changes in Kafka Connect. The Kafka classes would be similar to KafkaEmbedded in streams. The new classes would reuse the kafka.server.KafkaServer classes from :core, and provide a simple interface to bring up brokers in integration tests.

Signed-off-by: Arjun Satish <arjunconfluent.io>

Author: Arjun Satish <arjun@confluent.io>
Author: Arjun Satish <wicknicks@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5516 from wicknicks/connect-integration-test",6,68,0,28,163,4,4,68,68,68,1,1,68,68,68,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorStateInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorStateInfo.java,"KAFKA-7503: Connect integration test harness

Expose a programmatic way to bring up a Kafka and Zk cluster through Java API to facilitate integration tests for framework level changes in Kafka Connect. The Kafka classes would be similar to KafkaEmbedded in streams. The new classes would reuse the kafka.server.KafkaServer classes from :core, and provide a simple interface to bring up brokers in integration tests.

Signed-off-by: Arjun Satish <arjunconfluent.io>

Author: Arjun Satish <arjun@confluent.io>
Author: Arjun Satish <wicknicks@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5516 from wicknicks/connect-integration-test",17,9,2,101,531,4,15,139,108,28,5,2,149,108,30,10,7,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/integration/RuntimeHandles.java,connect/runtime/src/test/java/org/apache/kafka/connect/integration/RuntimeHandles.java,"KAFKA-7503: Connect integration test harness

Expose a programmatic way to bring up a Kafka and Zk cluster through Java API to facilitate integration tests for framework level changes in Kafka Connect. The Kafka classes would be similar to KafkaEmbedded in streams. The new classes would reuse the kafka.server.KafkaServer classes from :core, and provide a simple interface to bring up brokers in integration tests.

Signed-off-by: Arjun Satish <arjunconfluent.io>

Author: Arjun Satish <arjun@confluent.io>
Author: Arjun Satish <wicknicks@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5516 from wicknicks/connect-integration-test",4,63,0,18,118,4,4,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/test/MockAggregator.java,streams/src/test/java/org/apache/kafka/test/MockAggregator.java,"MINOR: code cleanup (#6054)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Ismael Juma <ismael@confuent.io>",1,1,6,9,101,1,1,29,43,6,5,1,58,43,12,29,18,6,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockInitializer.java,streams/src/test/java/org/apache/kafka/test/MockInitializer.java,"MINOR: code cleanup (#6054)

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Ryanne Dolan <ryannedolan@gmail.com>, Ismael Juma <ismael@confuent.io>",1,0,1,11,64,0,1,31,29,6,5,1,43,29,9,12,6,2,2,1,0,1
buildSrc/src/main/java/org/apache/kafka/task/ProcessMessagesTask.java,buildSrc/src/main/java/org/apache/kafka/task/ProcessMessagesTask.java,"KAFKA-7609; Add Protocol Generator for Kafka (#5893)

This patch adds a framework to automatically generate the request/response classes for Kafka's protocol. The code will be updated to use the generated classes in follow-up patches. Below is a brief summary of the included components:

**buildSrc/src**
The message generator code is here.  This code is automatically re-run by gradle when one of the schema files changes.  The entire directory is processed at once to minimize the number of times we have to start a new JVM.  We use Jackson to translate the JSON files into Java objects.

**clients/src/main/java/org/apache/kafka/common/protocol/Message.java**
This is the interface implemented by all automatically generated messages.

**clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java**
Some utility functions used by the generated message code.

**clients/src/main/java/org/apache/kafka/common/protocol/Readable.java, Writable.java, ByteBufferAccessor.java**
The generated message code uses these classes for writing to a buffer.

**clients/src/main/message/README.md**
This README file explains how the JSON schemas work.

**clients/src/main/message/\*.json**
The JSON files in this directory implement every supported version of every Kafka API.  The unit tests automatically validate that the generated schemas match the hand-written schemas in our code.  Additionally, there are some things like request and response headers that have schemas here.

**clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashSet.java**
I added an optimization here for empty sets.  This is useful here because I want all messages to start with empty sets by default prior to being loaded with data.  This is similar to the ""empty list"" optimizations in the `java.util.ArrayList` class.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Ismael Juma <ismael@juma.me.uk>, Bob Barrett <bob.barrett@outlook.com>, Jason Gustafson <jason@confluent.io>",4,68,0,30,180,3,3,68,68,68,1,1,68,68,68,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/ApiMessage.java,clients/src/main/java/org/apache/kafka/common/protocol/ApiMessage.java,"KAFKA-7609; Add Protocol Generator for Kafka (#5893)

This patch adds a framework to automatically generate the request/response classes for Kafka's protocol. The code will be updated to use the generated classes in follow-up patches. Below is a brief summary of the included components:

**buildSrc/src**
The message generator code is here.  This code is automatically re-run by gradle when one of the schema files changes.  The entire directory is processed at once to minimize the number of times we have to start a new JVM.  We use Jackson to translate the JSON files into Java objects.

**clients/src/main/java/org/apache/kafka/common/protocol/Message.java**
This is the interface implemented by all automatically generated messages.

**clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java**
Some utility functions used by the generated message code.

**clients/src/main/java/org/apache/kafka/common/protocol/Readable.java, Writable.java, ByteBufferAccessor.java**
The generated message code uses these classes for writing to a buffer.

**clients/src/main/message/README.md**
This README file explains how the JSON schemas work.

**clients/src/main/message/\*.json**
The JSON files in this directory implement every supported version of every Kafka API.  The unit tests automatically validate that the generated schemas match the hand-written schemas in our code.  Additionally, there are some things like request and response headers that have schemas here.

**clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashSet.java**
I added an optimization here for empty sets.  This is useful here because I want all messages to start with empty sets by default prior to being loaded with data.  This is similar to the ""empty list"" optimizations in the `java.util.ArrayList` class.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Ismael Juma <ismael@juma.me.uk>, Bob Barrett <bob.barrett@outlook.com>, Jason Gustafson <jason@confluent.io>",0,28,0,4,23,0,0,28,28,28,1,1,28,28,28,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashSetTest.java,clients/src/test/java/org/apache/kafka/common/utils/ImplicitLinkedHashSetTest.java,"KAFKA-7609; Add Protocol Generator for Kafka (#5893)

This patch adds a framework to automatically generate the request/response classes for Kafka's protocol. The code will be updated to use the generated classes in follow-up patches. Below is a brief summary of the included components:

**buildSrc/src**
The message generator code is here.  This code is automatically re-run by gradle when one of the schema files changes.  The entire directory is processed at once to minimize the number of times we have to start a new JVM.  We use Jackson to translate the JSON files into Java objects.

**clients/src/main/java/org/apache/kafka/common/protocol/Message.java**
This is the interface implemented by all automatically generated messages.

**clients/src/main/java/org/apache/kafka/common/protocol/MessageUtil.java**
Some utility functions used by the generated message code.

**clients/src/main/java/org/apache/kafka/common/protocol/Readable.java, Writable.java, ByteBufferAccessor.java**
The generated message code uses these classes for writing to a buffer.

**clients/src/main/message/README.md**
This README file explains how the JSON schemas work.

**clients/src/main/message/\*.json**
The JSON files in this directory implement every supported version of every Kafka API.  The unit tests automatically validate that the generated schemas match the hand-written schemas in our code.  Additionally, there are some things like request and response headers that have schemas here.

**clients/src/main/java/org/apache/kafka/common/utils/ImplicitLinkedHashSet.java**
I added an optimization here for empty sets.  This is useful here because I want all messages to start with empty sets by default prior to being loaded with data.  This is similar to the ""empty list"" optimizations in the `java.util.ArrayList` class.

Reviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Ismael Juma <ismael@juma.me.uk>, Bob Barrett <bob.barrett@outlook.com>, Jason Gustafson <jason@confluent.io>",28,13,7,203,1743,6,18,245,239,82,3,5,257,239,86,12,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorInfo.java,"KAFKA-7253; The returned connector type is always null when creating connector (#5470)

The null map returned from the current snapshot causes the null type in response. The connector class name can be taken from the config of request instead since we require the config should contain the connector class name.

Reviewers: Jason Gustafson <jason@confluent.io>",13,3,2,53,346,2,7,83,81,12,7,3,104,81,15,21,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/internals/KafkaFutureImpl.java,clients/src/main/java/org/apache/kafka/common/internals/KafkaFutureImpl.java,"MINOR: Hygiene fixes in KafkaFutureImpl (#5098)

Change-Id: Ia44c6c659418bbed5367645b814725365daba820",55,2,4,213,1331,1,23,316,264,35,9,2,336,264,37,20,6,2,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/ToolsUtils.java,tools/src/main/java/org/apache/kafka/tools/ToolsUtils.java,"MINOR: Switch anonymous classes to lambda expressions in tools module

Switch to lambda when ever possible instead of old anonymous way
in tools module

Author: Srinivas Reddy <srinivas96alluri@gmail.com>
Author: Srinivas Reddy <mrsrinivas@users.noreply.github.com>

Reviewers: Ryanne Dolan <ryannedolan@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #6013 from mrsrinivas/tools-switch-to-java8",7,1,7,30,288,1,1,55,55,18,3,2,66,55,22,11,7,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/token/delegation/internals/DelegationTokenCache.java,clients/src/main/java/org/apache/kafka/common/security/token/delegation/internals/DelegationTokenCache.java,"KAFKA-7742; Fixed removing hmac entry for a token being removed from DelegationTokenCache

Author: Satish Duggana <satishd@apache.org>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #6037 from satishd/KAFKA-7742",21,18,6,81,691,5,13,125,111,25,5,2,137,111,27,12,6,2,2,1,0,1
core/src/main/scala/kafka/api/Request.scala,core/src/main/scala/kafka/api/Request.scala,"MINOR: Include additional detail in fetch error message (#6036)

This patch adds additional information in the log message after a fetch failure to make debugging easier.

Reviewers: David Arthur <mumrah@gmail.com>",8,10,0,16,85,2,2,37,24,9,4,1.0,42,24,10,5,5,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/OffsetNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/OffsetNotAvailableException.java,"KAFKA-2334; Guard against non-monotonic offsets in the client (#5991)

After a recent leader election, the leaders high-water mark might lag behind the offset at the beginning of the new epoch (as well as the previous leader's HW). This can lead to offsets going backwards from a client perspective, which is confusing and leads to strange behavior in some clients.

This change causes Partition#fetchOffsetForTimestamp to throw an exception to indicate the offsets are not yet available from the leader. For new clients, a new OFFSET_NOT_AVAILABLE error is added. For existing clients, a LEADER_NOT_AVAILABLE is thrown.

This is an implementation of [KIP-207](https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change).

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Dhruvil Shah <dhruvil@confluent.io>, Jason Gustafson <jason@confluent.io>",1,29,0,7,39,1,1,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/KeyValueTest.java,streams/src/test/java/org/apache/kafka/streams/KeyValueTest.java,"Fix the missing ApiUtils tests in streams module. (#6003)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>",1,1,1,40,536,2,1,72,50,14,5,4,110,50,22,38,21,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Serialized.java,streams/src/main/java/org/apache/kafka/streams/kstream/Serialized.java,"MINOR: improve Streams checkstyle and code cleanup (#5954)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Nikolay Izhikov <nIzhikov@gmail.com>, Ismael Juma <ismael@confluent.io>, Bill Bejeck <bill@confluent.io>",5,1,1,25,208,0,5,87,88,17,5,1,98,88,20,11,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Window.java,streams/src/main/java/org/apache/kafka/streams/kstream/Window.java,"MINOR: improve Streams checkstyle and code cleanup (#5954)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Nikolay Izhikov <nIzhikov@gmail.com>, Ismael Juma <ismael@confluent.io>, Bill Bejeck <bill@confluent.io>",14,6,1,59,309,0,8,176,51,10,18,3.0,246,51,14,70,23,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Windowed.java,streams/src/main/java/org/apache/kafka/streams/kstream/Windowed.java,"MINOR: improve Streams checkstyle and code cleanup (#5954)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Nikolay Izhikov <nIzhikov@gmail.com>, Ismael Juma <ismael@confluent.io>, Bill Bejeck <bill@confluent.io>",9,4,4,35,215,1,6,83,38,5,16,1.0,140,38,9,57,12,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/QuickUnion.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/QuickUnion.java,"MINOR: improve Streams checkstyle and code cleanup (#5954)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Nikolay Izhikov <nIzhikov@gmail.com>, Ismael Juma <ismael@confluent.io>, Bill Bejeck <bill@confluent.io>",9,5,3,39,267,3,5,72,67,10,7,1,90,67,13,18,10,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/Stamped.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/Stamped.java,"MINOR: improve Streams checkstyle and code cleanup (#5954)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Nikolay Izhikov <nIzhikov@gmail.com>, Ismael Juma <ismael@confluent.io>, Bill Bejeck <bill@confluent.io>",8,10,7,32,186,3,4,56,38,11,5,4,72,38,14,16,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/GlobalKTableImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/GlobalKTableImpl.java,"KAFKA-6036: Local Materialization for Source KTable (#5779)

Refactor the materialization for source KTables in the way that:

If Materialized.as(queryableName) is specified, materialize;
If the downstream operator requires to fetch from this KTable via ValueGetters, materialize;
If the downstream operator requires to send old values, materialize.
Otherwise do not materialize the KTable. E.g. builder.table(""topic"").filter().toStream().to(""topic"") would not create any state stores.

There's a couple of minor changes along with PR as well:

KTableImpl's queryableStoreName and isQueryable are merged into queryableStoreName only, and if it is null it means not queryable. As long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).
To achieve this, splitted MaterializedInternal.storeName() and MaterializedInternal.queryableName(). The former can be internally generated and will not be exposed to users. QueryableName can be modified to set to the internal store name if we decide to materialize it during the DSL parsing / physical topology generation phase. And only if queryableName is specified the corresponding KTable is determined to be materialized.

Found some overlapping unit tests among KTableImplTest, and KTableXXTest, removed them.

There are a few typing bugs found along the way, fixed them as well.

-----------------------

This PR is an illustration of experimenting a poc towards logical materializations.

Today we've logically materialized the KTable for filter / mapValues / transformValues if queryableName is not specified via Materialized, but whenever users specify queryableName we will still always materialize. My original goal is to also consider logically materialize for queryable stores, but when implementing it via a wrapped store to apply the transformations on the fly I realized it is tougher than I thought, because we not only need to support fetch or get, but also needs to support range queries, approximateNumEntries, and isOpen etc as well, which are not efficient to support. So in the end I'd suggest we still stick with the rule of always materializing if queryableName is specified, and only consider logical materialization otherwise.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <mjsax@apache.org>",3,5,13,18,113,4,3,41,34,8,5,3,61,34,12,20,13,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/MaterializedInternalTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/MaterializedInternalTest.java,"KAFKA-6036: Local Materialization for Source KTable (#5779)

Refactor the materialization for source KTables in the way that:

If Materialized.as(queryableName) is specified, materialize;
If the downstream operator requires to fetch from this KTable via ValueGetters, materialize;
If the downstream operator requires to send old values, materialize.
Otherwise do not materialize the KTable. E.g. builder.table(""topic"").filter().toStream().to(""topic"") would not create any state stores.

There's a couple of minor changes along with PR as well:

KTableImpl's queryableStoreName and isQueryable are merged into queryableStoreName only, and if it is null it means not queryable. As long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).
To achieve this, splitted MaterializedInternal.storeName() and MaterializedInternal.queryableName(). The former can be internally generated and will not be exposed to users. QueryableName can be modified to set to the internal store name if we decide to materialize it during the DSL parsing / physical topology generation phase. And only if queryableName is specified the corresponding KTable is determined to be materialized.

Found some overlapping unit tests among KTableImplTest, and KTableXXTest, removed them.

There are a few typing bugs found along the way, fixed them as well.

-----------------------

This PR is an illustration of experimenting a poc towards logical materializations.

Today we've logically materialized the KTable for filter / mapValues / transformValues if queryableName is not specified via Materialized, but whenever users specify queryableName we will still always materialize. My original goal is to also consider logically materialize for queryable stores, but when implementing it via a wrapped store to apply the transformations on the fly I realized it is tougher than I thought, because we not only need to support fetch or get, but also needs to support range queries, approximateNumEntries, and isOpen etc as well, which are not efficient to support. So in the end I'd suggest we still stick with the rule of always materializing if queryableName is specified, and only consider logical materialization otherwise.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <mjsax@apache.org>",3,3,6,48,427,3,3,76,76,25,3,3,88,76,29,12,6,4,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java,connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java,"MINOR: Safe string conversion to avoid NPEs

Should be ported back to 2.0

Author: Cyrus Vafadari <cyrus@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #6004 from cyrusv/cyrus-npe",31,2,2,106,759,1,13,182,103,14,13,2,237,103,18,55,18,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/StaleBrokerEpochException.java,clients/src/main/java/org/apache/kafka/common/errors/StaleBrokerEpochException.java,"KAFKA-7235: Detect outdated control requests and bounced brokers using broker generation (#5821)

* KAFKA-7235: Detect outdated control requests and bounced brokers using broker generation

* Add broker_epoch in controlled shutdown request

* Move broker epoch check into controller for ControlledShutdownRequest

* Refactor schema definition for controler requests/responses

* Address comments

* Address comments

* Address comments

* Send back STALE_BROKER_EPOCH error in ControlledShutdown response

* Fix build issue

* Address comments

* Address comments

* Address comments

* Address comments

* Fix tests after rebase

* Address comments

* Address comments",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/common/requests/ControlRequestTest.java,clients/src/test/java/org/apache/kafka/common/requests/ControlRequestTest.java,"KAFKA-7235: Detect outdated control requests and bounced brokers using broker generation (#5821)

* KAFKA-7235: Detect outdated control requests and bounced brokers using broker generation

* Add broker_epoch in controlled shutdown request

* Move broker epoch check into controller for ControlledShutdownRequest

* Refactor schema definition for controler requests/responses

* Address comments

* Address comments

* Address comments

* Send back STALE_BROKER_EPOCH error in ControlledShutdown response

* Fix build issue

* Address comments

* Address comments

* Address comments

* Address comments

* Fix tests after rebase

* Address comments

* Address comments",9,87,0,60,636,4,4,87,87,87,1,1,87,87,87,0,0,0,2,1,0,1
tests/kafkatest/services/delegation_tokens.py,tests/kafkatest/services/delegation_tokens.py,"KAFKA-4544: Add system tests for delegation token based authentication

This change adds some basic system tests for delegation token based authentication:
- basic delegation token creation
- producing with a delegation token
- consuming with a delegation token
- expiring a delegation token
- producing with an expired delegation token

New files:
- delegation_tokens.py: a wrapper around kafka-delegation-tokens.sh  - executed in container where a secure Broker is running (taking advantage of automatic cleanup)
- delegation_tokens_test.py: basic test to validate the lifecycle of a delegation token

Changes were made in the following file to extend their functionality:
- config_property was updated to be able to configure Kafka brokers with delegation token related settings
- jaas.conf template because a broker needs to support multiple login modules when delegation tokens are used
- consule-consumer and verifiable_producer to override KAFKA_OPTS (to specify custom jaas.conf) and the client properties (to authenticate with delegation token).

Author: Attila Sasvari <asasvari@apache.org>

Reviewers: Reviewers: Viktor Somogyi <viktorsomogyi@gmail.com>, Andras Katona <41361962+akatona84@users.noreply.github.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #5660 from asasvari/KAFKA-4544",8,102,0,74,530,7,7,102,102,102,1,1,102,102,102,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/SetSchemaMetadata.java,"MINOR: Add logging to Connect SMTs

Includes Update to ConnectRecord string representation to give
visibility into schemas, useful in SMT tracing

Author: Cyrus Vafadari <cyrus@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5860 from cyrusv/cyrus-logging",18,6,1,101,860,1,9,157,124,31,5,4,170,124,34,13,8,3,2,1,0,1
core/src/main/scala/kafka/zk/ZkSecurityMigratorUtils.scala,core/src/main/scala/kafka/zk/ZkSecurityMigratorUtils.scala,"KAFKA-7259; Remove deprecated ZKUtils usage from ZkSecurityMigrator

- Remove ZKUtils usage from various tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Sriharsha Chintalapani <sriharsha@apache.org>, Ismael Juma <ismael@juma.me.uk>, Satish Duggana <satishd@apache.org>, Jun Rao <junrao@gmail.com>, Ryanne Dolan <ryannedolan@gmail.com>

Closes #5480 from omkreddy/zkutils",0,30,0,5,30,0,0,30,30,30,1,1,30,30,30,0,0,0,2,1,0,1
tests/kafkatest/services/trogdor/produce_bench_workload.py,tests/kafkatest/services/trogdor/produce_bench_workload.py,"KAFKA-7597: Add transaction support to ProduceBenchWorker (#5885)

KAFKA-7597: Add configurable transaction support to ProduceBenchWorker.  In order to get support for serializing Optional<> types to JSON, add a new library: jackson-datatype-jdk8. Once Jackson 3 comes out, this library will not be needed.

Reviewers: Colin McCabe <cmccabe@apache.org>, Ismael Juma <ismael@juma.me.uk>",6,3,1,32,254,2,6,56,65,11,5,2,82,65,16,26,21,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/RestoringTasks.java,"MINOR: Refactor code for restoring tasks (#5768)

Reviewers: Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>",0,2,0,5,37,0,0,25,23,8,3,1,26,23,9,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/server/policy/AlterConfigPolicy.java,clients/src/main/java/org/apache/kafka/server/policy/AlterConfigPolicy.java,"MINOR: Various javadoc improvement in clients and connect (#5878)

Fixed formatting issues and added links in a few classes",4,3,3,27,165,0,4,85,85,42,2,2.0,88,85,44,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/server/policy/CreateTopicPolicy.java,clients/src/main/java/org/apache/kafka/server/policy/CreateTopicPolicy.java,"MINOR: Various javadoc improvement in clients and connect (#5878)

Fixed formatting issues and added links in a few classes",8,3,3,47,290,0,7,128,67,26,5,3,151,67,30,23,10,5,2,1,0,1
core/src/main/scala/kafka/tools/PerfConfig.scala,core/src/main/scala/kafka/tools/PerfConfig.scala,"KAFKA-7418: Add the missing '--help' option to Kafka commands (KIP-374)

Changes made as part of this [KIP-374](https://cwiki.apache.org/confluence/x/FgSQBQ) and [KAFKA-7418](https://issues.apache.org/jira/browse/KAFKA-7418)
 - Checking for empty args or help option in command file to print Usage
 - Added new class to enforce help option to all commands
 - Refactored few lines (ex `PreferredReplicaLeaderElectionCommand`) to
   make use of `CommandDefaultOptions` attributes.
 - Made the changes in help text wordings

Run the unit tests in local(Windows) few Linux friendly tests are failing but
not any functionality, verified `--help` and no option response by running
Scala classes, since those all are having `main` method.

Author: Srinivas Reddy <srinivas96alluri@gmail.com>
Author: Srinivas Reddy <mrsrinivas@users.noreply.github.com>
Author: Srinivas <srinivas96alluri@gmail.com>

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Manikumar Reddy <manikumar.reddy@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Mickael Maison <mickael.maison@gmail.com>

Closes #5910 from mrsrinivas/KIP-374",0,2,4,20,136,0,0,40,48,3,14,1.0,76,48,5,36,17,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Avg.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Avg.java,"KAFKA-7528: Standardize on Min/Avg/Max Kafka metrics' default value - NaN (#5908)

While metrics like Min, Avg and Max make sense to respective use Double.MAX_VALUE, 0.0 and Double.MIN_VALUE as default values to ease computation logic, exposing those values makes reading them a bit misleading. For instance, how would you differentiate whether your -avg metric has a value of 0 because it was given samples of 0 or no samples were fed to it?

It makes sense to standardize on the output of these metrics with something that clearly denotes that no values have been recorded.

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,2,1,22,145,1,3,49,33,5,9,1,85,33,9,36,17,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Max.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Max.java,"KAFKA-7528: Standardize on Min/Avg/Max Kafka metrics' default value - NaN (#5908)

While metrics like Min, Avg and Max make sense to respective use Double.MAX_VALUE, 0.0 and Double.MIN_VALUE as default values to ease computation logic, exposing those values makes reading them a bit misleading. For instance, how would you differentiate whether your -avg metric has a value of 0 because it was given samples of 0 or no samples were fed to it?

It makes sense to standardize on the output of these metrics with something that clearly denotes that no values have been recorded.

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,5,2,22,160,1,3,48,29,6,8,1.5,84,29,10,36,17,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Min.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Min.java,"KAFKA-7528: Standardize on Min/Avg/Max Kafka metrics' default value - NaN (#5908)

While metrics like Min, Avg and Max make sense to respective use Double.MAX_VALUE, 0.0 and Double.MIN_VALUE as default values to ease computation logic, exposing those values makes reading them a bit misleading. For instance, how would you differentiate whether your -avg metric has a value of 0 because it was given samples of 0 or no samples were fed to it?

It makes sense to standardize on the output of these metrics with something that clearly denotes that no values have been recorded.

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",5,5,2,22,159,1,3,48,29,5,9,2,88,29,10,40,17,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/NetworkClientUtils.java,clients/src/main/java/org/apache/kafka/clients/NetworkClientUtils.java,"KAFKA-7576; Fix shutdown of replica fetcher threads (#5875)

ReplicaFetcherThread.shutdown attempts to close the fetcher's Selector while the thread is running. This in unsafe and can result in `Selector.close()` failing with an exception. The exception is caught and logged at debug level, but this can lead to socket leak if the shutdown is due to dynamic config update rather than broker shutdown. This PR changes the shutdown logic to close Selector after the replica fetcher thread is shutdown, with a wakeup() and flag to terminate blocking sends first.

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",18,22,11,59,458,1,4,118,103,30,4,1.5,131,103,33,13,11,3,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewUntypedDemo.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewUntypedDemo.java,"MINOR: Remove deprecated callers (#5911)

Callers of 1) Windows#until, 2) Windows#of, 3) Serialized are replaced when possible with the new APIs.

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bill Bejeck <bill@confluent.io>",2,2,2,66,803,1,1,117,107,4,31,2,294,107,9,177,37,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueMapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueMapper.java,"MINOR: Remove deprecated callers (#5911)

Callers of 1) Windows#until, 2) Windows#of, 3) Serialized are replaced when possible with the new APIs.

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bill Bejeck <bill@confluent.io>",0,2,2,5,46,0,0,57,23,4,13,2,96,34,7,39,13,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowReduceTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowReduceTest.java,"MINOR: Remove deprecated callers (#5911)

Callers of 1) Windows#until, 2) Windows#of, 3) Serialized are replaced when possible with the new APIs.

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bill Bejeck <bill@confluent.io>",3,3,3,98,1042,2,3,132,62,15,9,4,166,62,18,34,11,4,2,1,0,1
tests/kafkatest/services/trogdor/consume_bench_workload.py,tests/kafkatest/services/trogdor/consume_bench_workload.py,"KAFKA-7514: Add threads to ConsumeBenchWorker (#5864)

Add threads with separate consumers to ConsumeBenchWorker.  Update the Trogdor test scripts and documentation with the new functionality.

Reviewers: Colin McCabe <cmccabe@apache.org>",6,2,1,32,262,2,6,56,55,28,2,1.5,57,55,28,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/CredentialCache.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/CredentialCache.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",10,2,1,41,328,1,7,68,71,14,5,3,85,71,17,17,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientCallbackHandler.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",21,1,0,74,718,1,3,106,94,11,10,2.0,145,94,14,39,17,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Crc32.java,clients/src/main/java/org/apache/kafka/common/utils/Crc32.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",19,1,0,314,4924,0,8,400,2169,40,10,1.0,2526,2169,253,2126,2078,213,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/PureJavaCrc32C.java,clients/src/main/java/org/apache/kafka/common/utils/PureJavaCrc32C.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",13,1,0,582,4786,0,5,645,644,322,2,1.0,645,644,322,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/converters/NumberConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/NumberConverter.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",15,1,0,81,663,0,10,127,122,42,3,1,128,122,43,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/ConnectRestConfigurable.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/ConnectRestConfigurable.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",23,7,7,96,542,14,13,138,138,69,2,4.0,145,138,72,7,7,4,2,1,0,1
core/src/main/scala/kafka/utils/Mx4jLoader.scala,core/src/main/scala/kafka/utils/Mx4jLoader.scala,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",5,2,2,37,281,1,1,71,74,6,12,2.0,98,74,8,27,7,2,2,1,0,1
core/src/main/scala/kafka/utils/json/JsonValue.scala,core/src/main/scala/kafka/utils/json/JsonValue.scala,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",7,2,2,40,333,0,2,116,118,39,3,1,120,118,40,4,2,1,2,1,0,1
core/src/test/scala/unit/kafka/admin/TestAdminUtils.scala,core/src/test/scala/unit/kafka/admin/TestAdminUtils.scala,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",5,1,0,11,139,0,5,29,27,10,3,1,29,27,10,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/utils/ZkUtilsTest.scala,core/src/test/scala/unit/kafka/utils/ZkUtilsTest.scala,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",10,1,0,87,592,0,9,133,55,17,8,1.0,135,55,17,2,1,0,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerRecordBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/producer/ProducerRecordBenchmark.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",1,1,1,25,211,1,1,47,47,24,2,1.0,48,47,24,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SerializedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SerializedInternal.java,"KAFKA-7612: Fix javac warnings and enable warnings as errors (#5900)

- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions)
- Use the same javac options when compiling tests (seems accidental that
we didn't do this before)
- Replaced several deprecated method calls with non-deprecated ones:
  - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`
  - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)
  - `scala.Console` (deprecated in Scala 2.11)
  - `PartitionData` taking a timestamp (one of them seemingly a bug)
  - `JsonMappingException` single parameter constructor
- Fix unnecessary usage of raw types in several places.
- Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in
several places.
- Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls)
- Use lambdas to simplify code in a few places
- Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method

Reviewers: Matthias J. Sax <mjsax@apache.org>, Manikumar Reddy <manikumar.reddy@gmail.com>, Randall Hauch <rhauch@gmail.com>, Bill Bejeck <bill@confluent.io>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>",3,2,3,14,110,2,3,34,35,8,4,1.5,42,35,10,8,5,2,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala,core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala,"MINOR: Update zstd, easymock, powermock, zkclient and build plugins (#5846)

EasyMock 4.0.x includes a change that relies on the caller for inferring
the return type of mock creator methods. Updated a number of Scala
tests for compilation and execution to succeed.

The versions of EasyMock and PowerMock in this PR include full support
for Java 11.

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",2,1,1,15,106,1,2,35,35,12,3,1,37,35,12,2,1,1,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginDescTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginDescTest.java,"KAFKA-7431: Clean up connect unit tests

[KAFKA-7431](https://issues.apache.org/jira/browse/KAFKA-7431)

Changes made to improve the code readability:
 - Removed `throws Exception` from the place where there won't be an
 exception
 - Removed type arguments where those can be inferred explicitly by compiler
 - Rewritten Anonymous classes to Java 8 with lambdas

Author: Srinivas Reddy <mrsrinivas@users.noreply.github.com>
Author: Srinivas Reddy <srinivas96alluri@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Ryanne Dolan <ryannedolan@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5681 from mrsrinivas/cleanup-connect-uts",8,5,5,181,1037,5,8,241,241,120,2,3.0,246,241,123,5,5,2,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/PushHttpMetricsReporter.java,tools/src/main/java/org/apache/kafka/tools/PushHttpMetricsReporter.java,"KAFKA-7560; PushHttpMetricsReporter should not convert metric value to double

Currently PushHttpMetricsReporter will convert value from KafkaMetric.metricValue() to double. This will not work for non-numerical metrics such as version in AppInfoParser whose value can be string. This has caused issue for PushHttpMetricsReporter which in turn caused system test kafkatest.tests.client.quota_test.QuotaTest.test_quota to fail.

Since we allow metric value to be object, PushHttpMetricsReporter should also read metric value as object and pass it to the http server.

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5886 from lindong28/KAFKA-7560",36,4,5,256,1723,1,24,331,319,83,4,1.5,345,319,86,14,8,4,2,1,0,1
core/src/main/scala/kafka/utils/ZkUtils.scala,core/src/main/scala/kafka/utils/ZkUtils.scala,"MINOR: Use string/log interpolation instead of string concat in core and clients (#5850)

Also removed a few unused imports and tweaked the log message slightly.

Reviewers: Ismael Juma <ismael@juma.me.uk>",188,3,3,656,5105,2,90,921,280,6,154,3.0,3662,280,24,2741,303,18,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ReauthenticationContext.java,clients/src/main/java/org/apache/kafka/common/network/ReauthenticationContext.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",4,94,0,21,105,4,4,94,94,94,1,1,94,94,94,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslInternalConfigs.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslInternalConfigs.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",1,41,0,7,47,1,1,41,41,41,1,1,41,41,41,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServer.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServer.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",40,3,1,176,1391,1,16,245,224,35,7,4,288,224,41,43,23,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramServerCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramServerCallbackHandler.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",9,4,0,51,437,1,4,76,60,7,11,2,109,60,10,33,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/token/delegation/internals/DelegationTokenCredentialCallback.java,clients/src/main/java/org/apache/kafka/common/security/token/delegation/internals/DelegationTokenCredentialCallback.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",4,9,0,18,97,2,4,40,31,10,4,1.0,42,31,10,2,1,0,1,0,0,0
clients/src/test/java/org/apache/kafka/common/security/TestSecurityConfig.java,clients/src/test/java/org/apache/kafka/common/security/TestSecurityConfig.java,"KAFKA-7352; Allow SASL Connections to Periodically Re-Authenticate (KIP-368) (#5582)

KIP-368 implementation to enable periodic re-authentication of SASL clients. Also adds a broker configuration option to terminate client connections that do not re-authenticate within the configured interval.",1,2,0,27,244,0,1,47,38,9,5,1,64,38,13,17,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedInternal.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedInternal.java,"MINOR: Update Streams Scala API for addition of Grouped (#5793)

While working on the documentation updates I realized the Streams Scala API needs
to get updated for the addition of Grouped

Added a test for Grouped.scala ran all streams-scala tests and streams tests

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",4,1,1,17,111,1,4,40,40,20,2,1.0,41,40,20,1,1,0,1,0,1,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Joined.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Joined.scala,"MINOR: Update Streams Scala API for addition of Grouped (#5793)

While working on the documentation updates I realized the Streams Scala API needs
to get updated for the addition of Grouped

Added a test for Grouped.scala ran all streams-scala tests and streams tests

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,22,0,14,163,0,0,64,42,32,2,1.0,64,42,32,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/network/ChannelState.java,clients/src/main/java/org/apache/kafka/common/network/ChannelState.java,"KAFKA-7475 - capture remote address on connection authetication errors, and log it (#5729)

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",6,14,2,42,245,5,6,105,56,21,5,5,123,56,25,18,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnsupportedCompressionTypeException.java,clients/src/main/java/org/apache/kafka/common/errors/UnsupportedCompressionTypeException.java,"KAFKA-4514; Add Codec for ZStandard Compression (#2267)

This patch adds support for zstandard compression to Kafka as documented in KIP-110: https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression. 

Reviewers: Ivan Babrou <ibobrik@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",2,34,0,10,57,2,2,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModule.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModule.java,"KAFKA-7478: Reduce default logging verbosity in OAuthBearerLoginModule (#5738)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Xavier Léauté <xl+github@xvrl.net>, Rajini Sivaram <rajinisivaram@googlemail.com>",30,11,13,179,1201,5,8,428,330,61,7,7,494,330,71,66,33,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenCallback.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenCallback.java,"KAFKA-7462: Make token optional for OAuthBearerLoginModule (#5733)

OAuthBearerLoginModule is used both on the server-side and client-side (similar to login modules for other mechanisms). OAUTHBEARER tokens are client credentials used only on the client-side to authenticate with servers, but the current implementation requires tokens to be provided on the server-side even if OAUTHBEARER is not used for inter-broker communication. This commit makes tokens optional for server-side login context to allow brokers to be configured without a token when OAUTHBEARER is not used for inter-broker communication.

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Jun Rao <junrao@gmail.com>",7,2,2,37,211,1,6,122,122,61,2,1.5,124,122,62,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/errors/FencedLeaderEpochException.java,clients/src/main/java/org/apache/kafka/common/errors/FencedLeaderEpochException.java,"KAFKA-7395; Add fencing to replication protocol (KIP-320) (#5661)

This patch contains the broker-side support for the fencing improvements from KIP-320. This includes the leader epoch validation in the ListOffsets, OffsetsForLeaderEpoch, and Fetch APIs as well as the changes needed in the fetcher threads to maintain and use the current leader epoch. The client changes from KIP-320 will be left for a follow-up.

One notable change worth mentioning is that we now require the read lock in `Partition` in order to read from the log or to query offsets. This is necessary to ensure the safety of the leader epoch validation. Additionally, we forward all leader epoch changes to the replica fetcher thread and go through the truncation phase. This is needed to ensure the fetcher always has the latest epoch and to guarantee that we cannot miss needed truncation if we missed an epoch change.

Reviewers: Jun Rao <junrao@gmail.com>",2,35,0,10,57,2,2,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/UnknownLeaderEpochException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownLeaderEpochException.java,"KAFKA-7395; Add fencing to replication protocol (KIP-320) (#5661)

This patch contains the broker-side support for the fencing improvements from KIP-320. This includes the leader epoch validation in the ListOffsets, OffsetsForLeaderEpoch, and Fetch APIs as well as the changes needed in the fetcher threads to maintain and use the current leader epoch. The client changes from KIP-320 will be left for a follow-up.

One notable change worth mentioning is that we now require the read lock in `Partition` in order to read from the log or to query offsets. This is necessary to ensure the safety of the leader epoch validation. Additionally, we forward all leader epoch changes to the replica fetcher thread and go through the truncation phase. This is needed to ensure the fetcher always has the latest epoch and to guarantee that we cannot miss needed truncation if we missed an epoch change.

Reviewers: Jun Rao <junrao@gmail.com>",2,35,0,10,57,2,2,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
core/src/main/scala/kafka/cluster/BrokerEndPoint.scala,core/src/main/scala/kafka/cluster/BrokerEndPoint.scala,"KAFKA-7395; Add fencing to replication protocol (KIP-320) (#5661)

This patch contains the broker-side support for the fencing improvements from KIP-320. This includes the leader epoch validation in the ListOffsets, OffsetsForLeaderEpoch, and Fetch APIs as well as the changes needed in the fetcher threads to maintain and use the current leader epoch. The client changes from KIP-320 will be left for a follow-up.

One notable change worth mentioning is that we now require the read lock in `Partition` in order to read from the log or to query offsets. This is necessary to ensure the safety of the leader epoch validation. Additionally, we forward all leader epoch changes to the replica fetcher thread and go through the truncation phase. This is needed to ensure the fetcher always has the latest epoch and to guarantee that we cannot miss needed truncation if we missed an epoch change.

Reviewers: Jun Rao <junrao@gmail.com>",10,4,0,40,289,0,5,83,67,9,9,1,100,67,11,17,6,2,2,1,0,1
core/src/main/scala/kafka/utils/DelayedItem.scala,core/src/main/scala/kafka/utils/DelayedItem.scala,"KAFKA-7395; Add fencing to replication protocol (KIP-320) (#5661)

This patch contains the broker-side support for the fencing improvements from KIP-320. This includes the leader epoch validation in the ListOffsets, OffsetsForLeaderEpoch, and Fetch APIs as well as the changes needed in the fetcher threads to maintain and use the current leader epoch. The client changes from KIP-320 will be left for a follow-up.

One notable change worth mentioning is that we now require the read lock in `Partition` in order to read from the log or to query offsets. This is necessary to ensure the safety of the leader epoch validation. Additionally, we forward all leader epoch changes to the replica fetcher thread and go through the truncation phase. This is needed to ensure the fetcher always has the latest epoch and to guarantee that we cannot miss needed truncation if we missed an epoch change.

Reviewers: Jun Rao <junrao@gmail.com>",3,1,1,15,144,0,3,44,48,5,9,1,91,48,10,47,19,5,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/SchemaProjector.java,connect/api/src/main/java/org/apache/kafka/connect/data/SchemaProjector.java,"KAFKA-7476: Fix Date-based types in SchemaProjector

Various converters (AvroConverter and JsonConverter) produce a
SchemaAndValue consisting of a logical schema type and a java.util.Date.
This is a fix for SchemaProjector to properly handle the Date.

Author: Robert Yokota <rayokota@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5736 from rayokota/KAFKA-7476",52,1,1,150,1225,1,9,196,197,39,5,2,220,197,44,24,13,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/Cancellable.java,streams/src/main/java/org/apache/kafka/streams/processor/Cancellable.java,"KAFKA-7277: Migrate Streams API to Duration instead of longMs times (#5682)

Reviewers: Johne Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,3,1,5,28,0,0,32,24,6,5,3,41,24,8,9,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/PunctuationType.java,streams/src/main/java/org/apache/kafka/streams/processor/PunctuationType.java,"KAFKA-7277: Migrate Streams API to Duration instead of longMs times (#5682)

Reviewers: Johne Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,3,1,6,27,0,0,36,34,12,3,2,39,34,13,3,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/Punctuator.java,streams/src/main/java/org/apache/kafka/streams/processor/Punctuator.java,"KAFKA-7277: Migrate Streams API to Duration instead of longMs times (#5682)

Reviewers: Johne Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,3,1,5,30,0,0,34,26,11,3,2,36,26,12,2,1,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/WindowStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/WindowStoreIterator.java,"KAFKA-7277: Migrate Streams API to Duration instead of longMs times (#5682)

Reviewers: Johne Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,3,1,8,60,0,0,37,26,4,10,1.5,63,26,6,26,18,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/SuppressedTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/SuppressedTest.java,"KAFKA-7223: Add name config to Suppressed (#5731)

KIP-372 (allow naming all internal topics) was designed and developed concurrently with suppression.

Since suppression introduces a new internal topic, it also needs to be nameable.

Reviewers: Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",3,31,8,100,806,2,3,135,112,34,4,5.5,154,112,38,19,8,5,1,0,1,1
clients/src/test/java/org/apache/kafka/test/MockMetricsReporter.java,clients/src/test/java/org/apache/kafka/test/MockMetricsReporter.java,KAFKA-6123: Give client MetricsReporter auto-generated client.id (#5383),6,3,0,30,199,1,6,55,52,9,6,2.5,64,52,11,9,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullTimeWindowedSerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullTimeWindowedSerde.java,"KAFKA-7223: In-Memory Suppression Buffering (#5693)

Reviewer: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",1,32,0,14,139,1,1,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/InMemoryTimeOrderedKeyValueBuffer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/InMemoryTimeOrderedKeyValueBuffer.java,"KAFKA-7223: In-Memory Suppression Buffering (#5693)

Reviewer: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",15,116,0,80,606,6,6,116,116,116,1,1,116,116,116,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/TimeKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/TimeKey.java,"KAFKA-7223: In-Memory Suppression Buffering (#5693)

Reviewer: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",11,60,0,34,225,6,6,60,60,60,1,1,60,60,60,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/CompositeRestoreListener.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/CompositeRestoreListener.java,"KAFKA-7223: internally provide full consumer record during restore (#5710)

The Suppression buffer stores the full record context, not just the key and value,
so its changelog/restore loop will also need to preserve this information.

This change is a precondition to that, creating an option to register a
state restore callback to receive the full consumer record.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,14,19,70,472,7,9,116,109,23,5,3,171,109,34,55,27,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestoreCallbackAdapter.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StateRestoreCallbackAdapter.java,"KAFKA-7223: internally provide full consumer record during restore (#5710)

The Suppression buffer stores the full record context, not just the key and value,
so its changelog/restore loop will also need to preserve this information.

This change is a precondition to that, creating an option to register a
state restore callback to receive the full consumer record.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",6,52,0,31,267,2,2,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/serialization/ExtendedDeserializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ExtendedDeserializer.java,"KAFKA-6923; Refactor Serializer/Deserializer for KIP-336 (#5494)

This patch implements KIP-336. It adds a default implementation to the Serializer/Deserializer interface to support the use of headers and it deprecates the ExtendedSerializer and ExtendedDeserializer interfaces for later removal.

Reviewers: Satish Duggana <sduggana@hortonworks.com>, John Roesler <john@confluent.io>, Jason Gustafson <jason@confluent.io>",8,2,0,32,242,0,6,79,56,20,4,1.5,80,56,20,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/serialization/ExtendedSerializer.java,clients/src/main/java/org/apache/kafka/common/serialization/ExtendedSerializer.java,"KAFKA-6923; Refactor Serializer/Deserializer for KIP-336 (#5494)

This patch implements KIP-336. It adds a default implementation to the Serializer/Deserializer interface to support the use of headers and it deprecates the ExtendedSerializer and ExtendedDeserializer interfaces for later removal.

Reviewers: Satish Duggana <sduggana@hortonworks.com>, John Roesler <john@confluent.io>, Jason Gustafson <jason@confluent.io>",8,2,0,32,242,0,6,79,55,20,4,1.5,79,55,20,0,0,0,1,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/SessionBytesStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/SessionBytesStoreSupplier.java,"MINOR: cleanup some state store code (#5656)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,8,6,6,51,0,0,47,33,12,4,1.0,54,33,14,7,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/StoreBuilder.java,streams/src/main/java/org/apache/kafka/streams/state/StoreBuilder.java,"MINOR: cleanup some state store code (#5656)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",0,0,1,13,106,0,0,87,82,29,3,1,88,82,29,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractStatus.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractStatus.java,"KAFKA-6926: Simplified some logic to eliminate some suppressions of NPath complexity checks (#5051)

Modified several classes' `equals` methods and simplified a complex method to
reduce the NPath complexity so they could be removed from the checkstyle
suppressions that were required with the recent move to Java 8 and upgrade
of Checkstyle: https://github.com/apache/kafka/pull/5046.

Reviewers: Robert Yokota <rayokota@gmail.com>, Arjun Satish <arjun@confluent.io>, Ismael Juma <ismael@juma.me.uk>",20,7,6,71,389,1,9,102,100,26,4,1.5,115,100,29,13,7,3,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Consumed.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Consumed.scala,"KAFKA-7396: Materialized, Serialized, Joined, Consumed and Produced with implicit Serdes (#5551)

We want to make sure that we always have a serde for all Materialized, Serialized, Joined, Consumed and Produced.
For that we can make use of the implicit parameters in Scala.

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-365%3A+Materialized%2C+Serialized%2C+Joined%2C+Consumed+and+Produced+with+implicit+Serde

Reviewers: John Roesler <vvcephei@users.noreply.github.com>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Ted Yu <yuzhihong@gmail.com>",0,79,0,20,281,0,0,79,79,79,1,1,79,79,79,0,0,0,0,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Materialized.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Materialized.scala,"KAFKA-7396: Materialized, Serialized, Joined, Consumed and Produced with implicit Serdes (#5551)

We want to make sure that we always have a serde for all Materialized, Serialized, Joined, Consumed and Produced.
For that we can make use of the implicit parameters in Scala.

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-365%3A+Materialized%2C+Serialized%2C+Joined%2C+Consumed+and+Produced+with+implicit+Serde

Reviewers: John Roesler <vvcephei@users.noreply.github.com>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Ted Yu <yuzhihong@gmail.com>",0,107,0,23,363,0,0,107,107,107,1,1,107,107,107,0,0,0,0,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Produced.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/Produced.scala,"KAFKA-7396: Materialized, Serialized, Joined, Consumed and Produced with implicit Serdes (#5551)

We want to make sure that we always have a serde for all Materialized, Serialized, Joined, Consumed and Produced.
For that we can make use of the implicit parameters in Scala.

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-365%3A+Materialized%2C+Serialized%2C+Joined%2C+Consumed+and+Produced+with+implicit+Serde

Reviewers: John Roesler <vvcephei@users.noreply.github.com>, Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <guozhang@confluent.io>, Ted Yu <yuzhihong@gmail.com>",0,59,0,11,154,0,0,59,59,59,1,1,59,59,59,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java,clients/src/main/java/org/apache/kafka/clients/InFlightRequests.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",29,1,7,97,766,1,14,185,126,11,17,4,314,126,18,129,25,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/TopicPartition.java,clients/src/main/java/org/apache/kafka/common/TopicPartition.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",11,3,9,45,244,2,6,73,61,9,8,1.0,88,61,11,15,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntryFilter.java,clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntryFilter.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",21,1,3,62,424,1,13,143,117,29,5,1,150,117,30,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/memory/GarbageCollectedMemoryPool.java,clients/src/main/java/org/apache/kafka/common/memory/GarbageCollectedMemoryPool.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",22,1,1,110,757,1,11,168,168,84,2,1.0,169,168,84,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java,clients/src/main/java/org/apache/kafka/common/record/ByteBufferLogInputStream.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",12,1,2,47,383,1,3,88,119,11,8,3.0,186,119,23,98,65,12,2,1,0,1
clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java,clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",19,1,3,55,342,1,10,118,90,15,8,1.0,238,99,30,120,99,15,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClient.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClient.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",33,5,5,146,1051,4,14,197,178,39,5,1,207,178,41,10,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/plain/PlainLoginModule.java,clients/src/main/java/org/apache/kafka/common/security/plain/PlainLoginModule.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",7,4,5,38,238,4,5,65,66,13,5,1,81,66,16,16,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/scram/ScramLoginModule.java,clients/src/main/java/org/apache/kafka/common/security/scram/ScramLoginModule.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",8,4,5,47,328,4,5,75,67,11,7,2,93,67,13,18,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramExtensions.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramExtensions.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",4,1,1,20,158,1,4,43,78,7,6,1.0,91,78,15,48,26,8,2,1,0,1
clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaEntity.java,clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaEntity.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",0,2,2,15,56,0,0,62,62,31,2,1.5,64,62,32,2,2,1,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/authenticator/TestDigestLoginModule.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/TestDigestLoginModule.java,"MINOR: Code cleanup of 'clients' module (#5427)

Cleanup involves:
* Refactoring to use Java 8 constructs (lambdas,
diamond for `empty` collection methods) and library
methods (`computeIfAbsent`)
* Simplifying code (including unnecessarily complex
`equals` and `hashCode` implementations)
* Removing redundant code
* Fixing typos

Reviewers: Ryanne Dolan, Ismael Juma <ismael@juma.me.uk>",11,1,3,47,366,1,3,76,109,15,5,3,141,109,28,65,52,13,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetAndTimestamp.java,"KAFKA-7333; Protocol changes for KIP-320

This patch contains the protocol updates needed for KIP-320 as well as some of the basic consumer APIs (e.g. `OffsetAndMetadata` and `ConsumerRecord`). The inter-broker format version has not been changed and the brokers will continue to use the current API versions.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Dong Lin <lindong28@gmail.com>

Closes #5564 from hachikuji/KAFKA-7333",15,36,11,48,288,6,8,85,58,14,6,3.5,110,58,18,25,11,4,2,1,0,1
core/src/test/scala/integration/kafka/server/ReplicaFetcherThreadFatalErrorTest.scala,core/src/test/scala/integration/kafka/server/ReplicaFetcherThreadFatalErrorTest.scala,"MINOR: Move common out of range handling into AbstractFetcherThread (#5608)

This patch removes the duplication of the out of range handling between `ReplicaFetcherThread` and `ReplicaAlterLogDirsThread` and attempts to expose a cleaner API for extension. It also adds a mock implementation to facilitate testing and several new test cases.

Reviewers: Jun Rao <junrao@gmail.com>",12,1,1,98,818,4,12,146,145,10,15,1,173,145,12,27,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/resource/ResourcePatternFilter.java,clients/src/main/java/org/apache/kafka/common/resource/ResourcePatternFilter.java,"MINOR: Tidy up pattern type comparisons, remove unused producer-id (#5593)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Manikumar Reddy <manikumar.reddy@gmail.com>",34,1,1,85,581,1,11,170,169,57,3,1,194,169,65,24,23,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/PrintForeachAction.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/PrintForeachAction.java,"KAFKA-7326: KStream.print() should flush on each line for PrintStream (#5579)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>",6,3,0,36,276,1,3,67,59,13,5,1,91,59,18,24,16,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/AuthenticationException.java,clients/src/main/java/org/apache/kafka/common/errors/AuthenticationException.java,"KAFKA-6950: Delay response to failed client authentication to prevent potential DoS issues (KIP-306) (#5082)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",3,4,0,14,79,1,3,51,27,10,5,1,60,27,12,9,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/DelayedResponseAuthenticationException.java,clients/src/main/java/org/apache/kafka/common/network/DelayedResponseAuthenticationException.java,"KAFKA-6950: Delay response to failed client authentication to prevent potential DoS issues (KIP-306) (#5082)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>",1,27,0,8,52,1,1,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java,clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java,"KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)

Set empty extensions if null is passed in.

Reviewers: Satish Duggana <sduggana@hortonworks.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",8,4,0,28,196,0,5,61,57,30,2,1.0,61,57,30,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java,clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java,"KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)

Set empty extensions if null is passed in.

Reviewers: Satish Duggana <sduggana@hortonworks.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,12,4,12,78,1,2,51,43,26,2,3.0,55,43,28,4,4,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java,"KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)

Set empty extensions if null is passed in.

Reviewers: Satish Duggana <sduggana@hortonworks.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",23,58,5,95,827,3,9,189,96,63,3,12,206,96,69,17,12,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TopicDeletionDisabledException.java,clients/src/main/java/org/apache/kafka/common/errors/TopicDeletionDisabledException.java,"KAFKA-5975; No response when deleting topics and delete.topic.enable=false (#3960)

This PR implements [KIP-322](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=87295558).

Reviewers: Tom Bentley <tbentley@redhat.com>, Manikumar Reddy O <manikumar.reddy@gmail.com>, Jason Gustafson <jason@confluent.io>",2,29,0,9,45,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowingDefaults.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowingDefaults.java,"MINOR: restructure Windows to favor immutable implementation (#5536)

Update to KIP-328.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>",1,23,0,5,39,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/EqualityCheck.java,streams/src/test/java/org/apache/kafka/streams/EqualityCheck.java,"MINOR: restructure Windows to favor immutable implementation (#5536)

Update to KIP-328.

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Kamal Chandraprakash <kamal.chandraprakash@gmail.com>",27,153,0,104,687,5,5,153,153,153,1,1,153,153,153,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/resource/PatternType.java,clients/src/main/java/org/apache/kafka/common/resource/PatternType.java,"MINOR: Improved configuration formatting in documentation (#5532)

Reviewers: Jason Gustafson <jason@confluent.io>",8,1,1,44,302,0,6,124,91,31,4,1.5,143,91,36,19,18,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/TopologyWrapper.java,streams/src/test/java/org/apache/kafka/streams/TopologyWrapper.java,"KAFKA-6998: Disable Caching when max.cache.bytes are zero. (#5488)

1) As titled, add a rewriteTopology that 1) sets application id, 2) maybe disable caching, 3) adjust for source KTable. This optimization can hence be applied for both DSL or PAPI generated Topology.

2) Defer the building of globalStateStores in rewriteTopology so that we can also disable caching. But we still need to build the state stores before InternalTopologyBuilder.build() since we should only build global stores once for all threads.

3) Added withCachingDisabled to StoreBuilder, it is a public API change.

4) [Optional] Fixed unit test config setting functionalities, and set the necessary config to shorten the unit test latency (now it reduces from 5min to 3.5min on my laptop).

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Ted Yu <yuzhihong@gmail.com>",3,5,4,14,121,4,3,39,34,10,4,1.0,44,34,11,5,4,1,2,1,0,1
streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java,streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java,"KAFKA-7285: Create new producer on each rebalance if EOS enabled (#5501)

Reviewers: Guozhang Wang <guozhang@confluent.io>, John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>",42,1,1,207,1485,1,24,277,95,5,51,2,638,95,13,361,85,7,2,1,0,1
clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenMock.java,clients/src/test/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerTokenMock.java,"KAFKA-7169: Validate SASL extensions through callback on server side (#5497)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",5,46,0,24,90,5,5,46,46,46,1,1,46,46,46,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/ScramExtensionsCallback.java,clients/src/main/java/org/apache/kafka/common/security/scram/ScramExtensionsCallback.java,"KAFKA-7169: Custom SASL extensions for OAuthBearer authentication mechanism (KIP-342) (#5379)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",2,5,5,13,95,0,2,46,32,12,4,3.5,57,32,14,11,6,3,1,0,1,1
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramMessages.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramMessages.java,"KAFKA-7169: Custom SASL extensions for OAuthBearer authentication mechanism (KIP-342) (#5379)

Reviewers: Ron Dagostino <rndgstn@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>",42,4,1,217,1406,1,30,288,272,32,9,3,345,272,38,57,22,6,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamJoinWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamJoinWindow.java,"MINOR: clean up window store interface to avoid confusion (#5359)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>",5,3,10,32,247,3,4,62,68,5,13,3,132,68,10,70,24,5,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonPOJODeserializer.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonPOJODeserializer.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,4,4,31,205,4,4,61,66,12,5,2,77,66,15,16,7,3,2,1,0,1
streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonPOJOSerializer.java,streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/JsonPOJOSerializer.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,3,3,26,171,4,4,55,60,9,6,2.0,71,60,12,16,7,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/PunctuationQueue.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/PunctuationQueue.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",7,2,2,38,261,3,3,70,56,9,8,1.5,85,56,11,15,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/PunctuationSchedule.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/PunctuationSchedule.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",13,2,2,69,369,4,12,110,43,14,8,2.5,138,49,17,28,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StampedRecord.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StampedRecord.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",8,1,1,30,183,2,8,56,52,14,4,1.5,61,52,15,5,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueLoggedStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueLoggedStore.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",16,15,15,86,703,18,11,133,132,8,17,3,242,132,14,109,33,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/OrderedBytes.java,streams/src/main/java/org/apache/kafka/streams/state/internals/OrderedBytes.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,5,5,32,256,4,2,68,68,23,3,1,74,68,25,6,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/WindowedStreamPartitionerTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/WindowedStreamPartitionerTest.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",3,8,8,51,625,1,1,83,84,5,16,2.0,201,84,13,118,83,7,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/WallclockTimestampExtractorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/WallclockTimestampExtractorTest.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,4,4,34,255,8,5,62,62,21,3,3,72,62,24,10,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/QuickUnionTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/QuickUnionTest.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",6,9,9,61,634,4,3,95,97,24,4,2.5,109,97,27,14,9,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/WrappedBatchingStateRestoreCallbackTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/WrappedBatchingStateRestoreCallbackTest.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",1,1,1,25,269,1,1,51,51,26,2,1.0,52,51,26,1,1,0,1,0,1,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentedCacheFunctionTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SegmentedCacheFunctionTest.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",4,1,1,85,506,1,4,126,124,32,4,2.5,134,124,34,8,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/KTableValueGetterStub.java,streams/src/test/java/org/apache/kafka/test/KTableValueGetterStub.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,1,1,24,166,2,5,50,46,12,4,1.0,57,46,14,7,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockMapper.java,streams/src/test/java/org/apache/kafka/test/MockMapper.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",12,5,5,61,528,10,12,92,36,12,8,3.0,110,36,14,18,7,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockReducer.java,streams/src/test/java/org/apache/kafka/test/MockReducer.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",4,2,2,32,215,4,4,63,43,16,4,2.0,69,43,17,6,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockValueJoiner.java,streams/src/test/java/org/apache/kafka/test/MockValueJoiner.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",1,1,1,13,106,1,1,33,33,8,4,2.0,47,33,12,14,9,4,2,1,0,1
streams/src/test/java/org/apache/kafka/test/SegmentedBytesStoreStub.java,streams/src/test/java/org/apache/kafka/test/SegmentedBytesStoreStub.java,"MINOR: Require final variables in Streams (#5452)

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",13,2,2,79,519,4,13,112,95,22,5,1,120,95,24,8,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedStreamPartitioner.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedStreamPartitioner.java,"KAFKA-6761: Construct Physical Plan using Graph, Reduce streams footprint part III (#5201)

The specific changes in this PR from the second PR include:

1. Changed the types of graph nodes to names conveying more context
2. Build the entire physical plan from the graph, after StreamsBuilder.build() is called.

Other changes are addressed directly as review comments on the PR.

Testing consists of using all existing streams tests to validate building the physical plan with graph

Reviewers: Matthias J. Sax <matthias@confluent.io>, John Roesler <vvcephei@users.noreply.github.com>, Guozhang Wang <wangguoz@gmail.com>",2,1,1,16,169,1,2,51,52,5,10,2.5,78,52,8,27,4,3,2,1,0,1
connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSinkTask.java,connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSinkTask.java,"MINOR: FileStreamSinkTask should create file if it doesn't exist (#5406)

A recent change from `new FileOutputStream` to `Files.newOutputStream` missed the `CREATE` flag (which is necessary in addition to `APPEND`).

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,4,2,67,455,1,8,99,79,8,13,2,128,79,10,29,5,2,2,1,0,1
streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala,streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinWithIncompleteMetadataIntegrationTest.scala,"MINOR: Fix format violations streams scala tests (#5402)

@guozhangwang @mjsax hot fix for streams scala test format violations

Reviewers: Guozhang Wang <wangguoz@gmail.com>",3,1,1,44,414,1,1,88,88,44,2,1.0,89,88,44,1,1,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/KafkaStreamsWrapper.java,streams/src/test/java/org/apache/kafka/streams/KafkaStreamsWrapper.java,"KAFKA-5037: Fix infinite loop if all input topics are unknown at startup

1. At the beginning of assign, we first check that all the non-repartition source topics are included in the metadata. If not, we log an error at the leader and set an error in the Assignment userData bytes, indicating that leader cannot complete assignment and the error code would indicate the root cause of it.

2. Upon receiving the assignment, if the error is not NONE the streams will shutdown itself with a log entry re-stating the root cause interpreted from the error code.

Author: tedyu <yuzhihong@gmail.com>

Reviewers: Matthias J. Sax <mjsax@apache.org>, Guozhang Wang <wangguoz@gmail.com>

Closes #5322 from tedyu/trunk",4,51,0,20,124,2,2,51,51,51,1,1,51,51,51,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/provider/FileConfigProvider.java,clients/src/main/java/org/apache/kafka/common/config/provider/FileConfigProvider.java,"MINOR: Avoid FileInputStream/FileOutputStream (#5281)

They rely on finalizers (before Java 11), which create
unnecessary GC load. The alternatives are as easy to
use and don't have this issue.

Also use FileChannel directly instead of retrieving
it from RandomAccessFile whenever possible
since the indirection is unnecessary.

Finally, add a few try/finally blocks.

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Rajini Sivaram <rajinisivaram@googlemail.com>",15,3,5,61,452,1,5,103,101,26,4,1.5,110,101,28,7,5,2,1,0,1,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/FileOffsetBackingStore.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/FileOffsetBackingStore.java,"MINOR: Avoid FileInputStream/FileOutputStream (#5281)

They rely on finalizers (before Java 11), which create
unnecessary GC load. The alternatives are as easy to
use and don't have this issue.

Also use FileChannel directly instead of retrieving
it from RandomAccessFile whenever possible
since the indirection is unnecessary.

Finally, add a few try/finally blocks.

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Rajini Sivaram <rajinisivaram@googlemail.com>",16,6,7,70,627,2,6,104,111,9,11,3,157,111,14,53,19,5,2,1,0,1
core/src/test/scala/other/kafka/TestTruncate.scala,core/src/test/scala/other/kafka/TestTruncate.scala,"MINOR: Avoid FileInputStream/FileOutputStream (#5281)

They rely on finalizers (before Java 11), which create
unnecessary GC load. The alternatives are as easy to
use and don't have this issue.

Also use FileChannel directly instead of retrieving
it from RandomAccessFile whenever possible
since the indirection is unnecessary.

Finally, add a few try/finally blocks.

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Rajini Sivaram <rajinisivaram@googlemail.com>",1,3,1,19,142,1,1,41,38,14,3,1,48,38,16,7,6,2,2,1,0,1
core/src/main/scala/kafka/coordinator/group/OffsetConfig.scala,core/src/main/scala/kafka/coordinator/group/OffsetConfig.scala,"MINOR: KIP-211 Follow-up (#5272)

Updates the description of `offsets.retention.minutes` config, and fixes an upgrade note.",0,2,1,24,165,0,0,62,61,21,3,1,65,61,22,3,2,1,2,1,0,1
tests/kafkatest/benchmarks/streams/streams_simple_benchmark_test.py,tests/kafkatest/benchmarks/streams/streams_simple_benchmark_test.py,"MINOR: report streams benchmarks separately (#5275)

Specify each benchmark as a separate test so that we can see the results reported independently.

Reviewers: Guozhang Wang <wangguoz@gmail.com>",24,5,1,107,860,0,3,164,48,10,16,1.0,245,81,15,81,33,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowReduce.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowReduce.java,"KAFKA-6978: make window retention time strict (#5218)

Enforce window retention times strictly:

* records for windows that are expired get dropped
* queries for timestamps old enough to be expired immediately answered with null

Reviewers: Bill Bejeck <bill@confluent.io>, Damian Guy <damian@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",2,7,122,16,133,9,1,34,165,2,19,3,269,165,14,235,122,12,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampSupplier.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampSupplier.java,"KAFKA-6978: make window retention time strict (#5218)

Enforce window retention times strictly:

* records for windows that are expired get dropped
* queries for timestamps old enough to be expired immediately answered with null

Reviewers: Bill Bejeck <bill@confluent.io>, Damian Guy <damian@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",0,21,0,4,23,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/OffsetMetadataAndError.scala,core/src/main/scala/kafka/common/OffsetMetadataAndError.scala,"KAFKA-4682; Revise expiration semantics of consumer group offsets (KIP-211 - Part 1) (#4896)

This patch contains the improved offset expiration semantics proposed in KIP-211. Committed offsets will not be expired as long as a group is active. Once all members have left the group, then offsets will be expired after the timeout configured by `offsets.retention.minutes`. Note that the optimization for early expiration of unsubscribed topics will be implemented in a separate patch.",7,3,3,42,464,1,7,81,36,5,16,1.0,145,42,9,64,22,4,2,1,0,1
streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/WordCount.java,streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/WordCount.java,"MINOR: update web docs and examples of Streams with Java8 syntax (#5249)

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian@confluent.io>",2,0,22,49,472,1,1,81,97,27,3,2,109,97,36,28,22,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ListenerNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/ListenerNotFoundException.java,"KAFKA-6546: Use LISTENER_NOT_FOUND_ON_LEADER error for missing listener (#5189)

For metadata request version 6 and above, use a different error code to indicate missing listener on leader broker to enable diagnosis of listener configuration issues.

Reviewers: Ismael Juma <ismael@juma.me.uk>",2,38,0,10,57,2,2,38,38,38,1,1,38,38,38,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/IndexOffsetOverflowException.scala,core/src/main/scala/kafka/common/IndexOffsetOverflowException.scala,"MINOR: Use exceptions in o.a.k.common if possible and deprecate ZkUtils (#5255)

Also:
- Remove exceptions in `kafka.common` that are no longer used.
- Keep `kafka.common.KafkaException` as it's still used by `ZkUtils`,
`kafka.admin.AdminClient` and `kafka.security.auth` classes and
we would like to maintain compatibility for now.
- Add deprecated annotation to `kafka.admin.AdminClient`. The scaladoc
stated that the class is deprecated, but the annotation was missing.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",1,1,1,4,46,0,1,25,25,12,2,1.0,26,25,13,1,1,0,2,1,0,1
core/src/main/scala/kafka/common/LogSegmentOffsetOverflowException.scala,core/src/main/scala/kafka/common/LogSegmentOffsetOverflowException.scala,"MINOR: Use exceptions in o.a.k.common if possible and deprecate ZkUtils (#5255)

Also:
- Remove exceptions in `kafka.common` that are no longer used.
- Keep `kafka.common.KafkaException` as it's still used by `ZkUtils`,
`kafka.admin.AdminClient` and `kafka.security.auth` classes and
we would like to maintain compatibility for now.
- Add deprecated annotation to `kafka.admin.AdminClient`. The scaladoc
stated that the class is deprecated, but the annotation was missing.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",0,1,1,5,39,0,0,30,31,10,3,1,34,31,11,4,3,1,2,1,0,1
core/src/main/scala/kafka/consumer/BaseConsumerRecord.scala,core/src/main/scala/kafka/consumer/BaseConsumerRecord.scala,"KAFKA-2983: Remove Scala consumers and related code (#5230)

- Removed Scala consumers (`SimpleConsumer` and `ZooKeeperConsumerConnector`)
and their tests.
- Removed Scala request/response/message classes.
- Removed any mention of new consumer or new producer in the code
with the exception of MirrorMaker where the new.consumer option was
never deprecated so we have to keep it for now. The non-code
documentation has not been updated either, that will be done
separately.
- Removed a number of tools that only made sense in the context
of the Scala consumers (see upgrade notes).
- Updated some tools that worked with both Scala and Java consumers
so that they only support the latter (see upgrade notes).
- Removed `BaseConsumer` and related classes apart from `BaseRecord`
which is used in `MirrorMakerMessageHandler`. The latter is a pluggable
interface so effectively public API.
- Removed `ZkUtils` methods that were only used by the old consumers.
- Removed `ZkUtils.registerBroker` and `ZKCheckedEphemeral` since
the broker now uses the methods in `KafkaZkClient` and no-one else
should be using that method.
- Updated system tests so that they don't use the Scala consumers except
for multi-version tests.
- Updated LogDirFailureTest so that the consumer offsets topic would
continue to be available after all the failures. This was necessary for it
to work with the Java consumer.
- Some multi-version system tests had not been updated to include
recently released Kafka versions, fixed it.
- Updated findBugs and checkstyle configs not to refer to deleted
classes and packages.

Reviewers: Dong Lin <lindong28@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",0,13,5,14,110,0,0,33,23,7,5,1,48,23,10,15,6,3,2,1,0,1
tests/kafkatest/services/mirror_maker.py,tests/kafkatest/services/mirror_maker.py,"KAFKA-2983: Remove Scala consumers and related code (#5230)

- Removed Scala consumers (`SimpleConsumer` and `ZooKeeperConsumerConnector`)
and their tests.
- Removed Scala request/response/message classes.
- Removed any mention of new consumer or new producer in the code
with the exception of MirrorMaker where the new.consumer option was
never deprecated so we have to keep it for now. The non-code
documentation has not been updated either, that will be done
separately.
- Removed a number of tools that only made sense in the context
of the Scala consumers (see upgrade notes).
- Updated some tools that worked with both Scala and Java consumers
so that they only support the latter (see upgrade notes).
- Removed `BaseConsumer` and related classes apart from `BaseRecord`
which is used in `MirrorMakerMessageHandler`. The latter is a pluggable
interface so effectively public API.
- Removed `ZkUtils` methods that were only used by the old consumers.
- Removed `ZkUtils.registerBroker` and `ZKCheckedEphemeral` since
the broker now uses the methods in `KafkaZkClient` and no-one else
should be using that method.
- Updated system tests so that they don't use the Scala consumers except
for multi-version tests.
- Updated LogDirFailureTest so that the consumer offsets topic would
continue to be available after all the failures. This was necessary for it
to work with the Java consumer.
- Some multi-version system tests had not been updated to include
recently released Kafka versions, fixed it.
- Updated findBugs and checkstyle configs not to refer to deleted
classes and packages.

Reviewers: Dong Lin <lindong28@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",12,3,25,99,846,3,8,177,165,12,15,3,241,165,16,64,25,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/converters/DoubleConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/DoubleConverter.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",1,3,1,11,111,0,1,37,35,18,2,1.5,38,35,19,1,1,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/converters/FloatConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/FloatConverter.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",1,3,1,11,111,0,1,37,35,18,2,1.5,38,35,19,1,1,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/converters/IntegerConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/IntegerConverter.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",1,3,1,11,111,0,1,37,35,18,2,1.5,38,35,19,1,1,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/converters/LongConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/LongConverter.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",1,3,1,11,111,0,1,38,36,19,2,1.5,39,36,20,1,1,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/converters/NumberConverterConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/NumberConverterConfig.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",2,2,1,13,94,0,2,39,38,20,2,1.5,40,38,20,1,1,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/converters/ShortConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/ShortConverter.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",1,3,1,11,111,0,1,37,35,18,2,1.5,38,35,19,1,1,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/converters/DoubleConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/DoubleConverterTest.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",4,1,1,21,138,0,4,43,43,22,2,1.0,44,43,22,1,1,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/converters/FloatConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/FloatConverterTest.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",4,1,1,21,138,0,4,43,43,22,2,1.0,44,43,22,1,1,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/converters/IntegerConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/IntegerConverterTest.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",4,1,1,21,136,0,4,43,43,22,2,1.0,44,43,22,1,1,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/converters/LongConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/LongConverterTest.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",4,1,1,21,136,0,4,43,43,22,2,1.0,44,43,22,1,1,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/converters/ShortConverterTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/converters/ShortConverterTest.java,"KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)

KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.

All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.

It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.

This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5222 from rhauch/kafka-7056",4,1,1,21,136,0,4,44,44,22,2,1.0,45,44,22,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/resource/ResourcePattern.java,clients/src/main/java/org/apache/kafka/common/resource/ResourcePattern.java,"KAFKA-7010: Rename ResourceNameType to PatternType (#5205)

The initial PR for KIP-290 #5117 added a new `ResourceNameType`, which was initially a field on `Resource` and `ResourceFilter`. However, follow on PRs have now moved the name type fields to new `ResourcePattern` and `ResourcePatternFilter` classes. This means the old name is no longer valid and may be confusing. The PR looks to rename the class to a more intuitive `resource.PatternType`.

@cmccabe also requested that the current `ANY` value for this class be renamed to avoid confusion. `PatternType.ANY` currently causes `ResourcePatternFilter` to bring back all ACLs that would affect the supplied resource, i.e. it brings back literal, wildcard ACLs, and also does pattern matching to work out which prefix acls would affect the resource.  This is very different from the behaviour of `ResourceType.ANY`, which just means the filter ignores the type of resources. 

 `ANY` is to be renamed to `MATCH` to disambiguate it from other `ANY` filter types. A new `ANY` will be added that works in the same way as others, i.e. it will cause the filter to ignore the pattern type, (but won't do any pattern matching).

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Jun Rao <junrao@gmail.com>",19,14,14,55,353,9,9,119,119,60,2,6.5,133,119,66,14,14,7,2,1,0,1
core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala,core/src/main/scala/kafka/common/OffsetsOutOfOrderException.scala,"KAFKA-6975; Fix replica fetching from non-batch-aligned log start offset (#5133)

It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW. 

Added two integration tests:
1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset;
2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",0,25,0,3,18,0,0,25,25,25,1,1,25,25,25,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala,core/src/main/scala/kafka/common/UnexpectedAppendOffsetException.scala,"KAFKA-6975; Fix replica fetching from non-batch-aligned log start offset (#5133)

It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW. 

Added two integration tests:
1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset;
2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>, Jason Gustafson <jason@confluent.io>",0,29,0,5,29,0,0,29,29,29,1,1,29,29,29,0,0,0,2,1,0,1
core/src/test/scala/integration/kafka/api/ProducerBounceTest.scala,core/src/test/scala/integration/kafka/api/ProducerBounceTest.scala,"MINOR: Remove unnecessary old consumer usage in tests and other clean-ups (#5199)

- Update some tests to use the Java consumer.
- Remove ignored `ProducerBounceTest.testBrokerFailure`. This test
is flaky and it has been superseded by `TransactionBounceTest`.
- Use non-blocking poll for consumption methods in `TestUtils`.

This is a step on the road to remove the old consumers.",8,8,62,80,616,1,2,116,164,6,19,2,265,164,14,149,62,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigChangeCallback.java,clients/src/main/java/org/apache/kafka/common/config/ConfigChangeCallback.java,"MINOR: Move FileConfigProvider to provider subpackage (#5194)

This moves FileConfigProvider to the org.apache.common.config.provider package to more easily isolate provider implementations going forward.

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",0,2,0,5,41,0,0,33,31,16,2,1.0,33,31,16,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/config/ConfigData.java,clients/src/main/java/org/apache/kafka/common/config/ConfigData.java,"MINOR: Move FileConfigProvider to provider subpackage (#5194)

This moves FileConfigProvider to the org.apache.common.config.provider package to more easily isolate provider implementations going forward.

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",4,2,0,20,126,0,4,68,66,34,2,1.0,68,66,34,0,0,0,1,0,0,0
clients/src/main/java/org/apache/kafka/common/config/ConfigTransformerResult.java,clients/src/main/java/org/apache/kafka/common/config/ConfigTransformerResult.java,"MINOR: Move FileConfigProvider to provider subpackage (#5194)

This moves FileConfigProvider to the org.apache.common.config.provider package to more easily isolate provider implementations going forward.

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Randall Hauch <rhauch@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",3,2,0,17,119,0,3,63,61,32,2,1.0,63,61,32,0,0,0,1,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/InnerMeteredKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InnerMeteredKeyValueStore.java,"KAFKA-6538: Changes to enhance  ByteStore exceptions thrown from RocksDBStore with more human readable info (#5103)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",29,44,28,273,1780,3,18,339,323,56,6,1.0,402,323,67,63,30,10,2,1,0,1
core/src/main/scala/kafka/security/auth/ResourceNameType.scala,core/src/main/scala/kafka/security/auth/ResourceNameType.scala,"KAFKA-6841: Support Prefixed ACLs (KIP-290) (#5117)

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Jun Rao <junrao@gmail.com>

Co-authored-by: Piyush Vijay <pvijay@apple.com>
Co-authored-by: Andy Coates <big-andy-coates@users.noreply.github.com>",3,49,0,23,184,3,3,49,49,49,1,1,49,49,49,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ToleranceType.java,"KAFKA-6981: Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes (KIP-298)

Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes, and refactor the tests and classes to use these new properties.

Testing: Unit tests and running the connect-standalone script with a file sink connector.

Author: Arjun Satish <arjun@confluent.io>
Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Robert Yokota <rayokota@gmail.com>, Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5125 from wicknicks/KAFKA-6981",1,3,2,9,48,2,1,40,39,20,2,1.5,42,39,21,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerRefreshingLogin.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerRefreshingLogin.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),11,4,4,77,541,0,5,153,153,76,2,1.5,157,153,78,4,4,2,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientProvider.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientProvider.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),2,2,2,16,130,0,2,37,37,18,2,1.5,39,37,20,2,2,1,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServerProvider.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslServerProvider.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),2,2,2,16,130,0,2,37,37,18,2,1.5,39,37,20,2,2,1,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredential.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredential.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),0,1,1,7,42,0,0,66,66,33,2,1.0,67,66,34,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshConfig.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshConfig.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),6,1,1,34,217,0,6,124,124,62,2,1.0,125,124,62,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerConfigException.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerConfigException.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),2,1,1,11,75,0,2,35,35,18,2,1.0,36,35,18,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerIllegalTokenException.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerIllegalTokenException.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),3,1,1,16,109,0,2,53,53,26,2,1.0,54,53,27,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerScopeUtils.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerScopeUtils.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),6,1,1,25,200,0,3,75,75,38,2,1.0,76,75,38,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationResult.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationResult.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),13,1,1,44,254,0,9,126,126,63,2,1.0,127,126,64,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationUtils.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerValidationUtils.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),29,1,1,97,717,0,8,200,200,100,2,1.0,201,200,100,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainSaslServerProvider.java,clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainSaslServerProvider.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),2,2,2,15,120,0,2,37,38,7,5,2,52,38,10,15,10,3,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainServerCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/plain/internals/PlainServerCallbackHandler.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),9,1,1,49,384,0,4,76,76,38,2,1.0,77,76,38,1,1,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramCredentialUtils.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramCredentialUtils.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),14,1,1,53,488,0,5,86,86,11,8,2.0,117,86,15,31,11,4,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramMechanism.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramMechanism.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),8,1,1,47,278,0,8,77,79,19,4,1.0,89,79,22,12,10,3,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslClientProvider.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslClientProvider.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),3,2,2,16,133,0,2,38,39,8,5,2,53,39,11,15,10,3,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslServerProvider.java,clients/src/main/java/org/apache/kafka/common/security/scram/internals/ScramSaslServerProvider.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),3,2,2,16,133,0,2,38,39,8,5,2,53,39,11,15,10,3,0,0,0,0
clients/src/test/java/org/apache/kafka/common/security/authenticator/TestJaasConfig.java,clients/src/test/java/org/apache/kafka/common/security/authenticator/TestJaasConfig.java,MINOR: Rename package `internal` to `internals` for consistency (#5137),26,1,1,121,992,0,11,155,89,16,10,2.0,178,89,18,23,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/AuthenticationContext.java,clients/src/main/java/org/apache/kafka/common/security/auth/AuthenticationContext.java,"KAFKA-6750: Add listener name to authentication context (KIP-282) (#4829)

PrincipalBuilder implementations can now take the listener into account
when creating the Principal. This is especially interesting in deployments
where inter-broker traffic is on a different listener than client traffic or
when the same protocol is used by multiple listeners.

The change in itself is mostly ""plumbing"" as the listener name needs to be
passed from ChannelBuilders all the way down to all classes implementing
AuthenticationContext.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>
Co-authored-by: Mickael Maison <mickael.maison@gmail.com>",0,8,1,7,40,0,0,42,36,14,3,2,45,36,15,3,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/PlaintextAuthenticationContext.java,clients/src/main/java/org/apache/kafka/common/security/auth/PlaintextAuthenticationContext.java,"KAFKA-6750: Add listener name to authentication context (KIP-282) (#4829)

PrincipalBuilder implementations can now take the listener into account
when creating the Principal. This is especially interesting in deployments
where inter-broker traffic is on a different listener than client traffic or
when the same protocol is used by multiple listeners.

The change in itself is mostly ""plumbing"" as the listener name needs to be
passed from ChannelBuilders all the way down to all classes implementing
AuthenticationContext.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>
Co-authored-by: Mickael Maison <mickael.maison@gmail.com>",4,8,1,22,98,3,4,45,40,15,3,2,50,40,17,5,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/SslAuthenticationContext.java,clients/src/main/java/org/apache/kafka/common/security/auth/SslAuthenticationContext.java,"KAFKA-6750: Add listener name to authentication context (KIP-282) (#4829)

PrincipalBuilder implementations can now take the listener into account
when creating the Principal. This is especially interesting in deployments
where inter-broker traffic is on a different listener than client traffic or
when the same protocol is used by multiple listeners.

The change in itself is mostly ""plumbing"" as the listener name needs to be
passed from ChannelBuilders all the way down to all classes implementing
AuthenticationContext.

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>
Co-authored-by: Mickael Maison <mickael.maison@gmail.com>",5,8,1,28,131,3,5,51,46,17,3,2,56,46,19,5,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ClientResponse.java,clients/src/main/java/org/apache/kafka/clients/ClientResponse.java,"MINOR: Fix bug in AdminClient node reassignment following connection failure (#5112)

We added logic to reassign nodes in callToSend after a connection failure, but we do not handle the case when there is no node currently available to reassign the request to. This can happen when using MetadataUpdateNodeIdProvider if all of the known nodes are blacked out awaiting the retry backoff. To fix this, we need to ensure that the call is added to pendingCalls if a new node cannot be found.",13,8,0,79,343,2,12,126,78,18,7,5,164,78,23,38,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java,clients/src/main/java/org/apache/kafka/common/record/ConvertedRecords.java,"KAFKA-6927; Chunked down-conversion to prevent out of memory errors on broker [KIP-283] (#4871)

Implementation for lazy down-conversion in a chunked manner for efficient memory usage during down-conversion. This pull request is mainly to get initial feedback on the direction of the patch. The patch includes all the main components from KIP-283.

Reviewers: Jason Gustafson <jason@confluent.io>",3,5,5,15,74,4,3,36,36,18,2,2.5,41,36,20,5,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordConversionStats.java,clients/src/main/java/org/apache/kafka/common/record/RecordConversionStats.java,"KAFKA-6927; Chunked down-conversion to prevent out of memory errors on broker [KIP-283] (#4871)

Implementation for lazy down-conversion in a chunked manner for efficient memory usage during down-conversion. This pull request is mainly to get initial feedback on the direction of the patch. The patch includes all the main components from KIP-283.

Reviewers: Jason Gustafson <jason@confluent.io>",7,17,7,34,166,6,7,70,60,23,3,6,85,60,28,15,8,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderRequest.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderRequest.java,"KAFKA-6886: Externalize secrets from Connect configs (KIP-297)

This commit allows secrets in Connect configs to be externalized and replaced with variable references of the form `${provider:[path:]key}`, where the ""path"" is optional.

There are 2 main additions to `org.apache.kafka.common.config`: a `ConfigProvider` and a `ConfigTransformer`.  The `ConfigProvider` is an interface that allows key-value pairs to be provided by an external source for a given ""path"".  An a TTL can be associated with the key-value pairs returned from the path.  The `ConfigTransformer` will use instances of `ConfigProvider` to replace variable references in a set of configuration values.

In the Connect framework, `ConfigProvider` classes can be specified in the worker config, and then variable references can be used in the connector config.  In addition, the herder can be configured to restart connectors (or not) based on the TTL returned from a `ConfigProvider`.  The main class that performs restarts and transformations is `WorkerConfigTransformer`.

Finally, a `configs()` method has been added to both `SourceTaskContext` and `SinkTaskContext`.  This allows connectors to get configs with variables replaced by the latest values from instances of `ConfigProvider`.

Most of the other changes in the Connect framework are threading various objects through classes to enable the above functionality.

Author: Robert Yokota <rayokota@gmail.com>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5068 from rayokota/KAFKA-6886-connect-secrets",0,21,0,4,21,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java,"KAFKA-6886: Externalize secrets from Connect configs (KIP-297)

This commit allows secrets in Connect configs to be externalized and replaced with variable references of the form `${provider:[path:]key}`, where the ""path"" is optional.

There are 2 main additions to `org.apache.kafka.common.config`: a `ConfigProvider` and a `ConfigTransformer`.  The `ConfigProvider` is an interface that allows key-value pairs to be provided by an external source for a given ""path"".  An a TTL can be associated with the key-value pairs returned from the path.  The `ConfigTransformer` will use instances of `ConfigProvider` to replace variable references in a set of configuration values.

In the Connect framework, `ConfigProvider` classes can be specified in the worker config, and then variable references can be used in the connector config.  In addition, the herder can be configured to restart connectors (or not) based on the TTL returned from a `ConfigProvider`.  The main class that performs restarts and transformations is `WorkerConfigTransformer`.

Finally, a `configs()` method has been added to both `SourceTaskContext` and `SinkTaskContext`.  This allows connectors to get configs with variables replaced by the latest values from instances of `ConfigProvider`.

Most of the other changes in the Connect framework are threading various objects through classes to enable the above functionality.

Author: Robert Yokota <rayokota@gmail.com>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5068 from rayokota/KAFKA-6886-connect-secrets",3,15,1,25,151,3,3,48,35,12,4,3.0,57,35,14,9,5,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/RecordContext.java,streams/src/main/java/org/apache/kafka/streams/processor/RecordContext.java,"KAFKA-4936: Add dynamic routing in Streams (#5018)

implements KIP-303

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,11,12,9,54,0,0,56,45,11,5,2,74,45,15,18,12,4,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/StreamPartitioner.java,streams/src/main/java/org/apache/kafka/streams/processor/StreamPartitioner.java,"KAFKA-4936: Add dynamic routing in Streams (#5018)

implements KIP-303

Reviewers: Bill Bejeck <bill@confluent.io>, John Roesler <john@confluent.io>, Matthias J. Sax <matthias@confluent.io>",0,3,2,6,63,0,0,63,59,8,8,3.5,94,59,12,31,10,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/Operation.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/Operation.java,"KAFKA-6738: Implement error handling for source and sink tasks (KIP-298)

This PR implements the features described in this KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect

This PR changes the Connect framework to allow it to automatically deal with errors encountered while processing records in a Connector. The following behavior changes are introduced here:

**Retry on Failure**: Retry the failed operation a configurable number of times, with backoff between each retry.
**Task Tolerance Limits**: Tolerate a configurable number of failures in a task.

We also add the following ways to report errors, along with sufficient context to simplify the debugging process:

**Log Error Context**: The error information along with processing context is logged along with standard application logs.
**Dead Letter Queue**: Produce the original message into a Kafka topic (applicable only to sink connectors).

New **metrics** which will monitor the number of failures, and the behavior of the response handler are added.

The changes proposed here **are backward compatible**. The current behavior in Connect is to kill the task on the first error in any stage. This will remain the default behavior if the connector does not override any of the new configurations which are provided as part of this feature.

Testing: added multiple unit tests to test the retry and tolerance logic.

Author: Arjun Satish <arjun@confluent.io>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Magesh Nandakumar <magesh.n.kumar@gmail.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5065 from wicknicks/KAFKA-6378",0,28,0,4,35,0,0,28,28,28,1,1,28,28,28,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/Stage.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/Stage.java,"KAFKA-6738: Implement error handling for source and sink tasks (KIP-298)

This PR implements the features described in this KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect

This PR changes the Connect framework to allow it to automatically deal with errors encountered while processing records in a Connector. The following behavior changes are introduced here:

**Retry on Failure**: Retry the failed operation a configurable number of times, with backoff between each retry.
**Task Tolerance Limits**: Tolerate a configurable number of failures in a task.

We also add the following ways to report errors, along with sufficient context to simplify the debugging process:

**Log Error Context**: The error information along with processing context is logged along with standard application logs.
**Dead Letter Queue**: Produce the original message into a Kafka topic (applicable only to sink connectors).

New **metrics** which will monitor the number of failures, and the behavior of the response handler are added.

The changes proposed here **are backward compatible**. The current behavior in Connect is to kill the task on the first error in any stage. This will remain the default behavior if the connector does not override any of the new configurations which are provided as part of this feature.

Testing: added multiple unit tests to test the retry and tolerance logic.

Author: Arjun Satish <arjun@confluent.io>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Magesh Nandakumar <magesh.n.kumar@gmail.com>, Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #5065 from wicknicks/KAFKA-6378",0,63,0,11,33,0,0,63,63,63,1,1,63,63,63,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/ConnectionState.java,clients/src/main/java/org/apache/kafka/clients/ConnectionState.java,"KAFKA-6028: Improve the quota throttle communication (KIP-219)

This implements KIP-219, where a broker returns a response with throttle time on
quota violation immediately after processing the corresponding request.  After
the response is sent out, the broker will keep the channel muted until the
throttle time is over. Also, on receiving a response with throttle time, client
will block outgoing communication to the broker for the specified throttle time.

See PR 4830, 5064 and 5094 for all the review history

Author: Jon Lee <jonlee@jonlee-ld1.linkedin.biz>

Reviewers: Jun Rao <junrao@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>,  Dong Lin <lindong28@gmail.com>

Closes #5064 from jonlee2/kip-219",4,4,0,10,56,1,2,38,20,5,7,1,53,20,8,15,11,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/components/Versioned.java,connect/api/src/main/java/org/apache/kafka/connect/components/Versioned.java,"KAFKA-6776: ConnectRestExtension Interfaces & Rest integration (KIP-285)

This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <wicknicks@users.noreply.github.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4931 from mageshn/KIP-285",0,30,0,4,21,0,0,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorType.java,connect/api/src/main/java/org/apache/kafka/connect/health/ConnectorType.java,"KAFKA-6776: ConnectRestExtension Interfaces & Rest integration (KIP-285)

This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <wicknicks@users.noreply.github.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4931 from mageshn/KIP-285",1,43,0,11,52,1,1,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/rest/ConnectRestExtension.java,connect/api/src/main/java/org/apache/kafka/connect/rest/ConnectRestExtension.java,"KAFKA-6776: ConnectRestExtension Interfaces & Rest integration (KIP-285)

This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <wicknicks@users.noreply.github.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4931 from mageshn/KIP-285",0,58,0,9,80,0,0,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/rest/ConnectRestExtensionContext.java,connect/api/src/main/java/org/apache/kafka/connect/rest/ConnectRestExtensionContext.java,"KAFKA-6776: ConnectRestExtension Interfaces & Rest integration (KIP-285)

This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <wicknicks@users.noreply.github.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4931 from mageshn/KIP-285",0,44,0,7,55,0,0,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/ConnectRestExtensionContextImpl.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/ConnectRestExtensionContextImpl.java,"KAFKA-6776: ConnectRestExtension Interfaces & Rest integration (KIP-285)

This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule

Author: Magesh Nandakumar <magesh.n.kumar@gmail.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Arjun Satish <wicknicks@users.noreply.github.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4931 from mageshn/KIP-285",3,47,0,23,127,3,3,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/WakeupException.java,clients/src/main/java/org/apache/kafka/common/errors/WakeupException.java,"KAFKA-5697; Implement new consumer poll API from KIP-266 (#4855)

Add the new stricter-timeout version of `poll` proposed in KIP-266.

The pre-existing variant `poll(long timeout)` would block indefinitely for metadata
updates if they were needed, then it would issue a fetch and poll for `timeout` ms 
for new records. The initial indefinite metadata block caused applications to become
stuck when the brokers became unavailable. The existence of the timeout parameter
made the indefinite block especially unintuitive.

This PR adds `poll(Duration timeout)` with the semantics:
1. iff a metadata update is needed:
    1. send (asynchronous) metadata requests
    2. poll for metadata responses (counts against timeout)
        - if no response within timeout, **return an empty collection immediately**
2. if there is fetch data available, **return it immediately**
3. if there is no fetch request in flight, send fetch requests
4. poll for fetch responses (counts against timeout)
    - if no response within timeout, **return an empty collection** (leaving async fetch request for the next poll)
    - if we get a response, **return the response**

The old method, `poll(long timeout)` is deprecated, but we do not change its semantics, so it remains:
1. iff a metadata update is needed:
    1. send (asynchronous) metadata requests
    2. poll for metadata responses *indefinitely until we get it*
2. if there is fetch data available, **return it immediately**
3. if there is no fetch request in flight, send fetch requests
4. poll for fetch responses (counts against timeout)
    - if no response within timeout, **return an empty collection** (leaving async fetch request for the next poll)
    - if we get a response, **return the response**

One notable usage is prohibited by the new `poll`: previously, you could call `poll(0)` to block for metadata updates, for example to initialize the client, supposedly without fetching records. Note, though, that this behavior is not according to any contract, and there is no guarantee that `poll(0)` won't return records the first time it's called. Therefore, it has always been unsafe to ignore the response.",0,1,1,5,37,0,0,30,20,8,4,1.5,42,20,10,12,9,3,2,1,0,1
core/src/main/scala/kafka/consumer/PartitionAssignor.scala,core/src/main/scala/kafka/consumer/PartitionAssignor.scala,"MINOR: Replace unused variables by underscore (#5003)

And remove one unused expression.

Reviewers: Ismael Juma <ismael@juma.me.uk>",14,1,1,96,782,1,4,172,161,14,12,2.0,251,161,21,79,30,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerToken.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerToken.java,"KAFKA-6562: OAuth Authentication via SASL/OAUTHBEARER (KIP-255) (#4994)

This KIP adds the following functionality related to SASL/OAUTHBEARER:

1) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to flexibly retrieve an access token from an OAuth 2 authorization server based on the declaration of a custom login CallbackHandler implementation and have that access token transparently and automatically transmitted to a broker for authentication.

2) Allow brokers to flexibly validate provided access tokens when a client establishes a connection based on the declaration of a custom SASL Server CallbackHandler implementation.

3) Provide implementations of the above retrieval and validation features based on an unsecured JSON Web Token that function out-of-the-box with minimal configuration required (i.e. implementations of the two types of callback handlers mentioned above will be used by default with no need to explicitly declare them).

4) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to transparently retrieve a new access token in the background before the existing access token expires in case the client has to open new connections.",0,105,0,11,70,0,0,105,105,105,1,1,105,105,105,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerValidatorCallback.java,clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerValidatorCallback.java,"KAFKA-6562: OAuth Authentication via SASL/OAUTHBEARER (KIP-255) (#4994)

This KIP adds the following functionality related to SASL/OAUTHBEARER:

1) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to flexibly retrieve an access token from an OAuth 2 authorization server based on the declaration of a custom login CallbackHandler implementation and have that access token transparently and automatically transmitted to a broker for authentication.

2) Allow brokers to flexibly validate provided access tokens when a client establishes a connection based on the declaration of a custom SASL Server CallbackHandler implementation.

3) Provide implementations of the above retrieval and validation features based on an unsecured JSON Web Token that function out-of-the-box with minimal configuration required (i.e. implementations of the two types of callback handlers mentioned above will be used by default with no need to explicitly declare them).

4) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to transparently retrieve a new access token in the background before the existing access token expires in case the client has to open new connections.",10,154,0,46,265,8,8,154,154,154,1,1,154,154,154,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/metrics/LazyMBeanInfo.java,clients/src/main/java/org/apache/kafka/common/metrics/LazyMBeanInfo.java,"Improve kafka client sensor registration performance by lazily calculating JMX attributes

When any metric (e.g. per-partition metric) is created or deleted,
registerMBean() is called which in turn calls getMBeanInfo().getClassName().
However, KafkaMbean.getMBeanInfo() instantiates an array of all sensors even
though we only need the class name. This costs a lot of CPU to register
sensors when consumer with large partition assignment starts. For example, it
takes 5 minutes to start a consumer with 35k partitions. This patch reduces the
consumer startup time seconds.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Satish Duggana <satish.duggana@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5011 from radai-rosenblatt/fun-with-jmx",4,57,0,32,181,2,2,57,57,57,1,1,57,57,57,0,0,0,0,0,0,0
core/src/main/scala/kafka/client/ClientUtils.scala,core/src/main/scala/kafka/client/ClientUtils.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",28,17,31,145,987,2,5,203,92,6,35,2,348,96,10,145,31,4,2,1,0,1
core/src/test/scala/unit/kafka/api/RequestResponseSerializationTest.scala,core/src/test/scala/unit/kafka/api/RequestResponseSerializationTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",2,1,35,158,1380,2,2,205,206,3,64,2.5,764,206,12,559,117,9,2,1,0,1
core/src/test/scala/unit/kafka/common/ConfigTest.scala,core/src/test/scala/unit/kafka/common/ConfigTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",8,0,32,34,204,1,1,57,89,6,9,2,106,89,12,49,32,5,2,1,0,1
core/src/test/scala/unit/kafka/consumer/ZookeeperConsumerConnectorTest.scala,core/src/test/scala/unit/kafka/consumer/ZookeeperConsumerConnectorTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",21,21,20,311,2836,7,12,453,267,8,60,4.5,1143,267,19,690,91,12,2,1,0,1
core/src/test/scala/unit/kafka/integration/AutoOffsetResetTest.scala,core/src/test/scala/unit/kafka/integration/AutoOffsetResetTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",10,7,10,74,538,1,7,117,223,4,28,2.0,396,223,14,279,150,10,2,1,0,1
core/src/test/scala/unit/kafka/integration/FetcherTest.scala,core/src/test/scala/unit/kafka/integration/FetcherTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",7,2,2,67,486,1,5,99,105,2,44,2.0,265,105,6,166,17,4,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/consumer/ZookeeperConsumerConnectorTest.scala,core/src/test/scala/unit/kafka/javaapi/consumer/ZookeeperConsumerConnectorTest.scala,"KAFKA-6921; Remove old Scala producer and related code

* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders,
tests.
* Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer`
and several options that are not used by the Java producer).
* Updated a few Scala consumer tests to use the new producer (including a minor
refactor of `produceMessages` methods in `TestUtils`).
* Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of
`SyncProducer`.
* Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`.
* Minor import clean-ups

No new tests added since behaviour should remain the same after these changes.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5045 from ijuma/kafka-6921-remove-old-producer",7,15,29,69,612,4,3,108,287,3,40,2.0,512,287,13,404,185,10,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/StaleMetadataException.java,clients/src/main/java/org/apache/kafka/clients/StaleMetadataException.java,"MINOR: Remove dependence on __consumer_offsets in AdminClient listConsumerGroups

Avoid dependence on the internal __consumer_offsets topic to handle `listConsumerGroups()` since it unnecessarily requires users to have Describe access on an internal topic. Instead we query each broker independently. For most clusters, this amounts to the same thing since the default number of partitions for __consumer_offsets is 50. This also provides better encapsulation since it avoids exposing the use of __consumer_offsets, which gives us more flexibility in the future.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Dong Lin <lindong28@gmail.com>

Closes #5007 from hachikuji/remove-admin-use-of-offsets-topic",2,11,2,9,56,2,2,35,22,12,3,3,46,22,15,11,9,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordContextStub.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordContextStub.java,"KAFKA-6850: Add Record Header support to Kafka Streams Processor API (KIP-244) (#4955)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>",9,19,2,53,246,4,9,81,55,20,4,3.0,91,55,23,10,6,2,2,1,0,1
core/src/main/scala/kafka/utils/PasswordEncoder.scala,core/src/main/scala/kafka/utils/PasswordEncoder.scala,"MINOR: Remove o.a.kafka.common.utils.Base64 and IS_JAVA8_COMPATIBLE

We no longer need them since we now require Java 8.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Andras Beni <andrasbeni@cloudera.com>, Manikumar Reddy O <manikumar.reddy@gmail.com>, Dong Lin <lindong28@gmail.com>

Closes #5049 from ijuma/remove-base64",25,4,4,128,993,3,12,175,175,88,2,3.0,179,175,90,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Shell.java,clients/src/main/java/org/apache/kafka/common/utils/Shell.java,"KAFKA-4423: Drop support for Java 7 (KIP-118) and update deps (#5046)

* Set --source, --target and --release to 1.8.
* Build Scala 2.12 by default.
* Remove some conditionals in the build file now that Java 8
is the minimum version.
* Bump the version of Jetty, Jersey and Checkstyle (the newer
versions require Java 8).
* Fixed issues uncovered by the new version if Checkstyle.
* A couple of minor updates to handle an incompatible source
change in the new version of Jetty.
* Add dependency to jersey-hk2 to fix failing tests caused
by Jersey upgrade.
* Update release script to use Java 8 and to take into account
that Scala 2.12 is now built by default.
* While we're at it, bump the version of Gradle, Gradle plugins,
ScalaLogging, JMH and apache directory api.
* Minor documentation updates including the readme and upgrade
notes. A number of Streams Java 7 examples can be removed
subsequently.",35,1,1,168,1035,1,17,299,304,50,6,1.5,320,304,53,21,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupIdNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupIdNotFoundException.java,"KAFKA-6868; Fix buffer underflow and expose group state in the consumer groups API (#4980)

* The consumer groups API should expose group state and coordinator information.  This information is needed by administrative tools and scripts that access consume groups.

* The partition assignment will be empty when the group is rebalancing. Fix an issue where the adminclient attempted to deserialize this empty buffer.

* Remove nulls from the API and make all collections immutable.

* DescribeConsumerGroupsResult#all should return a result as expected, rather than Void

* Fix exception text for GroupIdNotFoundException, GroupNotEmptyException. It was being filled in as ""The group id The group id does not exist was not found"" and similar.

Reviewers: Attila Sasvari <asasvari@apache.org>, Andras Beni <andrasbeni@cloudera.com>, Dong Lin <lindong28@gmail.com>, Jason Gustafson <jason@confluent.io>",1,2,10,6,31,3,1,23,31,12,2,1.5,33,31,16,10,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupNotEmptyException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupNotEmptyException.java,"KAFKA-6868; Fix buffer underflow and expose group state in the consumer groups API (#4980)

* The consumer groups API should expose group state and coordinator information.  This information is needed by administrative tools and scripts that access consume groups.

* The partition assignment will be empty when the group is rebalancing. Fix an issue where the adminclient attempted to deserialize this empty buffer.

* Remove nulls from the API and make all collections immutable.

* DescribeConsumerGroupsResult#all should return a result as expected, rather than Void

* Fix exception text for GroupIdNotFoundException, GroupNotEmptyException. It was being filled in as ""The group id The group id does not exist was not found"" and similar.

Reviewers: Attila Sasvari <asasvari@apache.org>, Andras Beni <andrasbeni@cloudera.com>, Dong Lin <lindong28@gmail.com>, Jason Gustafson <jason@confluent.io>",1,2,10,6,31,3,1,23,31,12,2,1.5,33,31,16,10,10,5,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala,core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala,"KAFKA-6361: Fix log divergence between leader and follower after fast leader fail over (#4882)

Implementation of KIP-279 as described here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over

In summary:
- Added leader_epoch to OFFSET_FOR_LEADER_EPOCH_RESPONSE
- Leader replies with the pair( largest epoch less than or equal to the requested epoch, the end offset of this epoch)
- If Follower does not know about the leader epoch that leader replies with, it truncates to the end offset of largest leader epoch less than leader epoch that leader replied with, and sends another OffsetForLeaderEpoch request. That request contains the largest leader epoch less than leader epoch that leader replied with.

Reviewers: Dong Lin <lindong28@gmail.com>, Jun Rao <junrao@gmail.com>",19,2,2,115,941,2,11,158,63,4,38,2.0,273,73,7,115,17,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreSupplier.java,"KAFKA-6813: Remove deprecated APIs in KIP-182, Part II (#4976)

1. Remove the deprecated StateStoreSuppliers, and the corresponding Stores.create() functions and factories: only the base StateStoreSupplier and MockStoreSupplier were still preserved as they are needed by the deprecated TopologyBuilder and KStreamBuilder. Will remove them in a follow-up PR.

2. Add TopologyWrapper.java as the original InternalTopologyBuilderAccessor was removed, but I realized it is still needed as of now.

3. Minor: removed StateStoreTestUtils.java and inline its logic in its callers since now with StoreBuilder it is just a one-liner.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",5,0,1,28,254,0,3,60,276,3,22,4.5,504,276,23,444,250,20,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java,"KAFKA-6813: Remove deprecated APIs in KIP-182, Part II (#4976)

1. Remove the deprecated StateStoreSuppliers, and the corresponding Stores.create() functions and factories: only the base StateStoreSupplier and MockStoreSupplier were still preserved as they are needed by the deprecated TopologyBuilder and KStreamBuilder. Will remove them in a follow-up PR.

2. Add TopologyWrapper.java as the original InternalTopologyBuilderAccessor was removed, but I realized it is still needed as of now.

3. Minor: removed StateStoreTestUtils.java and inline its logic in its callers since now with StoreBuilder it is just a one-liner.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",36,1,1,349,3344,0,32,457,92,15,30,4.0,635,92,21,178,35,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java,streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java,"KAFKA-6813: Remove deprecated APIs in KIP-182, Part II (#4976)

1. Remove the deprecated StateStoreSuppliers, and the corresponding Stores.create() functions and factories: only the base StateStoreSupplier and MockStoreSupplier were still preserved as they are needed by the deprecated TopologyBuilder and KStreamBuilder. Will remove them in a follow-up PR.

2. Add TopologyWrapper.java as the original InternalTopologyBuilderAccessor was removed, but I realized it is still needed as of now.

3. Minor: removed StateStoreTestUtils.java and inline its logic in its callers since now with StoreBuilder it is just a one-liner.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",60,5,23,595,5653,4,56,752,99,16,48,3.5,1035,99,22,283,37,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableForeachTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableForeachTest.java,"KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [cleanup] (#4939)

* Add method to create test properties to StreamsTestUtils
* Make TopologyTestDriver protected constructor package-private
* Add comment suggesting the use of TopologyTestDriver to KStreamTestDriver
* Cleanup:
    - GlobalKTableJoinsTest
    - KGroupedStreamImplTest
    - KGroupedTableImplTest
    - KStreamBranchTest
    - KStreamFilterTest
    - KStreamFlatMapTest
    - KStreamFlatMapValuesTest
    - KStreamForeachTest
    - KStreamGlobalKTableJoinTest
    - KStreamGlobalKTableLeftJoinTest
    - KStreamImplTest
    - KStreamKStreamJoinTest
    - KStreamKStreamLeftJoinTest
    - KStreamGlobalKTableLeftJoinTest
    - KStreamKTableJoinTest
    - KStreamKTableLeftJoinTest
    - KStreamMapTest
    - KStreamMapValuesTest
    - KStreamPeekTest
    - StreamsBuilderTest
    - KStreamSelectKeyTest
    - KStreamTransformTest
    - KStreamTransformValuesTest
    - KStreamWindowAggregateTest
    - KTableForeachTest

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",4,9,34,79,772,3,2,111,85,7,15,4,194,85,13,83,34,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/ShutdownException.java,streams/src/main/java/org/apache/kafka/streams/errors/ShutdownException.java,"KAFKA-5697: issue Consumer#wakeup during Streams shutdown

Wakeup consumers during shutdown to break them out of any internally blocking calls.

Semantically, it should be fine to treat a WakeupException as ""no work to do"", which will then continue the threads' polling loops, leading them to discover that they are supposed to shut down, which they will do gracefully.

The existing tests should be sufficient to verify no regressions.

Author: John Roesler <john@confluent.io>

Reviewers: Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <wangguoz@gmail.com>

Closes #4930 from vvcephei/streams-client-wakeup-on-shutdown

minor javadoc updates",3,31,0,12,66,3,3,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/ConsumerUtils.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ConsumerUtils.java,"KAFKA-5697: issue Consumer#wakeup during Streams shutdown

Wakeup consumers during shutdown to break them out of any internally blocking calls.

Semantically, it should be fine to treat a WakeupException as ""no work to do"", which will then continue the threads' polling loops, leading them to discover that they are supposed to shut down, which they will do gracefully.

The existing tests should be sufficient to verify no regressions.

Author: John Roesler <john@confluent.io>

Reviewers: Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <wangguoz@gmail.com>

Closes #4930 from vvcephei/streams-client-wakeup-on-shutdown

minor javadoc updates",3,38,0,18,177,2,2,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/BaseJoinProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/BaseJoinProcessorNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",10,99,0,62,388,10,10,99,99,99,1,1,99,99,99,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/RepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/RepartitionNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",20,165,0,116,617,20,20,165,165,165,1,1,165,165,165,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulProcessorNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulProcessorNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",15,133,0,90,599,15,15,133,133,133,1,1,133,133,133,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulRepartitionNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulRepartitionNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",23,169,0,120,837,19,19,169,169,169,1,1,169,169,169,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulSourceNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StatefulSourceNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",26,197,0,138,835,24,24,197,197,197,1,1,197,197,197,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamsGraphNode.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamsGraphNode.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",16,106,0,67,327,16,16,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamsTopologyGraph.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/StreamsTopologyGraph.java,"KAFKA-6761: Part 1 of 3; Graph nodes (#4923)

This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.
I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:

1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the InternalTopologyBuilder when using the DSL. Graph nodes are stored in the StreamsTopologyGraph until the final topology needs building then the graph is traversed and optimizations are made at that point. There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice.

2. PR 2 will intercept all DSL calls and build the graph. The InternalStreamsBuilder uses the graph to provide the required info to the InternalTopologyBuilder and build a topology. The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph.

3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics. For example the following topology:

```
KStream<String, String> mappedStreamOther = inputStream.map(new KeyValueMapper<String, String, KeyValue<? extends String, ? extends String>>() {
            @Override
            public KeyValue<? extends String, ? extends String> apply(String key, String value) {

                return KeyValue.pair(key.substring(0, 3), value);
            }
        });


        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");
        mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out"");
```

would create 3 repartion topics, but after applying an optimization strategy, only one is created.

Reviewers: John Roesler <john@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",15,137,0,75,623,7,7,137,137,137,1,1,137,137,137,0,0,0,0,0,0,0
tests/kafkatest/services/trogdor/round_trip_workload.py,tests/kafkatest/services/trogdor/round_trip_workload.py,"MINOR: Fix Trogdor tests, partition assignments (#4892)",6,2,2,25,202,2,6,49,49,24,2,2.0,51,49,26,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreSupplierTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBKeyValueStoreSupplierTest.java,"MINOR: fix NamedCache metrics in Streams (#4917)

* Fixes a bug in which all NamedCache instances in a process shared
one parent metric.

* Also fixes a bug which incorrectly computed the per-cache metric tag
(which was undetected due to the former bug).

* Drop the StreamsMetricsConventions#xLevelSensorName convention
in favor of StreamsMetricsImpl#xLevelSensor to allow StreamsMetricsImpl
to track thread- and cache-level metrics, so that they may be cleanly declared
from anywhere but still unloaded at the appropriate time. This was necessary
right now so that the NamedCache could register a thread-level parent sensor
to be unloaded when the thread, not the cache, is closed.

* The above changes made it mostly unnecessary for the StreamsMetricsImpl to
expose a reference to the underlying Metrics registry, so I did a little extra work
to remove that reference, including removing inconsistently-used and unnecessary
calls to Metrics#close() in the tests.

The existing tests should be sufficient to verify this change.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",9,0,1,131,988,1,9,162,155,15,11,2,199,155,18,37,7,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreSupplierTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreSupplierTest.java,"MINOR: fix NamedCache metrics in Streams (#4917)

* Fixes a bug in which all NamedCache instances in a process shared
one parent metric.

* Also fixes a bug which incorrectly computed the per-cache metric tag
(which was undetected due to the former bug).

* Drop the StreamsMetricsConventions#xLevelSensorName convention
in favor of StreamsMetricsImpl#xLevelSensor to allow StreamsMetricsImpl
to track thread- and cache-level metrics, so that they may be cleanly declared
from anywhere but still unloaded at the appropriate time. This was necessary
right now so that the NamedCache could register a thread-level parent sensor
to be unloaded when the thread, not the cache, is closed.

* The above changes made it mostly unnecessary for the StreamsMetricsImpl to
expose a reference to the underlying Metrics registry, so I did a little extra work
to remove that reference, including removing inconsistently-used and unnecessary
calls to Metrics#close() in the tests.

The existing tests should be sufficient to verify this change.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",8,0,1,106,908,1,8,139,169,13,11,2,224,169,20,85,40,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreSupplierTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreSupplierTest.java,"MINOR: fix NamedCache metrics in Streams (#4917)

* Fixes a bug in which all NamedCache instances in a process shared
one parent metric.

* Also fixes a bug which incorrectly computed the per-cache metric tag
(which was undetected due to the former bug).

* Drop the StreamsMetricsConventions#xLevelSensorName convention
in favor of StreamsMetricsImpl#xLevelSensor to allow StreamsMetricsImpl
to track thread- and cache-level metrics, so that they may be cleanly declared
from anywhere but still unloaded at the appropriate time. This was necessary
right now so that the NamedCache could register a thread-level parent sensor
to be unloaded when the thread, not the cache, is closed.

* The above changes made it mostly unnecessary for the StreamsMetricsImpl to
expose a reference to the underlying Metrics registry, so I did a little extra work
to remove that reference, including removing inconsistently-used and unnecessary
calls to Metrics#close() in the tests.

The existing tests should be sufficient to verify this change.

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",11,0,1,144,1109,1,10,175,168,15,12,3.0,230,168,19,55,11,5,2,1,0,1
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/DefaultSerdes.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/DefaultSerdes.scala,"KAFKA-6670: Implement a Scala wrapper library for Kafka Streams

This PR implements a Scala wrapper library for Kafka Streams. The library is implemented as a project under streams, namely `:streams:streams-scala`. The PR contains the following:

* the library implementation of the wrapper abstractions
* the test suite
* the changes in `build.gradle` to build the library jar

The library has been tested running the tests as follows:

```
$ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdes streams:streams-scala:test
$ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro streams:streams-scala:test
$ ./gradlew -Dtest.single=WordCountTest streams:streams-scala:test
```

Author: Debasish Ghosh <ghosh.debasish@gmail.com>
Author: Sean Glover <seglo@randonom.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Ismael Juma <ismael@juma.me.uk>, John Roesler <john@confluent.io>, Damian Guy <damian@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4756 from debasishg/scala-streams",0,47,0,18,275,0,0,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/ScalaSerde.scala,streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/ScalaSerde.scala,"KAFKA-6670: Implement a Scala wrapper library for Kafka Streams

This PR implements a Scala wrapper library for Kafka Streams. The library is implemented as a project under streams, namely `:streams:streams-scala`. The PR contains the following:

* the library implementation of the wrapper abstractions
* the test suite
* the changes in `build.gradle` to build the library jar

The library has been tested running the tests as follows:

```
$ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdes streams:streams-scala:test
$ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro streams:streams-scala:test
$ ./gradlew -Dtest.single=WordCountTest streams:streams-scala:test
```

Author: Debasish Ghosh <ghosh.debasish@gmail.com>
Author: Sean Glover <seglo@randonom.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Ismael Juma <ismael@juma.me.uk>, John Roesler <john@confluent.io>, Damian Guy <damian@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4756 from debasishg/scala-streams",14,70,0,34,445,9,14,70,70,70,1,1,70,70,70,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsConventions.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/StreamsMetricsConventions.java,"KAFKA-6376; refactor skip metrics in Kafka Streams

* unify skipped records metering
* log warnings when things get skipped
* tighten up metrics usage a bit

### Testing strategy:
Unit testing of the metrics and the logs should be sufficient.

Author: John Roesler <john@confluent.io>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4812 from vvcephei/kip-274-streams-skip-metrics",4,39,0,19,135,3,3,39,39,39,1,1,39,39,39,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java,streams/src/test/java/org/apache/kafka/test/ProcessorTopologyTestDriver.java,"KAFKA-6376; refactor skip metrics in Kafka Streams

* unify skipped records metering
* log warnings when things get skipped
* tighten up metrics usage a bit

### Testing strategy:
Unit testing of the metrics and the logs should be sufficient.

Author: John Roesler <john@confluent.io>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4812 from vvcephei/kip-274-streams-skip-metrics",31,2,2,263,2498,1,13,490,317,9,53,2,837,317,16,347,77,7,2,1,0,1
clients/src/test/java/org/apache/kafka/test/MockDeserializer.java,clients/src/test/java/org/apache/kafka/test/MockDeserializer.java,"KAFKA-6592: Follow-up (#4864)

Do not require ConsoleConsumer to specify inner serde as s special property, but just a normal property of the message formatter.",6,5,0,43,317,1,6,71,59,18,4,2.5,83,59,21,12,9,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/StreamsRepeatingIntegerKeyProducer.java,streams/src/test/java/org/apache/kafka/streams/tests/StreamsRepeatingIntegerKeyProducer.java,"KAFKA-6611, PART II: Improve Streams SimpleBenchmark (#4854)

SimpleBenchmark:

1.a Do not rely on manual num.records / bytes collection on atomic integers.
1.b Rely on config files for num.threads, bootstrap.servers, etc.
1.c Add parameters for key skewness and value size.
1.d Refactor the tests for loading phase, adding tumbling-windowed count.
1.e For consumer / consumeproduce, collect metrics on consumer instead.
1.f Force stop the test after 3 minutes, this is based on empirical numbers of 10M records.

Other tests: use config for kafka bootstrap servers.

streams_simple_benchmark.py: only use scale 1 for system test, remove yahoo from benchmark tests.

Note that the JMX based metrics is more accurate than the manually collected metrics. 

Reviewers: John Roesler <john@confluent.io>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>",11,18,3,83,716,2,1,123,108,62,2,2.5,126,108,63,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/KafkaFuture.java,clients/src/main/java/org/apache/kafka/common/KafkaFuture.java,"KAFKA-6058: Refactor consumer API result return types (#4856)

Refactored the return types in consumer group APIs the following way:

```
Map<TopicPartition, KafkaFuture<Void>> DeleteConsumerGroupsResult#deletedGroups()

Map<TopicPartition, KafkaFuture<ConsumerGroupDescription>> DescribeConsumerGroupsResult#describedGroups()

KafkaFuture<Collection<ConsumerGroupListing>> ListConsumerGroupsResult#listings()

KafkaFuture<Map<TopicPartition, OffsetAndMetadata>> ListConsumerGroupOffsetsResult#partitionsToOffsetAndMetadata()
```

* For DeleteConsumerGroupsResult and DescribeConsumerGroupsResult, for each group id we have two round-trips to get the coordinator, and then send the delete / describe request; I leave the potential optimization of batching requests for future work.

* For ListConsumerGroupOffsetsResult, it is a simple single round-trip and hence the whole map is wrapped as a Future.

* ListConsumerGroupsResult, it is the most tricky one: we would only know how many futures we should wait for after the first listNode returns, and hence I constructed the flattened future in the middle wrapped with the underlying map of futures; also added an iterator API to compensate the ""fail the whole future if any broker returns error"" behavior. The iterator future will throw exception on the failing brokers, while return the consumer for other succeeded brokers.

Reviewers: Colin Patrick McCabe <colin@cmccabe.xyz>, Jason Gustafson <jason@confluent.io>",9,0,9,74,555,1,5,205,155,29,7,3,227,155,32,22,9,3,2,1,0,1
core/src/test/scala/unit/kafka/producer/AsyncProducerTest.scala,core/src/test/scala/unit/kafka/producer/AsyncProducerTest.scala,"MINOR: Fix AsyncProducerTest bug that hits when logging is turned up (#4450)

AsyncProducerTest gets an error about an incorrect mock when the logging
level is turned up.  Instead of usIng a mock, just create a real
SyncProducerConfig object, since the object is simple to create.

Reviewers: Ismael Juma <ismael@juma.me.uk>",42,5,2,393,3526,0,23,514,268,8,64,3.0,1350,304,21,836,206,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/JaasConfig.java,clients/src/main/java/org/apache/kafka/common/security/JaasConfig.java,KAFKA-4883: handle NullPointerException while parsing login modue control flag (#4849),20,3,0,83,587,1,4,124,123,25,5,1,133,123,27,9,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/Resource.java,clients/src/main/java/org/apache/kafka/common/requests/Resource.java,"Make [Config]Resource.toString() consistent with existing code (#4845)

The toString() for ConfigResource was using { } instead of ( ) which is inconsistent with the existing toStrings in the code, while toString for Resource was using a mix of ( and }.",10,1,1,34,186,1,6,60,60,30,2,1.0,61,60,30,1,1,0,1,0,1,1
connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTask.java,connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTask.java,"MINOR: Remove magic number and extract Pattern instance from method as class field (#4799)

* Remove magic number
* Extract Pattern instance from method as class field
* Add @Override declare

Reviewers: Randall Hauch <rhauch@gmail.com>",7,1,0,37,249,0,7,174,64,12,14,2.0,205,64,15,31,11,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/ReflectionsUtil.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/ReflectionsUtil.java,"MINOR: Remove magic number and extract Pattern instance from method as class field (#4799)

* Remove magic number
* Extract Pattern instance from method as class field
* Add @Override declare

Reviewers: Randall Hauch <rhauch@gmail.com>",8,2,0,59,368,0,5,94,90,31,3,1,108,90,36,14,14,5,2,1,0,1
clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java,clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java,"KAFKA-6576: Configurable Quota Management (KIP-257) (#4699)

Enable quota calculation to be customized using a configurable callback. See KIP-257 for details.

Reviewers: Jun Rao <junrao@gmail.com>",0,106,0,14,137,0,0,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/ListenerName.java,clients/src/main/java/org/apache/kafka/common/network/ListenerName.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",11,5,1,45,272,2,10,82,68,12,7,1,89,68,13,7,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/AuthenticateCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/auth/AuthenticateCallbackHandler.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",0,62,0,9,82,0,0,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/Login.java,clients/src/main/java/org/apache/kafka/common/security/auth/Login.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",0,16,5,14,110,0,0,68,57,11,6,2.5,87,57,14,19,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/AbstractLogin.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/AbstractLogin.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",12,29,9,74,494,8,8,115,108,19,6,4.5,160,108,27,45,15,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerCallbackHandler.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",9,16,21,46,341,5,5,79,81,9,9,6,144,81,16,65,21,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosClientCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosClientCallbackHandler.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",10,76,0,51,376,3,3,76,76,76,1,1,76,76,76,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/plain/PlainAuthenticateCallback.java,clients/src/main/java/org/apache/kafka/common/security/plain/PlainAuthenticateCallback.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",4,63,0,18,97,4,4,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/scram/ScramCredential.java,clients/src/main/java/org/apache/kafka/common/security/scram/ScramCredential.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",5,20,0,25,137,0,5,68,50,23,3,3,78,50,26,10,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/scram/ScramCredentialCallback.java,clients/src/main/java/org/apache/kafka/common/security/scram/ScramCredentialCallback.java,"KAFKA-4292: Configurable SASL callback handlers (KIP-86) (#2022)

Implementation of KIP-86. Client, server and login callback handlers have been made configurable for both brokers and clients.

Reviewers: Jun Rao <junrao@gmail.com>, Ron Dagostino <rndgstn@gmail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>",2,14,5,11,60,1,2,40,32,8,5,3,73,32,15,33,24,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/LogAndContinueExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/LogAndContinueExceptionHandler.java,"KAFKA-6702: Wrong className in LoggerFactory.getLogger method (#4772)

Reviewers: Manikumar Reddy, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",2,1,2,22,166,0,2,51,52,17,3,1,56,52,19,5,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/LogAndFailExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/LogAndFailExceptionHandler.java,"KAFKA-6702: Wrong className in LoggerFactory.getLogger method (#4772)

Reviewers: Manikumar Reddy, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Jason Gustafson <jason@confluent.io>",2,1,2,22,166,0,2,51,52,17,3,1,57,52,19,6,4,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/StateStoreTestUtils.java,streams/src/test/java/org/apache/kafka/streams/state/internals/StateStoreTestUtils.java,"KAFKA-6473: Add MockProcessorContext to public test-utils (#4736)

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>",1,2,2,33,252,1,1,56,125,6,9,2,156,125,17,100,57,11,2,1,0,1
clients/src/test/java/org/apache/kafka/test/MockProducerInterceptor.java,clients/src/test/java/org/apache/kafka/test/MockProducerInterceptor.java,"MINOR: Remove unnecessary null checks (#4708)

Remove unnecessary null check in StringDeserializer, MockProducerInterceptor and KStreamImpl.

Reviewers: Vahid Hashemian <vahidhashemian@us.ibm.com>, Jason Gustafson <jason@confluent.io>",13,1,1,73,591,1,7,105,85,18,6,2.5,116,85,19,11,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedSerializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedSerializer.java,"KAFKA-4831: Extract WindowedSerde to public APIs (#3307)

Now that we have augmented WindowSerde with non-arg parameters, extract it out as part of the public APIs so that users who want to I/O windowed streams can use it. This is originally introduced by @vitaly-pushkar

This PR grows out to be a much larger one, as I found a few tech debts and bugs while working on it. Here is a summary of the PR:

Public API changes (I will propose a KIP after a first round of reviews):
Add TimeWindowedSerializer, TimeWindowedDeserializer, SessionWindowedSerializer, SessionWindowedDeserializer into o.a.k.streams.kstream. The serializers would implemented an internal WindowedSerializer interface for the serializeBaseKey function used in 3) below.

Add WindowedSerdes into o.a.k.streams.kstream. The reason to now add them into o.a.k.clients's Serdes is that it then needs dependency of streams.

Add ""default.windowed.key.serde.inner"" and ""default.windowed.value.serde.inner"" into StreamsConfig, used when ""default.key.serde"" is specified to use time or session windowed serde. Note this requires the serde class, not the type class.

Consolidated serde format from multiple classes, including SessionKeySerde.java for session, and WindowStoreUtils for time window, into SessionKeySchema and WindowKeySchema.

Bug fix: WindowedStreamPartitioner needs to consider both time window and session window serdes.

Removed RocksDBWindowBytesStore etc optimization since after KIP-182 all the serde know happens on metered store, hence this optimization is not worth.

Bug fix: for time window, the serdes used for store and the serdes used for piping (source and sink node) are different: the former needs to append sequence number but not for the later.

Other minor cleanups: remove unnecessary throws, etc.

Authors: Guozhang Wang <wangguoz@gmail.com>, Vitaly Pushkar <vitaly.pushkar@gmail.com>

Reviewers: Matthias J. Sax <mjsax@apache.org>, Bill Bejeck <bill@confluent.io>, Xi Hu",0,2,65,6,70,7,0,25,57,3,8,2.5,102,57,13,77,65,10,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidTxnTimeoutException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidTxnTimeoutException.java,MINOR: Fix incorrect references to the max transaction timeout config (#4664),2,1,1,10,57,0,2,34,33,8,4,1.0,36,33,9,2,1,0,2,1,0,1
core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala,core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala,"KAFKA-3806: Increase offsets retention default to 7 days (KIP-186) (#4648)

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>",9,2,2,237,2609,1,8,331,174,8,40,2.0,585,174,15,254,66,6,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/tests/SystemTestUtil.java,streams/src/test/java/org/apache/kafka/streams/tests/SystemTestUtil.java,"MINOR: Add System test for standby task-rebalancing (#4554)

Author: Bill Bejeck <bill@confluent.io>

Reviewers: Damian Guy <damian@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>",4,62,0,25,200,1,1,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/header/Headers.java,clients/src/main/java/org/apache/kafka/common/header/Headers.java,MINOR: Fix javadoc typo in Headers (#4627),0,1,1,9,77,0,0,72,72,36,2,1.0,73,72,36,1,1,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java,streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java,"MINOR: Rename stream partition assignor to streams partition assignor (#4621)

This is a straight-forward change that make the name of the partition assignor to be aligned with Streams.

Reviewers: Matthias J. Sax <mjsax@apache.org>",74,1,1,391,2696,0,45,958,293,16,59,9,3089,293,52,2131,1028,36,2,1,0,1
core/src/main/scala/kafka/consumer/BaseConsumer.scala,core/src/main/scala/kafka/consumer/BaseConsumer.scala,"KAFKA-5327; Console Consumer should not commit messages not printed (#4546)

Ensure that the consumer's offsets are reset prior to closing so that any buffered messages which haven't been printed are not committed.",26,15,4,128,893,2,12,172,72,9,19,4,285,72,15,113,33,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/cache/LRUCache.java,clients/src/main/java/org/apache/kafka/common/cache/LRUCache.java,"MINOR: fixes lgtm.com warnings (#4582)

fixes lgmt.com warnings
cleanup PrintForeachAction and Printed

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Sebastian Bauersfeld <sebastianbauersfeld@gmx.de>, Damian Guy <damian@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>",5,1,1,30,187,1,5,56,57,14,4,1.0,63,57,16,7,5,2,2,1,0,1
core/src/main/scala/kafka/utils/Logging.scala,core/src/main/scala/kafka/utils/Logging.scala,"MINOR: Fix logger name override (#4600)

This regressed during the log4j -> scalalogging change.
Added unit tests, one of which failed before the fix.",14,7,4,41,484,0,13,90,77,6,16,2.5,237,77,15,147,89,9,2,1,0,1
connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSinkConnector.java,connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSinkConnector.java,"MINOR: Fix bug introduced by adding batch.size without default in FileStreamSourceConnector (#4579)

https://github.com/apache/kafka/pull/4356 added `batch.size` config property to `FileStreamSourceConnector` but the property was added as required without a default in config definition (`ConfigDef`). This results in validation error during connector startup. 

Unit tests were added for both `FileStreamSourceConnector` and `FileStreamSinkConnector` to avoid such issues in the future.

Reviewers: Randall Hauch <rhauch@gmail.com>, Jason Gustafson <jason@confluent.io>",8,3,1,49,366,1,6,81,62,9,9,2,100,62,11,19,7,2,2,1,0,1
connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java,connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java,"MINOR: Fix bug introduced by adding batch.size without default in FileStreamSourceConnector (#4579)

https://github.com/apache/kafka/pull/4356 added `batch.size` config property to `FileStreamSourceConnector` but the property was added as required without a default in config definition (`ConfigDef`). This results in validation error during connector startup. 

Unit tests were added for both `FileStreamSourceConnector` and `FileStreamSinkConnector` to avoid such issues in the future.

Reviewers: Randall Hauch <rhauch@gmail.com>, Jason Gustafson <jason@confluent.io>",8,13,17,64,500,1,6,98,70,10,10,3.5,139,70,14,41,17,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/DeleteConsumerGroupTest.scala,core/src/test/scala/unit/kafka/admin/DeleteConsumerGroupTest.scala,"MINOR: Update test classes to use KafkaZkClient methods (#4367)

Remove ZkUtils reference form ZooKeeperTestHarness plus some minor cleanups.",17,17,1,177,1438,2,14,236,212,18,13,2,297,212,23,61,24,5,2,1,0,1
core/src/test/scala/unit/kafka/zk/ZKEphemeralTest.scala,core/src/test/scala/unit/kafka/zk/ZKEphemeralTest.scala,"MINOR: Update test classes to use KafkaZkClient methods (#4367)

Remove ZkUtils reference form ZooKeeperTestHarness plus some minor cleanups.",18,7,6,120,825,2,8,177,98,7,25,3,307,98,12,130,35,5,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Struct.java,connect/api/src/main/java/org/apache/kafka/connect/data/Struct.java,"KAFKA-5550; Connect Struct.put() should include the field name if validation fails (#3507)

Changed call to use the overload of ConnectSchema.validateValue() method with the field name passed in. Ensure that field in put call is not null.

Reviewers: Randall Hauch <rhauch@gmail.com>, Jason Gustafson <jason@confluent.io>",43,3,1,138,1079,1,25,287,265,32,9,1,302,265,34,15,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/storage/HeaderConverter.java,connect/api/src/main/java/org/apache/kafka/connect/storage/HeaderConverter.java,"KAFKA-6513: Corrected how Converters and HeaderConverters are instantiated and configured

The commits for KIP-145 (KAFKA-5142) changed how the Connect workers instantiate and configure the Converters, and also added the ability to do the same for the new HeaderConverters. However, the last few commits removed the default value for the `converter.type` property for Converters and HeaderConverters, and this broke how the internal converters were being created.

This change corrects the behavior so that the `converter.type` property is always set by the worker (or by the Plugins class), which means the existing Converter implementations will not have to do this. The built-in JsonConverter, ByteArrayConverter, and StringConverter also implement HeaderConverter which implements Configurable, but the Worker and Plugins methods do not yet use the `Configurable.configure(Map)` method and instead still use the `Converter.configure(Map,boolean)`.

Several tests were modified, and a new PluginsTest was added to verify the new behavior in Plugins for instantiating and configuring the Converter and HeaderConverter instances.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4512 from rhauch/kafka-6513",0,1,1,12,128,0,0,53,53,26,2,1.0,54,53,27,1,1,0,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerFetcherManager.scala,core/src/main/scala/kafka/consumer/ConsumerFetcherManager.scala,"KAFKA-6519; Reduce log level for normal replica fetch errors (#4501)

Out of range and not leader errors are common in replica fetchers and not necessarily an indication of a problem. This patch therefore reduces the log level for log messages corresponding to these errors from `ERROR` to `INFO`. Additionally, this patch removes some redundant information in the log message which is already present in the log context.

Reviewers: Ismael Juma <ismael@juma.me.uk>",18,1,3,117,762,1,5,162,152,3,47,2,464,152,10,302,49,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueBytesStore.java,"KAFKA-4750: Bypass null value and treat it as deletes (#4508)

Here is the new rule for handling nulls:
* in the interface store, put(key, null) are handled normally and value serde will still be applied to null, hence needs to handle null values
* in the inner bytes store, null bytes after serialization will be treated as deletes.
* in the interface store, if null bytes get returned in get(key), it indicate the key is not available; and hence serde will be avoided and null object will be returned.

More changes:
* Update javadocs, add unit tests accordingly; augment MockContext to set serdes for the newly added tests.
* Fixed a discovered bug which is exposed by the newly added tests.
* Use the new API to remove all old APIs in the existing state store tests.
* Remove SerializedKeyValueIterator since it is not used any more.

This is originally contributed by @evis.

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Damian Guy <damian@confluent.io>",16,4,6,102,849,1,11,151,154,38,4,1.0,159,154,40,8,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerMetrics.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerMetrics.java,"MINOR:  exchange redundant Collections.addAll with parameterized constructor (#4521)

* Exchange manual copy to collection with Collections.addAll call
* Exchange redundant Collections.addAll with parameterized constructor call

Reviewers: Guozhang Wang <wangguoz@gmail.com>",4,1,2,26,217,1,4,50,51,25,2,1.0,52,51,26,2,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetrics.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetrics.java,"MINOR:  exchange redundant Collections.addAll with parameterized constructor (#4521)

* Exchange manual copy to collection with Collections.addAll call
* Exchange redundant Collections.addAll with parameterized constructor call

Reviewers: Guozhang Wang <wangguoz@gmail.com>",3,1,2,27,232,1,3,52,48,17,3,1,64,48,21,12,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/FetchSessionIdNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/FetchSessionIdNotFoundException.java,"KAFKA-6254; Incremental fetch requests

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #4418 from cmccabe/KAFKA-6254",2,29,0,9,45,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/InvalidFetchSessionEpochException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidFetchSessionEpochException.java,"KAFKA-6254; Incremental fetch requests

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #4418 from cmccabe/KAFKA-6254",2,29,0,9,45,2,2,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/FetchMetadata.java,clients/src/main/java/org/apache/kafka/common/requests/FetchMetadata.java,"KAFKA-6254; Incremental fetch requests

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #4418 from cmccabe/KAFKA-6254",21,154,0,73,471,11,11,154,154,154,1,1,154,154,154,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/Reconfigurable.java,clients/src/main/java/org/apache/kafka/common/Reconfigurable.java,"KAFKA-6246; Dynamic update of listeners and security configs (#4488)

Dynamic update of listeners as described in KIP-226. This includes:
  - Addition of new listeners with listener-prefixed security configs
  - Removal of existing listeners
  - Password encryption
  - sasl.jaas.config property for broker's JAAS config prefixed with listener and mechanism name",0,7,2,9,77,0,0,54,49,27,2,2.0,56,49,28,2,2,1,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Field.java,connect/api/src/main/java/org/apache/kafka/connect/data/Field.java,KAFKA-6515; Adding toString() method to o.a.k.connect.data.Field (#4509),12,9,0,42,228,1,7,85,77,21,4,1.0,91,77,23,6,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/header/Header.java,connect/api/src/main/java/org/apache/kafka/connect/header/Header.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",0,66,0,9,61,0,0,66,66,66,1,1,66,66,66,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/storage/ConverterConfig.java,connect/api/src/main/java/org/apache/kafka/connect/storage/ConverterConfig.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",3,58,0,22,219,3,3,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/storage/ConverterType.java,connect/api/src/main/java/org/apache/kafka/connect/storage/ConverterType.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",5,64,0,32,196,4,4,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/storage/SimpleHeaderConverter.java,connect/api/src/main/java/org/apache/kafka/connect/storage/SimpleHeaderConverter.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",10,85,0,54,401,5,5,85,85,85,1,1,85,85,85,0,0,0,0,0,0,0
connect/api/src/main/java/org/apache/kafka/connect/storage/StringConverter.java,connect/api/src/main/java/org/apache/kafka/connect/storage/StringConverter.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",13,44,15,65,528,6,9,109,81,27,4,5.0,137,81,34,28,15,7,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/converters/ByteArrayConverter.java,connect/runtime/src/main/java/org/apache/kafka/connect/converters/ByteArrayConverter.java,"KAFKA-5142: Add Connect support for message headers (KIP-145)

**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**

Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).

The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.

The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.

A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.

Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Arjun Satish <arjun@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Magesh Nandakumar <magesh.n.kumar@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4319 from rhauch/kafka-5142-b",12,31,1,45,362,5,8,82,52,27,3,5,89,52,30,7,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Base64.java,clients/src/main/java/org/apache/kafka/common/utils/Base64.java,"KAFKA-6464: Fix Base64URL encode padding issue under JRE 1.7 (#4455)

The org.apache.kafka.common.utils.Base64 class defers Base64 encoding/decoding to the java.util.Base64 class beginning with JRE 1.8 but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.  The implementation of the encodeToString(bytes[]) method returned under JRE 1.7 by Base64.urlEncoderNoPadding() blindly removed the last two trailing characters of the Base64 encoding under the assumption that they would always be the string ""=="" but that is incorrect; padding can be ""="", ""=="", or non-existent. This commit fixes that problem.

The commit also adds a Base64.urlDecoder() method that defers to java.util.Base64 under JRE 1.8+ but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.

Finally, there is a unit test to confirm that encode/decode are inverses in both the Base64 and Base64URL cases.",38,64,5,231,1361,7,28,320,261,160,2,6.0,325,261,162,5,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/Base64Test.java,clients/src/test/java/org/apache/kafka/common/utils/Base64Test.java,"KAFKA-6464: Fix Base64URL encode padding issue under JRE 1.7 (#4455)

The org.apache.kafka.common.utils.Base64 class defers Base64 encoding/decoding to the java.util.Base64 class beginning with JRE 1.8 but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.  The implementation of the encodeToString(bytes[]) method returned under JRE 1.7 by Base64.urlEncoderNoPadding() blindly removed the last two trailing characters of the Base64 encoding under the assumption that they would always be the string ""=="" but that is incorrect; padding can be ""="", ""=="", or non-existent. This commit fixes that problem.

The commit also adds a Base64.urlDecoder() method that defers to java.util.Base64 under JRE 1.8+ but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.

Finally, there is a unit test to confirm that encode/decode are inverses in both the Base64 and Base64URL cases.",4,45,0,22,186,3,3,45,45,45,1,1,45,45,45,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ListenerReconfigurable.java,clients/src/main/java/org/apache/kafka/common/network/ListenerReconfigurable.java,"KAFKA-6241; Enable dynamic updates of broker SSL keystore (#4263)

Enable dynamic broker configuration (see KIP-226 for details). Includes
 - Base implementation to allow specific broker configs and custom configs to be dynamically updated
 - Extend DescribeConfigsRequest/Response to return all synonym configs and their sources in the order of precedence
 - Extend AdminClient to alter dynamic broker configs
 - Dynamic update of SSL keystores

Reviewers: Ted Yu <yuzhihong@gmail.com>, Jason Gustafson <jason@confluent.io>",0,31,0,5,34,0,0,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginClassLoader.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginClassLoader.java,"KAFKA-6277: Ensure loadClass for plugin class loaders is thread-safe.

`loadClass` needs to be synchronized to protect subsequent calls to `defineClass`.

Details in the javadoc of this PR as well as here too: https://docs.oracle.com/javase/7/docs/technotes/guides/lang/cl-mt.html

/cc ewencp rhauch

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4428 from kkonstantine/KAFKA-6277-Make-loadClass-thread-safe-for-class-loaders-of-Connect-plugins",10,59,16,50,274,1,5,112,68,37,3,5,136,68,45,24,16,8,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueMapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueMapper.java,"KAFKA-4218: Enable access to key in ValueTransformer and ValueMapper

This PR is the partial implementation for KIP-149. As the discussion for this KIP is still ongoing, I made a PR on the ""safe"" portions of the KIP (so that it can be included in the next release) which are 1) `ValueMapperWithKey`, 2) `ValueTransformerWithKeySupplier`, and 3) `ValueTransformerWithKey`.

Author: Jeyhun Karimov <je.karimov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4309 from jeyhunkarimov/KIP-149_hope_last",0,4,0,4,29,0,0,49,23,4,11,2,71,23,6,22,8,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ValueMapperWithKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/ValueMapperWithKey.java,"KAFKA-4218: Enable access to key in ValueTransformer and ValueMapper

This PR is the partial implementation for KIP-149. As the discussion for this KIP is still ongoing, I made a PR on the ""safe"" portions of the KIP (so that it can be included in the next release) which are 1) `ValueMapperWithKey`, 2) `ValueTransformerWithKeySupplier`, and 3) `ValueTransformerWithKey`.

Author: Jeyhun Karimov <je.karimov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4309 from jeyhunkarimov/KIP-149_hope_last",0,52,0,4,35,0,0,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalValueTransformerWithKey.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalValueTransformerWithKey.java,"KAFKA-4218: Enable access to key in ValueTransformer and ValueMapper

This PR is the partial implementation for KIP-149. As the discussion for this KIP is still ongoing, I made a PR on the ""safe"" portions of the KIP (so that it can be included in the next release) which are 1) `ValueMapperWithKey`, 2) `ValueTransformerWithKeySupplier`, and 3) `ValueTransformerWithKey`.

Author: Jeyhun Karimov <je.karimov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4309 from jeyhunkarimov/KIP-149_hope_last",0,24,0,5,55,0,0,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalValueTransformerWithKeySupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalValueTransformerWithKeySupplier.java,"KAFKA-4218: Enable access to key in ValueTransformer and ValueMapper

This PR is the partial implementation for KIP-149. As the discussion for this KIP is still ongoing, I made a PR on the ""safe"" portions of the KIP (so that it can be included in the next release) which are 1) `ValueMapperWithKey`, 2) `ValueTransformerWithKeySupplier`, and 3) `ValueTransformerWithKey`.

Author: Jeyhun Karimov <je.karimov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4309 from jeyhunkarimov/KIP-149_hope_last",0,21,0,4,37,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenAuthorizationException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenDisabledException.java,clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenDisabledException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenExpiredException.java,clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenExpiredException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenNotFoundException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenOwnerMismatchException.java,clients/src/main/java/org/apache/kafka/common/errors/DelegationTokenOwnerMismatchException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/InvalidPrincipalTypeException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidPrincipalTypeException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/UnsupportedByAuthenticationException.java,clients/src/main/java/org/apache/kafka/common/errors/UnsupportedByAuthenticationException.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",2,33,0,10,57,2,2,33,33,33,1,1,33,33,33,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java,clients/src/main/java/org/apache/kafka/common/resource/ResourceType.java,"KAFKA-4541; Support for delegation token mechanism

- Add capability to create delegation token
- Add authentication based on delegation token.
- Add capability to renew/expire delegation tokens.
- Add units tests and integration tests

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3616 from omkreddy/KAFKA-4541",8,6,1,44,262,0,6,121,97,17,7,1,127,97,18,6,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionJoinTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionJoinTest.java,"KAFKA-6398: fix KTable.filter that does not include its parent's queryable storename

1. Include the parent's queryable store name in KTable.filter if this operator is not materialized.
2. Augment InternalTopologyBuilder checking on null processor / store names from the enum.
3. Unit test.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>

Closes #4384 from guozhangwang/K6398-topology-builder-exception",25,6,6,307,2587,5,23,386,565,14,28,5.0,935,565,33,549,235,20,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockPredicate.java,streams/src/test/java/org/apache/kafka/test/MockPredicate.java,"KAFKA-6398: fix KTable.filter that does not include its parent's queryable storename

1. Include the parent's queryable store name in KTable.filter if this operator is not materialized.
2. Augment InternalTopologyBuilder checking on null processor / store names from the enum.
3. Unit test.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>

Closes #4384 from guozhangwang/K6398-topology-builder-exception",2,33,0,13,90,2,2,33,33,33,1,1,33,33,33,0,0,0,2,1,0,1
core/src/main/scala/kafka/tools/UpdateOffsetsInZK.scala,core/src/main/scala/kafka/tools/UpdateOffsetsInZK.scala,"MINOR: Fix concurrency bug in MetadataCache and Metadata request when listeners inconsistent (#4374)

- Add missing locking/volatile in MetadataCache.aliveEndPoint
- Fix topic metadata not to throw BrokerNotAvailableException
when listeners are inconsistent. Add test verifying the fix. As
part of this fix, renamed Broker methods to follow Map
convention where the `get` version returns `Option`.

Reviewers: Jason Gustafson <jason@confluent.io>",14,1,1,62,557,1,3,98,73,4,24,2.0,183,73,8,85,13,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/LogContext.java,clients/src/main/java/org/apache/kafka/common/utils/LogContext.java,"KAFKA-6298; Added support for location aware logger which fixes log line numbers (#4311)

LogContext to have two different implementations of Logger. One will be picked based on availability of LocationAwareLogger API.

Reviewers: Jason Gustafson <jason@confluent.io>",177,484,76,631,4289,168,131,793,408,264,3,1,869,484,290,76,76,25,2,1,0,1
core/src/test/scala/unit/kafka/consumer/ConsumerIteratorTest.scala,core/src/test/scala/unit/kafka/consumer/ConsumerIteratorTest.scala,"MINOR: Fix zk client session state metric names and various async zk clean-ups

- Fix zk session state and session change rate metric names: type
should be SessionExpireListener instead of KafkaHealthCheck. Test
verifying the fix was included.
- Handle missing controller in controlled shutdown in the same way as if
the broker is not registered (i.e. retry after backoff).
- Restructure BrokerInfo to reduce duplication. It now contains a
Broker instance and the JSON serde is done in BrokerIdZNode
since `Broker` does not contain all the fields.
- Remove dead code from `ZooKeeperClient.initialize` and remove
redundant `close` calls.
- Move ACL handling and persistent paths definition from ZkUtils to
ZkData (and call ZkData from ZkUtils).
- Remove ZooKeeperClientWrapper and ZooKeeperClientMetrics from
ZkUtils (avoids metrics clash if third party users create a ZkUtils
instance in the same process as the broker).
- Introduce factory method in KafkaZkClient that creates
ZooKeeperClient and remove metric name defaults from
ZooKeeperClient.
- Fix a few instances where ZooKeeperClient was not closed in tests.
- Update a few TestUtils methods to use KafkaZkClient instead of
ZkUtils.
- Add test verifying SessionState metric.
- Various clean-ups.

Testing: mostly relying on existing tests, but added a couple
of new tests as mentioned above.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #4359 from ijuma/kafka-6320-kafka-health-zk-metrics-follow-up",6,1,2,84,636,1,4,121,88,5,24,2.0,196,88,8,75,17,3,2,1,0,1
core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala,core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala,"MINOR: Fix zk client session state metric names and various async zk clean-ups

- Fix zk session state and session change rate metric names: type
should be SessionExpireListener instead of KafkaHealthCheck. Test
verifying the fix was included.
- Handle missing controller in controlled shutdown in the same way as if
the broker is not registered (i.e. retry after backoff).
- Restructure BrokerInfo to reduce duplication. It now contains a
Broker instance and the JSON serde is done in BrokerIdZNode
since `Broker` does not contain all the fields.
- Remove dead code from `ZooKeeperClient.initialize` and remove
redundant `close` calls.
- Move ACL handling and persistent paths definition from ZkUtils to
ZkData (and call ZkData from ZkUtils).
- Remove ZooKeeperClientWrapper and ZooKeeperClientMetrics from
ZkUtils (avoids metrics clash if third party users create a ZkUtils
instance in the same process as the broker).
- Introduce factory method in KafkaZkClient that creates
ZooKeeperClient and remove metric name defaults from
ZooKeeperClient.
- Fix a few instances where ZooKeeperClient was not closed in tests.
- Update a few TestUtils methods to use KafkaZkClient instead of
ZkUtils.
- Add test verifying SessionState metric.
- Various clean-ups.

Testing: mostly relying on existing tests, but added a couple
of new tests as mentioned above.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #4359 from ijuma/kafka-6320-kafka-health-zk-metrics-follow-up",25,4,4,208,1939,4,10,276,261,5,60,3.0,819,261,14,543,101,9,2,1,0,1
core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala,core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala,"MINOR: Fix zk client session state metric names and various async zk clean-ups

- Fix zk session state and session change rate metric names: type
should be SessionExpireListener instead of KafkaHealthCheck. Test
verifying the fix was included.
- Handle missing controller in controlled shutdown in the same way as if
the broker is not registered (i.e. retry after backoff).
- Restructure BrokerInfo to reduce duplication. It now contains a
Broker instance and the JSON serde is done in BrokerIdZNode
since `Broker` does not contain all the fields.
- Remove dead code from `ZooKeeperClient.initialize` and remove
redundant `close` calls.
- Move ACL handling and persistent paths definition from ZkUtils to
ZkData (and call ZkData from ZkUtils).
- Remove ZooKeeperClientWrapper and ZooKeeperClientMetrics from
ZkUtils (avoids metrics clash if third party users create a ZkUtils
instance in the same process as the broker).
- Introduce factory method in KafkaZkClient that creates
ZooKeeperClient and remove metric name defaults from
ZooKeeperClient.
- Fix a few instances where ZooKeeperClient was not closed in tests.
- Update a few TestUtils methods to use KafkaZkClient instead of
ZkUtils.
- Add test verifying SessionState metric.
- Various clean-ups.

Testing: mostly relying on existing tests, but added a couple
of new tests as mentioned above.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #4359 from ijuma/kafka-6320-kafka-health-zk-metrics-follow-up",15,1,1,180,1723,1,9,280,178,4,64,3.0,786,178,12,506,71,8,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KStreamKTableJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KStreamKTableJoinIntegrationTest.java,"KAFKA-6256: fix flaky test KStreamKTableJoinIntegrationTest.shouldCountClicksPerRegionWithNonZeroByteCache

Increase commit interval to make it less likely that we flush the cache in-between.
To make it fool-proof, only compare the ""final"" result records if cache is enabled.

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4364 from mjsax/kafka-6256-flaky-kstream-ktable-join-with-caching-test",17,48,13,233,2009,2,8,339,266,12,29,2,498,266,17,159,33,5,2,1,0,1
core/src/test/scala/unit/kafka/integration/TopicMetadataTest.scala,core/src/test/scala/unit/kafka/integration/TopicMetadataTest.scala,MINOR: Update test classes to use KafkaZkClient/AdminZkClient methods (#4353),20,5,5,208,1884,4,13,289,128,5,54,4.0,642,128,12,353,65,7,2,1,0,1
core/src/test/scala/unit/kafka/producer/ProducerTest.scala,core/src/test/scala/unit/kafka/producer/ProducerTest.scala,MINOR: Update test classes to use KafkaZkClient/AdminZkClient methods (#4353),34,8,7,260,2198,5,9,357,689,5,72,4.0,1682,689,23,1325,471,18,2,1,0,1
tests/kafkatest/services/trogdor/files_unreadable_fault_spec.py,tests/kafkatest/services/trogdor/files_unreadable_fault_spec.py,"KAFKA-5849; Add process stop, round trip workload, partitioned test

* Implement process stop faults via SIGSTOP / SIGCONT

* Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.

* Allow Trogdor tasks to block until other Trogdor tasks are complete.

* Add CreateTopicsWorker, which can be a building block for a lot of tests.

* Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.

* Implement some fault injection tests in round_trip_workload_test.py

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4323 from cmccabe/KAFKA-5849",1,9,21,17,119,2,1,46,58,23,2,1.5,67,58,34,21,21,10,1,0,1,1
tests/kafkatest/services/trogdor/network_partition_fault_spec.py,tests/kafkatest/services/trogdor/network_partition_fault_spec.py,"KAFKA-5849; Add process stop, round trip workload, partitioned test

* Implement process stop faults via SIGSTOP / SIGCONT

* Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.

* Allow Trogdor tasks to block until other Trogdor tasks are complete.

* Add CreateTopicsWorker, which can be a building block for a lot of tests.

* Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.

* Implement some fault injection tests in round_trip_workload_test.py

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4323 from cmccabe/KAFKA-5849",2,2,17,12,70,2,1,39,54,13,3,1,59,54,20,20,17,7,2,1,0,1
tests/kafkatest/services/trogdor/no_op_task_spec.py,tests/kafkatest/services/trogdor/no_op_task_spec.py,"KAFKA-5849; Add process stop, round trip workload, partitioned test

* Implement process stop faults via SIGSTOP / SIGCONT

* Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.

* Allow Trogdor tasks to block until other Trogdor tasks are complete.

* Add CreateTopicsWorker, which can be a building block for a lot of tests.

* Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.

* Implement some fault injection tests in round_trip_workload_test.py

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4323 from cmccabe/KAFKA-5849",1,1,7,11,50,2,1,35,41,12,3,1,50,41,17,15,8,5,2,1,0,1
tests/kafkatest/services/trogdor/process_stop_fault_spec.py,tests/kafkatest/services/trogdor/process_stop_fault_spec.py,"KAFKA-5849; Add process stop, round trip workload, partitioned test

* Implement process stop faults via SIGSTOP / SIGCONT

* Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.

* Allow Trogdor tasks to block until other Trogdor tasks are complete.

* Add CreateTopicsWorker, which can be a building block for a lot of tests.

* Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.

* Implement some fault injection tests in round_trip_workload_test.py

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4323 from cmccabe/KAFKA-5849",1,38,0,10,74,1,1,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
tests/kafkatest/tests/tools/trogdor_test.py,tests/kafkatest/tests/tools/trogdor_test.py,"KAFKA-5849; Add process stop, round trip workload, partitioned test

* Implement process stop faults via SIGSTOP / SIGCONT

* Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.

* Allow Trogdor tasks to block until other Trogdor tasks are complete.

* Add CreateTopicsWorker, which can be a building block for a lot of tests.

* Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.

* Implement some fault injection tests in round_trip_workload_test.py

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4323 from cmccabe/KAFKA-5849",17,4,3,69,612,1,8,99,97,33,3,1,112,97,37,13,10,4,2,1,0,1
core/src/test/scala/unit/kafka/admin/ReplicationQuotaUtils.scala,core/src/test/scala/unit/kafka/admin/ReplicationQuotaUtils.scala,"KAFKA-5647; Use KafkaZkClient in ReassignPartitionsCommand and PreferredReplicaLeaderElectionCommand

*  Use KafkaZkClient in ReassignPartitionsCommand
*  Use KafkaZkClient in PreferredReplicaLeaderElectionCommand
*  Updated test classes to use new methods
*  All existing tests should pass

Author: Manikumar Reddy <manikumar.reddy@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #4260 from omkreddy/KAFKA-5647-ADMINCOMMANDS",10,10,9,37,356,4,2,56,51,7,8,3.0,93,51,12,37,9,5,2,1,0,1
core/src/main/scala/kafka/server/KafkaHealthcheck.scala,core/src/main/scala/kafka/server/KafkaHealthcheck.scala,"KAFKA-5473; handle ZK session expiration properly when a new session can't be established

(WIP: this commit isn't ready to be reviewed yet. I was checking the travis-ci build with the configuration changes in my account and opened the PR prematurely against trunk. I will make it consistent with Contribution guidelines once it's well tested.)

https://issues.apache.org/jira/browse/KAFKA-5473

Design:
`zookeeper.connection.retry.timeout.ms` => this determines how long to wait before triggering the shutdown. The default is 60000ms.

Currently the implementation only handles the `handleSessionEstablishmentError` by waiting for the sessionTimeout.

Author: Prasanna Gautam <prasannagautam@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #3990 from prasincs/KAFKA-5473",6,34,55,60,382,6,6,112,269,2,45,3,1117,293,25,1005,293,22,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/DefaultProductionExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/DefaultProductionExceptionHandler.java,"KAFKA-6086: Provide for custom error handling when Kafka Streams fails to produce

This PR creates and implements the `ProductionExceptionHandler` as described in [KIP-210](https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce).

I've additionally provided a default implementation preserving the existing behavior. I fixed various compile errors in the tests that resulted from my changing of method signatures, and added tests to cover the new behavior.

Author: Matt Farmer <mfarmer@rsglab.com>
Author: Matt Farmer <matt@frmr.me>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4165 from farmdawgnation/msf/kafka-6086",2,37,0,13,85,2,2,37,37,37,1,1,37,37,37,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/ProductionExceptionHandler.java,streams/src/main/java/org/apache/kafka/streams/errors/ProductionExceptionHandler.java,"KAFKA-6086: Provide for custom error handling when Kafka Streams fails to produce

This PR creates and implements the `ProductionExceptionHandler` as described in [KIP-210](https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce).

I've additionally provided a default implementation preserving the existing behavior. I fixed various compile errors in the tests that resulted from my changing of method signatures, and added tests to cover the new behavior.

Author: Matt Farmer <mfarmer@rsglab.com>
Author: Matt Farmer <matt@frmr.me>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4165 from farmdawgnation/msf/kafka-6086",1,59,0,18,115,1,1,59,59,59,1,1,59,59,59,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/errors/AlwaysContinueProductionExceptionHandler.java,streams/src/test/java/org/apache/kafka/streams/errors/AlwaysContinueProductionExceptionHandler.java,"KAFKA-6086: Provide for custom error handling when Kafka Streams fails to produce

This PR creates and implements the `ProductionExceptionHandler` as described in [KIP-210](https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce).

I've additionally provided a default implementation preserving the existing behavior. I fixed various compile errors in the tests that resulted from my changing of method signatures, and added tests to cover the new behavior.

Author: Matt Farmer <mfarmer@rsglab.com>
Author: Matt Farmer <matt@frmr.me>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4165 from farmdawgnation/msf/kafka-6086",2,37,0,13,85,2,2,37,37,37,1,1,37,37,37,0,0,0,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/StateTrackerTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/StateTrackerTest.java,"KAFKA-6102; Consolidate MockTime implementations between connect and clients

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Jason Gustafson <jason@confluent.io>

Closes #4105 from cmccabe/KAFKA-6102",5,1,1,69,919,0,4,100,100,50,2,1.0,101,100,50,1,1,0,1,0,1,1
core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala,core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala,"KAFKA-5631; Use Jackson for serialising to JSON

- Rename `encode` to `legacyEncodeAsString`, we
can remove this when we remove `ZkUtils`.
- Introduce `encodeAsString` that uses Jackson.
- Change `encodeAsBytes` to use Jackson.
- Avoid intermediate string when converting
Broker to json bytes.

The methods that use Jackson only support
Java collections unlike `legacyEncodeAsString`.

Tests were added `encodeAsString` and
`encodeAsBytes`.

Author: umesh chaudhary <umesh9794@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #4259 from umesh9794/KAFKA-5631",131,8,2,765,5318,1,38,1241,569,9,132,3.0,2890,569,22,1649,149,12,2,1,0,1
core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala,core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala,"KAFKA-5631; Use Jackson for serialising to JSON

- Rename `encode` to `legacyEncodeAsString`, we
can remove this when we remove `ZkUtils`.
- Introduce `encodeAsString` that uses Jackson.
- Change `encodeAsBytes` to use Jackson.
- Avoid intermediate string when converting
Broker to json bytes.

The methods that use Jackson only support
Java collections unlike `legacyEncodeAsString`.

Tests were added `encodeAsString` and
`encodeAsBytes`.

Author: umesh chaudhary <umesh9794@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #4259 from umesh9794/KAFKA-5631",15,3,3,201,1792,1,7,266,207,20,13,2,310,207,24,44,13,3,2,1,0,1
core/src/main/scala/kafka/log/CorruptIndexException.scala,core/src/main/scala/kafka/log/CorruptIndexException.scala,"KAFKA-6324; Change LogSegment.delete to deleteIfExists and harden log recovery

- Rename `delete()` to `deleteIfExists()` in `LogSegment`, `AbstractIndex`
and `TxnIndex`. Throw exception in case of IO errors for more informative
errors and to make it less likely that errors are ignored, `boolean` is used
for the case where the file does not exist (like `Files.deleteIfExists()`).
- Fix an instance of delete while open (should fix KAFKA-6322 and
KAFKA-6075).
- `LogSegment.deleteIfExists` no longer throws an exception if any of
the files it tries to delete does not exist (fixes KAFKA-6194).
- Remove unnecessary `FileChannel.force(true)` when deleting file.
- Introduce `LogSegment.open()` and use it to improve encapsulation
and reduce duplication.
- Expand functionality of `LogSegment.onBecomeInactiveSegment()`
to reduce duplication and improve encapsulation.
- Use `AbstractIndex.deleteIfExists()` instead of deleting files manually.
- Improve logging when deleting swap files.
- Use CorruptIndexException instead of IllegalArgumentException.
- Simplify `LogCleaner.cleanSegments()` to reduce duplication and
improve encapsulation.
- A few other clean-ups in Log, LogSegment, etc.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Ted Yu <yuzhihong@gmail.com>

Closes #4040 from ijuma/kafka-5829-follow-up",0,20,0,2,16,0,0,20,20,20,1,1,20,20,20,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManager.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManager.java,"KAFKA-6121: Restore and global consumer should not use auto.offset.reset

- set auto.offset.reste to ""none"" for restore and global consumer
- handle InvalidOffsetException for restore and global consumer
- add corresponding tests
- some minor cleanup

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com, Bill Bejeck <bill@confluent.io>, GuozhangWang <wangguoz@gmail.com>

Closes #4215 from mjsax/kafka-6121-restore-global-consumer-handle-reset",0,4,1,7,56,0,0,32,23,8,4,2.0,39,23,10,7,6,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/MockStateStoreSupplier.java,streams/src/test/java/org/apache/kafka/test/MockStateStoreSupplier.java,"KAFKA-6150: KIP-204 part III; Purge repartition topics with the admin client

1. Add the repartition topics information into ProcessorTopology: personally I do not like leaking this information into the topology but it seems not other simple way around.
2. StreamTask: added one more function to expose the consumed offsets from repartition topics only.
3. TaskManager: use the AdminClient to send the gathered offsets to delete only if a) previous call has completed and client intentionally ignore-and-log any errors, or b) no requests have ever called before.

NOTE that this code depends on the assumption that purge is only called right after the commit has succeeded, hence we presume all consumed offsets are committed.

4. MINOR: Added a few more constructor for ProcessorTopology for cleaner unit tests.
5. MINOR: Extracted MockStateStore out of the deprecated class.
6. MINOR: Made a pass over some unit test classes for clean ups.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4270 from guozhangwang/K6150-purge-repartition-topics",6,0,64,38,190,8,6,63,97,5,12,2.5,171,97,14,108,64,9,2,1,0,1
core/src/main/scala/kafka/tools/ProducerPerformance.scala,core/src/main/scala/kafka/tools/ProducerPerformance.scala,"MINOR: Shutdown ControllerEventThread via event instead of interruption

If the ControllerEventThread is interrupted when a request is
being sent, it may lead to an IllegalStateException being thrown.
This, in turn, can lead to a NullPointerException in
unregisterPartitionReassignmentIsrChangeHandlers,

To avoid these issues, we make the ControllerEventThread
uninterruptable and we shut it down by clearing the queue
and enqueuing a special event.

To make the code more robust, we also set
ReassignedPartitionsContext.reassignIsrChangeHandler
during construction instead of setting it to null first.

Finally, misleading log messages in ephemeral node
creation have been clarified.

For reference, the relevant log lines from the relevant
flaky test:

```text
[2017-11-15 10:30:13,869] ERROR Error while creating ephemeral at /controller with return code: OK (kafka.zk.KafkaZkClient$CheckedEphemeral:101)
[2017-11-15 10:30:14,155] ERROR Haven't been able to send leader and isr requests, current state of the map is Map(101 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 100 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 102 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false))). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101)
[2017-11-15 10:30:14,156] ERROR Haven't been able to send metadata update requests to brokers Set(102, 103, 104, 101, 105), current state of the partition info is Map(topic1-0 -> PartitionState(controllerEpoch=1, leader=101, leaderEpoch=2, isr=[101], zkVersion=2, replicas=[100, 102, 101], offlineReplicas=[100])). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101)
[2017-11-15 10:30:14,158] ERROR [Controller id=101] Forcing the controller to resign (kafka.controller.KafkaController:101)
[2017-11-15 10:30:14,158] ERROR [Controller id=101] Error completing reassignment of partition topic1-0 (kafka.controller.KafkaController:107)
java.lang.NullPointerException
	at kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:784)
	at kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:783)
```

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #4219 from ijuma/fix-npe-unregister-zk-listener",8,0,1,257,2037,0,3,569,277,11,51,2,1117,277,22,548,173,11,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java,"KAFKA-6170; KIP-220 Part 2: Break dependency of Assignor on StreamThread

This refactoring is discussed in https://github.com/apache/kafka/pull/3624#discussion_r132614639. More specifically:

1. Moved the access of `StreamThread` in `StreamPartitionAssignor` to `TaskManager`, removed any fields stored in `StreamThread` such as `processId` and `clientId` that are only to be used in `StreamPartitionAssignor`, and pass them to `TaskManager` if necessary.
2. Moved any in-memory states, `metadataWithInternalTopics`, `partitionsByHostState`, `standbyTasks`, `activeTasks` to `TaskManager` so that `StreamPartitionAssignor` becomes a stateless thin layer that access TaskManager directly.
3. Remove the reference of `StreamPartitionAssignor` in `StreamThread`, instead consolidate all related functionalities such as `cachedTasksIds ` in `TaskManager` which could be retrieved by the `StreamThread` and the `StreamPartitionAssignor` directly.
4. Finally, removed the two interfaces used for `StreamThread` and `StreamPartitionAssignor`.

5. Some minor fixes on logPrefixes, etc.

Future work: when replacing the StreamsKafkaClient, we would let `StreamPartitionAssignor` to retrieve it from `TaskManager` directly, and also its closing call do not need to be called (`KafkaStreams` will be responsible for closing it).

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>, Matthias J. Sax <matthias@confluent.io>

Closes #4224 from guozhangwang/K6170-refactor-assignor",39,5,59,267,2223,7,14,357,270,12,29,3,691,270,24,334,68,12,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClientTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClientTest.java,"KAFKA-6170; KIP-220 Part 2: Break dependency of Assignor on StreamThread

This refactoring is discussed in https://github.com/apache/kafka/pull/3624#discussion_r132614639. More specifically:

1. Moved the access of `StreamThread` in `StreamPartitionAssignor` to `TaskManager`, removed any fields stored in `StreamThread` such as `processId` and `clientId` that are only to be used in `StreamPartitionAssignor`, and pass them to `TaskManager` if necessary.
2. Moved any in-memory states, `metadataWithInternalTopics`, `partitionsByHostState`, `standbyTasks`, `activeTasks` to `TaskManager` so that `StreamPartitionAssignor` becomes a stateless thin layer that access TaskManager directly.
3. Remove the reference of `StreamPartitionAssignor` in `StreamThread`, instead consolidate all related functionalities such as `cachedTasksIds ` in `TaskManager` which could be retrieved by the `StreamThread` and the `StreamPartitionAssignor` directly.
4. Finally, removed the two interfaces used for `StreamThread` and `StreamPartitionAssignor`.

5. Some minor fixes on logPrefixes, etc.

Future work: when replacing the StreamsKafkaClient, we would let `StreamPartitionAssignor` to retrieve it from `TaskManager` directly, and also its closing call do not need to be called (`KafkaStreams` will be responsible for closing it).

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>, Matthias J. Sax <matthias@confluent.io>

Closes #4224 from guozhangwang/K6170-refactor-assignor",21,5,38,171,1514,7,17,219,113,31,7,5,278,119,40,59,38,8,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/SimpleMemoryRecordsTest.java,clients/src/test/java/org/apache/kafka/common/record/SimpleMemoryRecordsTest.java,"KAFKA-6261; Fix exception thrown by request logging if acks=0

Only expect responseAsString to be set if request logging is
enabled _and_ responseSend is defined.

Also fixed a couple of issues that would manifest themselves
if trace logging is enabled:

- `MemoryRecords.toString` should not throw exception if data is corrupted
- Generate `responseString` correctly if unsupported api versions request is
received.

Unit tests were added for every issue fixed. Also changed
SocketServerTest to run with trace logging enabled as
request logging breakage has been a common issue.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4250 from ijuma/fix-issues-when-trace-logging-is-enabled",1,41,0,15,132,1,1,41,41,41,1,1,41,41,41,0,0,0,2,1,0,1
core/src/main/scala/kafka/controller/StateChangeLogger.scala,core/src/main/scala/kafka/controller/StateChangeLogger.scala,"KAFKA-1044; Eliminate direct and non-optional log4j references from `core`

Use slf4j (via scala-logging) instead. Also:

- Log4jController is only initialised if log4j if in the classpath
- Use FATAL marker to support log4j's FATAL level (as the log4j-slf4j bridge does)
- Removed `Logging.swallow` in favour of CoreUtils.swallow, which logs to the
correct logger

Author: Viktor Somogyi <viktor.somogyi@cloudera.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3477 from viktorsomogyi/KAFKA-1044",2,3,3,19,143,0,2,50,50,25,2,2.5,53,50,26,3,3,2,2,1,0,1
core/src/main/scala/kafka/network/BlockingChannel.scala,core/src/main/scala/kafka/network/BlockingChannel.scala,"KAFKA-1044; Eliminate direct and non-optional log4j references from `core`

Use slf4j (via scala-logging) instead. Also:

- Log4jController is only initialised if log4j if in the classpath
- Use FATAL marker to support log4j's FATAL level (as the log4j-slf4j bridge does)
- Removed `Logging.swallow` in favour of CoreUtils.swallow, which logs to the
correct logger

Author: Viktor Somogyi <viktor.somogyi@cloudera.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3477 from viktorsomogyi/KAFKA-1044",15,4,4,92,557,1,5,135,93,10,14,2.0,187,93,13,52,22,4,2,1,0,1
core/src/main/scala/kafka/producer/SyncProducer.scala,core/src/main/scala/kafka/producer/SyncProducer.scala,"KAFKA-1044; Eliminate direct and non-optional log4j references from `core`

Use slf4j (via scala-logging) instead. Also:

- Log4jController is only initialised if log4j if in the classpath
- Use FATAL marker to support log4j's FATAL level (as the log4j-slf4j bridge does)
- Removed `Logging.swallow` in favour of CoreUtils.swallow, which logs to the
correct logger

Author: Viktor Somogyi <viktor.somogyi@cloudera.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3477 from viktorsomogyi/KAFKA-1044",22,1,1,116,705,1,8,175,228,4,47,2,500,228,11,325,46,7,2,1,0,1
core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala,core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala,"KAFKA-1044; Eliminate direct and non-optional log4j references from `core`

Use slf4j (via scala-logging) instead. Also:

- Log4jController is only initialised if log4j if in the classpath
- Use FATAL marker to support log4j's FATAL level (as the log4j-slf4j bridge does)
- Removed `Logging.swallow` in favour of CoreUtils.swallow, which logs to the
correct logger

Author: Viktor Somogyi <viktor.somogyi@cloudera.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3477 from viktorsomogyi/KAFKA-1044",64,5,4,292,2415,3,9,375,124,5,72,3.0,1183,189,16,808,84,11,2,1,0,1
core/src/main/scala/kafka/tools/SimpleConsumerPerformance.scala,core/src/main/scala/kafka/tools/SimpleConsumerPerformance.scala,"KAFKA-1044; Eliminate direct and non-optional log4j references from `core`

Use slf4j (via scala-logging) instead. Also:

- Log4jController is only initialised if log4j if in the classpath
- Use FATAL marker to support log4j's FATAL level (as the log4j-slf4j bridge does)
- Removed `Logging.swallow` in favour of CoreUtils.swallow, which logs to the
correct logger

Author: Viktor Somogyi <viktor.somogyi@cloudera.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3477 from viktorsomogyi/KAFKA-1044",11,2,4,123,942,0,1,259,143,13,20,1.5,309,143,15,50,9,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/ConnectExceptionMapper.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/ConnectExceptionMapper.java,"MINOR: Log unexpected exceptions in Connect REST calls that generate 500s at a higher log level

The ConnectExceptionMapper was originally intended to handle ConnectException errors for some expected cases where we just want to always convert them to a certain response and the ExceptionMapper was the easiest way to do that uniformly across the API. However, in the case that it's not an expected subclass, we should log the information at the error level so the user can track down the cause of the error.

This is only an initial improvement. We should probably also add a more general ExceptionMapper to handle other exceptions we may not have caught and converted to ConnectException.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Randall Hauch <rhauch@gmail.com>, Jason Gustafson <jason@confluent.io>

Closes #4227 from ewencp/better-connect-error-logging",6,22,6,49,435,2,1,75,60,19,4,4.5,96,60,24,21,10,5,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Date.java,connect/api/src/main/java/org/apache/kafka/connect/data/Date.java,"KAFKA-6218: Optimize condition in if statement to reduce the number of comparisons

Changed the condition in **if** statement
**(schema.name() == null || !(schema.name().equals(LOGICAL_NAME)))** which
requires two comparisons in worst case with
**(!LOGICAL_NAME.equals(schema.name()))**  which requires single comparison
in all cases and _avoids null pointer exception.
![kafka_optimize_if](https://user-images.githubusercontent.com/32234013/32872271-afe0b954-ca3a-11e7-838d-6a3bc416b807.JPG)
_

Author: sachinbhalekar <sachinbansibhalekar@gmail.com>
Author: sachinbhalekar <32234013+sachinbhalekar@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4225 from sachinbhalekar/trunk",9,2,2,32,301,2,3,75,76,19,4,3.0,86,76,22,11,5,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Time.java,connect/api/src/main/java/org/apache/kafka/connect/data/Time.java,"KAFKA-6218: Optimize condition in if statement to reduce the number of comparisons

Changed the condition in **if** statement
**(schema.name() == null || !(schema.name().equals(LOGICAL_NAME)))** which
requires two comparisons in worst case with
**(!LOGICAL_NAME.equals(schema.name()))**  which requires single comparison
in all cases and _avoids null pointer exception.
![kafka_optimize_if](https://user-images.githubusercontent.com/32234013/32872271-afe0b954-ca3a-11e7-838d-6a3bc416b807.JPG)
_

Author: sachinbhalekar <sachinbansibhalekar@gmail.com>
Author: sachinbhalekar <32234013+sachinbhalekar@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4225 from sachinbhalekar/trunk",9,2,2,33,276,2,3,76,77,15,5,2,88,77,18,12,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Timestamp.java,connect/api/src/main/java/org/apache/kafka/connect/data/Timestamp.java,"KAFKA-6218: Optimize condition in if statement to reduce the number of comparisons

Changed the condition in **if** statement
**(schema.name() == null || !(schema.name().equals(LOGICAL_NAME)))** which
requires two comparisons in worst case with
**(!LOGICAL_NAME.equals(schema.name()))**  which requires single comparison
in all cases and _avoids null pointer exception.
![kafka_optimize_if](https://user-images.githubusercontent.com/32234013/32872271-afe0b954-ca3a-11e7-838d-6a3bc416b807.JPG)
_

Author: sachinbhalekar <sachinbansibhalekar@gmail.com>
Author: sachinbhalekar <32234013+sachinbhalekar@users.noreply.github.com>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #4225 from sachinbhalekar/trunk",5,2,2,21,174,2,3,59,64,12,5,2,73,64,15,14,5,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/DeletedRecords.java,clients/src/main/java/org/apache/kafka/clients/admin/DeletedRecords.java,"KAFKA-5925: Adding records deletion operation to the new Admin Client API

This is the PR related to the [KIP-204](https://cwiki.apache.org/confluence/display/KAFKA/KIP-204+%3A+Adding+records+deletion+operation+to+the+new+Admin+Client+API) in order to add the `deleteRecords` operation to the new Admin Client (it's already available in the ""legacy"" one).
Other than that, unit test and integration tests are added as well (such integration tests come from the ""legacy"" integration tests in order to test the new addition in the same way as the ""legacy"" one).

Author: Paolo Patierno <ppatierno@live.com>

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4132 from ppatierno/kafka-5925",2,47,0,12,62,2,2,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
tests/kafkatest/directory_layout/kafka_path.py,tests/kafkatest/directory_layout/kafka_path.py,"MINOR: Add HttpMetricsReporter for system tests

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #4072 from ewencp/http-metrics",16,8,8,89,493,8,9,137,137,27,5,1,162,137,32,25,8,5,2,1,0,1
tests/kafkatest/services/performance/performance.py,tests/kafkatest/services/performance/performance.py,"MINOR: Add HttpMetricsReporter for system tests

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #4072 from ewencp/http-metrics",10,1,1,34,305,2,7,72,29,12,6,1.5,79,29,13,7,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java,"KAFKA-6179: Clear min timestamp tracker upon partition queue cleanup

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4186 from guozhangwang/K6179-cleanup-timestamp-tracker-on-clear",12,4,0,40,253,1,5,84,67,14,6,2.5,104,67,17,20,12,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampTracker.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/TimestampTracker.java,"KAFKA-6179: Clear min timestamp tracker upon partition queue cleanup

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4186 from guozhangwang/K6179-cleanup-timestamp-tracker-on-clear",0,5,1,9,62,0,0,61,58,20,3,2,66,58,22,5,4,2,2,1,0,1
core/src/main/scala/kafka/common/TopicAndPartition.scala,core/src/main/scala/kafka/common/TopicAndPartition.scala,"MINOR: Eliminate unnecessary Topic(And)Partition allocations in Controller

- Eliminated all the unnecessary allocations of `TopicPartition` and
`TopicAndPartition` in the Controller. We now use the former
in the Controller (bringing it inline with the rest of the non legacy
code).
- Fixed missed `Listener` -> `Handler` renames for companion
objects.
- More String.format -> String interpolation conversions (the former
is roughly 5 times more expensive).
- Some other minor clean-ups.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Onur Karaman <okaraman@linkedin.com>, Viktor Somogyi <viktorsomogyi@gmail.com>

Closes #4152 from ijuma/controller-topic-partition-and-other-clean-ups",1,0,11,6,54,4,1,30,21,2,12,2.0,55,21,5,25,11,2,2,1,0,1
core/src/main/scala/kafka/utils/LogDirUtils.scala,core/src/main/scala/kafka/utils/LogDirUtils.scala,"MINOR: Eliminate unnecessary Topic(And)Partition allocations in Controller

- Eliminated all the unnecessary allocations of `TopicPartition` and
`TopicAndPartition` in the Controller. We now use the former
in the Controller (bringing it inline with the rest of the non legacy
code).
- Fixed missed `Listener` -> `Handler` renames for companion
objects.
- More String.format -> String interpolation conversions (the former
is roughly 5 times more expensive).
- Some other minor clean-ups.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Onur Karaman <okaraman@linkedin.com>, Viktor Somogyi <viktorsomogyi@gmail.com>

Closes #4152 from ijuma/controller-topic-partition-and-other-clean-ups",6,2,2,35,267,1,4,60,66,20,3,2,80,66,27,20,18,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskAction.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskAction.java,"KAFKA-6115: TaskManager should be type aware

 - remove type specific methods from Task interface
 - add generics to preserve task type
 - add sub classes for different task types

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Bill Bejeck <bill@confluent.io>, Damian Guy <damian.guy@gmail.com>, Guozhang Wang <wangguoz@gmail.com>

Closes #4129 from mjsax/kafka-6115-taskManager-should-be-type-aware",0,2,2,5,35,0,0,22,22,11,2,1.5,24,22,12,2,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Sanitizer.java,clients/src/main/java/org/apache/kafka/common/utils/Sanitizer.java,"KAFKA-6156; Metric tag values with colons must be sanitized

Windows directory paths often contain colons which are not allowed in
yammer metrics. Metric tag values with special characters must be
quoted.

Author: huxihx <huxi_2b@hotmail.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #4173 from huxihx/KAFKA-6156",9,1,1,41,295,0,3,94,61,24,4,1.5,99,61,25,5,2,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java,streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java,"KAFKA-6157; Fix repeated words words in JavaDoc and comments.

Author: Adem Efe Gencer <agencer@linkedin.com>

Reviewers: Jiangjie Qin <becket.qin@gmail.com>

Closes #4170 from efeg/bug/typoFix",60,6,6,306,3044,0,40,1269,361,30,42,3.5,1796,420,43,527,161,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Scheduler.java,clients/src/main/java/org/apache/kafka/common/utils/Scheduler.java,"KAFKA-6060; Add workload generation capabilities to Trogdor

Previously, Trogdor only handled ""Faults.""  Now, Trogdor can handle
""Tasks"" which may be either faults, or workloads to execute in the
background.

The Agent and Coordinator have been refactored from a
mutexes-and-condition-variables paradigm into a message passing
paradigm.  No locks are necessary, because only one thread can access
the task state or worker state.  This makes them a lot easier to reason
about.

The MockTime class can now handle mocking deferred message passing
(adding a message to an ExecutorService with a delay).  I added a
MockTimeTest.

MiniTrogdorCluster now starts up Agent and Coordinator classes in
paralle in order to minimize junit test time.

RPC messages now inherit from a common Message.java class.  This class
handles implementing serialization, equals, hashCode, etc.

Remove FaultSet, since it is no longer necessary.

Previously, if CoordinatorClient or AgentClient hit a networking
problem, they would throw an exception.  They now retry several times
before giving up.  Additionally, the REST RPCs to the Coordinator and
Agent have been changed to be idempotent.  If a response is lost, and
the request is resent, no harm will be done.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #4073 from cmccabe/KAFKA-6060",0,49,0,10,80,0,0,49,49,49,1,1,49,49,49,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/SystemScheduler.java,clients/src/main/java/org/apache/kafka/common/utils/SystemScheduler.java,"KAFKA-6060; Add workload generation capabilities to Trogdor

Previously, Trogdor only handled ""Faults.""  Now, Trogdor can handle
""Tasks"" which may be either faults, or workloads to execute in the
background.

The Agent and Coordinator have been refactored from a
mutexes-and-condition-variables paradigm into a message passing
paradigm.  No locks are necessary, because only one thread can access
the task state or worker state.  This makes them a lot easier to reason
about.

The MockTime class can now handle mocking deferred message passing
(adding a message to an ExecutorService with a delay).  I added a
MockTimeTest.

MiniTrogdorCluster now starts up Agent and Coordinator classes in
paralle in order to minimize junit test time.

RPC messages now inherit from a common Message.java class.  This class
handles implementing serialization, equals, hashCode, etc.

Remove FaultSet, since it is no longer necessary.

Previously, if CoordinatorClient or AgentClient hit a networking
problem, they would throw an exception.  They now retry several times
before giving up.  Additionally, the REST RPCs to the Coordinator and
Agent have been changed to be idempotent.  If a response is lost, and
the request is resent, no harm will be done.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #4073 from cmccabe/KAFKA-6060",3,43,0,18,115,3,3,43,43,43,1,1,43,43,43,0,0,0,2,1,0,1
tools/src/main/java/org/apache/kafka/trogdor/rest/CreateWorkerResponse.java,tools/src/main/java/org/apache/kafka/trogdor/rest/CreateWorkerResponse.java,"KAFKA-6060; Add workload generation capabilities to Trogdor

Previously, Trogdor only handled ""Faults.""  Now, Trogdor can handle
""Tasks"" which may be either faults, or workloads to execute in the
background.

The Agent and Coordinator have been refactored from a
mutexes-and-condition-variables paradigm into a message passing
paradigm.  No locks are necessary, because only one thread can access
the task state or worker state.  This makes them a lot easier to reason
about.

The MockTime class can now handle mocking deferred message passing
(adding a message to an ExecutorService with a delay).  I added a
MockTimeTest.

MiniTrogdorCluster now starts up Agent and Coordinator classes in
paralle in order to minimize junit test time.

RPC messages now inherit from a common Message.java class.  This class
handles implementing serialization, equals, hashCode, etc.

Remove FaultSet, since it is no longer necessary.

Previously, if CoordinatorClient or AgentClient hit a networking
problem, they would throw an exception.  They now retry several times
before giving up.  Additionally, the REST RPCs to the Coordinator and
Agent have been changed to be idempotent.  If a response is lost, and
the request is resent, no harm will be done.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #4073 from cmccabe/KAFKA-6060",2,39,0,15,91,2,2,39,39,39,1,1,39,39,39,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/zk/ZKPathTest.scala,core/src/test/scala/unit/kafka/zk/ZKPathTest.scala,"MINOR: Rename and change package of async ZooKeeper classes

- kafka.controller.ZookeeperClient -> kafka.zookeeper.ZooKeeperClient
- kafka.controller.ControllerZkUtils -> kafka.zk.KafkaZkClient
- kafka.controller.ZkData -> kafka.zk.ZkData
- Renamed various fields to match new names and for consistency
- A few clean-ups in ZkData
- Document intent

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Onur Karaman <okaraman@linkedin.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #4112 from ijuma/rename-zookeeper-client-and-move-to-zookeper-package",16,4,4,102,651,4,8,130,147,9,15,8,266,147,18,136,38,9,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/DelayedTxnMarker.scala,core/src/main/scala/kafka/coordinator/transaction/DelayedTxnMarker.scala,"KAFKA-6042: Avoid deadlock between two groups with delayed operations

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #4103 from rajinisivaram/KAFKA-6042-group-deadlock

(cherry picked from commit 5ee157126d595b913761cf1887963460bbe12855)
Signed-off-by: Guozhang Wang <wangguoz@gmail.com>",4,5,3,21,143,1,3,49,48,10,5,1,57,48,11,8,4,2,2,1,0,1
core/src/main/scala/kafka/api/RequestOrResponse.scala,core/src/main/scala/kafka/api/RequestOrResponse.scala,"KAFKA-5163; Support replicas movement between log directories (KIP-113)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Tom Bentley <tbentley@redhat.com>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #3874 from lindong28/KAFKA-5163",2,3,2,16,122,0,2,48,27,3,17,1,77,27,5,29,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractStoreSupplier.java,"MINOR: add suppress warnings annotations in Streams API

 - fixes examples with regard to new API
 - fixes `Topology#addGlobalStore` parameters

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #4003 from mjsax/minor-deprecated",4,2,3,36,241,0,4,59,58,5,11,2,113,58,10,54,27,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreSupplier.java,"MINOR: add suppress warnings annotations in Streams API

 - fixes examples with regard to new API
 - fixes `Topology#addGlobalStore` parameters

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #4003 from mjsax/minor-deprecated",4,1,0,18,223,0,3,53,145,3,21,5,330,145,16,277,164,13,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryLRUCacheStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryLRUCacheStoreSupplier.java,"MINOR: add suppress warnings annotations in Streams API

 - fixes examples with regard to new API
 - fixes `Topology#addGlobalStore` parameters

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #4003 from mjsax/minor-deprecated",4,1,0,20,244,0,3,51,180,4,13,4,265,180,20,214,144,16,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreSupplier.java,"MINOR: add suppress warnings annotations in Streams API

 - fixes examples with regard to new API
 - fixes `Topology#addGlobalStore` parameters

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #4003 from mjsax/minor-deprecated",5,1,0,32,243,0,3,65,68,7,9,4,157,68,17,92,36,10,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreSupplier.java,"MINOR: add suppress warnings annotations in Streams API

 - fixes examples with regard to new API
 - fixes `Topology#addGlobalStore` parameters

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #4003 from mjsax/minor-deprecated",7,1,1,42,351,0,4,75,60,4,21,4,214,60,10,139,45,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnknownTopicOrPartitionException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownTopicOrPartitionException.java,"KAFKA-5856; AdminClient.createPartitions() follow up

- Improve tests and javadoc (including expected exceptions)
- Return correct authorization error if no describe topic
permission

Author: Tom Bentley <tbentley@redhat.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3937 from tombentley/KAFKA-5856-AdminClient.createPartitions-follow-up",4,5,1,15,76,0,4,45,22,6,7,1,74,22,11,29,15,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/SaslAuthenticationException.java,clients/src/main/java/org/apache/kafka/common/errors/SaslAuthenticationException.java,"KAFKA-6004; Allow authentication providers to override error message

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #4015 from rajinisivaram/KAFKA-6004-auth-exception",2,8,0,11,66,0,2,48,31,16,3,2,51,31,17,3,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProtocol.java,clients/src/main/java/org/apache/kafka/common/security/auth/SecurityProtocol.java,"MINOR: Use SecurityProtocol in AuthenticationContext

Since we removed the unused `TRACE` option from `SecurityProtocol`, it now seems safer to expose it from `AuthenticationContext`. Additionally this patch exposes javadocs under security.auth and relocates the `Login` and `AuthCallbackHandler` to a non-public package.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3863 from hachikuji/use-security-protocol-in-auth-context",5,1,1,41,295,0,5,75,63,7,11,1,126,63,11,51,32,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/AuthCallbackHandler.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/AuthCallbackHandler.java,"MINOR: Use SecurityProtocol in AuthenticationContext

Since we removed the unused `TRACE` option from `SecurityProtocol`, it now seems safer to expose it from `AuthenticationContext`. Additionally this patch exposes javadocs under security.auth and relocates the `Login` and `AuthCallbackHandler` to a non-public package.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3863 from hachikuji/use-security-protocol-in-auth-context",0,1,1,9,86,0,0,45,46,15,3,1,50,46,17,5,4,2,2,1,0,1
core/src/test/scala/integration/kafka/api/SslConsumerTest.scala,core/src/test/scala/integration/kafka/api/SslConsumerTest.scala,"MINOR: Use SecurityProtocol in AuthenticationContext

Since we removed the unused `TRACE` option from `SecurityProtocol`, it now seems safer to expose it from `AuthenticationContext`. Additionally this patch exposes javadocs under security.auth and relocates the `Login` and `AuthCallbackHandler` to a non-public package.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3863 from hachikuji/use-security-protocol-in-auth-context",0,1,1,7,55,0,0,44,22,15,3,1,45,22,15,1,1,0,2,1,0,1
core/src/test/scala/integration/kafka/api/SslProducerSendTest.scala,core/src/test/scala/integration/kafka/api/SslProducerSendTest.scala,"MINOR: Use SecurityProtocol in AuthenticationContext

Since we removed the unused `TRACE` option from `SecurityProtocol`, it now seems safer to expose it from `AuthenticationContext`. Additionally this patch exposes javadocs under security.auth and relocates the `Login` and `AuthCallbackHandler` to a non-public package.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3863 from hachikuji/use-security-protocol-in-auth-context",0,1,1,7,55,0,0,27,27,14,2,1.0,28,27,14,1,1,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/StateTracker.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/StateTracker.java,"KAFKA-5902: Added sink task metrics (KIP-196)

Added Connect metrics specific to source tasks, and builds upon #3864 and #3911 that have already been merged into `trunk`, and #3959 that has yet to be merged.

I'll rebase this PR when the latter is merged.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #3975 from rhauch/kafka-5902",22,1,1,96,560,1,7,173,173,86,2,1.0,174,173,87,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/Aggregator.java,streams/src/main/java/org/apache/kafka/streams/kstream/Aggregator.java,"MINOR: fix JavaDocs warnings

 - add some missing annotations for deprecated methods

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Michael G. Noll <michael@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4005 from mjsax/minor-fix-javadoc-warnings",0,6,6,4,39,0,0,51,38,4,13,2,122,38,9,71,15,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Initializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/Initializer.java,"MINOR: fix JavaDocs warnings

 - add some missing annotations for deprecated methods

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Michael G. Noll <michael@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4005 from mjsax/minor-fix-javadoc-warnings",0,6,6,4,24,0,0,41,36,3,12,2.0,80,36,7,39,15,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Reducer.java,streams/src/main/java/org/apache/kafka/streams/kstream/Reducer.java,"MINOR: fix JavaDocs warnings

 - add some missing annotations for deprecated methods

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Michael G. Noll <michael@confluent.io>, Damian Guy <damian.guy@gmail.com>

Closes #4005 from mjsax/minor-fix-javadoc-warnings",0,6,6,5,42,0,0,49,23,4,12,2.0,81,23,7,32,7,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Gauge.java,clients/src/main/java/org/apache/kafka/common/metrics/Gauge.java,"KAFKA-5746; Add new metrics to support health checks (KIP-188)

Adds new metrics to support health checks:
1. Error rates for each request type, per-error code
2. Request size and temporary memory size
3. Message conversion rate and time
4. Successful and failed authentication rates
5. ZooKeeper latency and status
6. Client version

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3705 from rajinisivaram/KAFKA-5746-new-metrics",0,31,0,4,34,0,0,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Measurable.java,clients/src/main/java/org/apache/kafka/common/metrics/Measurable.java,"KAFKA-5746; Add new metrics to support health checks (KIP-188)

Adds new metrics to support health checks:
1. Error rates for each request type, per-error code
2. Request size and temporary memory size
3. Message conversion rate and time
4. Successful and failed authentication rates
5. ZooKeeper latency and status
6. Client version

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3705 from rajinisivaram/KAFKA-5746-new-metrics",0,2,2,4,31,0,0,32,16,5,7,1,64,16,9,32,16,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/MetricValueProvider.java,clients/src/main/java/org/apache/kafka/common/metrics/MetricValueProvider.java,"KAFKA-5746; Add new metrics to support health checks (KIP-188)

Adds new metrics to support health checks:
1. Error rates for each request type, per-error code
2. Request size and temporary memory size
3. Message conversion rate and time
4. Successful and failed authentication rates
5. ZooKeeper latency and status
6. Client version

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3705 from rajinisivaram/KAFKA-5746-new-metrics",0,28,0,2,19,0,0,28,28,28,1,1,28,28,28,0,0,0,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerInfo.java,"KAFKA-5867: Log Kafka Connect worker info during startup

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #3932 from kkonstantine/KAFKA-5867-Kafka-Connect-applications-should-log-info-message-when-starting-up",7,107,0,63,481,5,5,107,107,107,1,1,107,107,107,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/SslAuthenticationException.java,clients/src/main/java/org/apache/kafka/common/errors/SslAuthenticationException.java,"KAFKA-5920; Handle SSL handshake failures as authentication exceptions

1. Propagate `SSLException` as `SslAuthenticationException` to enable clients to report these and avoid retries
2. Updates to `SslTransportLayer` to process bytes received even if end-of-stream
3. Some tidy up of authentication handling
4. Report exceptions in SaslClientAuthenticator as AuthenticationExceptions

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3918 from rajinisivaram/KAFKA-5920-SSL-handshake-failure",2,44,0,11,66,2,2,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/IllegalSaslStateException.java,clients/src/main/java/org/apache/kafka/common/errors/IllegalSaslStateException.java,"KAFKA-5947; Handle authentication failure in admin client, txn producer

1. Raise AuthenticationException for authentication failures in admin client
2. Handle AuthenticationException as a fatal error for transactional producer
3. Add comments to authentication exceptions

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Vahid Hashemian <vahidhashemian@us.ibm.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3928 from rajinisivaram/KAFKA-5947-auth-failure",2,5,0,10,57,0,2,36,27,12,3,1,45,27,15,9,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnsupportedSaslMechanismException.java,clients/src/main/java/org/apache/kafka/common/errors/UnsupportedSaslMechanismException.java,"KAFKA-5947; Handle authentication failure in admin client, txn producer

1. Raise AuthenticationException for authentication failures in admin client
2. Handle AuthenticationException as a fatal error for transactional producer
3. Add comments to authentication exceptions

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Vahid Hashemian <vahidhashemian@us.ibm.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3928 from rajinisivaram/KAFKA-5947-auth-failure",2,4,0,10,57,0,2,35,27,12,3,1,44,27,15,9,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ReassignmentInProgressException.java,clients/src/main/java/org/apache/kafka/common/errors/ReassignmentInProgressException.java,"KAFKA-5856; Add AdminClient.createPartitions() (KIP-195)

The contribution is my original work and I license the work to the project under the project's open source license.

This patch adds AdminClient.createPartitions() and the network protocol is
uses. The broker-side algorithm is as follows:

1. KafkaApis makes some initial checks on the request, then delegates to the
   new AdminManager.createPartitions() method.
2. AdminManager.createPartitions() performs some validation then delegates to
   AdminUtils.addPartitions().

Aside: I felt it was safer to add the extra validation in
AdminManager.createPartitions() than in AdminUtils.addPartitions() since the
latter is used on other code paths which might fail differently with the
introduction of extra checks.

3. AdminUtils.addPartitions() does its own checks and adds the partitions.
4. AdminManager then uses the existing topic purgatory to wait for the
   PartitionInfo available from the metadata cache to become consistent with
   the new total number of partitions.

The messages of exceptions thrown in AdminUtils affecting this new API have
been made consistent with initial capital letter and terminating period.
A few have been reworded for clarity.

Author: Tom Bentley <tbentley@redhat.com>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3870 from tombentley/KAFKA-5856-AdminClient.createPartitions",2,32,0,9,49,2,2,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/UnknownProducerIdException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownProducerIdException.java,"KAFKA-5793; Tighten up the semantics of the OutOfOrderSequenceException

Description of the solution can be found here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Exactly+Once+-+Solving+the+problem+of+spurious+OutOfOrderSequence+errors

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #3865 from apurvam/KAFKA-5793-tighten-up-out-of-order-sequence-v2",1,32,0,6,31,1,1,32,32,32,1,1,32,32,32,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNodeRecordDeserializer.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/SourceNodeRecordDeserializer.java,"MINOR: various random minor fixes and improve KafkaConsumer JavaDocs

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Jason Gustafson <jason@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3884 from mjsax/minor-fixed-discoverd-via-exception-handling-investigation",8,2,2,62,485,2,4,90,57,15,6,2.0,101,57,17,11,6,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/ProtoUtils.java,clients/src/main/java/org/apache/kafka/common/protocol/ProtoUtils.java,"MINOR: Move request/response schemas to the corresponding object representation

This refactor achieves the following:

1. Breaks up the increasingly unmanageable `Protocol` class and moves schemas closer to their actual usage.
2. Removes the need for redundant field identifiers maintained separately in `Protocol` and the respective request/response objects.
3. Provides a better mechanism for sharing common fields between different schemas (e.g. topics, partitions, error codes, etc.).
4. Adds convenience helpers to `Struct` for common patterns (such as setting a field only if it exists).

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3813 from hachikuji/protocol-schema-refactor",7,3,3,28,220,1,2,120,95,10,12,1.5,199,95,17,79,49,7,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/types/BoundField.java,clients/src/main/java/org/apache/kafka/common/protocol/types/BoundField.java,"MINOR: Move request/response schemas to the corresponding object representation

This refactor achieves the following:

1. Breaks up the increasingly unmanageable `Protocol` class and moves schemas closer to their actual usage.
2. Removes the need for redundant field identifiers maintained separately in `Protocol` and the respective request/response objects.
3. Provides a better mechanism for sharing common fields between different schemas (e.g. topics, partitions, error codes, etc.).
4. Adds convenience helpers to `Struct` for common patterns (such as setting a field only if it exists).

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3813 from hachikuji/protocol-schema-refactor",2,37,0,15,83,2,2,37,37,37,1,1,37,37,37,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/ApiVersionsTest.scala,core/src/test/scala/unit/kafka/server/ApiVersionsTest.scala,"MINOR: Move request/response schemas to the corresponding object representation

This refactor achieves the following:

1. Breaks up the increasingly unmanageable `Protocol` class and moves schemas closer to their actual usage.
2. Removes the need for redundant field identifiers maintained separately in `Protocol` and the respective request/response objects.
3. Provides a better mechanism for sharing common fields between different schemas (e.g. topics, partitions, error codes, etc.).
4. Adds convenience helpers to `Struct` for common patterns (such as setting a field only if it exists).

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3813 from hachikuji/protocol-schema-refactor",4,7,7,26,220,1,1,51,51,6,8,2.0,72,51,9,21,7,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/JoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/JoinIntegrationTest.java,"KAFKA-5873; add materialized overloads to StreamsBuilder

Add overloads for `table` and `globalTable` that use `Materialized`

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3837 from dguy/kafka-5873",17,2,2,322,2413,1,15,400,433,33,12,1.0,459,433,38,59,39,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableJoinIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableJoinIntegrationTest.java,"KAFKA-5873; add materialized overloads to StreamsBuilder

Add overloads for `table` and `globalTable` that use `Materialized`

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3837 from dguy/kafka-5873",38,3,3,327,2949,1,25,399,280,22,18,2.0,630,280,35,231,85,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java,"KAFKA-5754; Refactor Streams to use LogContext

This PR utilizes `org.apache.kafka.common.utils.LogContext` for logging in `KafkaStreams`. hachikuji, ijuma please review this and let me know your thoughts.

Author: umesh chaudhary <umesh9794@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #3727 from umesh9794/KAFKA-5754",22,2,1,182,1511,1,22,225,207,25,9,2,263,207,29,38,19,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/DuplicateSequenceException.java,clients/src/main/java/org/apache/kafka/common/errors/DuplicateSequenceException.java,"KAFKA-5494; Enable idempotence with max.in.flight.requests.per.connection > 1

Here we introduce client and broker changes to support multiple inflight requests while still guaranteeing idempotence. Two major problems to be solved:

1. Sequence number management on the client when there are request failures. When a batch fails,  future inflight batches will also fail with `OutOfOrderSequenceException`. This must be handled on the client with intelligent sequence reassignment. We must also deal with the fatal failure of some batch: the future batches must get different sequence numbers when the come back.
2. On the broker, when we have multiple inflights, we can get duplicates of multiple old batches. With this patch, we retain the record metadata for 5 older batches.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3743 from apurvam/KAFKA-5494-increase-max-in-flight-for-idempotent-producer",1,2,2,6,31,2,1,24,24,12,2,1.5,26,24,13,2,2,1,0,0,0,0
core/src/main/scala/kafka/javaapi/consumer/ZookeeperConsumerConnector.scala,core/src/main/scala/kafka/javaapi/consumer/ZookeeperConsumerConnector.scala,"MINOR: Remove unused SecurityProtocol.TRACE

It adds complexity for no benefit since we don't use
it anywhere.

Also removed a few unused imports, variables and
default parameters.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jason Gustafson <jason@confluent.io>

Closes #3856 from ijuma/remove-security-protocol-trace",9,0,1,58,577,0,9,141,88,7,20,2.0,218,88,11,77,22,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/DefaultPrincipalBuilder.java,clients/src/main/java/org/apache/kafka/common/security/auth/DefaultPrincipalBuilder.java,"KAFKA-5783; Add KafkaPrincipalBuilder with support for SASL (KIP-189)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #3795 from hachikuji/KAFKA-5783",4,5,1,18,136,0,3,45,43,11,4,1.0,51,43,13,6,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalBuilder.java,clients/src/main/java/org/apache/kafka/common/security/auth/KafkaPrincipalBuilder.java,"KAFKA-5783; Add KafkaPrincipalBuilder with support for SASL (KIP-189)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #3795 from hachikuji/KAFKA-5783",0,36,0,4,25,0,0,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/security/auth/PrincipalBuilder.java,clients/src/main/java/org/apache/kafka/common/security/auth/PrincipalBuilder.java,"KAFKA-5783; Add KafkaPrincipalBuilder with support for SASL (KIP-189)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Manikumar Reddy <manikumar.reddy@gmail.com>

Closes #3795 from hachikuji/KAFKA-5783",0,4,1,15,132,0,0,52,51,10,5,3,68,51,14,16,11,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedDeserializer.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowedDeserializer.java,"KAFKA-4468: Correctly calculate the window end timestamp after read from state stores

I have decided to use the following approach to fixing this bug:

1) Since the Window Size in WindowedDeserializer was originally unknown, I have initialized
a field _windowSize_ and created a constructor to allow it to be instantiated

2) The default size for __windowSize__ is _Long.MAX_VALUE_. If that is the case, then the
deserialize method will return an Unlimited Window, or else will return Timed one.

3) Temperature Demo was modified to demonstrate how to use this new constructor, given
that the window size is known.

Author: Richard Yu <richardyu@Richards-Air.attlocal.net>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3745 from ConcurrencyPractitioner/trunk",14,30,10,63,510,7,9,106,59,21,5,4,123,59,25,17,10,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreUtils.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WindowStoreUtils.java,"KAFKA-4468: Correctly calculate the window end timestamp after read from state stores

I have decided to use the following approach to fixing this bug:

1) Since the Window Size in WindowedDeserializer was originally unknown, I have initialized
a field _windowSize_ and created a constructor to allow it to be instantiated

2) The default size for __windowSize__ is _Long.MAX_VALUE_. If that is the case, then the
deserialize method will return an Unlimited Window, or else will return Timed one.

3) Temperature Demo was modified to demonstrate how to use this new constructor, given
that the window size is known.

Author: Richard Yu <richardyu@Richards-Air.attlocal.net>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3745 from ConcurrencyPractitioner/trunk",9,1,1,47,499,1,8,86,55,5,16,2.0,137,55,9,51,18,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/FanoutIntegrationTest.java,"KAFKA-5531; throw concrete exceptions in streams tests

1. Now instead of just generic `Exception` methods declare more concrete
exceptions throwing or don't declare any throwing at all, if not needed.
2. `SimpleBenchmark.run()` throws `RuntimeException`
3. `SimpleBenchmark.produce()` throws `IllegalArgumentException`
4. Expect `ProcessorStateException` in
`StandbyTaskTest.testUpdateNonPersistentStore()`

/cc enothereska

Author: Evgeny Veretennikov <evg.veretennikov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3485 from evis/5531-throw-concrete-exceptions",3,1,1,105,1039,1,2,166,166,10,16,2.5,239,166,15,73,27,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionKeySerdeTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionKeySerdeTest.java,"KAFKA-5531; throw concrete exceptions in streams tests

1. Now instead of just generic `Exception` methods declare more concrete
exceptions throwing or don't declare any throwing at all, if not needed.
2. `SimpleBenchmark.run()` throws `RuntimeException`
3. `SimpleBenchmark.produce()` throws `IllegalArgumentException`
4. Expect `ProcessorStateException` in
`StandbyTaskTest.testUpdateNonPersistentStore()`

/cc enothereska

Author: Evgeny Veretennikov <evg.veretennikov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3485 from evis/5531-throw-concrete-exceptions",11,11,11,76,704,11,11,107,87,18,6,5.5,161,87,27,54,23,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java,"KAFKA-5531; throw concrete exceptions in streams tests

1. Now instead of just generic `Exception` methods declare more concrete
exceptions throwing or don't declare any throwing at all, if not needed.
2. `SimpleBenchmark.run()` throws `RuntimeException`
3. `SimpleBenchmark.produce()` throws `IllegalArgumentException`
4. Expect `ProcessorStateException` in
`StandbyTaskTest.testUpdateNonPersistentStore()`

/cc enothereska

Author: Evgeny Veretennikov <evg.veretennikov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3485 from evis/5531-throw-concrete-exceptions",8,7,7,50,351,7,8,78,93,20,4,5.5,147,93,37,69,58,17,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/SerializedKeyValueIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/SerializedKeyValueIteratorTest.java,"KAFKA-5531; throw concrete exceptions in streams tests

1. Now instead of just generic `Exception` methods declare more concrete
exceptions throwing or don't declare any throwing at all, if not needed.
2. `SimpleBenchmark.run()` throws `RuntimeException`
3. `SimpleBenchmark.produce()` throws `IllegalArgumentException`
4. Expect `ProcessorStateException` in
`StandbyTaskTest.testUpdateNonPersistentStore()`

/cc enothereska

Author: Evgeny Veretennikov <evg.veretennikov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3485 from evis/5531-throw-concrete-exceptions",8,5,5,63,513,5,7,94,95,31,3,4,106,95,35,12,7,4,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreUtilsTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreUtilsTest.java,"KAFKA-5531; throw concrete exceptions in streams tests

1. Now instead of just generic `Exception` methods declare more concrete
exceptions throwing or don't declare any throwing at all, if not needed.
2. `SimpleBenchmark.run()` throws `RuntimeException`
3. `SimpleBenchmark.produce()` throws `IllegalArgumentException`
4. Expect `ProcessorStateException` in
`StandbyTaskTest.testUpdateNonPersistentStore()`

/cc enothereska

Author: Evgeny Veretennikov <evg.veretennikov@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>

Closes #3485 from evis/5531-throw-concrete-exceptions",1,1,1,22,216,1,1,42,44,10,4,1.0,65,44,16,23,17,6,2,1,0,1
tests/kafkatest/services/trogdor/__init__.py,tests/kafkatest/services/trogdor/__init__.py,"KAFKA-5777; Add ducktape integration for Trogdor

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3726 from cmccabe/KAFKA-5777",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/trogdor/fault/AbstractFault.java,tools/src/main/java/org/apache/kafka/trogdor/fault/AbstractFault.java,"KAFKA-5777; Add ducktape integration for Trogdor

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3726 from cmccabe/KAFKA-5777",15,106,0,73,460,10,10,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/trogdor/fault/SendingState.java,tools/src/main/java/org/apache/kafka/trogdor/fault/SendingState.java,"KAFKA-5777; Add ducktape integration for Trogdor

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3726 from cmccabe/KAFKA-5777",6,64,0,32,223,3,3,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/trogdor/rest/FaultDataMap.java,tools/src/main/java/org/apache/kafka/trogdor/rest/FaultDataMap.java,"KAFKA-5777; Add ducktape integration for Trogdor

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3726 from cmccabe/KAFKA-5777",17,1,1,64,401,2,10,98,98,49,2,1.0,99,98,50,1,1,0,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/processor/StateStoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/processor/StateStoreSupplier.java,"KAFKA-5650; add StateStoreBuilder interface and implementations

Part of KIP-182

- Add `StateStoreBuilder` interface and `WindowStateStoreBuilder`, `KeyValueStateStoreBuilder`, and `SessionStateStoreBuilder` implementations
- Add `StoreSupplier`, `WindowBytesStoreSupplier`, `KeyValueBytesStoreSupplier`, `SessionBytesStoreSupplier` interfaces and implementations
- Add new methods to `Stores` to create the newly added `StoreSupplier` and `StateStoreBuilder` implementations
- Update `Topology` and `InternalTopology` to use the interfaces

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3767 from dguy/kafka-5650",0,2,0,9,55,0,0,59,25,7,8,2.0,70,25,9,11,7,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/StoreSupplier.java,streams/src/main/java/org/apache/kafka/streams/state/StoreSupplier.java,"KAFKA-5650; add StateStoreBuilder interface and implementations

Part of KIP-182

- Add `StateStoreBuilder` interface and `WindowStateStoreBuilder`, `KeyValueStateStoreBuilder`, and `SessionStateStoreBuilder` implementations
- Add `StoreSupplier`, `WindowBytesStoreSupplier`, `KeyValueBytesStoreSupplier`, `SessionBytesStoreSupplier` interfaces and implementations
- Add new methods to `Stores` to create the newly added `StoreSupplier` and `StateStoreBuilder` implementations
- Update `Topology` and `InternalTopology` to use the interfaces

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3767 from dguy/kafka-5650",0,47,0,7,49,0,0,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/KafkaStorageException.java,clients/src/main/java/org/apache/kafka/common/errors/KafkaStorageException.java,"KAFKA-5694; Add AlterReplicaDirRequest and DescribeReplicaDirRequest (KIP-113 part-1)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>, Jiangjie Qin <becket.qin@gmail.com>, Colin P. Mccabe <cmccabe@confluent.io>

Closes #3621 from lindong28/KAFKA-5694",4,1,1,16,80,0,4,50,50,25,2,1.0,51,50,26,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/errors/LogDirNotFoundException.java,clients/src/main/java/org/apache/kafka/common/errors/LogDirNotFoundException.java,"KAFKA-5694; Add AlterReplicaDirRequest and DescribeReplicaDirRequest (KIP-113 part-1)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>, Jiangjie Qin <becket.qin@gmail.com>, Colin P. Mccabe <cmccabe@confluent.io>

Closes #3621 from lindong28/KAFKA-5694",3,37,0,13,70,3,3,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
tests/kafkatest/services/simple_consumer_shell.py,tests/kafkatest/services/simple_consumer_shell.py,"KAFKA-5768; Upgrade to ducktape 0.7.1

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3721 from cmccabe/KAFKA-5768",8,6,3,41,343,4,7,70,69,18,4,3.5,84,69,21,14,7,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionKeySerde.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/SessionKeySerde.java,"KAFKA-5749; Add MeteredSessionStore and ChangeloggingSessionBytesStore.

Make MeteredSessionStore the outermost store.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3729 from dguy/kafka-5749",25,1,1,116,1048,1,22,170,149,19,9,2,200,149,22,30,9,3,2,1,0,1
tools/src/main/java/org/apache/kafka/trogdor/fault/FaultSet.java,tools/src/main/java/org/apache/kafka/trogdor/fault/FaultSet.java,"KAFKA-5776; Add the Trogdor fault injection daemon

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3699 from cmccabe/trogdor-review",18,146,0,83,592,10,10,146,146,146,1,1,146,146,146,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/trogdor/fault/FaultSpec.java,tools/src/main/java/org/apache/kafka/trogdor/fault/FaultSpec.java,"KAFKA-5776; Add the Trogdor fault injection daemon

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3699 from cmccabe/trogdor-review",2,59,0,27,191,1,1,59,59,59,1,1,59,59,59,0,0,0,0,0,0,0
tools/src/main/java/org/apache/kafka/trogdor/rest/CreateCoordinatorFaultRequest.java,tools/src/main/java/org/apache/kafka/trogdor/rest/CreateCoordinatorFaultRequest.java,"KAFKA-5776; Add the Trogdor fault injection daemon

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3699 from cmccabe/trogdor-review",10,69,0,40,246,6,6,69,69,69,1,1,69,69,69,0,0,0,0,0,0,0
tools/src/test/java/org/apache/kafka/trogdor/common/ExpectedFaults.java,tools/src/test/java/org/apache/kafka/trogdor/common/ExpectedFaults.java,"KAFKA-5776; Add the Trogdor fault injection daemon

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3699 from cmccabe/trogdor-review",24,193,0,156,1176,10,10,193,193,193,1,1,193,193,193,0,0,0,0,0,0,0
tools/src/test/java/org/apache/kafka/trogdor/fault/FaultSetTest.java,tools/src/test/java/org/apache/kafka/trogdor/fault/FaultSetTest.java,"KAFKA-5776; Add the Trogdor fault injection daemon

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3699 from cmccabe/trogdor-review",12,126,0,96,787,4,4,126,126,126,1,1,126,126,126,0,0,0,0,0,0,0
core/src/test/scala/other/kafka/TestOffsetManager.scala,core/src/test/scala/other/kafka/TestOffsetManager.scala,"MINOR: Consolidate broker request/response handling

This patch contains a few small improvements to make request/response handling more consistent. Primarily it consolidates request/response serialization logic so that `SaslServerAuthenticator` and `KafkaApis` follow the same path. It also reduces the amount of custom logic needed to handle unsupported versions of the ApiVersions requests.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3673 from hachikuji/consolidate-response-handling",27,1,2,243,1707,1,10,309,291,17,18,1.5,355,291,20,46,12,3,2,1,0,1
core/src/test/scala/unit/kafka/utils/IteratorTemplateTest.scala,core/src/test/scala/unit/kafka/utils/IteratorTemplateTest.scala,"MINOR: Consolidate broker request/response handling

This patch contains a few small improvements to make request/response handling more consistent. Primarily it consolidates request/response serialization logic so that `SaslServerAuthenticator` and `KafkaApis` follow the same path. It also reduces the amount of custom logic needed to handle unsupported versions of the ApiVersions requests.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3673 from hachikuji/consolidate-response-handling",4,1,1,36,184,0,2,57,41,10,6,1.0,62,41,10,5,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/OutOfOrderSequenceException.java,clients/src/main/java/org/apache/kafka/common/errors/OutOfOrderSequenceException.java,"KAFKA-5342; Clarify producer fatal/abortable errors and fix inconsistencies

This patch improves documentation on the handling of errors for the idempotent/transactional producer. It also fixes a couple minor inconsistencies and improves test coverage. In particular:
- UnsupportedForMessageFormat should be a fatal error for TxnOffsetCommit responses
- UnsupportedVersion should be fatal for Produce responses and should be returned instead of InvalidRequest

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #3716 from hachikuji/KAFKA-5342",1,8,0,6,31,0,1,32,24,16,2,1.0,32,24,16,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ProducerFencedException.java,clients/src/main/java/org/apache/kafka/common/errors/ProducerFencedException.java,"KAFKA-5342; Clarify producer fatal/abortable errors and fix inconsistencies

This patch improves documentation on the handling of errors for the idempotent/transactional producer. It also fixes a couple minor inconsistencies and improves test coverage. In particular:
- UnsupportedForMessageFormat should be a fatal error for TxnOffsetCommit responses
- UnsupportedVersion should be fatal for Produce responses and should be returned instead of InvalidRequest

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #3716 from hachikuji/KAFKA-5342",1,6,0,6,31,0,1,30,24,15,2,1.0,30,24,15,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnsupportedForMessageFormatException.java,clients/src/main/java/org/apache/kafka/common/errors/UnsupportedForMessageFormatException.java,"KAFKA-5342; Clarify producer fatal/abortable errors and fix inconsistencies

This patch improves documentation on the handling of errors for the idempotent/transactional producer. It also fixes a couple minor inconsistencies and improves test coverage. In particular:
- UnsupportedForMessageFormat should be a fatal error for TxnOffsetCommit responses
- UnsupportedVersion should be fatal for Produce responses and should be returned instead of InvalidRequest

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #3716 from hachikuji/KAFKA-5342",2,2,1,10,57,0,2,34,29,11,3,1,44,29,15,10,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnsupportedVersionException.java,clients/src/main/java/org/apache/kafka/common/errors/UnsupportedVersionException.java,"KAFKA-5342; Clarify producer fatal/abortable errors and fix inconsistencies

This patch improves documentation on the handling of errors for the idempotent/transactional producer. It also fixes a couple minor inconsistencies and improves test coverage. In particular:
- UnsupportedForMessageFormat should be a fatal error for TxnOffsetCommit responses
- UnsupportedVersion should be fatal for Produce responses and should be returned instead of InvalidRequest

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #3716 from hachikuji/KAFKA-5342",2,12,0,11,64,0,2,41,25,14,3,1,50,25,17,9,9,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/ForeachAction.java,streams/src/main/java/org/apache/kafka/streams/kstream/ForeachAction.java,"MINOR: Typographical error corrected in the StreamsBuilder Javadoc.

Author: Kamal C <kamal.chandraprakash@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3695 from Kamal15/typo_error",0,0,1,4,33,0,0,43,35,5,9,1,69,35,8,26,12,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Crc32C.java,clients/src/main/java/org/apache/kafka/common/utils/Crc32C.java,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",8,3,1,57,333,1,7,114,112,57,2,1.5,115,112,58,1,1,0,2,1,0,1
core/src/main/scala/kafka/api/FetchResponse.scala,core/src/main/scala/kafka/api/FetchResponse.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",22,1,1,115,893,1,14,174,199,5,37,4,677,199,18,503,138,14,2,1,0,1
core/src/main/scala/kafka/api/ProducerRequest.scala,core/src/main/scala/kafka/api/ProducerRequest.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",7,1,1,109,713,1,5,148,82,3,44,2.0,429,83,10,281,52,6,2,1,0,1
core/src/main/scala/kafka/javaapi/TopicMetadata.scala,core/src/main/scala/kafka/javaapi/TopicMetadata.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",2,0,1,33,293,0,2,77,62,8,10,4.0,124,62,12,47,17,5,2,1,0,1
core/src/main/scala/kafka/message/ByteBufferMessageSet.scala,core/src/main/scala/kafka/message/ByteBufferMessageSet.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",16,1,1,79,618,1,11,232,317,4,56,2.5,1516,382,27,1284,597,23,2,1,0,1
core/src/main/scala/kafka/message/Message.scala,core/src/main/scala/kafka/message/Message.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",34,2,2,172,1439,0,14,376,182,16,24,2.0,708,213,30,332,84,14,2,1,0,1
core/src/main/scala/kafka/producer/ProducerPool.scala,core/src/main/scala/kafka/producer/ProducerPool.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",9,2,1,59,379,1,4,90,189,4,22,2.0,362,189,16,272,139,12,2,1,0,1
core/src/main/scala/kafka/tools/ExportZkOffsets.scala,core/src/main/scala/kafka/tools/ExportZkOffsets.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",11,1,1,77,549,0,3,259,123,14,19,1,322,123,17,63,17,3,2,1,0,1
core/src/main/scala/kafka/tools/ImportZkOffsets.scala,core/src/main/scala/kafka/tools/ImportZkOffsets.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",10,0,1,58,454,0,3,222,112,15,15,1,264,112,18,42,14,3,2,1,0,1
core/src/main/scala/kafka/tools/ReplayLogProducer.scala,core/src/main/scala/kafka/tools/ReplayLogProducer.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",13,1,1,128,950,0,3,159,197,5,29,2,302,197,10,143,38,5,2,1,0,1
core/src/test/scala/unit/kafka/message/ByteBufferMessageSetTest.scala,core/src/test/scala/unit/kafka/message/ByteBufferMessageSetTest.scala,"KAFKA-4501; Java 9 compilation and runtime fixes

Compilation error fixes:
- Avoid ambiguity error when appending to Properties in Scala
code (https://github.com/scala/bug/issues/10418)
- Use position() and limit() to fix ambiguity issue (
https://github.com/scala/bug/issues/10418#issuecomment-316364778)
- Disable findBugs if Java 9 is used (
https://github.com/findbugsproject/findbugs/issues/105)

Compilation warning fixes:
- Avoid deprecated Class.newInstance in Utils.newInstance
- Silence a few Java 9 deprecation warnings
- var -> val and unused fixes

Runtime error fixes:
- Introduce Base64 class that works in Java 7 and Java 9

Also:
- Set --release option if building with Java 9

Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193)
or PowerMock (https://github.com/powermock/powermock/issues/783)
will fail as neither supports Java 9 currently.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3647 from ijuma/kafka-4501-support-java-9",9,2,2,115,998,1,8,185,234,7,26,2.0,745,247,29,560,348,22,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/NextIteratorFunction.java,streams/src/main/java/org/apache/kafka/streams/state/internals/NextIteratorFunction.java,"KAFKA-5668; fetch across stores in CompositeReadOnlyWindowStore & CompositeReadOnlySessionStore

Fix range queries in `CompositeReadOnlyWindowStore` and `CompositeReadOnlySessionStore` to fetch across all stores (was previously just looking in the first store)

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3685 from dguy/kafka-5668",0,24,0,5,50,0,0,24,24,24,1,1,24,24,24,0,0,0,2,1,0,1
streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/Pipe.java,streams/quickstart/java/src/main/resources/archetype-resources/src/main/java/Pipe.java,"KAFKA-5727: Add Streams quickstart tutorial as an archetype project

0. Minor fixes on the existing examples to merge all on a single input topic; also do not use `common.utils.Exit` as it is for internal usage only.

1. Add the archetype project for the quickstart. Steps to try it out:

  a. `mvn install` on the quickstart directory.
  b. `mvn archetype:generate \
-DarchetypeGroupId=org.apache.kafka \
-DarchetypeArtifactId=streams-quickstart-java \
-DarchetypeVersion=1.0.0-SNAPSHOT \
-DgroupId=streams-quickstart \
-DartifactId=streams-quickstart \
-Dversion=0.1 \
-Dpackage=StreamsQuickstart \
-DinteractiveMode=false` at any directory to create the project.
  c. build the streams jar with version `1.0.0-SNAPSHOT` to local maven repository with `./gradlew installAll`; `cd streams-quickstart; mvn clean package`
  d. create the input / output topics, start the console producer and consumer.
  e. start the program: `mvn exec:java -Dexec.mainClass=StreamsQuickstart.Pipe/LineSplit/WordCount`.
  f. type data on console producer and observe data on console consumer.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Ewen Cheslack-Postava <me@ewencp.org>, Eno Thereska <eno.thereska@gmail.com>

Closes #3630 from guozhangwang/KMinor-streams-quickstart-tutorial",2,67,0,36,293,1,1,67,67,67,1,1,67,67,67,0,0,0,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadDataProvider.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadDataProvider.java,"KAFKA-5702; extract refactor StreamThread

Extracted `TaskManager` to handle all task related activities.
Make `StandbyTaskCreator`, `TaskCreator`, and `RebalanceListener` static classes so they must define their dependencies and can be testing independently of `StreamThread`
Added interfaces between `StreamPartitionAssignor` & `StreamThread` to reduce coupling.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Eno Thereska <eno.thereska@gmail.com>

Closes #3624 from dguy/stream-thread-refactor",0,36,0,16,117,0,0,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadMetadataProvider.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadMetadataProvider.java,"KAFKA-5702; extract refactor StreamThread

Extracted `TaskManager` to handle all task related activities.
Make `StandbyTaskCreator`, `TaskCreator`, and `RebalanceListener` static classes so they must define their dependencies and can be testing independently of `StreamThread`
Added interfaces between `StreamPartitionAssignor` & `StreamThread` to reduce coupling.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Eno Thereska <eno.thereska@gmail.com>

Closes #3624 from dguy/stream-thread-refactor",0,36,0,14,129,0,0,36,36,36,1,1,36,36,36,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/FetchRequest.scala,core/src/main/scala/kafka/api/FetchRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",23,1,18,213,1426,1,14,270,117,5,53,2,560,136,11,290,99,5,2,1,0,1
core/src/main/scala/kafka/api/GroupCoordinatorRequest.scala,core/src/main/scala/kafka/api/GroupCoordinatorRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",3,1,9,42,265,1,3,74,79,6,13,2,116,79,9,42,9,3,2,1,0,1
core/src/main/scala/kafka/api/OffsetCommitRequest.scala,core/src/main/scala/kafka/api/OffsetCommitRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",13,1,11,130,909,1,3,172,100,10,17,3,279,100,16,107,24,6,2,1,0,1
core/src/main/scala/kafka/api/OffsetFetchRequest.scala,core/src/main/scala/kafka/api/OffsetFetchRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",4,2,21,75,484,1,3,96,89,6,15,2,170,89,11,74,21,5,2,1,0,1
core/src/main/scala/kafka/api/OffsetRequest.scala,core/src/main/scala/kafka/api/OffsetRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",7,1,12,94,626,1,4,126,99,5,27,2,266,99,10,140,54,5,2,1,0,1
core/src/main/scala/kafka/api/TopicMetadataRequest.scala,core/src/main/scala/kafka/api/TopicMetadataRequest.scala,"MINOR: Remove unneeded error handlers in deprecated request objects

These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3646 from hachikuji/remove-old-request-error-handlers",4,1,11,46,300,1,3,72,146,2,30,2.0,275,146,9,203,59,7,2,1,0,1
core/src/main/scala/kafka/api/GroupCoordinatorResponse.scala,core/src/main/scala/kafka/api/GroupCoordinatorResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",4,2,0,33,228,0,3,60,57,5,12,4.0,99,57,8,39,6,3,2,1,0,1
core/src/main/scala/kafka/api/OffsetCommitResponse.scala,core/src/main/scala/kafka/api/OffsetCommitResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",5,2,0,53,366,0,3,82,93,7,11,2,136,93,12,54,22,5,2,1,0,1
core/src/main/scala/kafka/api/OffsetFetchResponse.scala,core/src/main/scala/kafka/api/OffsetFetchResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",7,2,0,74,516,0,4,107,100,13,8,3.5,142,100,18,35,13,4,2,1,0,1
core/src/main/scala/kafka/api/OffsetResponse.scala,core/src/main/scala/kafka/api/OffsetResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",5,2,0,70,479,0,3,102,49,6,16,2.0,153,59,10,51,25,3,2,1,0,1
core/src/main/scala/kafka/api/ProducerResponse.scala,core/src/main/scala/kafka/api/ProducerResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",5,2,0,75,495,0,3,110,51,5,21,2,259,72,12,149,50,7,2,1,0,1
core/src/main/scala/kafka/api/TopicMetadataResponse.scala,core/src/main/scala/kafka/api/TopicMetadataResponse.scala,"MINOR: Add missing deprecations on old request objects

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3651 from hachikuji/add-missing-request-deprecations",3,2,0,32,263,0,3,57,53,4,13,2,106,53,8,49,14,4,2,1,0,1
core/src/test/scala/unit/kafka/message/MessageCompressionTest.scala,core/src/test/scala/unit/kafka/message/MessageCompressionTest.scala,"MINOR: Update dependencies for 1.0.0 release

Notable updates:

1. Gradle 4.1 includes a number of performance and
CLI improvements as well as initial Java 9 support.

2. Scala 2.12.3 has substantial compilation time
improvements.

3. lz4-java 1.4 allows us to remove a workaround in
KafkaLZ4BlockInputStream (not done in this PR).

4. snappy-java 1.1.4 improved performance of compression (5%)
and decompression (20%). There was a slight increase in the
compressed size in one of our tests.

Not updated:

1. PowerMock due to a couple of regressions. I investigated one of them
and filed https://github.com/powermock/powermock/issues/828.

2. Jackson, which will be done via #3631.

3. Rocksdb, which will be done via #3519.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3619 from ijuma/update-deps-for-1.0.0",9,2,3,58,458,1,4,87,65,5,17,2,158,65,9,71,21,4,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/Requirements.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/Requirements.java,"KAFKA-5535: Handle null values in ExtractField

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Randall Hauch <rhauch@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3559 from ewencp/kafka-5535-extract-field-null",14,14,0,48,372,2,7,75,61,15,5,1,84,61,17,9,8,2,2,1,0,1
core/src/main/scala/kafka/consumer/ZookeeperTopicEventWatcher.scala,core/src/main/scala/kafka/consumer/ZookeeperTopicEventWatcher.scala,"KAFKA-5388; Replace ZkClient.subscribe*Changes methods with equivalent ZkUtils methods

Author: Balint Molnar <balintmolnar91@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3281 from baluchicken/KAFKA-5388",12,7,7,66,360,3,7,99,115,6,18,2.0,190,115,11,91,18,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSegmentedBytesStore.java,"HOTFIX: Fixes to metric names of Streams

A couple of fixes to metric names to match the KIP
- Removed extra strings in the metric names that are already in the tags
- add a separate metric for ""all""

Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3491 from enothereska/hotfix-metric-names",15,15,8,131,995,1,14,169,177,21,8,2.0,219,177,27,50,22,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TopologyBuilderException.java,streams/src/main/java/org/apache/kafka/streams/errors/TopologyBuilderException.java,"KAFKA-5670: (KIP-120) Add Topology and deprecate TopologyBuilder

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3590 from mjsax/kafka-3856-replace-topology-builder-by-topology",5,4,1,14,100,0,3,42,38,6,7,3,65,38,9,23,14,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TopologyException.java,streams/src/main/java/org/apache/kafka/streams/errors/TopologyException.java,"KAFKA-5670: (KIP-120) Add Topology and deprecate TopologyBuilder

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3590 from mjsax/kafka-3856-replace-topology-builder-by-topology",5,40,0,14,98,3,3,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/AbstractNotifyingBatchingRestoreCallback.java,streams/src/main/java/org/apache/kafka/streams/processor/AbstractNotifyingBatchingRestoreCallback.java,"KAFKA-5363 (KIP-167): implementing bulk load, restoration event notification

Author: Bill Bejeck <bill@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3325 from bbejeck/KAFKA-5363_add_ability_to_batch_restore",4,83,0,26,127,4,4,83,83,83,1,1,83,83,83,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/processor/AbstractNotifyingRestoreCallback.java,streams/src/main/java/org/apache/kafka/streams/processor/AbstractNotifyingRestoreCallback.java,"KAFKA-5363 (KIP-167): implementing bulk load, restoration event notification

Author: Bill Bejeck <bill@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #3325 from bbejeck/KAFKA-5363_add_ability_to_batch_restore",3,72,0,21,100,3,3,72,72,72,1,1,72,72,72,0,0,0,0,0,0,0
core/src/main/scala/kafka/consumer/TopicCount.scala,core/src/main/scala/kafka/consumer/TopicCount.scala,"KAFKA-1595; Remove deprecated and slower Scala JSON parser

In a test by onurkaraman involving 3066 topics and 95895 partitions,
Controller initialisation time spent on JSON parsing would be reduced from
37.1 seconds to 0.7 seconds by switching from the current JSON parser to
Jackson. See the following JIRA comment for more details:

https://issues.apache.org/jira/browse/KAFKA-5328?focusedCommentId=16027086

I tested that we only use Jackson methods introduced in 2.0 in the main
codebase by compiling it with the older version locally. We use a
constructor introduced in 2.4 in one test, but I didn't remove it as it
seemed harmless. The reasoning for this is explained in the mailing list
thread:

http://search-hadoop.com/m/uyzND1FWbWw1qUbWe

Finally, this PR only handles the parsing side. It would be good to use Jackson
for serialising to JSON as well. I filed KAFKA-5631 for that.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Onur Karaman <okaraman@linkedin.com>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #83 from ijuma/kafka-1595-remove-deprecated-json-parser-jackson",20,4,4,104,794,1,6,232,92,9,27,2,468,120,17,236,73,9,2,1,0,1
clients/src/main/java/org/apache/kafka/common/memory/MemoryPool.java,clients/src/main/java/org/apache/kafka/common/memory/MemoryPool.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",1,95,0,34,144,1,1,95,95,95,1,1,95,95,95,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/memory/SimpleMemoryPool.java,clients/src/main/java/org/apache/kafka/common/memory/SimpleMemoryPool.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",23,137,0,92,597,10,10,137,137,137,1,1,137,137,137,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/Receive.java,clients/src/main/java/org/apache/kafka/common/network/Receive.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",0,11,1,11,70,0,0,55,35,8,7,3,70,35,10,15,7,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/protocol/SchemaVisitor.java,clients/src/main/java/org/apache/kafka/common/protocol/SchemaVisitor.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",0,27,0,9,82,0,0,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/protocol/SchemaVisitorAdapter.java,clients/src/main/java/org/apache/kafka/common/protocol/SchemaVisitorAdapter.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",3,38,0,15,97,3,3,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/common/network/PlaintextSender.java,clients/src/test/java/org/apache/kafka/common/network/PlaintextSender.java,"KAFKA-4602; KIP-72 - Allow putting a bound on memory consumed by Incoming requests

this is the initial implementation.

Author: radai-rosenblatt <radai.rosenblatt@gmail.com>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Jun Rao <junrao@gmail.com>

Closes #2330 from radai-rosenblatt/broker-memory-pool-with-muting",2,44,0,22,152,1,1,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/DelayedCreateTopics.scala,core/src/main/scala/kafka/server/DelayedCreateTopics.scala,"KAFKA-5627; Reduce classes needed for LeaderAndIsrPartitionState and MetadataPartitionState

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jiangjie Qin <becket.qin@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #3565 from lindong28/KAFKA-5627",12,2,2,46,360,1,5,92,91,18,5,2,105,91,21,13,5,3,2,1,0,1
core/src/main/scala/kafka/common/KafkaStorageException.scala,core/src/main/scala/kafka/common/KafkaStorageException.scala,"KAFKA-4763; Handle disk failure for JBOD (KIP-112)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jiangjie Qin <becket.qin@gmail.com>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Onur Karaman <okaraman@linkedin.com>

Closes #2929 from lindong28/KAFKA-4763",2,4,2,6,54,0,2,27,25,14,2,1.0,29,25,14,2,2,1,1,0,1,1
core/src/main/scala/kafka/controller/PartitionLeaderSelector.scala,core/src/main/scala/kafka/controller/PartitionLeaderSelector.scala,"KAFKA-4763; Handle disk failure for JBOD (KIP-112)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jiangjie Qin <becket.qin@gmail.com>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Onur Karaman <okaraman@linkedin.com>

Closes #2929 from lindong28/KAFKA-4763",20,6,7,125,844,1,6,325,120,10,31,2,535,120,17,210,60,7,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/TopologyDescription.java,streams/src/main/java/org/apache/kafka/streams/processor/TopologyDescription.java,"KAFKA-3856; Cleanup Kafka Stream builder API (KIP-120)

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Damian Guy <damian.guy@gmail.com>, Bill Bejeck <bbejeck@gmail.com>

Closes #2301 from mjsax/kafka-3856-topology-builder-API",79,476,0,299,1720,46,46,476,476,476,1,1,476,476,476,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingSegmentedBytesStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingSegmentedBytesStore.java,"MINOR: Code Cleanup

Clean up includes:

- Switching try-catch-finally blocks to try-with-resources when possible
- Removing some seemingly unnecessary `SuppressWarnings` annotations
- Resolving some Java warnings
- Closing unclosed Closable objects
- Removing unused code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Balint Molnar <balintmolnar91@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #3222 from vahidhashemian/minor/code_cleanup_1706",8,0,1,49,360,0,7,80,95,11,7,2,117,95,17,37,26,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingSegmentedBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingSegmentedBytesStoreTest.java,"MINOR: Code Cleanup

Clean up includes:

- Switching try-catch-finally blocks to try-with-resources when possible
- Removing some seemingly unnecessary `SuppressWarnings` annotations
- Resolving some Java warnings
- Closing unclosed Closable objects
- Removing unused code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Balint Molnar <balintmolnar91@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #3222 from vahidhashemian/minor/code_cleanup_1706",9,0,3,94,703,0,9,125,114,18,7,3,158,114,23,33,17,5,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSegmentedBytesStoreTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSegmentedBytesStoreTest.java,"MINOR: Code Cleanup

Clean up includes:

- Switching try-catch-finally blocks to try-with-resources when possible
- Removing some seemingly unnecessary `SuppressWarnings` annotations
- Resolving some Java warnings
- Closing unclosed Closable objects
- Removing unused code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Balint Molnar <balintmolnar91@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #3222 from vahidhashemian/minor/code_cleanup_1706",10,0,1,123,879,0,9,165,122,24,7,1,183,122,26,18,7,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/CorruptRecordException.java,clients/src/main/java/org/apache/kafka/common/errors/CorruptRecordException.java,"MINOR: Add another common error case for CorruptRecordException's error message

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3541 from ewencp/corrupt-record-null-key-compacted-topic",4,1,1,16,81,1,4,43,23,5,9,2,79,23,9,36,15,4,2,1,0,1
core/src/main/scala/kafka/api/ControlledShutdownResponse.scala,core/src/main/scala/kafka/api/ControlledShutdownResponse.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",5,1,1,47,267,1,3,72,70,8,9,2,89,70,10,17,4,2,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerConnector.scala,core/src/main/scala/kafka/consumer/ConsumerConnector.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",4,1,1,36,291,1,4,142,74,9,16,2.0,194,74,12,52,11,3,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerIterator.scala,core/src/main/scala/kafka/consumer/ConsumerIterator.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",13,1,1,85,562,0,3,131,82,4,35,1,249,82,7,118,13,3,2,1,0,1
core/src/main/scala/kafka/consumer/KafkaStream.scala,core/src/main/scala/kafka/consumer/KafkaStream.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",1,1,1,22,168,1,1,51,38,3,17,2,112,38,7,61,13,4,2,1,0,1
core/src/main/scala/kafka/javaapi/TopicMetadataRequest.scala,core/src/main/scala/kafka/javaapi/TopicMetadataRequest.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",7,1,1,39,336,1,4,68,35,5,14,2.0,99,35,7,31,5,2,2,1,0,1
core/src/main/scala/kafka/javaapi/producer/Producer.scala,core/src/main/scala/kafka/javaapi/producer/Producer.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",4,1,1,18,165,1,4,52,122,5,11,1,146,122,13,94,74,9,2,1,0,1
core/src/main/scala/kafka/producer/Producer.scala,core/src/main/scala/kafka/producer/Producer.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",17,1,1,101,650,1,5,138,178,4,35,2,568,178,16,430,160,12,2,1,0,1
core/src/main/scala/kafka/producer/async/EventHandler.scala,core/src/main/scala/kafka/producer/async/EventHandler.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",0,1,1,7,49,0,0,37,44,6,6,1.0,60,44,10,23,14,4,2,1,0,1
core/src/main/scala/kafka/producer/async/ProducerSendThread.scala,core/src/main/scala/kafka/producer/async/ProducerSendThread.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",13,1,1,78,534,1,3,117,124,4,31,1,261,124,8,144,51,5,2,1,0,1
core/src/main/scala/kafka/utils/IteratorTemplate.scala,core/src/main/scala/kafka/utils/IteratorTemplate.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",9,2,2,52,234,2,5,88,76,8,11,1,109,76,10,21,6,2,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/message/BaseMessageSetTestCases.scala,core/src/test/scala/unit/kafka/javaapi/message/BaseMessageSetTestCases.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",5,1,1,41,270,1,5,67,74,5,13,1,102,74,8,35,7,3,2,1,0,1
core/src/test/scala/unit/kafka/message/MessageTest.scala,core/src/test/scala/unit/kafka/message/MessageTest.scala,"MINOR: Enable a number of xlint scalac warnings

Update the code where possible to fix the warnings. The unused
warning introduced in Scala 2.12 is quite handy and provides
a reason to compile with Scala 2.12.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #3464 from ijuma/scala-xlint",20,1,1,99,835,1,10,140,72,7,19,1,248,73,13,108,36,6,2,1,0,1
core/src/main/scala/kafka/network/RequestOrResponseSend.scala,core/src/main/scala/kafka/network/RequestOrResponseSend.scala,"KAFKA-5127; Replace pattern matching with foreach where the case None is ignored

Author: Balint Molnar <balintmolnar91@gmail.com>

Reviewers: Vahid Hashemian <vahidhashemian@us.ibm.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2919 from baluchicken/KAFKA-5127",4,1,5,29,195,1,3,53,57,13,4,1.0,60,57,15,7,5,2,2,1,0,1
core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala,core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala,"KAFKA-5127; Replace pattern matching with foreach where the case None is ignored

Author: Balint Molnar <balintmolnar91@gmail.com>

Reviewers: Vahid Hashemian <vahidhashemian@us.ibm.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2919 from baluchicken/KAFKA-5127",30,16,31,159,1410,4,5,218,147,6,38,2.0,436,147,11,218,34,6,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadStateTransitionValidator.java,streams/src/main/java/org/apache/kafka/streams/processor/internals/ThreadStateTransitionValidator.java,"KAFKA-5372; fixes to state transitions

Several fixes to state transition logic:
- Kafka streams will now be in ERROR when all threads are DEAD or when global thread stops unexpectedly
- Fixed transition logic in corner cases when thread is already dead or Kafka Streams is already closed
- Fixed incorrect transition diagram in StreamThread
- Unit tests to verify transitions

Also:
- re-enabled throwing an exception when an unexpected state change happens
- fixed a whole bunch of EoS tests that did not start a thread
- added more comments.

Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #3432 from enothereska/KAFKA-5372-state-transitions",0,24,0,4,26,0,0,24,24,24,1,1,24,24,24,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/ByteBufferOutputStream.java,clients/src/main/java/org/apache/kafka/common/utils/ByteBufferOutputStream.java,"KAFKA-5490; Cleaner should retain empty batch if needed to preserve producer last sequence

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #3406 from hachikuji/KAFKA-5490",16,14,7,65,416,6,14,133,57,15,9,5,229,57,25,96,32,11,2,1,0,1
core/src/main/scala/kafka/serializer/Encoder.scala,core/src/main/scala/kafka/serializer/Encoder.scala,"MINOR: remove unnecessary null check

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3445 from mjsax/minor-remove-null-check",8,1,4,35,267,1,6,79,31,13,6,1.0,97,34,16,18,6,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestSinkConnector.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestSinkConnector.java,"KAFKA-5475: Connector config validation should include fields for defined transformation aliases

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3399 from ewencp/kafka-5475-validation-transformations",6,61,0,33,210,6,6,61,61,61,1,1,61,61,61,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestSourceConnector.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/TestSourceConnector.java,"KAFKA-5475: Connector config validation should include fields for defined transformation aliases

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3399 from ewencp/kafka-5475-validation-transformations",6,61,0,33,210,6,6,61,61,61,1,1,61,61,61,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/internals/FatalExitError.java,clients/src/main/java/org/apache/kafka/common/internals/FatalExitError.java,"MINOR: Remove unused logger

Author: Kamal C <kamal.chandraprakash@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3365 from Kamal15/logger",4,0,4,17,92,0,3,47,47,16,3,2,60,47,20,13,9,4,2,1,0,1
core/src/main/scala/kafka/tools/SimpleConsumerShell.scala,core/src/main/scala/kafka/tools/SimpleConsumerShell.scala,"MINOR: Consolidate Utils.newThread, Utils.daemonThread and KafkaThread

Removed the first two in favour of the latter.

Author: Kamal C <kamal.chandraprakash@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3350 from Kamal15/utilcleanup",31,3,3,219,1607,1,2,262,100,6,43,2,463,146,11,201,48,5,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/TopicListing.java,clients/src/main/java/org/apache/kafka/clients/admin/TopicListing.java,"KAFKA-5275; AdminClient API consistency

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3339 from ijuma/kafka-5275-admin-client-api-consistency",4,1,1,19,89,2,4,57,44,14,4,1.0,59,44,15,2,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntry.java,clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntry.java,"KAFKA-5275; AdminClient API consistency

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3339 from ijuma/kafka-5275-admin-client-api-consistency",13,6,4,51,292,3,10,112,86,28,4,2.0,119,86,30,7,4,2,1,0,1,1
clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntryData.java,clients/src/main/java/org/apache/kafka/common/acl/AccessControlEntryData.java,"KAFKA-5275; AdminClient API consistency

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3339 from ijuma/kafka-5275-admin-client-api-consistency",23,2,2,65,364,2,10,105,105,35,3,1,108,105,36,3,2,1,1,0,1,1
clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java,clients/src/main/java/org/apache/kafka/common/acl/AclPermissionType.java,"KAFKA-5275; AdminClient API consistency

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Colin P. Mccabe <cmccabe@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3339 from ijuma/kafka-5275-admin-client-api-consistency",8,1,1,41,236,2,6,106,92,26,4,1.0,108,92,27,2,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/annotation/InterfaceStability.java,clients/src/main/java/org/apache/kafka/common/annotation/InterfaceStability.java,"KAFKA-5274: AdminClient Javadoc improvements

Publish Javadoc for common.annotation package, which contains
InterfaceStability.

Finally, mark AdminClient classes with `Evolving` instead of `Unstable`.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Colin Mccabe, Gwen Shapira

Closes #3316 from ijuma/kafka-5274-admin-client-javadoc",0,11,9,16,92,0,0,54,48,18,3,3,72,48,24,18,9,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginDesc.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginDesc.java,"HOTFIX: Handle Connector version returning 'null' during plugin loading.

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #3321 from kkonstantine/HOTFIX-Handle-null-version-returned-from-Connector-interface-during-plugin-loading",18,2,2,80,471,1,11,110,110,55,2,1.0,112,110,56,2,2,1,2,1,0,1
jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/TopicBenchmark.java,jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/TopicBenchmark.java,"MINOR: Optimise performance of `Topic.validate()`

I included a JMH benchmark and the results follow. The
implementation in this PR takes no more than 1/10th
of the time when compared to trunk. I also included
results for an alternative implementation that is a little
slower than the one in the PR.

Trunk:
```text
TopicBenchmark.testValidate                                topic  avgt   15  134.107 ±  3.956  ns/op
TopicBenchmark.testValidate                    longer-topic-name  avgt   15  316.241 ± 13.379  ns/op
TopicBenchmark.testValidate  very-long-topic-name_with_more_text  avgt   15  636.026 ± 30.272  ns/op
```

Implementation in the PR:
```text
TopicBenchmark.testValidate                                topic  avgt   15  13.153 ± 0.383  ns/op
TopicBenchmark.testValidate                    longer-topic-name  avgt   15  26.139 ± 0.896  ns/op
TopicBenchmark.testValidate  very-long-topic-name.with_more_text  avgt   15  44.829 ± 1.390  ns/op
```

Alternative implementation where boolean validChar = Character.isLetterOrDigit(c) || c == '.' || c == '_' || c == '-';
```text
TopicBenchmark.testValidate                                topic  avgt   15  18.883 ± 1.044  ns/op
TopicBenchmark.testValidate                    longer-topic-name  avgt   15  36.696 ± 1.220  ns/op
TopicBenchmark.testValidate  very-long-topic-name_with_more_text  avgt   15  65.956 ± 0.669  ns/op
```

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3234 from ijuma/optimise-topic-is-valid",1,53,0,31,241,1,1,53,53,53,1,1,53,53,53,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/OperationNotAttemptedException.java,clients/src/main/java/org/apache/kafka/common/errors/OperationNotAttemptedException.java,"KAFKA-5322; Add `OPERATION_NOT_ATTEMPTED` error code to resolve AddPartitionsToTxn inconsistency

In the `AddPartitionsToTxn` request handling, if even one partition fails authorization checks, the entire request is essentially failed. However, the `AddPartitionsToTxnResponse` today will only contain the error codes for the topics which failed authorization. It will have no error code for the topics which succeeded, making it inconsistent with other APIs.

This patch adds a new error code `OPERATION_NOT_ATTEMPTED` which is returned for the successful partitions to indicate that they were not added to the transaction.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #3204 from apurvam/KAFKA-5322-add-operation-not-attempted-for-add-partitions",1,27,0,6,32,1,1,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerConfig.scala,core/src/main/scala/kafka/consumer/ConsumerConfig.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",16,13,9,113,726,0,7,191,85,5,37,2,329,85,9,138,18,4,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerTopicStats.scala,core/src/main/scala/kafka/consumer/ConsumerTopicStats.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",4,3,0,36,298,0,4,69,41,12,6,3.0,84,41,14,15,6,2,1,0,1,1
core/src/main/scala/kafka/consumer/FetchRequestAndResponseStats.scala,core/src/main/scala/kafka/consumer/FetchRequestAndResponseStats.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",7,3,0,46,372,0,4,82,58,14,6,3.0,98,58,16,16,10,3,1,0,1,1
core/src/main/scala/kafka/consumer/PartitionTopicInfo.scala,core/src/main/scala/kafka/consumer/PartitionTopicInfo.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",8,4,2,48,378,0,6,82,84,4,21,2,161,84,8,79,15,4,2,1,0,1
core/src/main/scala/kafka/consumer/SimpleConsumer.scala,core/src/main/scala/kafka/consumer/SimpleConsumer.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",24,2,0,119,749,0,14,200,233,4,54,2.0,585,233,11,385,74,7,2,1,0,1
core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java,core/src/main/scala/kafka/javaapi/consumer/ConsumerConnector.java,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",0,4,0,23,259,0,0,110,43,7,16,3.0,254,56,16,144,56,9,2,1,0,1
core/src/main/scala/kafka/javaapi/consumer/ConsumerRebalanceListener.java,core/src/main/scala/kafka/javaapi/consumer/ConsumerRebalanceListener.java,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",0,4,0,9,72,0,0,54,42,11,5,1,61,42,12,7,4,1,2,1,0,1
core/src/main/scala/kafka/javaapi/consumer/SimpleConsumer.scala,core/src/main/scala/kafka/javaapi/consumer/SimpleConsumer.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",7,4,2,40,287,2,7,107,70,11,10,2.0,146,70,15,39,16,4,2,1,0,1
core/src/main/scala/kafka/tools/VerifyConsumerRebalance.scala,core/src/main/scala/kafka/tools/VerifyConsumerRebalance.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",17,2,0,96,632,1,2,277,137,12,23,1,333,137,14,56,10,2,2,1,0,1
core/src/test/scala/kafka/tools/TestLogCleaning.scala,core/src/test/scala/kafka/tools/TestLogCleaning.scala,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",41,18,17,263,2097,8,15,323,216,16,20,2.0,457,216,23,134,50,7,2,1,0,1
examples/src/main/java/kafka/examples/SimpleConsumerDemo.java,examples/src/main/java/kafka/examples/SimpleConsumerDemo.java,"KAFKA-3264; Deprecate the old Scala consumer (KIP-109)

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

This patch had conflicts when merged, resolved by
Committer: Ismael Juma <ismael@juma.me.uk>

Closes #2328 from vahidhashemian/KAFKA-3264",7,4,0,68,536,0,3,93,85,6,15,2,219,85,15,126,55,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/BrokerNotFoundException.java,streams/src/main/java/org/apache/kafka/streams/errors/BrokerNotFoundException.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",3,0,2,13,74,0,3,42,44,21,2,1.5,44,44,22,2,2,1,1,0,0,0
streams/src/main/java/org/apache/kafka/streams/errors/LockException.java,streams/src/main/java/org/apache/kafka/streams/errors/LockException.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",3,0,2,13,74,0,3,41,36,10,4,2.5,60,36,15,19,11,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/ProcessorStateException.java,streams/src/main/java/org/apache/kafka/streams/errors/ProcessorStateException.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",3,0,2,13,74,0,3,40,35,8,5,2,52,35,10,12,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/StreamsException.java,streams/src/main/java/org/apache/kafka/streams/errors/StreamsException.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",3,0,2,14,85,0,3,39,40,10,4,3.0,52,40,13,13,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TaskIdFormatException.java,streams/src/main/java/org/apache/kafka/streams/errors/TaskIdFormatException.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",5,0,2,13,98,0,3,41,27,8,5,2,50,27,10,9,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/Merger.java,streams/src/main/java/org/apache/kafka/streams/kstream/Merger.java,"KAFKA-5350: Modify unstable annotations in Streams API

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #3172 from guozhangwang/K5350-compatibility-annotations",0,0,2,4,37,0,0,37,40,7,5,2,46,40,9,9,4,2,2,1,0,1
core/src/main/scala/kafka/message/MessageAndOffset.scala,core/src/main/scala/kafka/message/MessageAndOffset.scala,"KAFKA-5093; Avoid loading full batch data when possible when iterating FileRecords

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3160 from hachikuji/KAFKA-5093",6,21,3,25,146,3,2,53,22,6,9,2,87,22,10,34,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/admin/ApiVersionsResult.java,clients/src/main/java/org/apache/kafka/clients/admin/ApiVersionsResult.java,"KAFKA-5265; Move ACLs, Config, Topic classes into org.apache.kafka.common

Also introduce TopicConfig.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3120 from cmccabe/KAFKA-5265",5,2,2,35,301,2,3,63,63,32,2,1.5,65,63,32,2,2,1,0,0,0,0
clients/src/test/java/org/apache/kafka/common/metrics/SampleMetrics.java,clients/src/test/java/org/apache/kafka/common/metrics/SampleMetrics.java,"KAFKA-5191: Autogenerate Consumer Fetcher metrics

Autogenerate docs for the Consumer Fetcher's metrics. This is a smaller subset of the original PR https://github.com/apache/kafka/pull/1202.

CC ijuma benstopford hachikuji

Author: James Cheng <jylcheng@yahoo.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>

Closes #2993 from wushujames/fetcher_metrics_docs",0,31,0,7,87,0,0,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/TransactionalIdAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/TransactionalIdAuthorizationException.java,"KAFKA-5259; TransactionalId auth implies ProducerId auth

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>

Closes #3075 from hachikuji/KAFKA-5259-FIXED",1,1,1,6,32,0,1,23,23,12,2,1.0,24,23,12,1,1,0,2,1,0,1
core/src/main/scala/kafka/metrics/KafkaTimer.scala,core/src/main/scala/kafka/metrics/KafkaTimer.scala,"KAFKA-5135; Controller Health Metrics (KIP-143)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Onur Karaman <okaraman@linkedin.com>

Closes #2983 from ijuma/kafka-5135-controller-health-metrics-kip-143",0,2,6,9,53,0,0,35,40,12,3,1,45,40,15,10,6,3,2,1,0,1
tests/kafkatest/sanity_checks/test_kafka_version.py,tests/kafkatest/sanity_checks/test_kafka_version.py,"MINOR: Fix race condition in TestVerifiableProducer sanity test

## Fixes race condition in TestVerifiableProducer sanity test:
The test starts a producer, waits for at least 5 acks, and then
logs in to the worker to grep for the producer process to figure
out what version it is running.

The problem was that the producer was set up to produce 1000 messages
at a rate of 1000 msgs/s and then exit. This means it will have a
typical runtime slightly above 1 second.

Logging in to the vagrant instance might take longer than that thus
resulting in the process grep to fail, failing the test.

This commit doesn't really fix the issue - a proper fix would be to tell
the producer to stick around until explicitly killed - but it increases
the chances of the test passing, at the expense of a slightly longer
runtime.

## Improves error reporting when is_version() fails

Author: Magnus Edenhill <magnus@edenhill.se>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2765 from edenhill/trunk",4,3,3,31,327,2,4,58,55,12,5,3,66,55,13,8,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/CacheFunction.java,streams/src/main/java/org/apache/kafka/streams/state/internals/CacheFunction.java,"KAFKA-5192: add WindowStore range scan (KIP-155)

Implements range scan for keys in windowed and session stores

Modifies caching session and windowed stores to use segmented cache keys.
Cache keys are internally prefixed with their segment id to ensure key ordering in the cache matches the ordering in the underlying store for keys spread across multiple segments.
This should also result in fewer cache keys getting scanned for queries spanning only some segments.

Author: Xavier Léauté <xavier@confluent.io>

Reviewers: Damian Guy, Guozhang Wang

Closes #3027 from xvrl/windowstore-range-scan",0,25,0,6,44,0,0,25,25,25,1,1,25,25,25,0,0,0,2,1,0,1
core/src/main/scala/kafka/api/ControlledShutdownRequest.scala,core/src/main/scala/kafka/api/ControlledShutdownRequest.scala,"MINOR: Small refactor of request quotas handling in KafkaApis

- Avoid unnecessary inner methods
- Remove redundant parameter in `sendResponseExemptThrottle`
- Go through `sendResponseExemptThrottle` for produce requests with acks=0
- Tighten how we handle cases where there’s no response

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #3087 from ijuma/kafka-apis-improvements",5,1,1,55,396,1,4,84,73,6,13,2,118,73,9,34,6,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorPluginInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorPluginInfo.java,"KAFKA-3487: Support classloading isolation in Connect (KIP-146)

Author: Konstantine Karantasis <konstantine@confluent.io>

Reviewers: Randall Hauch <rhauch@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #3028 from kkonstantine/KAFKA-3487-Support-classloading-isolation-in-Connect",13,3,34,62,400,3,8,89,61,22,4,3.5,142,69,36,53,34,13,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/BrokerAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/BrokerAuthorizationException.java,"KAFKA-3267; Describe and Alter Configs Admin APIs (KIP-133)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3076 from ijuma/kafka-3267-describe-alter-configs-protocol",1,23,0,6,32,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/ResourceType.java,clients/src/main/java/org/apache/kafka/common/requests/ResourceType.java,"KAFKA-3267; Describe and Alter Configs Admin APIs (KIP-133)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #3076 from ijuma/kafka-3267-describe-alter-configs-protocol",5,42,0,19,138,3,3,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/SecurityDisabledException.java,clients/src/main/java/org/apache/kafka/common/errors/SecurityDisabledException.java,"KAFKA-3266; Describe, Create and Delete ACLs Admin APIs (KIP-140)

Includes server-side code, protocol and AdminClient.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2941 from cmccabe/KAFKA-3266",2,32,0,10,57,2,2,32,32,32,1,1,32,32,32,0,0,0,0,0,0,0
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/SchemaUtil.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/SchemaUtil.java,"KAFKA-4714; Flatten and Cast single message transforms (KIP-66)

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Shikhar Bhushan <shikhar@confluent.io>, Jason Gustafson <jason@confluent.io>

Closes #2458 from ewencp/kafka-3209-even-more-transforms",3,4,0,19,156,1,2,43,40,14,3,1,51,40,17,8,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ProducerIdAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/ProducerIdAuthorizationException.java,"KAFKA-5129; Add ACL checks for Transactional APIs

Add ACL checks for Transactional APIs

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Apurva Mehta <apurva@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2979 from dguy/kafka-5129",1,23,0,6,32,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorIntegrationTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionCoordinatorIntegrationTest.scala,"MINOR: Consolidate Topic classes

During the 0.11.0.0 cycle, a Java version of the class
was introduced so that Streams could use it. Given that
it includes the bulk of the functionality of the Scala
version of the class, it makes sense to consolidate them.

While doing this, I noticed that one of the tests for
the Java class (`shouldThrowOnInvalidTopicNames`) was
broken as it only checked if the first topic name in
the list was invalid.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #3046 from ijuma/consolidate-topic-classes",5,2,2,52,376,1,3,102,98,20,5,3,115,98,23,13,8,3,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/NonEmptyListValidator.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/NonEmptyListValidator.java,"MINOR: Handle nulls in NonEmptyListValidator

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #3045 from ewencp/minor-non-empty-list-validator-nulls",4,1,1,16,111,1,2,38,39,13,3,1,47,39,16,9,8,3,2,1,0,1
core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannel.scala,core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannel.scala,"KAFKA-5132: abort long running transactions

Abort any ongoing transactions that haven't been touched for longer than the transaction timeout

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Jason Gustafson, Apurva Mehta, Ismael Juma, Guozhang Wang

Closes #2957 from dguy/kafka-5132",31,2,1,133,1126,3,14,186,168,62,3,2,227,168,76,41,40,14,1,0,1,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PluginDiscovery.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/PluginDiscovery.java,"KAFKA-4343: Expose Connector type in REST API (KIP-151)

https://cwiki.apache.org/confluence/display/KAFKA/KIP-151+Expose+Connector+type+in+REST+API

Author: dan norwood <norwood@confluent.io>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2960 from norwood/KIP-151",21,2,2,90,811,1,7,126,127,32,4,1.5,137,127,34,11,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorType.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorType.java,"KAFKA-4343: Expose Connector type in REST API (KIP-151)

https://cwiki.apache.org/confluence/display/KAFKA/KIP-151+Expose+Connector+type+in+REST+API

Author: dan norwood <norwood@confluent.io>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2960 from norwood/KIP-151",5,52,0,28,196,3,3,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorTypeTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/entities/ConnectorTypeTest.java,"KAFKA-4343: Expose Connector type in REST API (KIP-151)

https://cwiki.apache.org/confluence/display/KAFKA/KIP-151+Expose+Connector+type+in+REST+API

Author: dan norwood <norwood@confluent.io>

Reviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2960 from norwood/KIP-151",5,44,0,23,158,2,2,44,44,44,1,1,44,44,44,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/TransactionCoordinatorFencedException.java,clients/src/main/java/org/apache/kafka/common/errors/TransactionCoordinatorFencedException.java,"KAFKA-5121; Implement transaction index for KIP-98

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2910 from hachikuji/eos-txn-index",2,30,0,10,57,2,2,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelTest.scala,core/src/test/scala/unit/kafka/coordinator/transaction/TransactionMarkerChannelTest.scala,"KAFKA-5136: move coordinatorEpoch from WriteTxnMarkerRequest to TxnMarkerEntry

Moving the coordinatorEpoch from WriteTxnMarkerRequest to TxnMarkerEntry will generate fewer broker send requests

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Ismael Juma, Guozhang Wang

Closes #2925 from dguy/tc-write-txn-request-follow-up",14,19,19,123,1682,6,11,179,179,90,2,5.0,198,179,99,19,19,10,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/admin/ApiVersionsOptions.java,clients/src/main/java/org/apache/kafka/clients/admin/ApiVersionsOptions.java,"KAFKA-3265; Add a public AdminClient API in Java (KIP-117)

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Dan Norwood <norwood@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #2472 from cmccabe/KAFKA-3265",2,37,0,13,67,2,2,37,37,37,1,1,37,37,37,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/header/Header.java,clients/src/main/java/org/apache/kafka/common/header/Header.java,"KAFKA-4208; Add Record Headers

As per KIP-82

Adding record headers api to ProducerRecord, ConsumerRecord
Support to convert from protocol to api added Kafka Producer, Kafka Fetcher (Consumer)
Updated MirrorMaker, ConsoleConsumer and scala BaseConsumer
Add RecordHeaders and RecordHeader implementation of the interfaces Headers and Header

Some bits using are reverted to being Java 7 compatible, for the moment until KIP-118 is implemented.

Author: Michael Andre Pearce <Michael.Andre.Pearce@me.com>

Reviewers: Radai Rosenblatt <radai.rosenblatt@gmail.com>, Jiangjie Qin <becket.qin@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2772 from michaelandrepearce/KIP-82",0,25,0,5,28,0,0,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureTransactionalResult.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/FutureTransactionalResult.java,"KAFKA-4818; Exactly once transactional clients

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2840 from apurvam/exactly-once-transactional-clients",8,64,0,38,214,6,6,64,64,64,1,1,64,64,64,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/ConcurrentTransactionsException.java,clients/src/main/java/org/apache/kafka/common/errors/ConcurrentTransactionsException.java,"KAFKA-5059: Implement Transactional Coordinator

Author: Damian Guy <damian.guy@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Guozhang Wang, Jason Gustafson, Apurva Mehta, Jun Rao

Closes #2849 from dguy/exactly-once-tc",1,25,0,7,40,1,1,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/Topic.scala,core/src/main/scala/kafka/common/Topic.scala,"KAFKA-5059: Implement Transactional Coordinator

Author: Damian Guy <damian.guy@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Guozhang Wang, Jason Gustafson, Apurva Mehta, Jun Rao

Closes #2849 from dguy/exactly-once-tc",12,2,1,33,296,0,4,76,40,5,15,2,101,40,7,25,6,2,2,1,0,1
core/src/main/scala/kafka/log/ProducerIdMapping.scala,core/src/main/scala/kafka/log/ProducerIdMapping.scala,"KAFKA-5108; Add support for reading PID snapshot files to DumpLogSegments

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #2922 from hachikuji/KAFKA-5108",50,3,3,260,1959,1,22,384,394,77,5,3,513,394,103,129,120,26,2,1,0,1
core/src/test/scala/unit/kafka/log/ProducerIdMappingTest.scala,core/src/test/scala/unit/kafka/log/ProducerIdMappingTest.scala,"MINOR: Improvements to PID snapshot management

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2866 from hachikuji/improve-snapshot-management",15,90,22,208,1695,10,15,291,224,97,3,2,316,224,105,25,22,8,2,1,0,1
core/src/main/scala/kafka/server/ZookeeperLeaderElector.scala,core/src/main/scala/kafka/server/ZookeeperLeaderElector.scala,"KAFKA-4814; Enable ZK ACLs only when zookeeper.set.acl is set

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2845 from rajinisivaram/KAFKA-4814",9,1,1,98,528,0,4,158,98,6,27,3,421,98,16,263,77,10,2,1,0,1
core/src/test/scala/unit/kafka/server/ControlledShutdownLeaderSelectorTest.scala,core/src/test/scala/unit/kafka/server/ControlledShutdownLeaderSelectorTest.scala,"MINOR: Make LeaderAndIsr immutable case class

Also include a few code readability improvements.

Author: jozi-k <jozef.koval@protonmail.ch>

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2731 from jozi-k/immutable_LeaderAndIsr",1,1,1,43,360,1,1,73,73,18,4,1.0,76,73,19,3,1,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/CoordinatorLoadInProgressException.java,clients/src/main/java/org/apache/kafka/common/errors/CoordinatorLoadInProgressException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",2,39,0,10,57,2,2,39,39,39,1,1,39,39,39,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/CoordinatorNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/CoordinatorNotAvailableException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",3,43,0,14,78,3,3,43,43,43,1,1,43,43,43,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupCoordinatorNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupCoordinatorNotAvailableException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",4,4,1,18,93,0,4,47,40,9,5,4,63,40,13,16,10,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/GroupLoadInProgressException.java,clients/src/main/java/org/apache/kafka/common/errors/GroupLoadInProgressException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",4,4,1,17,82,0,4,46,40,9,5,4,65,40,13,19,10,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotCoordinatorException.java,clients/src/main/java/org/apache/kafka/common/errors/NotCoordinatorException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",2,38,0,10,57,2,2,38,38,38,1,1,38,38,38,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotCoordinatorForGroupException.java,clients/src/main/java/org/apache/kafka/common/errors/NotCoordinatorForGroupException.java,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",4,4,1,17,82,0,4,46,40,12,4,4.0,62,40,16,16,10,4,2,1,0,1
core/src/main/scala/kafka/coordinator/TransactionCoordinator.scala,core/src/main/scala/kafka/coordinator/TransactionCoordinator.scala,"KAFKA-5043; Rename GroupCoordinator to FindCoordinator (KIP-98)

Also:
1. FindCoordinator is more general and takes a coordinator_type
so that it can be used for the group and transaction coordinators.
2. Include an error message in FindCoordinatorResponse to make the
errors at the client side more informative. We have just added the
field to the protocol in this PR, a subsequent PR will update the
code to use it.
3. Rename `Errors` names for FindCoordinator to be more generic. This
is a compatible change as the ids remain the same.
4. Since the exception classes for the error codes are in a public
package, we introduce new ones and deprecate the old ones.
The classes were not thrown back to the user (KAFKA-5052 aside),
so this is a compatible change.
5. Update InitPidRequest for transactions. Since this protocol API
was introduced recently and is not used by default, we did not bump
its version.

Author: Apurva Mehta <apurva@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2825 from apurvam/exactly-once-rpc-stubs",7,1,1,43,305,1,5,92,92,46,2,1.0,93,92,46,1,1,0,2,1,0,1
core/src/main/scala/kafka/javaapi/FetchRequest.scala,core/src/main/scala/kafka/javaapi/FetchRequest.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",5,6,6,42,301,3,2,72,61,10,7,3,108,61,15,36,18,5,2,1,0,1
core/src/main/scala/kafka/javaapi/FetchResponse.scala,core/src/main/scala/kafka/javaapi/FetchResponse.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",8,6,5,19,166,4,5,44,33,7,6,1.0,63,33,10,19,9,3,2,1,0,1
core/src/main/scala/kafka/javaapi/GroupCoordinatorResponse.scala,core/src/main/scala/kafka/javaapi/GroupCoordinatorResponse.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",5,6,5,23,141,3,2,50,42,6,9,2,72,42,8,22,5,2,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetCommitRequest.scala,core/src/main/scala/kafka/javaapi/OffsetCommitRequest.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",5,6,10,34,202,3,2,59,55,7,9,2,88,55,10,29,10,3,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetFetchRequest.scala,core/src/main/scala/kafka/javaapi/OffsetFetchRequest.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",5,6,10,33,175,3,2,60,58,10,6,2.0,81,58,14,21,10,4,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetRequest.scala,core/src/main/scala/kafka/javaapi/OffsetRequest.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",4,6,11,26,145,3,1,51,61,7,7,2,78,61,11,27,11,4,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetResponse.scala,core/src/main/scala/kafka/javaapi/OffsetResponse.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",7,6,10,19,157,3,4,45,49,15,3,2,57,49,19,12,10,4,2,1,0,1
core/src/main/scala/kafka/javaapi/TopicMetadataResponse.scala,core/src/main/scala/kafka/javaapi/TopicMetadataResponse.scala,"KAFKA-4899; Fix findbugs warnings in kafka-core

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jozef Koval <jozef.koval@protonmail.ch>, Ismael Juma <ismael@juma.me.uk>

Closes #2687 from cmccabe/KAFKA-4899",4,6,5,17,110,3,1,38,26,13,3,1,43,26,14,5,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidPidMappingException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidPidMappingException.java,"KAFKA-4990; Request/response classes for transactions (KIP-98)

Author: Matthias J. Sax <matthias@confluent.io>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2799 from mjsax/kafka-4990-add-api-stub-config-parameters-request-types",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/InvalidTxnStateException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidTxnStateException.java,"KAFKA-4990; Request/response classes for transactions (KIP-98)

Author: Matthias J. Sax <matthias@confluent.io>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2799 from mjsax/kafka-4990-add-api-stub-config-parameters-request-types",1,23,0,6,31,1,1,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/requests/TransactionResult.java,clients/src/main/java/org/apache/kafka/common/requests/TransactionResult.java,"KAFKA-4990; Request/response classes for transactions (KIP-98)

Author: Matthias J. Sax <matthias@confluent.io>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2799 from mjsax/kafka-4990-add-api-stub-config-parameters-request-types",3,34,0,14,70,2,2,34,34,34,1,1,34,34,34,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java,"HOTFIX: WindowedStreamPartitioner does not provide topic name to serializer

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Eno Thereska <eno@confluent.io>, Damian Guy <damian.guy@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2777 from mjsax/hotfix-window-serdes-trunk",17,4,3,91,760,1,11,123,127,25,5,2,146,127,29,23,8,5,2,1,0,1
tests/kafkatest/tests/streams/streams_bounce_test.py,tests/kafkatest/tests/streams/streams_bounce_test.py,"KAFKA-4916: test streams with brokers failing

Several fixes for handling broker failures:
- default replication value for internal topics is now 3 in test itself (not in streams code, that will require a KIP.
- streams producer waits for acks from all replicas in test itself (not in streams code, that will require a KIP.
- backoff time for streams client to try again after a failure to contact controller.
- fix bug related to state store locks (this helps in multi-threaded scenarios)
- fix related to catching exceptions property for network errors.
- system test for all the above

Author: Eno Thereska <eno@confluent.io>
Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Matthias J. Sax <matthias@confluent.io>, Damian Guy <damian.guy@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Dan Norwood <norwood@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2719 from enothereska/KAFKA-4916-broker-bounce-test",2,2,2,36,310,1,2,75,71,11,7,2,84,71,12,9,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/producer/TransactionState.java,clients/src/main/java/org/apache/kafka/clients/producer/TransactionState.java,"KAFKA-4817; Add idempotent producer semantics

This is from the KIP-98 proposal.

The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.

There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.

In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following in the error callback:

1. Close the producer so that no queued batches are sent. This is important for guaranteeing ordering.
2. Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.

Author: Apurva Mehta <apurva@confluent.io>
Author: hachikuji <jason@confluent.io>
Author: Apurva Mehta <apurva.1618@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: fpj <fpj@apache.org>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2735 from apurvam/exactly-once-idempotent-producer",15,135,0,67,429,10,10,135,135,135,1,1,135,135,135,0,0,0,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionStateTest.java,clients/src/test/java/org/apache/kafka/clients/producer/internals/TransactionStateTest.java,"KAFKA-4817; Add idempotent producer semantics

This is from the KIP-98 proposal.

The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.

There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.

In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following in the error callback:

1. Close the producer so that no queued batches are sent. This is important for guaranteeing ordering.
2. Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.

Author: Apurva Mehta <apurva@confluent.io>
Author: hachikuji <jason@confluent.io>
Author: Apurva Mehta <apurva.1618@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: fpj <fpj@apache.org>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2735 from apurvam/exactly-once-idempotent-producer",4,61,0,35,280,4,4,61,61,61,1,1,61,61,61,0,0,0,2,1,0,1
core/src/main/scala/kafka/coordinator/PidMetadata.scala,core/src/main/scala/kafka/coordinator/PidMetadata.scala,"KAFKA-4817; Add idempotent producer semantics

This is from the KIP-98 proposal.

The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.

There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.

In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following in the error callback:

1. Close the producer so that no queued batches are sent. This is important for guaranteeing ordering.
2. Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.

Author: Apurva Mehta <apurva@confluent.io>
Author: hachikuji <jason@confluent.io>
Author: Apurva Mehta <apurva.1618@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: fpj <fpj@apache.org>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2735 from apurvam/exactly-once-idempotent-producer",4,31,0,10,67,1,1,31,31,31,1,1,31,31,31,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/TransactionCoordinatorTest.scala,core/src/test/scala/unit/kafka/coordinator/TransactionCoordinatorTest.scala,"KAFKA-4817; Add idempotent producer semantics

This is from the KIP-98 proposal.

The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.

There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.

In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following in the error callback:

1. Close the producer so that no queued batches are sent. This is important for guaranteeing ordering.
2. Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.

Author: Apurva Mehta <apurva@confluent.io>
Author: hachikuji <jason@confluent.io>
Author: Apurva Mehta <apurva.1618@gmail.com>
Author: Guozhang Wang <wangguoz@gmail.com>
Author: fpj <fpj@apache.org>
Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #2735 from apurvam/exactly-once-idempotent-producer",7,93,0,61,470,5,6,93,93,93,1,1,93,93,93,0,0,0,2,1,0,1
core/src/main/scala/kafka/utils/Annotations.scala,core/src/main/scala/kafka/utils/Annotations.scala,"MINOR: Fix typos in javadoc and code comments

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2595 from vahidhashemian/minor/fix_typos_1702",0,1,1,5,22,0,0,76,38,19,4,1.0,77,38,19,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/Checksums.java,clients/src/main/java/org/apache/kafka/common/utils/Checksums.java,"KAFKA-1449; Use CRC32C for checksum of V2 message format

I manually tested that Crc32CTest and AbstractChecksums pass with JDK 9. I also verified that `Java9ChecksumFactory` is used in that case.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #2739 from ijuma/kafka-1449-crc32c",7,67,0,35,360,5,5,67,67,67,1,1,67,67,67,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/network/DualSocketChannel.java,clients/src/main/java/org/apache/kafka/common/network/DualSocketChannel.java,"KAFKA-4586; Add purgeDataBefore() API (KIP-107)

Author: Dong Lin <lindong28@gmail.com>

Reviewers: Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Jiangjie Qin <becket.qin@gmail.com>

Closes #2476 from lindong28/KAFKA-4586",0,4,0,3,16,0,0,4,4,4,1,1,4,4,4,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/LogInputStream.java,clients/src/main/java/org/apache/kafka/common/record/LogInputStream.java,"KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>, Apurva Mehta <apurva@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2614 from hachikuji/exactly-once-message-format",0,14,10,5,34,0,0,43,35,11,4,3.5,64,35,16,21,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordBatchIterator.java,clients/src/main/java/org/apache/kafka/common/record/RecordBatchIterator.java,"KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>, Apurva Mehta <apurva@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2614 from hachikuji/exactly-once-message-format",4,43,0,21,129,2,2,43,43,43,1,1,43,43,43,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/TimestampType.java,clients/src/main/java/org/apache/kafka/common/record/TimestampType.java,"KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>, Apurva Mehta <apurva@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2614 from hachikuji/exactly-once-message-format",5,0,13,21,131,2,3,46,62,8,6,1.0,82,62,14,36,19,6,2,1,0,1
core/src/main/scala/kafka/message/MessageSet.scala,core/src/main/scala/kafka/message/MessageSet.scala,"KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>, Apurva Mehta <apurva@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2614 from hachikuji/exactly-once-message-format",6,1,1,46,309,0,3,111,90,7,17,1,203,90,12,92,30,5,2,1,0,1
core/src/test/scala/unit/kafka/message/BaseMessageSetTestCases.scala,core/src/test/scala/unit/kafka/message/BaseMessageSetTestCases.scala,"KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>, Apurva Mehta <apurva@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2614 from hachikuji/exactly-once-message-format",13,2,1,86,699,1,11,132,69,7,19,1,187,69,10,55,11,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Decimal.java,connect/api/src/main/java/org/apache/kafka/connect/data/Decimal.java,"KAFKA-4924: Fix Kafka Connect API findbugs warnings

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2715 from cmccabe/KAFKA-4924",8,1,1,35,259,1,5,86,87,22,4,2.0,95,87,24,9,5,2,2,1,0,1
clients/src/test/java/org/apache/kafka/test/IntegrationTest.java,clients/src/test/java/org/apache/kafka/test/IntegrationTest.java,"KAFKA-4594; Annotate integration tests and provide gradle build targets to run subsets of tests

This uses JUnit Categories to identify integration tests. Adds 2 new build targets:
`integrationTest` and `unitTest`.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Eno Thereska <eno@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #2695 from dguy/junit-categories",0,20,0,3,14,0,0,20,20,20,1,1,20,20,20,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/producer/internals/ErrorLoggingCallback.java,clients/src/main/java/org/apache/kafka/clients/producer/internals/ErrorLoggingCallback.java,"KAFKA-4894; Fix findbugs ""default character set in use"" warnings

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2683 from cmccabe/KAFKA-4894",9,4,2,33,266,1,2,56,43,9,6,3.0,77,43,13,21,10,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KeyValuePrinter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KeyValuePrinter.java,"KAFKA-4894; Fix findbugs ""default character set in use"" warnings

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2683 from cmccabe/KAFKA-4894",16,21,22,83,592,11,10,120,124,24,5,4,166,124,33,46,22,9,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KeyValuePrinterProcessorTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KeyValuePrinterProcessorTest.java,"KAFKA-4894; Fix findbugs ""default character set in use"" warnings

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2683 from cmccabe/KAFKA-4894",21,23,13,141,1148,4,14,207,165,34,6,4.0,239,165,40,32,13,5,2,1,0,1
core/src/test/scala/integration/kafka/api/FixedPortTestUtils.scala,core/src/test/scala/integration/kafka/api/FixedPortTestUtils.scala,"KAFKA-4861; GroupMetadataManager record is rejected if broker configured with LogAppendTime

The record should be created with CreateTime (like in the producer). The conversion to
LogAppendTime happens automatically (if necessary).

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #2657 from ijuma/kafka-4861-log-append-time-breaks-group-data-manager",4,4,5,26,175,2,2,51,55,13,4,1.5,68,55,17,17,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/RecordsIterator.java,clients/src/main/java/org/apache/kafka/common/record/RecordsIterator.java,"MINOR: Add varint serde utilities for new message format

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2647 from hachikuji/add-varint-serdes",32,1,0,153,1025,0,10,222,170,32,7,2,305,170,44,83,68,12,2,1,0,1
clients/src/test/java/org/apache/kafka/common/record/SimpleRecordTest.java,clients/src/test/java/org/apache/kafka/common/record/SimpleRecordTest.java,"MINOR: Add varint serde utilities for new message format

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2647 from hachikuji/add-varint-serdes",12,1,0,100,1066,0,8,144,66,24,6,3.5,160,66,27,16,11,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/InvalidOffsetException.java,clients/src/main/java/org/apache/kafka/clients/consumer/InvalidOffsetException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,13,9,10,71,0,1,38,34,19,2,2.0,47,34,24,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetResetStrategy.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetResetStrategy.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,13,9,4,21,0,0,21,17,10,2,2.0,30,17,15,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerInterceptors.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerInterceptors.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,13,9,47,342,0,4,103,99,52,2,2.0,112,99,56,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/NoAvailableBrokersException.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/NoAvailableBrokersException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,13,9,5,41,0,0,27,23,14,2,2.0,36,23,18,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/NoOpConsumerRebalanceListener.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/NoOpConsumerRebalanceListener.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,15,12,10,79,0,2,32,30,6,5,2,53,30,11,21,12,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFutureAdapter.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFutureAdapter.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,13,9,7,61,0,1,32,28,16,2,2.0,41,28,20,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFutureListener.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/RequestFutureListener.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,13,9,5,35,0,0,27,23,14,2,2.0,36,23,18,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/Configurable.java,clients/src/main/java/org/apache/kafka/common/Configurable.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,3,3,5,33,0,0,31,16,6,5,1,36,16,7,5,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/KafkaException.java,clients/src/main/java/org/apache/kafka/common/KafkaException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,3,3,16,78,0,4,42,26,10,4,1.0,46,26,12,4,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/cache/Cache.java,clients/src/main/java/org/apache/kafka/common/cache/Cache.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,4,5,7,50,0,0,52,53,17,3,1,58,53,19,6,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/cache/SynchronizedCache.java,clients/src/main/java/org/apache/kafka/common/cache/SynchronizedCache.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,4,5,23,138,0,5,50,51,25,2,2.5,55,51,28,5,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/Config.java,clients/src/main/java/org/apache/kafka/common/config/Config.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,11,61,0,2,32,28,11,3,1,42,28,14,10,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigException.java,clients/src/main/java/org/apache/kafka/common/config/ConfigException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,3,3,14,107,0,3,40,24,10,4,1.5,45,24,11,5,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/ConfigValue.java,clients/src/main/java/org/apache/kafka/common/config/ConfigValue.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",21,7,8,79,479,0,14,112,113,22,5,2,125,113,25,13,8,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/config/types/Password.java,clients/src/main/java/org/apache/kafka/common/config/types/Password.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",6,4,5,26,124,0,5,67,68,34,2,3.0,72,68,36,5,5,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ApiException.java,clients/src/main/java/org/apache/kafka/common/errors/ApiException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,3,3,21,103,0,5,51,35,10,5,1,57,35,11,6,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/AuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/AuthorizationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,9,49,0,2,29,19,7,4,1.5,38,19,10,9,9,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/BrokerNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/BrokerNotAvailableException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,10,57,0,2,31,32,16,2,2.5,35,32,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ClusterAuthorizationException.java,clients/src/main/java/org/apache/kafka/common/errors/ClusterAuthorizationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,30,26,15,2,2.0,39,26,20,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/ControllerMovedException.java,clients/src/main/java/org/apache/kafka/common/errors/ControllerMovedException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,10,57,0,2,31,32,16,2,2.5,35,32,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/DisconnectException.java,clients/src/main/java/org/apache/kafka/common/errors/DisconnectException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,9,17,91,0,4,44,39,15,3,1,53,39,18,9,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/IllegalGenerationException.java,clients/src/main/java/org/apache/kafka/common/errors/IllegalGenerationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,9,16,80,0,4,37,33,12,3,1,47,33,16,10,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InconsistentGroupProtocolException.java,clients/src/main/java/org/apache/kafka/common/errors/InconsistentGroupProtocolException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,29,25,14,2,2.0,38,25,19,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InterruptException.java,clients/src/main/java/org/apache/kafka/common/errors/InterruptException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,15,11,17,116,0,3,43,34,14,3,1,54,34,18,11,11,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidCommitOffsetSizeException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidCommitOffsetSizeException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,29,25,14,2,2.0,38,25,19,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidConfigurationException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidConfigurationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,31,22,10,3,4,40,22,13,9,5,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidFetchSizeException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidFetchSizeException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,31,27,16,2,2.0,40,27,20,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidGroupIdException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidGroupIdException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,29,25,14,2,2.0,38,25,19,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidMetadataException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidMetadataException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,15,12,16,81,0,4,42,39,21,2,1.5,54,39,27,12,12,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidOffsetException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidOffsetException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,10,57,0,2,36,37,18,2,2.5,40,37,20,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidPartitionsException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidPartitionsException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,31,43,6,5,3,73,43,15,42,27,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidReplicaAssignmentException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidReplicaAssignmentException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,31,31,16,2,2.5,35,31,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidReplicationFactorException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidReplicationFactorException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,31,31,16,2,2.5,35,31,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidRequestException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidRequestException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,36,36,18,2,2.5,40,36,20,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidRequiredAcksException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidRequiredAcksException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,4,4,7,39,0,1,25,25,12,2,2.5,29,25,14,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidSessionTimeoutException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidSessionTimeoutException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,29,25,14,2,2.0,38,25,19,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/InvalidTimestampException.java,clients/src/main/java/org/apache/kafka/common/errors/InvalidTimestampException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,10,57,0,2,33,34,16,2,2.5,37,34,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/LeaderNotAvailableException.java,clients/src/main/java/org/apache/kafka/common/errors/LeaderNotAvailableException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,10,57,0,2,35,19,6,6,2.0,70,19,12,35,23,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NetworkException.java,clients/src/main/java/org/apache/kafka/common/errors/NetworkException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,15,11,16,80,0,4,43,23,9,5,1,70,23,14,27,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotControllerException.java,clients/src/main/java/org/apache/kafka/common/errors/NotControllerException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,10,10,57,0,2,31,28,16,2,2.5,41,28,20,10,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotEnoughReplicasAfterAppendException.java,clients/src/main/java/org/apache/kafka/common/errors/NotEnoughReplicasAfterAppendException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,4,4,7,39,0,1,30,43,8,4,3.0,63,43,16,33,17,8,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/NotEnoughReplicasException.java,clients/src/main/java/org/apache/kafka/common/errors/NotEnoughReplicasException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,4,4,16,80,0,4,40,40,13,3,4,58,40,19,18,14,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/OffsetMetadataTooLarge.java,clients/src/main/java/org/apache/kafka/common/errors/OffsetMetadataTooLarge.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,9,15,76,0,4,41,22,7,6,1.5,67,22,11,26,14,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/OffsetOutOfRangeException.java,clients/src/main/java/org/apache/kafka/common/errors/OffsetOutOfRangeException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,10,57,0,2,35,36,18,2,2.5,39,36,20,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/PolicyViolationException.java,clients/src/main/java/org/apache/kafka/common/errors/PolicyViolationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,9,49,0,2,31,29,10,3,1,35,29,12,4,4,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/RebalanceInProgressException.java,clients/src/main/java/org/apache/kafka/common/errors/RebalanceInProgressException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,3,3,16,80,0,4,37,37,18,2,2.0,40,37,20,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/RecordBatchTooLargeException.java,clients/src/main/java/org/apache/kafka/common/errors/RecordBatchTooLargeException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,9,16,80,0,4,43,39,22,2,2.0,52,39,26,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/RecordTooLargeException.java,clients/src/main/java/org/apache/kafka/common/errors/RecordTooLargeException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",6,15,11,26,151,0,6,55,23,9,6,1.0,81,23,14,26,14,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/SerializationException.java,clients/src/main/java/org/apache/kafka/common/errors/SerializationException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,13,9,21,103,0,5,50,46,25,2,2.0,59,46,30,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TimeoutException.java,clients/src/main/java/org/apache/kafka/common/errors/TimeoutException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,15,11,16,80,0,4,42,23,8,5,1,69,23,14,27,15,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/TopicExistsException.java,clients/src/main/java/org/apache/kafka/common/errors/TopicExistsException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,57,0,2,31,23,6,5,2,51,23,10,20,6,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnknownMemberIdException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownMemberIdException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,9,16,80,0,4,37,33,9,4,2.0,52,33,13,15,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/UnknownServerException.java,clients/src/main/java/org/apache/kafka/common/errors/UnknownServerException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,15,11,15,76,0,4,43,22,9,5,1,69,22,14,26,14,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/internals/ClusterResourceListeners.java,clients/src/main/java/org/apache/kafka/common/internals/ClusterResourceListeners.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",7,3,4,26,155,0,4,62,63,31,2,2.5,66,63,33,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/Quota.java,clients/src/main/java/org/apache/kafka/common/metrics/Quota.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",17,3,3,45,272,0,9,75,36,9,8,1.0,82,36,10,7,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentile.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentile.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,3,3,17,90,0,3,40,32,8,5,1,57,32,11,17,13,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/metrics/stats/Value.java,clients/src/main/java/org/apache/kafka/common/metrics/stats/Value.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,13,9,14,92,0,2,37,33,18,2,2.0,46,33,23,9,9,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/ByteBufferReceive.java,clients/src/main/java/org/apache/kafka/common/network/ByteBufferReceive.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,3,3,30,156,0,4,57,43,8,7,1,71,43,10,14,7,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/DefaultAuthenticator.java,clients/src/main/java/org/apache/kafka/common/network/DefaultAuthenticator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",6,3,4,25,166,0,5,63,63,21,3,3,69,63,23,6,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/InvalidReceiveException.java,clients/src/main/java/org/apache/kafka/common/network/InvalidReceiveException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,10,60,0,2,30,39,6,5,1,62,39,12,32,21,6,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/Mode.java,clients/src/main/java/org/apache/kafka/common/network/Mode.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,3,3,2,19,0,0,22,19,6,4,1.0,26,19,6,4,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/network/TransportLayers.java,clients/src/main/java/org/apache/kafka/common/network/TransportLayers.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,3,4,10,63,0,2,33,34,16,2,2.5,37,34,18,4,4,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/record/LogEntry.java,clients/src/main/java/org/apache/kafka/common/record/LogEntry.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",23,3,3,85,591,0,16,171,123,28,6,1.0,189,137,32,18,14,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/GroupCoordinatorRequest.java,clients/src/main/java/org/apache/kafka/common/requests/GroupCoordinatorRequest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,13,9,59,397,0,9,89,47,6,15,3,157,47,10,68,11,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/RequestAndSize.java,clients/src/main/java/org/apache/kafka/common/requests/RequestAndSize.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,3,3,9,49,0,1,27,27,14,2,2.0,30,27,15,3,3,2,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultLogin.java,clients/src/main/java/org/apache/kafka/common/security/authenticator/DefaultLogin.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,8,10,10,41,0,2,30,32,15,2,2.0,40,32,20,10,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/BadFormatString.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/BadFormatString.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,8,10,10,56,0,2,28,30,14,2,2.0,38,30,19,10,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosName.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosName.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",11,8,10,51,308,0,6,105,114,26,4,3.5,150,114,38,45,31,11,2,1,0,1
clients/src/main/java/org/apache/kafka/common/security/kerberos/NoMatchingRule.java,clients/src/main/java/org/apache/kafka/common/security/kerberos/NoMatchingRule.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,8,10,7,39,0,1,25,27,12,2,2.0,35,27,18,10,10,5,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/AbstractIterator.java,clients/src/main/java/org/apache/kafka/common/utils/AbstractIterator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",13,3,3,56,257,0,6,88,72,18,5,1,94,72,19,6,3,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/utils/CopyOnWriteMap.java,clients/src/main/java/org/apache/kafka/common/utils/CopyOnWriteMap.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",24,15,11,105,655,0,18,146,130,29,5,1,173,130,35,27,15,5,2,1,0,1
clients/src/test/java/org/apache/kafka/common/metrics/FakeMetricsReporter.java,clients/src/test/java/org/apache/kafka/common/metrics/FakeMetricsReporter.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,15,11,15,93,0,5,39,32,13,3,1,51,32,17,12,11,4,2,1,0,1
clients/src/test/java/org/apache/kafka/common/utils/Serializer.java,clients/src/test/java/org/apache/kafka/common/utils/Serializer.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,13,10,30,213,0,4,52,49,26,2,2.5,62,49,31,10,10,5,2,1,0,1
clients/src/test/java/org/apache/kafka/test/DelayedReceive.java,clients/src/test/java/org/apache/kafka/test/DelayedReceive.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,3,3,16,80,0,3,40,40,20,2,2.0,43,40,22,3,3,2,2,1,0,1
clients/src/test/java/org/apache/kafka/test/Microbenchmarks.java,clients/src/test/java/org/apache/kafka/test/Microbenchmarks.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",19,15,11,158,1327,0,4,196,143,24,8,1.5,230,143,29,34,15,4,2,1,0,1
clients/src/test/java/org/apache/kafka/test/MockClusterResourceListener.java,clients/src/test/java/org/apache/kafka/test/MockClusterResourceListener.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,3,4,16,98,0,2,38,39,19,2,2.5,42,39,21,4,4,2,2,1,0,1
clients/src/test/java/org/apache/kafka/test/MockPartitioner.java,clients/src/test/java/org/apache/kafka/test/MockPartitioner.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,3,3,27,180,0,5,51,51,26,2,2.0,54,51,27,3,3,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectorContext.java,connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectorContext.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,4,5,5,28,0,0,36,33,7,5,2,46,33,9,10,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/connector/Task.java,connect/api/src/main/java/org/apache/kafka/connect/connector/Task.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,4,5,7,45,0,0,52,49,9,6,2.0,67,49,11,15,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/Schema.java,connect/api/src/main/java/org/apache/kafka/connect/data/Schema.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",12,4,5,72,487,0,3,222,157,32,7,1,231,157,33,9,5,1,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/data/SchemaAndValue.java,connect/api/src/main/java/org/apache/kafka/connect/data/SchemaAndValue.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,4,5,36,201,0,6,61,62,20,3,1,67,62,22,6,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/AlreadyExistsException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/AlreadyExistsException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,34,35,8,4,1.5,41,35,10,7,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/ConnectException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/ConnectException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,13,73,0,3,37,40,9,4,3.0,50,40,12,13,6,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/DataException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/DataException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,34,36,8,4,3.5,54,36,14,20,12,5,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/IllegalWorkerStateException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/IllegalWorkerStateException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,34,24,8,4,3.0,47,24,12,13,6,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/NotFoundException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/NotFoundException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,34,35,8,4,1.5,41,35,10,7,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/RetriableException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/RetriableException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,34,35,8,4,1.5,41,35,10,7,5,2,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/SchemaBuilderException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/SchemaBuilderException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,12,62,0,3,31,33,8,4,2.5,47,33,12,16,10,4,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/errors/SchemaProjectorException.java,connect/api/src/main/java/org/apache/kafka/connect/errors/SchemaProjectorException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,14,12,12,62,0,3,31,29,10,3,1,44,29,15,13,12,4,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/storage/OffsetStorageReader.java,connect/api/src/main/java/org/apache/kafka/connect/storage/OffsetStorageReader.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,4,5,7,83,0,0,61,59,10,6,3.0,78,59,13,17,5,3,2,1,0,1
connect/api/src/main/java/org/apache/kafka/connect/util/ConnectorUtils.java,connect/api/src/main/java/org/apache/kafka/connect/util/ConnectorUtils.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,4,5,23,200,0,1,62,66,16,4,1.5,71,66,18,9,5,2,2,1,0,1
connect/api/src/test/java/org/apache/kafka/connect/data/FakeSchema.java,connect/api/src/test/java/org/apache/kafka/connect/data/FakeSchema.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",12,4,5,53,186,0,12,82,83,41,2,2.5,87,83,44,5,5,2,2,1,0,1
connect/json/src/main/java/org/apache/kafka/connect/json/JsonSchema.java,connect/json/src/main/java/org/apache/kafka/connect/json/JsonSchema.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,4,5,58,511,0,3,81,114,16,5,1,143,114,29,62,56,12,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorFactory.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorFactory.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",18,13,16,61,524,0,5,99,102,50,2,2.5,115,102,58,16,16,8,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorStatus.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorStatus.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,4,17,134,0,2,73,58,24,3,3,78,58,26,5,4,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TargetState.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TargetState.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,7,7,5,20,0,0,36,36,18,2,2.5,43,36,22,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TaskConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TaskConfig.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,4,5,23,175,0,2,53,54,13,4,2.0,62,54,16,9,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/NotAssignedException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/NotAssignedException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,7,7,6,38,0,1,29,29,14,2,2.5,36,29,18,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/NotLeaderException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/NotLeaderException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,4,5,6,38,0,1,29,38,6,5,4,61,38,12,32,19,6,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/RebalanceNeededException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/RebalanceNeededException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,7,7,7,46,0,1,27,27,9,3,2,36,27,12,9,7,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/RequestTargetException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/RequestTargetException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,7,7,20,119,0,4,47,47,24,2,2.5,54,47,27,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigInfo.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,7,8,39,239,0,6,65,66,32,2,2.5,73,66,36,8,8,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigInfos.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigInfos.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",14,7,8,69,417,0,8,101,102,50,2,2.5,109,102,54,8,8,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigKeyInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigKeyInfo.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",28,7,8,135,769,0,15,170,171,57,3,3,181,171,60,11,8,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigValueInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ConfigValueInfo.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",16,7,8,76,443,0,9,105,106,35,3,4,119,106,40,14,8,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/CreateConnectorRequest.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/CreateConnectorRequest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",9,4,5,34,229,0,5,58,59,19,3,1,64,59,21,6,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ErrorMessage.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/ErrorMessage.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",9,4,5,33,210,0,5,62,63,16,4,1.5,72,63,18,10,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/TaskInfo.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/entities/TaskInfo.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",9,4,5,33,219,0,5,57,58,19,3,2,64,58,21,7,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/BadRequestException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/BadRequestException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,7,7,7,52,0,1,27,27,14,2,2.5,34,27,17,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/ConnectRestException.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/errors/ConnectRestException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,4,5,39,308,0,10,69,70,23,3,4,85,70,28,16,11,5,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneConfig.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneConfig.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,4,5,19,123,0,1,44,35,7,6,2.0,55,35,9,11,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java,connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,4,5,31,268,0,2,56,46,8,7,1,66,46,9,10,5,1,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSinkConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSinkConnector.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,7,7,51,318,0,10,84,84,42,2,2.5,91,84,46,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSinkTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSinkTask.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,7,7,53,363,0,6,88,71,29,3,4,96,71,32,8,7,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSourceConnector.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSourceConnector.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,7,7,51,318,0,10,84,84,42,2,2.5,91,84,46,7,7,4,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSourceTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/MockSourceTask.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",8,7,7,45,308,0,4,73,66,24,3,4,81,66,27,8,7,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSinkTask.java,connect/runtime/src/main/java/org/apache/kafka/connect/tools/VerifiableSinkTask.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,4,5,74,549,0,5,108,110,27,4,1.0,114,110,28,6,5,2,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/Callback.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/Callback.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,7,8,4,29,0,0,30,31,10,3,1,39,31,13,9,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/FutureCallback.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/FutureCallback.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,7,8,13,67,0,3,33,76,6,6,2.0,94,76,16,61,49,10,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/ShutdownableThread.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/ShutdownableThread.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",12,7,8,58,366,0,9,144,145,48,3,1,153,145,51,9,8,3,2,1,0,1
connect/runtime/src/main/java/org/apache/kafka/connect/util/SinkUtils.java,connect/runtime/src/main/java/org/apache/kafka/connect/util/SinkUtils.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,7,7,7,38,0,2,27,27,14,2,2.5,34,27,17,7,7,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/storage/MemoryStatusBackingStoreTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/storage/MemoryStatusBackingStoreTest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,7,7,41,376,0,4,66,66,33,2,2.5,73,66,36,7,7,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ByteArrayProducerRecordEquals.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ByteArrayProducerRecordEquals.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",11,4,5,30,319,0,4,53,53,13,4,1.0,59,53,15,6,5,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/MockTime.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/MockTime.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,4,4,26,175,0,5,55,49,14,4,2.5,65,49,16,10,5,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ShutdownableThreadTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ShutdownableThreadTest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,7,8,44,242,0,2,71,72,24,3,1,80,72,27,9,8,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/TableTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/TableTest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,7,7,24,241,0,1,48,48,24,2,2.5,55,48,28,7,7,4,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/TestBackgroundThreadExceptionHandler.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/TestBackgroundThreadExceptionHandler.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,7,8,13,78,0,2,36,37,12,3,1,45,37,15,9,8,3,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/TestFuture.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/TestFuture.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",22,4,5,105,498,0,13,160,81,32,5,1,171,81,34,11,5,2,2,1,0,1
connect/runtime/src/test/java/org/apache/kafka/connect/util/ThreadedTest.java,connect/runtime/src/test/java/org/apache/kafka/connect/util/ThreadedTest.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",2,7,8,16,77,0,2,42,43,14,3,1,51,43,17,9,8,3,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/RegexRouter.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/RegexRouter.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,7,8,46,381,0,4,74,75,37,2,2.5,82,75,41,8,8,4,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/RegexValidator.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/RegexValidator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,7,8,18,120,0,2,40,41,20,2,2.5,48,41,24,8,8,4,2,1,0,1
connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/SimpleConfig.java,connect/transforms/src/main/java/org/apache/kafka/connect/transforms/util/SimpleConfig.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",1,7,8,9,78,0,1,33,34,16,2,2.5,41,34,20,8,8,4,2,1,0,1
core/src/main/scala/kafka/server/LeaderElector.scala,core/src/main/scala/kafka/server/LeaderElector.scala,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,3,3,8,28,0,0,33,35,11,3,1,38,35,13,5,3,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/errors/TaskAssignmentException.java,streams/src/main/java/org/apache/kafka/streams/errors/TaskAssignmentException.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,3,3,13,74,0,3,39,32,8,5,3,55,32,11,16,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggProcessorSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggProcessorSupplier.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,3,4,6,62,0,0,27,28,14,2,2.5,31,28,16,4,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamForeach.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamForeach.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",3,3,4,21,165,0,3,43,44,22,2,2.5,47,44,24,4,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinValueGetter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableJoinValueGetter.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",5,6,6,36,297,0,3,62,62,31,2,2.0,68,62,34,6,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoinValueGetter.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableLeftJoinValueGetter.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",4,6,6,34,273,0,3,58,58,29,2,2.0,64,58,32,6,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableValueGetterSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableValueGetterSupplier.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,3,4,5,40,0,0,24,24,8,3,1,28,24,9,4,4,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/HasNextCondition.java,streams/src/main/java/org/apache/kafka/streams/state/internals/HasNextCondition.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,6,6,6,56,0,0,24,25,8,3,2,35,25,12,11,6,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/PeekingKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/PeekingKeyValueIterator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,6,6,6,64,0,0,25,24,8,3,2,32,24,11,7,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/SerializedKeyValueIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/SerializedKeyValueIterator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",8,6,6,43,319,0,6,70,70,23,3,1,77,70,26,7,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/StateStoreProvider.java,streams/src/main/java/org/apache/kafka/streams/state/internals/StateStoreProvider.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",0,13,11,8,83,0,0,43,41,14,3,1,55,41,18,12,11,4,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedWindowStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/WrappedWindowStoreIterator.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",10,15,17,58,478,0,9,91,93,46,2,1.0,108,93,54,17,17,8,2,1,0,1
tools/src/main/java/org/apache/kafka/tools/ThroughputThrottler.java,tools/src/main/java/org/apache/kafka/tools/ThroughputThrottler.java,"MINOR: improve license header check by providing head file instead of (prefix) header regex

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #2303 from mjsax/licenseHeader",15,3,4,67,359,0,4,139,118,15,9,2,176,118,20,37,14,4,2,1,0,1
core/src/main/scala/kafka/utils/NotNothing.scala,core/src/main/scala/kafka/utils/NotNothing.scala,"MINOR: Make it impossible to invoke `Request.body` without an explicit type parameter

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #2579 from ijuma/safer-body",0,41,0,8,70,0,0,41,41,41,1,1,41,41,41,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/TopicMetadata.scala,core/src/main/scala/kafka/api/TopicMetadata.scala,"MINOR: Use an explicit `Errors` object when possible instead of a numeric error code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2475 from vahidhashemian/minor/use_explicit_Errors_type_when_possible",5,10,10,105,848,2,4,153,244,6,24,2.0,414,244,17,261,89,11,2,1,0,1
core/src/main/scala/kafka/common/ErrorMapping.scala,core/src/main/scala/kafka/common/ErrorMapping.scala,"MINOR: Use an explicit `Errors` object when possible instead of a numeric error code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2475 from vahidhashemian/minor/use_explicit_Errors_type_when_possible",5,10,10,66,667,0,4,115,62,3,39,2,223,62,6,108,16,3,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetCommitResponse.scala,core/src/main/scala/kafka/javaapi/OffsetCommitResponse.scala,"MINOR: Use an explicit `Errors` object when possible instead of a numeric error code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2475 from vahidhashemian/minor/use_explicit_Errors_type_when_possible",3,4,2,14,130,2,3,39,30,6,6,1.5,48,30,8,9,5,2,2,1,0,1
core/src/main/scala/kafka/producer/BrokerPartitionInfo.scala,core/src/main/scala/kafka/producer/BrokerPartitionInfo.scala,"MINOR: Use an explicit `Errors` object when possible instead of a numeric error code

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>

Closes #2475 from vahidhashemian/minor/use_explicit_Errors_type_when_possible",13,6,6,69,533,2,2,105,49,4,29,2,298,55,10,193,36,7,2,1,0,1
core/src/main/scala/kafka/tools/ZooKeeperMainWrapper.scala,core/src/main/scala/kafka/tools/ZooKeeperMainWrapper.scala,"KAFKA-4039; Fix deadlock during shutdown due to log truncation not allowed

Author: Maysam Yabandeh <myabandeh@dropbox.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2474 from ijuma/kafka-4039-deadlock-during-shutdown",2,2,1,15,90,1,2,40,39,13,3,1,42,39,14,2,1,1,2,1,0,1
core/src/test/scala/other/kafka/TestKafkaAppender.scala,core/src/test/scala/other/kafka/TestKafkaAppender.scala,"KAFKA-4039; Fix deadlock during shutdown due to log truncation not allowed

Author: Maysam Yabandeh <myabandeh@dropbox.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2474 from ijuma/kafka-4039-deadlock-during-shutdown",6,3,3,25,159,1,2,50,50,6,9,1,72,50,8,22,6,2,2,1,0,1
tests/unit/directory_layout/check_project_paths.py,tests/unit/directory_layout/check_project_paths.py,"KAFKA-4450; Add upgrade tests for 0.10.1 and rename TRUNK to DEV_BRANCH to reduce confusion

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2457 from ewencp/kafka-4450-upgrade-tests",7,7,7,42,280,2,7,90,90,45,2,3.0,97,90,48,7,7,4,2,1,0,1
tests/unit/version/check_version.py,tests/unit/version/check_version.py,"KAFKA-4450; Add upgrade tests for 0.10.1 and rename TRUNK to DEV_BRANCH to reduce confusion

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2457 from ewencp/kafka-4450-upgrade-tests",1,3,3,12,71,1,1,33,33,16,2,2.0,36,33,18,3,3,2,2,1,0,1
core/src/test/scala/integration/kafka/api/SaslTestHarness.scala,core/src/test/scala/integration/kafka/api/SaslTestHarness.scala,"KAFKA-4636; Per listener security settings overrides (KIP-103)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Rajini Sivaram <rajinisivaram@googlemail.com>

Closes #2406 from ijuma/kafka-4636-per-listener-security-settings",3,4,2,23,117,1,2,39,63,6,7,4,127,63,18,88,52,13,2,1,0,1
core/src/test/scala/unit/kafka/integration/ProducerConsumerTestHarness.scala,core/src/test/scala/unit/kafka/integration/ProducerConsumerTestHarness.scala,"KAFKA-4565; Separation of Internal and External traffic (KIP-103)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira <cshapi@gmail.com>, Jason Gustafson <jason@confluent.io>

Closes #2354 from ijuma/kafka-4565-separation-of-internal-and-external-traffic",2,1,1,27,179,1,2,48,51,3,16,2.0,117,51,7,69,19,4,2,1,0,1
core/src/test/scala/unit/kafka/server/SessionExpireListenerTest.scala,core/src/test/scala/unit/kafka/server/SessionExpireListenerTest.scala,"KAFKA-4565; Separation of Internal and External traffic (KIP-103)

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira <cshapi@gmail.com>, Jason Gustafson <jason@confluent.io>

Closes #2354 from ijuma/kafka-4565-separation-of-internal-and-external-traffic",5,1,1,43,309,1,3,73,68,24,3,1,79,68,26,6,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/InMemoryKeyValueStore.java,streams/src/test/java/org/apache/kafka/test/InMemoryKeyValueStore.java,"KAFKA-4490: Add Global Table support to Kafka Streams

Add Global Tables to KafkaStreams. Global Tables are fully replicated once-per instance of KafkaStreams. A single thread is used to update them. They can be used to join with KStreams, KTables, and other GlobalKTables. When participating in a join a GlobalKTable is only ever used to perform a lookup, i.e., it will never cause data to be forwarded to downstream processor nodes.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Matthias J. Sax, Eno Thereska, Guozhang Wang

Closes #2244 from dguy/global-tables",24,6,5,108,692,3,21,152,146,51,3,3,159,146,53,7,5,2,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/ObsoleteBrokerException.java,clients/src/main/java/org/apache/kafka/common/errors/ObsoleteBrokerException.java,"KAFKA-4507; Clients should support older brokers (KIP-97)

The client should send older versions of requests to the broker if necessary.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2264 from cmccabe/KAFKA-4507",2,31,0,10,57,2,2,31,31,31,1,1,31,31,31,0,0,0,0,0,0,0
core/src/main/scala/kafka/utils/NetworkClientBlockingOps.scala,core/src/main/scala/kafka/utils/NetworkClientBlockingOps.scala,"KAFKA-4507; Clients should support older brokers (KIP-97)

The client should send older versions of requests to the broker if necessary.

Author: Colin P. Mccabe <cmccabe@confluent.io>

Reviewers: Jason Gustafson <jason@confluent.io>, Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #2264 from cmccabe/KAFKA-4507",12,3,2,65,460,1,5,145,142,13,11,2,233,142,21,88,34,8,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCachedWindowStoreIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/MergedSortedCachedWindowStoreIterator.java,"KAFKA-3452: Support session windows

Add support for SessionWindows based on design detailed in https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows.
This includes refactoring of the RocksDBWindowStore such that functionality common with the RocksDBSessionStore isn't duplicated.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Eno Thereska <eno.thereska@gmail.com>, Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>

Closes #2166 from dguy/kafka-3452-session-merge",15,12,10,69,508,6,7,107,105,54,2,5.5,117,105,58,10,10,5,1,0,1,1
core/src/test/scala/unit/kafka/common/TopicTest.scala,core/src/test/scala/unit/kafka/common/TopicTest.scala,"MINOR: Scala code readability improvements

Author: Himani Arora <1himani.arora@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2297 from himani1/refactored_code",13,2,2,70,449,1,3,102,61,9,11,2,117,61,11,15,3,1,2,1,0,1
core/src/main/scala/kafka/controller/ControllerZkListener.scala,core/src/main/scala/kafka/controller/ControllerZkListener.scala,"KAFKA-4447; Controller resigned but it also acts as a controller for a long time

Author: Ismael Juma <ismael@juma.me.uk>
Author: xiguantiaozhan <kafkausr@126.com>
Author: tuyang <tuyang@meituan.com>

Reviewers: Jiangjie Qin <becket.qin@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Onur Karaman <okaraman@linkedin.com>

Closes #2191 from xiguantiaozhan/avoid_swamp_controllerLog",6,62,0,34,232,3,3,62,62,62,1,1,62,62,62,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/NetworkClientApiVersionsCheckTest.java,clients/src/test/java/org/apache/kafka/clients/NetworkClientApiVersionsCheckTest.java,"KAFKA-3600; Use ApiVersions to check if broker supports required api versions

Author: Ashish Singh <asingh@cloudera.com>

Reviewers: Jason Gustafson <jason@confluent.io>, Colin P. Mccabe <cmccabe@confluent.io>, Dana Powers <dana.powers@gmail.com>, Gwen Shapira <cshapi@gmail.com>, Grant Henke <granthenke@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1251 from SinghAsDev/KAFKA-3600",9,89,0,60,641,5,5,89,89,89,1,1,89,89,89,0,0,0,0,0,0,0
tests/kafkatest/tests/core/simple_consumer_shell_test.py,tests/kafkatest/tests/core/simple_consumer_shell_test.py,"KAFKA-4140: Upgrade to ducktape 0.6.0 and make system tests parallel friendly

Updates to take advantage of soon-to-be-released ducktape features.

Author: Geoff Anderson <geoff@confluent.io>
Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1834 from granders/systest-parallel-friendly",6,4,0,46,344,0,6,79,75,16,5,0,79,75,16,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/smoketest/TestTimestampExtractor.java,streams/src/test/java/org/apache/kafka/streams/smoketest/TestTimestampExtractor.java,"KAFKA-4393: Improve invalid/negative TS handling

Author: Matthias J. Sax <matthias@confluent.io>

Reviewers: Michael G. Noll, Eno Thereska, Damian Guy, Guozhang Wang

Closes #2117 from mjsax/kafka-4393-improveInvalidTsHandling",2,1,1,15,108,2,1,37,37,18,2,1.0,38,37,19,1,1,0,1,0,1,1
core/src/main/scala/kafka/log/FileMessageSet.scala,core/src/main/scala/kafka/log/FileMessageSet.scala,"KAFKA-2247; Merge kafka.utils.Time and kafka.common.utils.Time

Also:
* Make all implementations of `Time` thread-safe as they are accessed from multiple threads in some cases.
* Change default implementation of `MockTime` to use two separate variables for `nanoTime` and `currentTimeMillis` as they have different `origins`.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>, Shikhar Bhushan <shikhar@confluent.io>, Jason Gustafson <jason@confluent.io>, Eno Thereska <eno.thereska@gmail.com>, Damian Guy <damian.guy@gmail.com>

Closes #2095 from ijuma/kafka-2247-consolidate-time-interfaces",58,1,2,253,1765,0,23,443,204,12,38,3.0,752,204,20,309,36,8,2,1,0,1
core/src/main/scala/kafka/javaapi/OffsetFetchResponse.scala,core/src/main/scala/kafka/javaapi/OffsetFetchResponse.scala,"KAFKA-4377; remove deprecated scala.collection.JavaConversions calls

JavaConversions are deprecated in 2.12 in favour of JavaConverters.

Author: Bernard Leach <leachbj@bouncycastle.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #2101 from leachbj/4377-java-converters",1,2,5,10,86,0,1,33,30,8,4,1.5,39,30,10,6,5,2,2,1,0,1
core/src/main/scala/kafka/api/GenericRequestAndHeader.scala,core/src/main/scala/kafka/api/GenericRequestAndHeader.scala,"KAFKA-2066; Use client-side FetchRequest/FetchResponse on server

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2069 from hachikuji/KAFKA-2066",3,3,2,36,217,0,3,56,55,14,4,2.0,61,55,15,5,2,1,2,1,0,1
core/src/main/scala/kafka/api/GenericResponseAndHeader.scala,core/src/main/scala/kafka/api/GenericResponseAndHeader.scala,"KAFKA-2066; Use client-side FetchRequest/FetchResponse on server

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #2069 from hachikuji/KAFKA-2066",3,3,2,27,161,0,3,47,45,9,5,2,60,45,12,13,8,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/processor/ConsumerRecordTimestampExtractor.java,streams/src/main/java/org/apache/kafka/streams/processor/ConsumerRecordTimestampExtractor.java,revert streams/src/main/java/org/apache/kafka/streams/processor/ConsumerRecordTimestampExtractor.java,1,11,41,8,54,2,1,40,39,10,4,4.0,93,41,23,53,41,13,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorTest.java,"KAFKA-4117: Stream partitionassignro cleanup

1. Create a new `ClientMetadata` to collapse `Set<String> consumerMemberIds`, `ClientState<TaskId> state`, and `HostInfo hostInfo`.

2. Stop reusing `stateChangelogTopicToTaskIds` and `internalSourceTopicToTaskIds` to access the (sub-)topology's internal repartition and changelog topics for clarity; also use the source topics num.partitions to set the num.partitions for repartition topics, and clarify to NOT have cycles since otherwise the while loop will fail.

3. `ensure-copartition` at the end to modify the number of partitions for repartition topics if necessary to be equal to other co-partition topics.

4. Refactor `ClientState` as well and update the logic of `TaskAssignor` for clarity as well.

5. Change default `clientId` from `applicationId-suffix` to `applicationId-processId` where `processId` is an UUID to avoid conflicts of clientIds that are from different JVMs, and hence conflicts in metrics.

6. Enforce `assignment` partitions to have the same size, and hence 1-1 mapping to `activeTask` taskIds.

7. Remove the `AssignmentSupplier` class by always construct the `partitionsByHostState` before assigning tasks to consumers within a client.

8. Remove all unnecessary member variables in `StreamPartitionAssignor`.

9. Some other minor fixes on unit tests, e.g. remove `test only` functions with java field reflection.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Xavier Léauté, Matthias J. Sax, Eno Thereska, Jason Gustafson

Closes #2012 from guozhangwang/K4117-stream-partitionassignro-cleanup",23,69,46,247,2440,4,4,312,289,104,3,12,370,289,123,58,46,19,2,1,0,1
core/src/main/scala/kafka/producer/DefaultPartitioner.scala,core/src/main/scala/kafka/producer/DefaultPartitioner.scala,"MINOR: A bunch of clean-ups related to usage of unused variables

There should be only one cases where these clean-ups have a functional impact: replaced repeated identical logs with a single log for the stale controller epoch case.

The rest should just make the code easier to read and make it a bit less wasteful. I did this exercise because unused variables sometimes mask bugs.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #1985 from ijuma/remove-unused",1,0,2,10,70,0,1,30,28,3,11,1,48,28,4,18,6,2,2,1,0,1
core/src/test/scala/other/kafka/TestCrcPerformance.scala,core/src/test/scala/other/kafka/TestCrcPerformance.scala,"MINOR: A bunch of clean-ups related to usage of unused variables

There should be only one cases where these clean-ups have a functional impact: replaced repeated identical logs with a single log for the stale controller epoch case.

The rest should just make the code easier to read and make it a bit less wasteful. I did this exercise because unused variables sometimes mask bugs.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #1985 from ijuma/remove-unused",3,5,9,20,169,1,1,44,31,11,4,1.0,54,31,14,10,9,2,2,1,0,1
core/src/test/scala/unit/kafka/log/FileMessageSetTest.scala,core/src/test/scala/unit/kafka/log/FileMessageSetTest.scala,"MINOR: A bunch of clean-ups related to usage of unused variables

There should be only one cases where these clean-ups have a functional impact: replaced repeated identical logs with a single log for the stale controller epoch case.

The rest should just make the code easier to read and make it a bit less wasteful. I did this exercise because unused variables sometimes mask bugs.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #1985 from ijuma/remove-unused",23,4,5,245,1951,3,18,351,86,18,20,2.0,430,86,22,79,18,4,2,1,0,1
core/src/test/scala/unit/kafka/api/FetchRequestTest.scala,core/src/test/scala/unit/kafka/api/FetchRequestTest.scala,"MINOR: Tweak implementation of `FetchRequest.shuffle` and upgrade.html improvements

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jason Gustafson <jason@confluent.io>

Closes #1955 from ijuma/kip-74-follow-up",9,63,0,38,324,2,2,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/DelegatingPeekingWindowIterator.java,streams/src/main/java/org/apache/kafka/streams/state/internals/DelegatingPeekingWindowIterator.java,"KAFKA-3776: Unify store and downstream caching in streams

This is joint work between dguy and enothereska. The work implements KIP-63. Overview of main changes:

- New byte-based cache that acts as a buffer for any persistent store and for forwarding changes downstream.
- Forwarding record path changes: previously a record in a task completed end-to-end. Now it may be buffered in a processor node while other records complete in the task.
- Cleanup and state stores and decoupling of cache from state store and forwarding.
- More than 80 new unit and integration tests.

Author: Damian Guy <damian.guy@gmail.com>
Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Matthias J. Sax, Guozhang Wang

Closes #1752 from enothereska/KAFKA-3776-poc",10,73,0,46,246,6,6,73,73,73,1,1,73,73,73,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCacheMetrics.java,streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCacheMetrics.java,"KAFKA-3776: Unify store and downstream caching in streams

This is joint work between dguy and enothereska. The work implements KIP-63. Overview of main changes:

- New byte-based cache that acts as a buffer for any persistent store and for forwarding changes downstream.
- Forwarding record path changes: previously a record in a task completed end-to-end. Now it may be buffered in a processor node while other records complete in the task.
- Cleanup and state stores and decoupling of cache from state store and forwarding.
- More than 80 new unit and integration tests.

Author: Damian Guy <damian.guy@gmail.com>
Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Matthias J. Sax, Guozhang Wang

Closes #1752 from enothereska/KAFKA-3776-poc",0,40,0,6,55,0,0,40,40,40,1,1,40,40,40,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/state/internals/DelegatingPeekingWindowIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/state/internals/DelegatingPeekingWindowIteratorTest.java,"KAFKA-3776: Unify store and downstream caching in streams

This is joint work between dguy and enothereska. The work implements KIP-63. Overview of main changes:

- New byte-based cache that acts as a buffer for any persistent store and for forwarding changes downstream.
- Forwarding record path changes: previously a record in a task completed end-to-end. Now it may be buffered in a processor node while other records complete in the task.
- Cleanup and state stores and decoupling of cache from state store and forwarding.
- More than 80 new unit and integration tests.

Author: Damian Guy <damian.guy@gmail.com>
Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Matthias J. Sax, Guozhang Wang

Closes #1752 from enothereska/KAFKA-3776-poc",7,92,0,63,649,5,5,92,92,92,1,1,92,92,92,0,0,0,0,0,0,0
core/src/main/scala/kafka/utils/Time.scala,core/src/main/scala/kafka/utils/Time.scala,"KAFKA-1464; Add a throttling option to the Kafka replication

This applies to Replication Quotas
based on KIP-73 [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas) originally motivated by KAFKA-1464.

System Tests Run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/544/

**This first PR demonstrates the approach**.

**_Overview of Change_**
The guts of this change are relatively small. Throttling occurs on both leader and follower sides. A single class tracks the throttled throughput in and out of each broker (**_ReplicationQuotaManager_**).

On the follower side, the Follower Throttled Rate is calculated as fetch responses arrive. Then, before the next fetch request is sent, we check to see if the quota is violated, removing throttled partitions from the request if it is. This is all encapsulated in a few lines of code in the **_ReplicaFetcherThread_**. There is existing code to handle temporal back off, if the request ends up being empty.

On the leader side it's a little more complex. When a fetch request arrives in the leader, it is built, partition by partition, in **_ReplicaManager.readFromLocalLog_**. As we put each partition into the fetch response, we check if the total size fits in the current quota. If the quota is exceeded, the partition will not be added to the fetch response. Importantly, we don't increase the quota at this point, we just check to see if the bytes will fit.

Now, if there aren't enough bytes to send the response immediately, which is common if we're catching up and throttled, then the request will be put in purgatory. I've added some simple code to **_DelayedFetch_** to handle throttled partitions (throttled partitions are checked against the quota, rather than the messages available in the log).

When the delayed fetch completes, and exits purgatory, _**ReplicaManager.readFromLocalLog**_ will be called again. This is why _**ReplicaManager.readFromLocalLog**_ does not actually increase the quota, it just checks whether enough bytes are available for a partition.

Finally, when there are enough bytes to be sent, or the delayed fetch times out, the response will be sent. Before it is sent the throttled-outbound-rate is increased, based on the size of throttled partitions being sent. This is at the end of _**KafkaApis.handleFetchRequest**_, exactly where client quotas are recorded.

There is an acceptance test which asserts the whole throttling process stabilises on the desired value. This covers a number of use cases including many-to-many replication. See **_ReplicationQuotaTest_**.

Note:
It should be noted that this protocol can over-request. The request is built, based on the quota at time t1 (_ReplicaManager.readFromLocalLog_). The bytes in the response are recorded at time t2 (end of _KafkaApis.handleFetchRequest_), where t2 > t1. For this reason I originally included an OverRequestedRate as a JMX metric, but testing has not seen revealed any obvious issue. Over-requesting is quickly compensated by subsequent requests, stabilising close to the quota value.

_**Main stuff left to do:**_
- The fetch size is currently unbounded. This will be addressed in KIP-74, but we need to ensure this ensures requests don’t go beyond the throttle window.
- There are two failures showing up in the system tests on this branch:  StreamsSmokeTest.test_streams (which looks like it fails regularly) and OffsetValidationTest.test_broker_rolling_bounce (which I need to look into)

_**Stuff left to do that could be deferred:**_
- Add the extra metrics specified in the KIP.
- There are no system tests.
- There is no validation for the cluster size / throttle combination that could lead to ISR dropouts

Author: Ben Stopford <benstopford@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>

Closes #1776 from benstopford/rep-quotas-v2",2,1,1,27,158,0,2,65,60,16,4,1.0,72,60,18,7,6,2,2,1,0,1
core/src/test/scala/integration/kafka/api/ClientQuotasTest.scala,core/src/test/scala/integration/kafka/api/ClientQuotasTest.scala,"KAFKA-1464; Add a throttling option to the Kafka replication

This applies to Replication Quotas
based on KIP-73 [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas) originally motivated by KAFKA-1464.

System Tests Run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/544/

**This first PR demonstrates the approach**.

**_Overview of Change_**
The guts of this change are relatively small. Throttling occurs on both leader and follower sides. A single class tracks the throttled throughput in and out of each broker (**_ReplicationQuotaManager_**).

On the follower side, the Follower Throttled Rate is calculated as fetch responses arrive. Then, before the next fetch request is sent, we check to see if the quota is violated, removing throttled partitions from the request if it is. This is all encapsulated in a few lines of code in the **_ReplicaFetcherThread_**. There is existing code to handle temporal back off, if the request ends up being empty.

On the leader side it's a little more complex. When a fetch request arrives in the leader, it is built, partition by partition, in **_ReplicaManager.readFromLocalLog_**. As we put each partition into the fetch response, we check if the total size fits in the current quota. If the quota is exceeded, the partition will not be added to the fetch response. Importantly, we don't increase the quota at this point, we just check to see if the bytes will fit.

Now, if there aren't enough bytes to send the response immediately, which is common if we're catching up and throttled, then the request will be put in purgatory. I've added some simple code to **_DelayedFetch_** to handle throttled partitions (throttled partitions are checked against the quota, rather than the messages available in the log).

When the delayed fetch completes, and exits purgatory, _**ReplicaManager.readFromLocalLog**_ will be called again. This is why _**ReplicaManager.readFromLocalLog**_ does not actually increase the quota, it just checks whether enough bytes are available for a partition.

Finally, when there are enough bytes to be sent, or the delayed fetch times out, the response will be sent. Before it is sent the throttled-outbound-rate is increased, based on the size of throttled partitions being sent. This is at the end of _**KafkaApis.handleFetchRequest**_, exactly where client quotas are recorded.

There is an acceptance test which asserts the whole throttling process stabilises on the desired value. This covers a number of use cases including many-to-many replication. See **_ReplicationQuotaTest_**.

Note:
It should be noted that this protocol can over-request. The request is built, based on the quota at time t1 (_ReplicaManager.readFromLocalLog_). The bytes in the response are recorded at time t2 (end of _KafkaApis.handleFetchRequest_), where t2 > t1. For this reason I originally included an OverRequestedRate as a JMX metric, but testing has not seen revealed any obvious issue. Over-requesting is quickly compensated by subsequent requests, stabilising close to the quota value.

_**Main stuff left to do:**_
- The fetch size is currently unbounded. This will be addressed in KIP-74, but we need to ensure this ensures requests don’t go beyond the throttle window.
- There are two failures showing up in the system tests on this branch:  StreamsSmokeTest.test_streams (which looks like it fails regularly) and OffsetValidationTest.test_broker_rolling_bounce (which I need to look into)

_**Stuff left to do that could be deferred:**_
- Add the extra metrics specified in the KIP.
- There are no system tests.
- There is no validation for the cluster size / throttle combination that could lead to ISR dropouts

Author: Ben Stopford <benstopford@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Apurva Mehta <apurva@confluent.io>, Jun Rao <junrao@gmail.com>

Closes #1776 from benstopford/rep-quotas-v2",12,4,5,155,1435,1,7,206,194,19,11,2,254,194,23,48,16,4,0,0,0,0
clients/src/main/java/org/apache/kafka/common/record/Compressor.java,clients/src/main/java/org/apache/kafka/common/record/Compressor.java,"KAFKA-3977; Defer fetch parsing for space efficiency and to ensure exceptions are raised to the user

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ewen Cheslack-Postava <me@ewencp.org>, Ismael Juma <ismael@juma.me.uk>

Closes #1656 from hachikuji/KAFKA-3977",41,2,2,251,1697,2,18,332,244,24,14,3.5,423,244,30,91,18,6,2,1,0,1
tests/kafkatest/benchmarks/core/__init__.py,tests/kafkatest/benchmarks/core/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/benchmarks/streams/__init__.py,tests/kafkatest/benchmarks/streams/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/services/__init__.py,tests/kafkatest/services/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/tests/__init__.py,tests/kafkatest/tests/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/tests/connect/__init__.py,tests/kafkatest/tests/connect/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/tests/tools/__init__.py,tests/kafkatest/tests/tools/__init__.py,"MINOR: cleanup apache license in python files

ijuma
As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several __init__.py files, and a few others as well

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1659 from granders/minor-cleanup-init-files",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
tests/kafkatest/tests/streams/__init__.py,tests/kafkatest/tests/streams/__init__.py,"HOTFIX: Adding init file so streams benchmark is autodiscovered

Without this file the benchmark does not run nightly.

Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Geoff Anderson <geoff@confluent.io>, Ismael Juma <ismael@juma.me.uk>

Closes #1645 from enothereska/hotfix-streams-test",0,0,1,0,0,0,0,14,15,7,2,1.0,15,15,8,1,1,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/WordCountIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/WordCountIntegrationTest.java,"KAFKA-3942; Change IntegrationTestUtils.purgeLocalStreamsState to use java.io.tmpdir

It was previously only deleting files/folders where the path started with /tmp. Changed it to delete from the value of the System Property `java.io.tmpdir`. Also changed the tests that were creating State dirs under /tmp to just use `TestUtils.tempDirectory(..)`

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1600 from dguy/kafka-3942",3,2,6,107,1069,1,2,154,149,22,7,2,176,149,25,22,6,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/state/StateTestUtils.java,streams/src/test/java/org/apache/kafka/streams/state/StateTestUtils.java,"MINOR: speed up streams integration tests

... by
a) merging some for startup/shutdown efficiency.
b) use independent state dirs.
c) remove some tests that are covered elsewhere

guozhangwang ewencp - tests are running much quicker now, i.e, down to about 1 minute on my laptop (from about 2 - 3 minutes). There were some issues with state-dirs in some of the integration tests that was causing the shutdown of the streams apps to take a long time.

Author: Damian Guy <damian.guy@gmail.com>

Reviewers: Ismael Juma, Guozhang Wang

Closes #1525 from dguy/integration-tests",6,1,1,47,307,1,2,79,77,16,5,1,95,77,19,16,13,3,2,1,0,1
core/src/main/scala/kafka/cluster/Cluster.scala,core/src/main/scala/kafka/cluster/Cluster.scala,"KAFKA-3771; Improving Kafka core code

- Used flatMap instead of map and flatten
- Use isEmpty, NonEmpty, isDefined as appropriate
- Used head, keys and keySet where appropriate
- Used contains, diff and find where appropriate
- Removed redundant val modifier for case class constructor
- toString has no parameters, no side effect hence without () consistent usage
- Removed unnecessary return , parentheses and semi colons.

Author: Joshi <rekhajoshm@gmail.com>
Author: Rekha Joshi <rekhajoshm@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1451 from rekhajoshm/KAFKA-3771",5,1,1,16,138,1,4,44,45,7,6,1.0,55,45,9,11,6,2,2,1,0,1
core/src/main/scala/kafka/common/AppInfo.scala,core/src/main/scala/kafka/common/AppInfo.scala,"KAFKA-3771; Improving Kafka core code

- Used flatMap instead of map and flatten
- Use isEmpty, NonEmpty, isDefined as appropriate
- Used head, keys and keySet where appropriate
- Used contains, diff and find where appropriate
- Removed redundant val modifier for case class constructor
- toString has no parameters, no side effect hence without () consistent usage
- Removed unnecessary return , parentheses and semi colons.

Author: Joshi <rekhajoshm@gmail.com>
Author: Rekha Joshi <rekhajoshm@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1451 from rekhajoshm/KAFKA-3771",1,1,1,30,124,1,1,54,66,18,3,1,83,66,28,29,28,10,2,1,0,1
core/src/main/scala/kafka/producer/ProducerRequestStats.scala,core/src/main/scala/kafka/producer/ProducerRequestStats.scala,"KAFKA-3771; Improving Kafka core code

- Used flatMap instead of map and flatten
- Use isEmpty, NonEmpty, isDefined as appropriate
- Used head, keys and keySet where appropriate
- Used contains, diff and find where appropriate
- Removed redundant val modifier for case class constructor
- toString has no parameters, no side effect hence without () consistent usage
- Removed unnecessary return , parentheses and semi colons.

Author: Joshi <rekhajoshm@gmail.com>
Author: Rekha Joshi <rekhajoshm@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1451 from rekhajoshm/KAFKA-3771",4,1,1,36,327,0,4,69,56,10,7,1,83,56,12,14,9,2,2,1,0,1
core/src/main/scala/kafka/message/ByteBufferBackedInputStream.scala,core/src/main/scala/kafka/message/ByteBufferBackedInputStream.scala,"KAFKA-3768; Replace all pattern match on boolean value by if/else block.

Author: Satendra kumar <satendra@knoldus.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>

Closes #1445 from satendrakumar06/remove_boolean_pattern_match",4,11,13,19,114,2,2,39,26,8,5,1,54,26,11,15,13,3,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/MapFunctionIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/MapFunctionIntegrationTest.java,"KAFKA-3678: Removed sleep from streams integration tests

Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #1439 from enothereska/KAFKA-3678-timeouts1",3,1,4,79,788,1,2,122,127,30,4,1.5,132,127,33,10,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/integration/PassThroughIntegrationTest.java,streams/src/test/java/org/apache/kafka/streams/integration/PassThroughIntegrationTest.java,"KAFKA-3678: Removed sleep from streams integration tests

Author: Eno Thereska <eno.thereska@gmail.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #1439 from enothereska/KAFKA-3678-timeouts1",2,1,4,66,624,1,2,108,113,27,4,1.5,118,113,30,10,5,2,2,1,0,1
core/src/main/scala/kafka/message/InvalidMessageException.scala,core/src/main/scala/kafka/message/InvalidMessageException.scala,"HOTFIX: KAFKA-3160 follow-up, catch decompression errors in constructor

After testing KAFKA-3160 a bit more, I found that the error code was not being set properly in ProduceResponse. This happened because the validation error is raised in the CompressionFactory constructor, which was not wrapped in a try / catch.

ijuma junrao

(This contribution is my original work and I license the work under Apache 2.0.)

Author: Dana Powers <dana.powers@gmail.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Gwen Shapira <cshapi@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1344 from dpkp/decompress_error_code",2,2,1,6,59,1,2,35,22,6,6,1.0,46,22,8,11,6,2,2,1,0,1
core/src/main/scala/kafka/message/CompressionFactory.scala,core/src/main/scala/kafka/message/CompressionFactory.scala,"KAFKA-3160; Fix LZ4 Framing

This contribution is my original work and I license the work under Apache 2.0.

Author: Dana Powers <dana.powers@gmail.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1212 from dpkp/KAFKA-3160",12,4,4,34,222,4,2,56,55,9,6,2.5,74,55,12,18,8,3,2,1,0,1
core/src/test/scala/unit/kafka/message/MessageWriterTest.scala,core/src/test/scala/unit/kafka/message/MessageWriterTest.scala,"KAFKA-3160; Fix LZ4 Framing

This contribution is my original work and I license the work under Apache 2.0.

Author: Dana Powers <dana.powers@gmail.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>, Ismael Juma <ismael@juma.me.uk>

Closes #1212 from dpkp/KAFKA-3160",12,3,3,96,842,3,10,131,130,33,4,2.0,139,130,35,8,4,2,2,1,0,1
tests/kafkatest/directory_layout/__init__.py,tests/kafkatest/directory_layout/__init__.py,"KAFKA-3592: System test - configurable paths

This patch adds logic for the following:
- remove hard-coded paths to various scripts and jars in kafkatest service classes
- provide a mechanism for overriding path resolution logic with a ""pluggable"" path resolver class

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1245 from granders/configurable-install-path",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,2,1,0,1
tests/unit/__init__.py,tests/unit/__init__.py,"KAFKA-3592: System test - configurable paths

This patch adds logic for the following:
- remove hard-coded paths to various scripts and jars in kafkatest service classes
- provide a mechanism for overriding path resolution logic with a ""pluggable"" path resolver class

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1245 from granders/configurable-install-path",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,2,1,0,1
tests/unit/directory_layout/__init__.py,tests/unit/directory_layout/__init__.py,"KAFKA-3592: System test - configurable paths

This patch adds logic for the following:
- remove hard-coded paths to various scripts and jars in kafkatest service classes
- provide a mechanism for overriding path resolution logic with a ""pluggable"" path resolver class

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1245 from granders/configurable-install-path",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,2,1,0,1
tests/unit/version/__init__.py,tests/unit/version/__init__.py,"KAFKA-3592: System test - configurable paths

This patch adds logic for the following:
- remove hard-coded paths to various scripts and jars in kafkatest service classes
- provide a mechanism for overriding path resolution logic with a ""pluggable"" path resolver class

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1245 from granders/configurable-install-path",0,15,0,0,0,0,0,15,15,15,1,1,15,15,15,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/ResponseSend.java,clients/src/main/java/org/apache/kafka/common/requests/ResponseSend.java,"KAFKA-3149; Extend SASL implementation to support more mechanisms

Code changes corresponding to KIP-43 to enable review of the KIP.

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Jun Rao <junrao@apache.org>, Ismael Juma <ismael@juma.me.uk>

Closes #812 from rajinisivaram/KAFKA-3149",3,1,1,19,162,1,3,41,41,20,2,1.0,42,41,21,1,1,0,1,0,1,1
clients/src/main/java/org/apache/kafka/common/network/LoginType.java,clients/src/main/java/org/apache/kafka/common/network/LoginType.java,"MINOR: Fix typos in code comments

This patch fixes all occurances of two consecutive 'the's in the code comments.

Author: Ishita Mandhan (imandhaus.ibm.com)

Author: Ishita Mandhan <imandha@us.ibm.com>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #1240 from imandhan/typofixes",2,1,1,13,71,0,2,39,39,20,2,1.0,40,39,20,1,1,0,2,1,0,1
core/src/main/scala/kafka/message/MessageAndMetadata.scala,core/src/main/scala/kafka/message/MessageAndMetadata.scala,"KAFKA-3563: Maintain MessageAndMetadata constructor compatibility

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ashish Singh <asingh@cloudera.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1226 from granthenke/message_constructor",4,3,3,14,161,0,2,60,21,9,7,1,69,21,10,9,3,1,2,1,0,1
core/src/main/scala/kafka/utils/timer/TimingWheel.scala,core/src/main/scala/kafka/utils/timer/TimingWheel.scala,"KAFKA-3470: treat commits as member heartbeats

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>

Closes #1206 from hachikuji/KAFKA-3470",11,1,1,49,330,1,3,166,160,42,4,1.0,169,160,42,3,1,1,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableLeftJoin.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamKTableLeftJoin.java,"KAFKA-3521: validate null keys in Streams DSL implementations

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1197 from guozhangwang/K3521",6,5,1,36,301,1,5,66,62,22,3,1,70,62,23,4,3,1,1,0,1,1
tests/kafkatest/benchmarks/__init__.py,tests/kafkatest/benchmarks/__init__.py,"KAFKA-3483: Restructure ducktape tests to simplify running subsets of tests

… tests

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Geoff Anderson <geoff@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #1162 from granthenke/ducktape-structure",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/integration/RollingBounceTest.scala,core/src/test/scala/unit/kafka/integration/RollingBounceTest.scala,"MINOR: Remove a couple of redundant `CoreUtils.rm` methods

Also:
* Rename remaining `CoreUtils.rm` to `delete` for consistency
* Use `try with resources` in `Utils` to simplify code
* Silence compiler warning due to exception catch clause in `TestUtils`

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Guozhang Wang <wangguoz@gmail.com>

Closes #1153 from ijuma/remove-redundant-core-utils-rm",5,1,1,53,446,1,3,96,140,7,14,2.0,181,140,13,85,31,6,2,1,0,1
core/src/main/scala/kafka/producer/BaseProducer.scala,core/src/main/scala/kafka/producer/BaseProducer.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",6,6,0,41,375,1,5,74,69,18,4,2.0,78,69,20,4,2,1,1,0,1,1
core/src/main/scala/kafka/producer/ByteArrayPartitioner.scala,core/src/main/scala/kafka/producer/ByteArrayPartitioner.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",1,2,0,10,85,0,1,30,27,8,4,1.0,31,27,8,1,1,0,1,0,1,1
core/src/main/scala/kafka/producer/KeyedMessage.scala,core/src/main/scala/kafka/producer/KeyedMessage.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",2,2,0,18,131,0,2,44,38,6,8,1.0,84,38,10,40,21,5,1,0,1,1
core/src/main/scala/kafka/producer/Partitioner.scala,core/src/main/scala/kafka/producer/Partitioner.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",0,2,0,6,30,0,0,35,25,7,5,1,43,25,9,8,6,2,2,1,0,1
core/src/main/scala/kafka/producer/ProducerClosedException.scala,core/src/main/scala/kafka/producer/ProducerClosedException.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",0,1,0,4,22,0,0,22,23,7,3,1,26,23,9,4,4,1,1,0,0,0
core/src/main/scala/kafka/producer/ProducerConfig.scala,core/src/main/scala/kafka/producer/ProducerConfig.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",9,4,0,48,309,0,5,139,60,5,26,1.0,235,60,9,96,23,4,2,1,0,1
core/src/main/scala/kafka/producer/ProducerStats.scala,core/src/main/scala/kafka/producer/ProducerStats.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",2,2,0,22,177,0,2,46,40,9,5,1,50,40,10,4,3,1,1,0,1,1
core/src/main/scala/kafka/producer/ProducerTopicStats.scala,core/src/main/scala/kafka/producer/ProducerTopicStats.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",4,3,1,37,309,0,4,69,57,14,5,3,84,57,17,15,9,3,1,0,1,1
core/src/main/scala/kafka/producer/SyncProducerConfig.scala,core/src/main/scala/kafka/producer/SyncProducerConfig.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",1,6,0,29,184,0,1,74,44,4,18,1.0,135,44,8,61,14,3,2,1,0,1
core/src/main/scala/kafka/producer/async/AsyncProducerConfig.scala,core/src/main/scala/kafka/producer/async/AsyncProducerConfig.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",0,2,0,13,96,0,0,49,52,5,9,1,87,52,10,38,18,4,2,1,0,1
core/src/main/scala/kafka/producer/async/IllegalQueueStateException.scala,core/src/main/scala/kafka/producer/async/IllegalQueueStateException.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",1,1,0,5,36,0,1,26,25,13,2,1.0,26,25,13,0,0,0,2,1,0,1
core/src/main/scala/kafka/producer/async/MissingConfigException.scala,core/src/main/scala/kafka/producer/async/MissingConfigException.scala,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",1,1,0,5,36,0,1,24,22,8,3,1,30,22,10,6,6,2,2,1,0,1
core/src/main/scala/kafka/tools/KafkaMigrationTool.java,core/src/main/scala/kafka/tools/KafkaMigrationTool.java,"KAFKA-2982; Mark the old Scala producer and related classes as deprecated

Also update server tests to always use new producer.

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Gwen Shapira

Closes #1092 from ijuma/kafka-2982-deprecate-old-producers",46,1,1,382,2896,0,18,486,362,26,19,2,1060,399,56,574,398,30,2,1,0,1
clients/src/main/java/org/apache/kafka/common/internals/TopicConstants.java,clients/src/main/java/org/apache/kafka/common/internals/TopicConstants.java,"KAFKA-2832: Add a consumer config option to exclude internal topics

A new consumer config option 'exclude.internal.topics' was added to
allow excluding internal topics when wildcards are used to specify
consumers.
The new option takes a boolean value, with a default 'false' value (i.e.
no exclusion).

This patch is co-authored with rajinisivaram edoardocomar mimaison

Author: edoardo <ecomar@uk.ibm.com>
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Ismael Juma, Jun Rao, Gwen Shapira

Closes #1082 from edoardocomar/KAFKA-2832",1,33,0,11,87,1,1,33,33,33,1,1,33,33,33,0,0,0,0,0,0,0
core/src/main/scala/kafka/admin/BrokerMetadata.scala,core/src/main/scala/kafka/admin/BrokerMetadata.scala,"KAFKA-1215; Rack-Aware replica assignment option

Please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment for the overall design.

The update to TopicMetadataRequest/TopicMetadataResponse will be done in a different PR.

Author: Allen Wang <awang@netflix.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>, 	Grant Henke <granthenke@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #132 from allenxwang/KAFKA-1215",0,23,0,2,19,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/main/scala/kafka/admin/RackAwareMode.scala,core/src/main/scala/kafka/admin/RackAwareMode.scala,"KAFKA-1215; Rack-Aware replica assignment option

Please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment for the overall design.

The update to TopicMetadataRequest/TopicMetadataResponse will be done in a different PR.

Author: Allen Wang <awang@netflix.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>, 	Grant Henke <granthenke@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #132 from allenxwang/KAFKA-1215",0,42,0,7,26,0,0,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/Serdes.java,streams/src/main/java/org/apache/kafka/streams/state/Serdes.java,"MINOR: Improve JavaDoc for some public classes.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Mastuda <yasuhiro.mastuda@gmail.com>

Closes #999 from guozhangwang/KJavaDoc",25,6,0,86,787,0,13,136,164,23,6,3.0,224,164,37,88,41,15,2,1,0,1
tests/kafkatest/tests/compatibility_test.py,tests/kafkatest/tests/compatibility_test.py,"KAFKA-3214: Added system tests for compressed topics

Added CompressionTest that tests 4 producers, each using a different compression type and one not using compression.

Enabled VerifiableProducer to run producers with different compression types (passed in the constructor). This includes enabling each producer to output unique values, so that the verification process in ProduceConsumeValidateTest is correct (counts acks from all producers).

Also a fix for console consumer to raise an exception if it sees the incorrect consumer output (before we swallowed an exception, so was hard to debug the issue).

Author: Anna Povzner <anna@confluent.io>

Reviewers: Geoff Anderson, Jason Gustafson

Closes #958 from apovzner/kafka-3214",6,2,1,58,539,0,4,102,101,34,3,1,104,101,35,2,1,1,2,1,0,1
core/src/main/scala/kafka/common/LongRef.scala,core/src/main/scala/kafka/common/LongRef.scala,"KAFKA-3259 KAFKA-3253; KIP-31/KIP-32 Follow-up

This PR includes a number of clean-ups:
* Code style
* Documentation wording improvements
* Efficiency improvements

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #943 from ijuma/kafka-3259-kip-31-32-clean-ups",6,61,0,30,110,6,6,61,61,61,1,1,61,61,61,0,0,0,0,0,0,0
core/src/main/scala/kafka/javaapi/message/ByteBufferMessageSet.scala,core/src/main/scala/kafka/javaapi/message/ByteBufferMessageSet.scala,"KAFKA-3259 KAFKA-3253; KIP-31/KIP-32 Follow-up

This PR includes a number of clean-ups:
* Code style
* Documentation wording improvements
* Efficiency improvements

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #943 from ijuma/kafka-3259-kip-31-32-clean-ups",7,3,2,35,270,1,5,69,96,5,15,2,147,96,10,78,20,5,2,1,0,1
core/src/main/scala/kafka/message/MessageWriter.scala,core/src/main/scala/kafka/message/MessageWriter.scala,"KAFKA-3259 KAFKA-3253; KIP-31/KIP-32 Follow-up

This PR includes a number of clean-ups:
* Code style
* Documentation wording improvements
* Efficiency improvements

Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Jun Rao <junrao@gmail.com>

Closes #943 from ijuma/kafka-3259-kip-31-32-clean-ups",37,1,1,163,1142,1,15,229,206,57,4,1.0,233,206,58,4,2,1,2,1,0,1
clients/src/main/java/org/apache/kafka/common/BrokerEndPoint.java,clients/src/main/java/org/apache/kafka/common/BrokerEndPoint.java,"KAFKA-2757; Consolidate BrokerEndPoint and EndPoint

Author: zhuchen1018 <amandazhu19620701@gmail.com>

Reviewers: Dong Lin <lindong28@gmail.com>, Guozhang Wang <wangguoz@gmail.com>

Closes #911 from zhuchen1018/KAFKA-2757",17,88,0,58,319,7,7,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/internals/RawStoreChangeLogger.java,streams/src/main/java/org/apache/kafka/streams/state/internals/RawStoreChangeLogger.java,"KAFKA-3207: Fix StateChangeLogger to use the right topic name

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #865 from guozhangwang/K3207",7,4,4,31,275,4,4,56,56,28,2,1.5,60,56,30,4,4,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/HoppingWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/HoppingWindows.java,"MINOR: Some more Kafka Streams Javadocs

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda <yasuhiro@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #853 from guozhangwang/KJavaDoc",9,3,0,42,326,0,6,95,79,32,3,1,100,79,33,5,5,2,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/TumblingWindows.java,streams/src/main/java/org/apache/kafka/streams/kstream/TumblingWindows.java,"MINOR: Some more Kafka Streams Javadocs

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda <yasuhiro@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #853 from guozhangwang/KJavaDoc",6,3,1,32,231,0,5,74,67,18,4,2.0,90,67,22,16,13,4,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/examples/KStreamJob.java,streams/src/main/java/org/apache/kafka/streams/examples/KStreamJob.java,"KAFKA-3136: Rename KafkaStreaming to KafkaStreams

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Gwen Shapira

Closes #800 from guozhangwang/KRename",1,12,12,57,502,1,1,84,84,17,5,1,101,84,20,17,12,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/requests/RequestSend.java,clients/src/main/java/org/apache/kafka/common/requests/RequestSend.java,"KAFKA-2071: Replace Producer Request/Response with their org.apache.kafka.common.requests equivalents

This PR replaces all occurrences of kafka.api.ProducerRequest/ProducerResponse by their common equivalents.

Author: David Jacot <david.jacot@gmail.com>

Reviewers: Grant Henke <granthenke@gmail.com>, Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #110 from dajac/KAFKA-2071",5,1,1,30,207,1,5,55,38,9,6,1.0,75,38,12,20,15,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SumAsLong.java,streams/src/main/java/org/apache/kafka/streams/kstream/SumAsLong.java,"KAFKA-3121: Remove aggregatorSupplier and add Reduce functions

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #795 from guozhangwang/K3121s1",3,12,28,15,86,8,3,36,52,18,2,4.0,64,52,32,28,28,14,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamAggWindow.java,"KAFKA-3104: add windowed aggregation to KStream

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Mastuda

Closes #781 from guozhangwang/K3104",3,51,0,24,206,3,3,51,51,51,1,1,51,51,51,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TumblingWindow.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TumblingWindow.java,"KAFKA-3104: add windowed aggregation to KStream

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Mastuda

Closes #781 from guozhangwang/K3104",5,4,4,15,115,6,3,38,38,19,2,2.5,42,38,21,4,4,2,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/WindowsTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/WindowsTest.java,"KAFKA-3104: add windowed aggregation to KStream

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Mastuda

Closes #781 from guozhangwang/K3104",3,70,0,32,324,3,3,70,70,70,1,1,70,70,70,0,0,0,0,0,0,0
connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThread.java,connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThread.java,"KAFKA-2886: Handle sink task rebalance failures by stopping worker task

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Liquan Pei <liquanpei@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #767 from hachikuji/KAFKA-2886",13,15,19,73,443,2,5,112,112,14,8,2.0,145,112,18,33,19,4,1,0,1,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/LongSumSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/LongSumSupplier.java,"KAFKA-3081: KTable Aggregation

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #761 from guozhangwang/K3081",5,52,0,27,167,5,5,52,52,52,1,1,52,52,52,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/examples/KTableJob.java,streams/src/main/java/org/apache/kafka/streams/examples/KTableJob.java,"KAFKA-2653: Add KStream/KTable Aggregation and KTable Join APIs

ping ymatsuda for reviews.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #730 from guozhangwang/K2653r",4,111,0,76,719,1,1,111,111,111,1,1,111,111,111,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToDoubleMapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToDoubleMapper.java,"KAFKA-2653: Add KStream/KTable Aggregation and KTable Join APIs

ping ymatsuda for reviews.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #730 from guozhangwang/K2653r",0,23,0,4,31,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToIntMapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToIntMapper.java,"KAFKA-2653: Add KStream/KTable Aggregation and KTable Join APIs

ping ymatsuda for reviews.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #730 from guozhangwang/K2653r",0,23,0,4,31,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToLongMapper.java,streams/src/main/java/org/apache/kafka/streams/kstream/KeyValueToLongMapper.java,"KAFKA-2653: Add KStream/KTable Aggregation and KTable Join APIs

ping ymatsuda for reviews.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #730 from guozhangwang/K2653r",0,23,0,4,31,0,0,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TopKSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/TopKSupplier.java,"KAFKA-2653: Add KStream/KTable Aggregation and KTable Join APIs

ping ymatsuda for reviews.

Author: Guozhang Wang <wangguoz@gmail.com>

Reviewers: Yasuhiro Matsuda

Closes #730 from guozhangwang/K2653r",11,106,0,62,538,6,6,106,106,106,1,1,106,106,106,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/UpdateMetadataRequest.scala,core/src/main/scala/kafka/api/UpdateMetadataRequest.scala,"KAFKA-2929: Migrate duplicate error mapping functionality

Deprecates ErrorMapping.scala in core in favor or Errors.java in common.
Duplicated exceptions in core are deprecated as well, to ensure the mapping is correct.

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #616 from granthenke/error-mapping",24,5,5,111,862,2,7,147,104,13,11,2,181,104,16,34,10,3,2,1,0,1
core/src/main/scala/kafka/api/UpdateMetadataResponse.scala,core/src/main/scala/kafka/api/UpdateMetadataResponse.scala,"KAFKA-2929: Migrate duplicate error mapping functionality

Deprecates ErrorMapping.scala in core in favor or Errors.java in common.
Duplicated exceptions in core are deprecated as well, to ensure the mapping is correct.

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>

Closes #616 from granthenke/error-mapping",4,3,7,20,126,1,4,42,44,10,4,1.5,51,44,13,9,7,2,2,1,0,1
core/src/main/scala/kafka/common/MessageStreamsExistException.scala,core/src/main/scala/kafka/common/MessageStreamsExistException.scala,"MINOR: Fix typos in code comments

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Gwen Shapira

Closes #673 from vahidhashemian/typo02/fix_typos_in_code_comments",0,1,1,3,24,0,0,23,23,12,2,1.0,24,23,12,1,1,0,2,1,0,1
core/src/main/scala/kafka/message/MessageLengthException.scala,core/src/main/scala/kafka/message/MessageLengthException.scala,"MINOR: Fix typos in code comments

Author: Vahid Hashemian <vahidhashemian@us.ibm.com>

Reviewers: Gwen Shapira

Closes #673 from vahidhashemian/typo02/fix_typos_in_code_comments",0,1,1,2,16,0,0,24,23,8,3,1,31,23,10,7,6,2,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowedImpl.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowedImpl.java,"KAFKA-2962: stream-table table-table joins

guozhangwang

Author: Yasuhiro Matsuda <yasuhiro@confluent.io>

Reviewers: Guozhang Wang

Closes #644 from ymatsuda/join_methods",4,1,1,38,482,1,2,67,54,11,6,2.5,83,54,14,16,6,3,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMerge.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableMerge.java,"KAFKA-2962: stream-table table-table joins

guozhangwang

Author: Yasuhiro Matsuda <yasuhiro@confluent.io>

Reviewers: Guozhang Wang

Closes #644 from ymatsuda/join_methods",12,92,0,57,445,7,7,92,92,92,1,1,92,92,92,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/LeaderAndIsrRequest.scala,core/src/main/scala/kafka/api/LeaderAndIsrRequest.scala,"KAFKA-2958: Remove duplicate API key mapping functionality

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Gwen Shapira

Closes #637 from granthenke/api-keys",22,5,4,163,1197,1,13,206,121,6,32,4.5,443,121,14,237,38,7,2,1,0,1
core/src/main/scala/kafka/api/StopReplicaRequest.scala,core/src/main/scala/kafka/api/StopReplicaRequest.scala,"KAFKA-2958: Remove duplicate API key mapping functionality

Author: Grant Henke <granthenke@gmail.com>

Reviewers: Gwen Shapira

Closes #637 from granthenke/api-keys",15,3,2,98,667,1,7,126,70,5,25,4,251,70,10,125,12,5,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindowSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindowSupplier.java,"KAFKA-2804: manage changelog topics through ZK in PartitionAssignor

Author: Guozhang Wang <wangguoz@gmail.com>
Author: wangguoz@gmail.com <guozhang@Guozhang-Macbook.local>
Author: Guozhang Wang <guozhang@Guozhang-Macbook.local>

Reviewers: Yasuhiro Matsuda

Closes #579 from guozhangwang/K2804",31,1,1,196,1449,1,17,266,265,38,7,2,279,265,40,13,5,2,2,1,0,1
streams/src/test/java/org/apache/kafka/test/UnlimitedWindowDef.java,streams/src/test/java/org/apache/kafka/test/UnlimitedWindowDef.java,"KAFKA-2804: manage changelog topics through ZK in PartitionAssignor

Author: Guozhang Wang <wangguoz@gmail.com>
Author: wangguoz@gmail.com <guozhang@Guozhang-Macbook.local>
Author: Guozhang Wang <guozhang@Guozhang-Macbook.local>

Reviewers: Yasuhiro Matsuda

Closes #579 from guozhangwang/K2804",16,1,1,68,499,1,13,104,104,35,3,1,108,104,36,4,3,1,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableDerivedValueGetterSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableDerivedValueGetterSupplier.java,"KAFKA-2856: Add KTable non-stateful APIs along with standby task support

guozhangwang
* added KTable API and impl
* added standby support for KTable

Author: Yasuhiro Matsuda <yasuhiro@confluent.io>

Reviewers: Guozhang Wang

Closes #604 from ymatsuda/add_ktable",1,28,0,7,62,1,1,28,28,28,1,1,28,28,28,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssginmentInfoTest.java,streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/AssginmentInfoTest.java,"KAFKA-2856: Add KTable non-stateful APIs along with standby task support

guozhangwang
* added KTable API and impl
* added standby support for KTable

Author: Yasuhiro Matsuda <yasuhiro@confluent.io>

Reviewers: Guozhang Wang

Closes #604 from ymatsuda/add_ktable",1,8,3,24,279,1,1,50,45,25,2,2.5,53,45,26,3,3,2,1,0,1,1
core/src/test/scala/unit/kafka/integration/SaslPlaintextTopicMetadataTest.scala,core/src/test/scala/unit/kafka/integration/SaslPlaintextTopicMetadataTest.scala,"KAFKA-2732; Add class for ZK Auth.

Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ben Stopford <benstopford@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #410 from fpj/KAFKA-2732",0,1,0,8,48,0,0,27,26,14,2,1.0,27,26,14,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/integration/SaslSslTopicMetadataTest.scala,core/src/test/scala/unit/kafka/integration/SaslSslTopicMetadataTest.scala,"KAFKA-2732; Add class for ZK Auth.

Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ben Stopford <benstopford@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #410 from fpj/KAFKA-2732",0,1,0,9,65,0,0,29,28,14,2,1.0,29,28,14,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/SaslPlaintextReplicaFetchTest.scala,core/src/test/scala/unit/kafka/server/SaslPlaintextReplicaFetchTest.scala,"KAFKA-2732; Add class for ZK Auth.

Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ben Stopford <benstopford@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #410 from fpj/KAFKA-2732",0,1,0,8,48,0,0,27,26,14,2,1.0,27,26,14,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/SaslSslReplicaFetchTest.scala,core/src/test/scala/unit/kafka/server/SaslSslReplicaFetchTest.scala,"KAFKA-2732; Add class for ZK Auth.

Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Ben Stopford <benstopford@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #410 from fpj/KAFKA-2732",0,1,0,9,65,0,0,29,28,14,2,1.0,29,28,14,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/StreamEndException.scala,core/src/main/scala/kafka/common/StreamEndException.scala,"Minor: Missing License

Author: Sriharsha Chintalapani <harsha@hortonworks.com>

Reviewers: Gwen Shapira

Closes #524 from harshach/missing-license",0,16,0,3,12,0,0,23,16,12,2,1.0,23,16,12,0,0,0,1,0,0,0
core/src/main/scala/kafka/common/AuthorizationException.scala,core/src/main/scala/kafka/common/AuthorizationException.scala,"KAFKA-2691: Improve handling of authorization failure during metadata refresh

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jun Rao

Closes #394 from hachikuji/KAFKA-2691",3,12,1,12,88,1,3,36,24,12,3,1,37,24,12,1,1,0,2,1,0,1
tests/kafkatest/services/security/__init__.py,tests/kafkatest/services/security/__init__.py,"KAFKA-2644; Run relevant ducktape tests with SASL_PLAINTEXT and SASL_SSL

Run sanity check, replication tests and benchmarks with SASL/Kerberos using MiniKdc.

Author: Rajini Sivaram <rajinisivaram@googlemail.com>

Reviewers: Geoff Anderson <geoff@confluent.io>, Jun Rao <junrao@gmail.com>

Closes #358 from rajinisivaram/KAFKA-2644",0,15,0,0,0,0,0,15,15,15,1,1,15,15,15,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/RequestKeys.scala,core/src/main/scala/kafka/api/RequestKeys.scala,"KAFKA-2687: Add support for ListGroups and DescribeGroup APIs

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Guozhang Wang, Jun Rao

Closes #388 from hachikuji/K2687",6,7,3,47,340,0,2,73,25,4,18,1.5,112,31,6,39,6,2,2,1,0,1
tests/kafkatest/services/kafka/directory.py,tests/kafkatest/services/kafka/directory.py,"KAFKA-1888: rolling upgrade test

ewencp gwenshap
This needs some refactoring to avoid the duplicated code between replication test and upgrade test, but in shape for initial feedback.

I'm interested in feedback on the added `KafkaConfig` class and `kafka_props` file. This addition makes it:
- easier to attach different configs to different nodes (e.g. during broker upgrade process)
- easier to reason about the configuration of a particular node

Notes:
- in the default values in the KafkaConfig class, I removed many properties which were in kafka.properties before. This is because most of those properties were set to what is already the default value.
- when running non-trunk VerifiableProducer, I append the trunk tools jar to the classpath, and run it with the non-trunk kafka-run-class.sh script

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Dong Lin, Ewen Cheslack-Postava

Closes #229 from granders/KAFKA-1888-upgrade-test",3,32,0,7,39,1,1,32,32,32,1,1,32,32,32,0,0,0,2,1,0,1
tests/kafkatest/services/monitor/__init__.py,tests/kafkatest/services/monitor/__init__.py,"KAFKA-1888: rolling upgrade test

ewencp gwenshap
This needs some refactoring to avoid the duplicated code between replication test and upgrade test, but in shape for initial feedback.

I'm interested in feedback on the added `KafkaConfig` class and `kafka_props` file. This addition makes it:
- easier to attach different configs to different nodes (e.g. during broker upgrade process)
- easier to reason about the configuration of a particular node

Notes:
- in the default values in the KafkaConfig class, I removed many properties which were in kafka.properties before. This is because most of those properties were set to what is already the default value.
- when running non-trunk VerifiableProducer, I append the trunk tools jar to the classpath, and run it with the non-trunk kafka-run-class.sh script

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Dong Lin, Ewen Cheslack-Postava

Closes #229 from granders/KAFKA-1888-upgrade-test",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamJoinTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamJoinTest.java,"KAFKA-2652: integrate new group protocol into partition grouping

guozhangwang

* added ```PartitionGrouper``` (abstract class)
 * This class is responsible for grouping partitions. Each group forms a task.
 * Users may implement this class for custom grouping.
* added ```DefaultPartitionGrouper```
 * our default implementation of ```PartitionGrouper```
* added ```KafkaStreamingPartitionAssignor```
 * We always use this as ```PartitionAssignor``` of stream consumers.
 * Actual grouping is delegated to ```PartitionGrouper```.
* ```TopologyBuilder```
 * added ```topicGroups()```
   * This returns groups of related topics according to the topology
 * added ```copartitionSources(sourceNodes...)```
   * This is used by DSL layer. It asserts the specified source nodes must be copartitioned.
 * added ```copartitionGroups()```
   * This returns groups of copartitioned topics
* KStream layer
 * keep track of source nodes to determine copartition sources when steams are joined
 * source nodes are set to null when partitioning property is not preserved (ex. ```map()```, ```transform()```), and this indicates the stream is no longer joinable

Author: Yasuhiro Matsuda <yasuhiro@confluent.io>

Reviewers: Guozhang Wang

Closes #353 from ymatsuda/grouping",14,32,1,129,1331,2,7,195,164,65,3,3,199,164,66,4,3,1,2,1,0,1
core/src/test/scala/other/kafka/DeleteZKPath.scala,core/src/test/scala/other/kafka/DeleteZKPath.scala,"KAFKA-2641; Upgrade path for ZK authentication

This pull request adds a configuration parameter and a migration tool. It is also based on pull request #303, which should go in first.

Author: flavio junqueira <fpj@apache.org>
Author: Flavio Junqueira <fpj@apache.org>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #313 from fpj/KAFKA-2641",4,0,1,22,158,1,1,44,44,6,7,2,59,44,8,15,6,2,2,1,0,1
core/src/main/scala/kafka/coordinator/CoordinatorMetadata.scala,core/src/main/scala/kafka/coordinator/CoordinatorMetadata.scala,"KAFKA-2464: client-side assignment for new consumer

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jiangjie Qin, Onur Karaman, Ewen Cheslack-Postava, Guozhang Wang

Closes #165 from hachikuji/KAFKA-2464",5,8,152,32,186,18,4,81,225,14,6,3.5,247,225,41,166,152,28,2,1,0,1
core/src/main/scala/kafka/server/OffsetManager.scala,core/src/main/scala/kafka/server/OffsetManager.scala,"KAFKA-2464: client-side assignment for new consumer

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jiangjie Qin, Onur Karaman, Ewen Cheslack-Postava, Guozhang Wang

Closes #165 from hachikuji/KAFKA-2464",68,8,8,419,3168,5,22,627,480,30,21,2,809,480,39,182,72,9,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/CoordinatorMetadataTest.scala,core/src/test/scala/unit/kafka/coordinator/CoordinatorMetadataTest.scala,"KAFKA-2464: client-side assignment for new consumer

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Jiangjie Qin, Onur Karaman, Ewen Cheslack-Postava, Guozhang Wang

Closes #165 from hachikuji/KAFKA-2464",5,8,152,42,235,17,5,71,213,14,5,1,250,213,50,179,152,36,2,1,0,1
core/src/test/scala/unit/kafka/integration/PlaintextTopicMetadataTest.scala,core/src/test/scala/unit/kafka/integration/PlaintextTopicMetadataTest.scala,"KAFKA-1686; Implement SASL/Kerberos

This PR implements SASL/Kerberos which was originally submitted by harshach as https://github.com/apache/kafka/pull/191.

I've been submitting PRs to Harsha's branch with fixes and improvements and he has integrated all, but the most recent one. I'm creating this PR so that the Jenkins can run the tests on the branch (they pass locally).

Author: Ismael Juma <ismael@juma.me.uk>
Author: Sriharsha Chintalapani <harsha@hortonworks.com>
Author: Harsha <harshach@users.noreply.github.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Parth Brahmbhatt <brahmbhatt.parth@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #334 from ijuma/KAFKA-1686-V1",0,3,0,6,34,0,0,26,23,13,2,1.5,26,23,13,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/integration/SslTopicMetadataTest.scala,core/src/test/scala/unit/kafka/integration/SslTopicMetadataTest.scala,"KAFKA-1686; Implement SASL/Kerberos

This PR implements SASL/Kerberos which was originally submitted by harshach as https://github.com/apache/kafka/pull/191.

I've been submitting PRs to Harsha's branch with fixes and improvements and he has integrated all, but the most recent one. I'm creating this PR so that the Jenkins can run the tests on the branch (they pass locally).

Author: Ismael Juma <ismael@juma.me.uk>
Author: Sriharsha Chintalapani <harsha@hortonworks.com>
Author: Harsha <harshach@users.noreply.github.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Parth Brahmbhatt <brahmbhatt.parth@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #334 from ijuma/KAFKA-1686-V1",0,4,1,7,51,0,0,27,24,14,2,1.5,28,24,14,1,1,0,2,1,0,1
core/src/test/scala/unit/kafka/server/PlaintextReplicaFetchTest.scala,core/src/test/scala/unit/kafka/server/PlaintextReplicaFetchTest.scala,"KAFKA-1686; Implement SASL/Kerberos

This PR implements SASL/Kerberos which was originally submitted by harshach as https://github.com/apache/kafka/pull/191.

I've been submitting PRs to Harsha's branch with fixes and improvements and he has integrated all, but the most recent one. I'm creating this PR so that the Jenkins can run the tests on the branch (they pass locally).

Author: Ismael Juma <ismael@juma.me.uk>
Author: Sriharsha Chintalapani <harsha@hortonworks.com>
Author: Harsha <harshach@users.noreply.github.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Parth Brahmbhatt <brahmbhatt.parth@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #334 from ijuma/KAFKA-1686-V1",0,3,0,6,34,0,0,25,22,12,2,1.5,25,22,12,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/server/SslReplicaFetchTest.scala,core/src/test/scala/unit/kafka/server/SslReplicaFetchTest.scala,"KAFKA-1686; Implement SASL/Kerberos

This PR implements SASL/Kerberos which was originally submitted by harshach as https://github.com/apache/kafka/pull/191.

I've been submitting PRs to Harsha's branch with fixes and improvements and he has integrated all, but the most recent one. I'm creating this PR so that the Jenkins can run the tests on the branch (they pass locally).

Author: Ismael Juma <ismael@juma.me.uk>
Author: Sriharsha Chintalapani <harsha@hortonworks.com>
Author: Harsha <harshach@users.noreply.github.com>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>, Parth Brahmbhatt <brahmbhatt.parth@gmail.com>, Jun Rao <junrao@gmail.com>

Closes #334 from ijuma/KAFKA-1686-V1",0,4,1,7,51,0,0,27,24,14,2,1.5,28,24,14,1,1,0,2,1,0,1
core/src/main/scala/kafka/coordinator/ConsumerCoordinator.scala,core/src/main/scala/kafka/coordinator/ConsumerCoordinator.scala,"KAFKA-2639: Refactoring of ZkUtils

I've split the work of KAFKA-1695 because this refactoring touches a large number of files. Most of the changes are trivial, but I feel it will be easier to review this way.

This pull request includes the one Parth-Brahmbhatt started to address KAFKA-1695.

Author: flavio junqueira <fpj@apache.org>
Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #303 from fpj/KAFKA-2639",88,8,8,404,2978,7,32,535,333,38,14,4.0,907,333,65,372,256,27,2,1,0,1
core/src/test/scala/integration/kafka/api/SSLConsumerTest.scala,core/src/test/scala/integration/kafka/api/SSLConsumerTest.scala,"KAFKA-2639: Refactoring of ZkUtils

I've split the work of KAFKA-1695 because this refactoring touches a large number of files. Most of the changes are trivial, but I feel it will be easier to review this way.

This pull request includes the one Parth-Brahmbhatt started to address KAFKA-1695.

Author: flavio junqueira <fpj@apache.org>
Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #303 from fpj/KAFKA-2639",17,3,3,172,1560,2,11,231,251,33,7,2,269,251,38,38,22,5,2,1,0,1
core/src/test/scala/integration/kafka/api/SSLProducerSendTest.scala,core/src/test/scala/integration/kafka/api/SSLProducerSendTest.scala,"KAFKA-2639: Refactoring of ZkUtils

I've split the work of KAFKA-1695 because this refactoring touches a large number of files. Most of the changes are trivial, but I feel it will be easier to review this way.

This pull request includes the one Parth-Brahmbhatt started to address KAFKA-1695.

Author: flavio junqueira <fpj@apache.org>
Author: Flavio Junqueira <fpj@apache.org>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #303 from fpj/KAFKA-2639",23,3,3,150,1357,3,7,240,240,120,2,2.0,243,240,122,3,3,2,1,0,1,1
core/src/main/scala/kafka/coordinator/ConsumerMetadata.scala,core/src/main/scala/kafka/coordinator/ConsumerMetadata.scala,"KAFKA-2397: add leave group request to force coordinator trigger rebalance

Let's say every consumer in a group has session timeout s. Currently, if a consumer leaves the group, the worst case time to stabilize the group is 2s (s to detect the consumer failure + s for the rebalance window). If a consumer instead can declare they are leaving the group, the worst case time to stabilize the group would just be the s associated with the rebalance window.

This is a low priority optimization!

Author: Onur Karaman <okaraman@linkedin.com>

Reviewers: Jason Gustafson, Guozhang Wang

Closes #103 from onurkaraman/leave-group",0,1,0,13,91,0,0,50,36,12,4,2.5,67,36,17,17,11,4,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/ConsumerCoordinatorResponseTest.scala,core/src/test/scala/unit/kafka/coordinator/ConsumerCoordinatorResponseTest.scala,"KAFKA-2397: add leave group request to force coordinator trigger rebalance

Let's say every consumer in a group has session timeout s. Currently, if a consumer leaves the group, the worst case time to stabilize the group is 2s (s to detect the consumer failure + s for the rebalance window). If a consumer instead can declare they are leaving the group, the worst case time to stabilize the group would just be the s associated with the rebalance window.

This is a low priority optimization!

Author: Onur Karaman <okaraman@linkedin.com>

Reviewers: Jason Gustafson, Guozhang Wang

Closes #103 from onurkaraman/leave-group",28,69,0,354,2578,6,28,447,293,56,8,3.5,470,293,59,23,6,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/Coordinator.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/Coordinator.java,"MINOR: ignore wakeups when committing offsets on consumer close

Author: Jason Gustafson <jason@confluent.io>

Reviewers: Gwen Shapira

Closes #306 from hachikuji/handle-wakeup-in-consumer-close",108,13,1,629,4720,2,35,848,595,34,25,3,1497,595,60,649,243,26,2,1,0,1
streams/src/main/java/org/apache/kafka/streams/kstream/WindowSupplier.java,streams/src/main/java/org/apache/kafka/streams/kstream/WindowSupplier.java,"KAFKA-2600: Align Kafka Streams' interfaces with Java 8 functional interfaces

A few of Kafka Stream's interfaces and classes are not as well-aligned with Java 8's functional interfaces. By making these changes, when Kafka moves to Java 8 these classes can extend standard Java 8 functional interfaces while remaining backward compatible. This will make it easier for developers to use Kafka Streams, and may allow us to eventually remove these custom interfaces and just use the standard Java 8 interfaces.

The changes include:

1. The 'apply' method of KStream's `Predicate` functional interface was renamed to `test` to match the method name on `java.util.function.BiPredicate`. This will allow KStream's `Predicate` to extend `BiPredicate` when Kafka moves to Java 8, and for the `KStream.filter` and `filterOut` methods to accept `BiPredicate`.
2. Renamed the `ProcessorDef` and `WindowDef` interfaces to `ProcessorSupplier` and `WindowSupplier`, respectively. Also the `SlidingWindowDef` class was renamed to `SlidingWindowSupplier`, and the `MockProcessorDef` test class was renamed to `MockProcessorSupplier`. The `instance()` method in all were renamed to `get()`, so that all of these can extend/implement Java 8's `java.util.function.Supplier<T>` interface in the future with no other changes and while remaining backward compatible. Variable names that used some form of ""def"" were changed to use ""supplier"".

These two sets of changes were made in separate commits.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Ismael Juma, Guozhang Wang

Closes #270 from rhauch/kafka-2600",0,2,2,5,36,0,0,25,25,12,2,1.5,27,25,14,2,2,1,2,1,0,1
streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowedTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowedTest.java,"KAFKA-2600: Align Kafka Streams' interfaces with Java 8 functional interfaces

A few of Kafka Stream's interfaces and classes are not as well-aligned with Java 8's functional interfaces. By making these changes, when Kafka moves to Java 8 these classes can extend standard Java 8 functional interfaces while remaining backward compatible. This will make it easier for developers to use Kafka Streams, and may allow us to eventually remove these custom interfaces and just use the standard Java 8 interfaces.

The changes include:

1. The 'apply' method of KStream's `Predicate` functional interface was renamed to `test` to match the method name on `java.util.function.BiPredicate`. This will allow KStream's `Predicate` to extend `BiPredicate` when Kafka moves to Java 8, and for the `KStream.filter` and `filterOut` methods to accept `BiPredicate`.
2. Renamed the `ProcessorDef` and `WindowDef` interfaces to `ProcessorSupplier` and `WindowSupplier`, respectively. Also the `SlidingWindowDef` class was renamed to `SlidingWindowSupplier`, and the `MockProcessorDef` test class was renamed to `MockProcessorSupplier`. The `instance()` method in all were renamed to `get()`, so that all of these can extend/implement Java 8's `java.util.function.Supplier<T>` interface in the future with no other changes and while remaining backward compatible. Variable names that used some form of ""def"" were changed to use ""supplier"".

These two sets of changes were made in separate commits.

Author: Randall Hauch <rhauch@gmail.com>

Reviewers: Ismael Juma, Guozhang Wang

Closes #270 from rhauch/kafka-2600",5,4,4,53,544,1,2,91,91,46,2,2.5,95,91,48,4,4,2,2,1,0,1
tests/kafkatest/sanity_checks/test_mirror_maker.py,tests/kafkatest/sanity_checks/test_mirror_maker.py,"KAFKA-2573: Mirror maker system test hangs and eventually fails

Author: Ashish Singh <asingh@cloudera.com>

Reviewers: Geoff Anderson, Guozhang Wang

Closes #234 from SinghAsDev/KAFKA-2573",3,2,2,42,430,1,3,90,90,45,2,1.5,92,90,46,2,2,1,1,0,1,1
core/src/main/scala/kafka/common/BaseEnum.scala,core/src/main/scala/kafka/common/BaseEnum.scala,"KAFKA-2212: Authorizer CLI implementation.

Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Author: Ismael Juma <ismael@juma.me.uk>

Reviewers: Ismael Juma <ismael@juma.me.uk>, Jun Rao <junrao@gmail.com>

Closes #230 from Parth-Brahmbhatt/KAFKA-2212",0,26,0,4,16,0,0,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/FilteredIterator.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/FilteredIterator.java,"KIP-28: Add a processor client for Kafka Streaming

This work has been contributed by Jesse Anderson, Randall Hauch, Yasuhiro Matsuda and Guozhang Wang. The detailed design can be found in https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client.

Author: Guozhang Wang <wangguoz@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro.matsuda@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro@confluent.io>
Author: ymatsuda <yasuhiro.matsuda@gmail.com>
Author: Randall Hauch <rhauch@gmail.com>
Author: Jesse Anderson <jesse@smokinghand.com>
Author: Ismael Juma <ismael@juma.me.uk>
Author: Jesse Anderson <eljefe6a@gmail.com>

Reviewers: Ismael Juma, Randall Hauch, Edward Ribeiro, Gwen Shapira, Jun Rao, Jay Kreps, Yasuhiro Matsuda, Guozhang Wang

Closes #130 from guozhangwang/streaming",7,63,0,35,176,5,5,63,63,63,1,1,63,63,63,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowSupport.java,streams/src/main/java/org/apache/kafka/streams/kstream/internals/WindowSupport.java,"KIP-28: Add a processor client for Kafka Streaming

This work has been contributed by Jesse Anderson, Randall Hauch, Yasuhiro Matsuda and Guozhang Wang. The detailed design can be found in https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client.

Author: Guozhang Wang <wangguoz@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro.matsuda@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro@confluent.io>
Author: ymatsuda <yasuhiro.matsuda@gmail.com>
Author: Randall Hauch <rhauch@gmail.com>
Author: Jesse Anderson <jesse@smokinghand.com>
Author: Ismael Juma <ismael@juma.me.uk>
Author: Jesse Anderson <eljefe6a@gmail.com>

Reviewers: Ismael Juma, Randall Hauch, Edward Ribeiro, Gwen Shapira, Jun Rao, Jay Kreps, Yasuhiro Matsuda, Guozhang Wang

Closes #130 from guozhangwang/streaming",28,159,0,111,787,19,19,159,159,159,1,1,159,159,159,0,0,0,0,0,0,0
streams/src/main/java/org/apache/kafka/streams/state/Entry.java,streams/src/main/java/org/apache/kafka/streams/state/Entry.java,"KIP-28: Add a processor client for Kafka Streaming

This work has been contributed by Jesse Anderson, Randall Hauch, Yasuhiro Matsuda and Guozhang Wang. The detailed design can be found in https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client.

Author: Guozhang Wang <wangguoz@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro.matsuda@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro@confluent.io>
Author: ymatsuda <yasuhiro.matsuda@gmail.com>
Author: Randall Hauch <rhauch@gmail.com>
Author: Jesse Anderson <jesse@smokinghand.com>
Author: Ismael Juma <ismael@juma.me.uk>
Author: Jesse Anderson <eljefe6a@gmail.com>

Reviewers: Ismael Juma, Randall Hauch, Edward Ribeiro, Gwen Shapira, Jun Rao, Jay Kreps, Yasuhiro Matsuda, Guozhang Wang

Closes #130 from guozhangwang/streaming",4,42,0,18,96,4,4,42,42,42,1,1,42,42,42,0,0,0,0,0,0,0
streams/src/test/java/org/apache/kafka/streams/kstream/internals/FilteredIteratorTest.java,streams/src/test/java/org/apache/kafka/streams/kstream/internals/FilteredIteratorTest.java,"KIP-28: Add a processor client for Kafka Streaming

This work has been contributed by Jesse Anderson, Randall Hauch, Yasuhiro Matsuda and Guozhang Wang. The detailed design can be found in https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client.

Author: Guozhang Wang <wangguoz@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro.matsuda@gmail.com>
Author: Yasuhiro Matsuda <yasuhiro@confluent.io>
Author: ymatsuda <yasuhiro.matsuda@gmail.com>
Author: Randall Hauch <rhauch@gmail.com>
Author: Jesse Anderson <jesse@smokinghand.com>
Author: Ismael Juma <ismael@juma.me.uk>
Author: Jesse Anderson <eljefe6a@gmail.com>

Reviewers: Ismael Juma, Randall Hauch, Edward Ribeiro, Gwen Shapira, Jun Rao, Jay Kreps, Yasuhiro Matsuda, Guozhang Wang

Closes #130 from guozhangwang/streaming",9,94,0,57,493,3,3,94,94,94,1,1,94,94,94,0,0,0,0,0,0,0
tests/kafkatest/services/performance.py,tests/kafkatest/services/performance.py,"KAFKA-2453: Enable new consumer in EndToEndLatency

Author: Ben Stopford <benstopford@gmail.com>

Reviewers: Gwen Shapira, Jason Gustafson

Closes #158 from benstopford/KAFKA-2453b",20,2,2,128,1231,1,9,163,163,82,2,1.0,165,163,82,2,2,1,2,1,0,1
core/src/main/scala/kafka/server/ThrottledResponse.scala,core/src/main/scala/kafka/server/ThrottledResponse.scala,"KAFKA-2136; Add throttle time (on quota violation) in fetch/produce
responses; reviewed by Joel Koshy, Dong Lin and Jun Rao",5,4,4,16,147,1,3,46,46,23,2,2.0,50,46,25,4,4,2,1,0,1,1
copycat/data/src/main/java/org/apache/kafka/copycat/data/ObjectProperties.java,copycat/data/src/main/java/org/apache/kafka/copycat/data/ObjectProperties.java,"KAFKA-2366; Initial patch for Copycat

This is an initial patch implementing the basics of Copycat for KIP-26.

The intent here is to start a review of the key pieces of the core API and get a reasonably functional, baseline, non-distributed implementation of Copycat in place to get things rolling. The current patch has a number of known issues that need to be addressed before a final version:

* Some build-related issues. Specifically, requires some locally-installed dependencies (see below), ignores checkstyle for the runtime data library because it's lifted from Avro currently and likely won't last in its current form, and some Gradle task dependencies aren't quite right because I haven't gotten rid of the dependency on `core` (which should now be an easy patch since new consumer groups are in a much better state).
* This patch currently depends on some Confluent trunk code because I prototyped with our Avro serializers w/ schema-registry support. We need to figure out what we want to provide as an example built-in set of serializers. Unlike core Kafka where we could ignore the issue, providing only ByteArray or String serializers, this is pretty central to how Copycat works.
* This patch uses a hacked up version of Avro as its runtime data format. Not sure if we want to go through the entire API discussion just to get some basic code committed, so I filed KAFKA-2367 to handle that separately. The core connector APIs and the runtime data APIs are entirely orthogonal.
* This patch needs some updates to get aligned with recent new consumer changes (specifically, I'm aware of the ConcurrentModificationException issue on exit). More generally, the new consumer is in flux but Copycat depends on it, so there are likely to be some negative interactions.
* The layout feels a bit awkward to me right now because I ported it from a Maven layout. We don't have nearly the same level of granularity in Kafka currently (core and clients, plus the mostly ignored examples, log4j-appender, and a couple of contribs). We might want to reorganize, although keeping data+api separate from runtime and connector plugins is useful for minimizing dependencies.
* There are a variety of other things (e.g., I'm not happy with the exception hierarchy/how they are currently handled, TopicPartition doesn't really need to be duplicated unless we want Copycat entirely isolated from the Kafka APIs, etc), but I expect those we'll cover in the review.

Before commenting on the patch, it's probably worth reviewing https://issues.apache.org/jira/browse/KAFKA-2365 and https://issues.apache.org/jira/browse/KAFKA-2366 to get an idea of what I had in mind for a) what we ultimately want with all the Copycat patches and b) what we aim to cover in this initial patch. My hope is that we can use a WIP patch (after the current obvious deficiencies are addressed) while recognizing that we want to make iterative progress with a bunch of subsequent PRs.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma, Gwen Shapira

Closes #99 from ewencp/copycat and squashes the following commits:

a3a47a6 [Ewen Cheslack-Postava] Simplify Copycat exceptions, make them a subclass of KafkaException.
8c108b0 [Ewen Cheslack-Postava] Rename Coordinator to Herder to avoid confusion with the consumer coordinator.
7bf8075 [Ewen Cheslack-Postava] Make Copycat CLI speific to standalone mode, clean up some config and get rid of config storage in standalone mode.
656a003 [Ewen Cheslack-Postava] Clarify and expand the explanation of the Copycat Coordinator interface.
c0e5fdc [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
0fa7a36 [Ewen Cheslack-Postava] Mark Copycat classes as unstable and reduce visibility of some classes where possible.
d55d31e [Ewen Cheslack-Postava] Reorganize Copycat code to put it all under one top-level directory.
b29cb2c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
d713a21 [Ewen Cheslack-Postava] Address Gwen's review comments.
6787a85 [Ewen Cheslack-Postava] Make Converter generic to match serializers since some serialization formats do not require a base class of Object; update many other classes to have generic key and value class type parameters to match this change.
b194c73 [Ewen Cheslack-Postava] Split Copycat converter option into two options for key and value.
0b5a1a0 [Ewen Cheslack-Postava] Normalize naming to use partition for both source and Kafka, adjusting naming in CopycatRecord classes to clearly differentiate.
e345142 [Ewen Cheslack-Postava] Remove Copycat reflection utils, use existing Utils and ConfigDef functionality from clients package.
be5c387 [Ewen Cheslack-Postava] Minor cleanup
122423e [Ewen Cheslack-Postava] Style cleanup
6ba87de [Ewen Cheslack-Postava] Remove most of the Avro-based mock runtime data API, only preserving enough schema functionality to support basic primitive types for an initial patch.
4674d13 [Ewen Cheslack-Postava] Address review comments, clean up some code styling.
25b5739 [Ewen Cheslack-Postava] Fix sink task offset commit concurrency issue by moving it to the worker thread and waking up the consumer to ensure it exits promptly.
0aefe21 [Ewen Cheslack-Postava] Add log4j settings for Copycat.
220e42d [Ewen Cheslack-Postava] Replace Avro serializer with JSON serializer.
1243a7c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
5a618c6 [Ewen Cheslack-Postava] Remove offset serializers, instead reusing the existing serializers and removing schema projection support.
e849e10 [Ewen Cheslack-Postava] Remove duplicated TopicPartition implementation.
dec1379 [Ewen Cheslack-Postava] Switch to using new consumer coordinator instead of manually assigning partitions. Remove dependency of copycat-runtime on core.
4a9b4f3 [Ewen Cheslack-Postava] Add some helpful Copycat-specific build and test targets that cover all Copycat packages.
31cd1ca [Ewen Cheslack-Postava] Add CLI tools for Copycat.
e14942c [Ewen Cheslack-Postava] Add Copycat file connector.
0233456 [Ewen Cheslack-Postava] Add copycat-avro and copycat-runtime
11981d2 [Ewen Cheslack-Postava] Add copycat-data and copycat-api",10,85,0,34,243,5,5,85,85,85,1,1,85,85,85,0,0,0,2,1,0,1
copycat/data/src/main/java/org/apache/kafka/copycat/data/Schema.java,copycat/data/src/main/java/org/apache/kafka/copycat/data/Schema.java,"KAFKA-2366; Initial patch for Copycat

This is an initial patch implementing the basics of Copycat for KIP-26.

The intent here is to start a review of the key pieces of the core API and get a reasonably functional, baseline, non-distributed implementation of Copycat in place to get things rolling. The current patch has a number of known issues that need to be addressed before a final version:

* Some build-related issues. Specifically, requires some locally-installed dependencies (see below), ignores checkstyle for the runtime data library because it's lifted from Avro currently and likely won't last in its current form, and some Gradle task dependencies aren't quite right because I haven't gotten rid of the dependency on `core` (which should now be an easy patch since new consumer groups are in a much better state).
* This patch currently depends on some Confluent trunk code because I prototyped with our Avro serializers w/ schema-registry support. We need to figure out what we want to provide as an example built-in set of serializers. Unlike core Kafka where we could ignore the issue, providing only ByteArray or String serializers, this is pretty central to how Copycat works.
* This patch uses a hacked up version of Avro as its runtime data format. Not sure if we want to go through the entire API discussion just to get some basic code committed, so I filed KAFKA-2367 to handle that separately. The core connector APIs and the runtime data APIs are entirely orthogonal.
* This patch needs some updates to get aligned with recent new consumer changes (specifically, I'm aware of the ConcurrentModificationException issue on exit). More generally, the new consumer is in flux but Copycat depends on it, so there are likely to be some negative interactions.
* The layout feels a bit awkward to me right now because I ported it from a Maven layout. We don't have nearly the same level of granularity in Kafka currently (core and clients, plus the mostly ignored examples, log4j-appender, and a couple of contribs). We might want to reorganize, although keeping data+api separate from runtime and connector plugins is useful for minimizing dependencies.
* There are a variety of other things (e.g., I'm not happy with the exception hierarchy/how they are currently handled, TopicPartition doesn't really need to be duplicated unless we want Copycat entirely isolated from the Kafka APIs, etc), but I expect those we'll cover in the review.

Before commenting on the patch, it's probably worth reviewing https://issues.apache.org/jira/browse/KAFKA-2365 and https://issues.apache.org/jira/browse/KAFKA-2366 to get an idea of what I had in mind for a) what we ultimately want with all the Copycat patches and b) what we aim to cover in this initial patch. My hope is that we can use a WIP patch (after the current obvious deficiencies are addressed) while recognizing that we want to make iterative progress with a bunch of subsequent PRs.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma, Gwen Shapira

Closes #99 from ewencp/copycat and squashes the following commits:

a3a47a6 [Ewen Cheslack-Postava] Simplify Copycat exceptions, make them a subclass of KafkaException.
8c108b0 [Ewen Cheslack-Postava] Rename Coordinator to Herder to avoid confusion with the consumer coordinator.
7bf8075 [Ewen Cheslack-Postava] Make Copycat CLI speific to standalone mode, clean up some config and get rid of config storage in standalone mode.
656a003 [Ewen Cheslack-Postava] Clarify and expand the explanation of the Copycat Coordinator interface.
c0e5fdc [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
0fa7a36 [Ewen Cheslack-Postava] Mark Copycat classes as unstable and reduce visibility of some classes where possible.
d55d31e [Ewen Cheslack-Postava] Reorganize Copycat code to put it all under one top-level directory.
b29cb2c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
d713a21 [Ewen Cheslack-Postava] Address Gwen's review comments.
6787a85 [Ewen Cheslack-Postava] Make Converter generic to match serializers since some serialization formats do not require a base class of Object; update many other classes to have generic key and value class type parameters to match this change.
b194c73 [Ewen Cheslack-Postava] Split Copycat converter option into two options for key and value.
0b5a1a0 [Ewen Cheslack-Postava] Normalize naming to use partition for both source and Kafka, adjusting naming in CopycatRecord classes to clearly differentiate.
e345142 [Ewen Cheslack-Postava] Remove Copycat reflection utils, use existing Utils and ConfigDef functionality from clients package.
be5c387 [Ewen Cheslack-Postava] Minor cleanup
122423e [Ewen Cheslack-Postava] Style cleanup
6ba87de [Ewen Cheslack-Postava] Remove most of the Avro-based mock runtime data API, only preserving enough schema functionality to support basic primitive types for an initial patch.
4674d13 [Ewen Cheslack-Postava] Address review comments, clean up some code styling.
25b5739 [Ewen Cheslack-Postava] Fix sink task offset commit concurrency issue by moving it to the worker thread and waking up the consumer to ensure it exits promptly.
0aefe21 [Ewen Cheslack-Postava] Add log4j settings for Copycat.
220e42d [Ewen Cheslack-Postava] Replace Avro serializer with JSON serializer.
1243a7c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
5a618c6 [Ewen Cheslack-Postava] Remove offset serializers, instead reusing the existing serializers and removing schema projection support.
e849e10 [Ewen Cheslack-Postava] Remove duplicated TopicPartition implementation.
dec1379 [Ewen Cheslack-Postava] Switch to using new consumer coordinator instead of manually assigning partitions. Remove dependency of copycat-runtime on core.
4a9b4f3 [Ewen Cheslack-Postava] Add some helpful Copycat-specific build and test targets that cover all Copycat packages.
31cd1ca [Ewen Cheslack-Postava] Add CLI tools for Copycat.
e14942c [Ewen Cheslack-Postava] Add Copycat file connector.
0233456 [Ewen Cheslack-Postava] Add copycat-avro and copycat-runtime
11981d2 [Ewen Cheslack-Postava] Add copycat-data and copycat-api",239,1054,0,784,4610,124,137,1054,1054,1054,1,1,1054,1054,1054,0,0,0,2,1,0,1
copycat/data/src/main/java/org/apache/kafka/copycat/data/SchemaBuilder.java,copycat/data/src/main/java/org/apache/kafka/copycat/data/SchemaBuilder.java,"KAFKA-2366; Initial patch for Copycat

This is an initial patch implementing the basics of Copycat for KIP-26.

The intent here is to start a review of the key pieces of the core API and get a reasonably functional, baseline, non-distributed implementation of Copycat in place to get things rolling. The current patch has a number of known issues that need to be addressed before a final version:

* Some build-related issues. Specifically, requires some locally-installed dependencies (see below), ignores checkstyle for the runtime data library because it's lifted from Avro currently and likely won't last in its current form, and some Gradle task dependencies aren't quite right because I haven't gotten rid of the dependency on `core` (which should now be an easy patch since new consumer groups are in a much better state).
* This patch currently depends on some Confluent trunk code because I prototyped with our Avro serializers w/ schema-registry support. We need to figure out what we want to provide as an example built-in set of serializers. Unlike core Kafka where we could ignore the issue, providing only ByteArray or String serializers, this is pretty central to how Copycat works.
* This patch uses a hacked up version of Avro as its runtime data format. Not sure if we want to go through the entire API discussion just to get some basic code committed, so I filed KAFKA-2367 to handle that separately. The core connector APIs and the runtime data APIs are entirely orthogonal.
* This patch needs some updates to get aligned with recent new consumer changes (specifically, I'm aware of the ConcurrentModificationException issue on exit). More generally, the new consumer is in flux but Copycat depends on it, so there are likely to be some negative interactions.
* The layout feels a bit awkward to me right now because I ported it from a Maven layout. We don't have nearly the same level of granularity in Kafka currently (core and clients, plus the mostly ignored examples, log4j-appender, and a couple of contribs). We might want to reorganize, although keeping data+api separate from runtime and connector plugins is useful for minimizing dependencies.
* There are a variety of other things (e.g., I'm not happy with the exception hierarchy/how they are currently handled, TopicPartition doesn't really need to be duplicated unless we want Copycat entirely isolated from the Kafka APIs, etc), but I expect those we'll cover in the review.

Before commenting on the patch, it's probably worth reviewing https://issues.apache.org/jira/browse/KAFKA-2365 and https://issues.apache.org/jira/browse/KAFKA-2366 to get an idea of what I had in mind for a) what we ultimately want with all the Copycat patches and b) what we aim to cover in this initial patch. My hope is that we can use a WIP patch (after the current obvious deficiencies are addressed) while recognizing that we want to make iterative progress with a bunch of subsequent PRs.

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ismael Juma, Gwen Shapira

Closes #99 from ewencp/copycat and squashes the following commits:

a3a47a6 [Ewen Cheslack-Postava] Simplify Copycat exceptions, make them a subclass of KafkaException.
8c108b0 [Ewen Cheslack-Postava] Rename Coordinator to Herder to avoid confusion with the consumer coordinator.
7bf8075 [Ewen Cheslack-Postava] Make Copycat CLI speific to standalone mode, clean up some config and get rid of config storage in standalone mode.
656a003 [Ewen Cheslack-Postava] Clarify and expand the explanation of the Copycat Coordinator interface.
c0e5fdc [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
0fa7a36 [Ewen Cheslack-Postava] Mark Copycat classes as unstable and reduce visibility of some classes where possible.
d55d31e [Ewen Cheslack-Postava] Reorganize Copycat code to put it all under one top-level directory.
b29cb2c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
d713a21 [Ewen Cheslack-Postava] Address Gwen's review comments.
6787a85 [Ewen Cheslack-Postava] Make Converter generic to match serializers since some serialization formats do not require a base class of Object; update many other classes to have generic key and value class type parameters to match this change.
b194c73 [Ewen Cheslack-Postava] Split Copycat converter option into two options for key and value.
0b5a1a0 [Ewen Cheslack-Postava] Normalize naming to use partition for both source and Kafka, adjusting naming in CopycatRecord classes to clearly differentiate.
e345142 [Ewen Cheslack-Postava] Remove Copycat reflection utils, use existing Utils and ConfigDef functionality from clients package.
be5c387 [Ewen Cheslack-Postava] Minor cleanup
122423e [Ewen Cheslack-Postava] Style cleanup
6ba87de [Ewen Cheslack-Postava] Remove most of the Avro-based mock runtime data API, only preserving enough schema functionality to support basic primitive types for an initial patch.
4674d13 [Ewen Cheslack-Postava] Address review comments, clean up some code styling.
25b5739 [Ewen Cheslack-Postava] Fix sink task offset commit concurrency issue by moving it to the worker thread and waking up the consumer to ensure it exits promptly.
0aefe21 [Ewen Cheslack-Postava] Add log4j settings for Copycat.
220e42d [Ewen Cheslack-Postava] Replace Avro serializer with JSON serializer.
1243a7c [Ewen Cheslack-Postava] Merge remote-tracking branch 'origin/trunk' into copycat
5a618c6 [Ewen Cheslack-Postava] Remove offset serializers, instead reusing the existing serializers and removing schema projection support.
e849e10 [Ewen Cheslack-Postava] Remove duplicated TopicPartition implementation.
dec1379 [Ewen Cheslack-Postava] Switch to using new consumer coordinator instead of manually assigning partitions. Remove dependency of copycat-runtime on core.
4a9b4f3 [Ewen Cheslack-Postava] Add some helpful Copycat-specific build and test targets that cover all Copycat packages.
31cd1ca [Ewen Cheslack-Postava] Add CLI tools for Copycat.
e14942c [Ewen Cheslack-Postava] Add Copycat file connector.
0233456 [Ewen Cheslack-Postava] Add copycat-avro and copycat-runtime
11981d2 [Ewen Cheslack-Postava] Add copycat-data and copycat-api",280,2415,0,1119,8069,260,260,2415,2415,2415,1,1,2415,2415,2415,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/coordinator/PartitionAssignorTest.scala,core/src/test/scala/unit/kafka/coordinator/PartitionAssignorTest.scala,"KAFKA-1782: fix JUnit3 Misuse

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ewen Cheslack-Postava, Guozhang Wang

Closes #135 from ewencp/kafka-1782-junit3-misusage and squashes the following commits:

0ae6258 [Ewen Cheslack-Postava] KAFKA-1782: Junit3 Misusage",20,1,1,266,2027,0,19,305,300,102,3,1,309,300,103,4,3,1,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/message/ByteBufferMessageSetTest.scala,core/src/test/scala/unit/kafka/javaapi/message/ByteBufferMessageSetTest.scala,"KAFKA-1782: fix JUnit3 Misuse

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ewen Cheslack-Postava, Guozhang Wang

Closes #135 from ewencp/kafka-1782-junit3-misusage and squashes the following commits:

0ae6258 [Ewen Cheslack-Postava] KAFKA-1782: Junit3 Misusage",3,1,1,23,198,0,3,45,86,6,8,1.0,105,86,13,60,26,8,2,1,0,1
core/src/test/scala/unit/kafka/utils/ByteBoundedBlockingQueueTest.scala,core/src/test/scala/unit/kafka/utils/ByteBoundedBlockingQueueTest.scala,"KAFKA-1782: fix JUnit3 Misuse

Author: Ewen Cheslack-Postava <me@ewencp.org>

Reviewers: Ewen Cheslack-Postava, Guozhang Wang

Closes #135 from ewencp/kafka-1782-junit3-misusage and squashes the following commits:

0ae6258 [Ewen Cheslack-Postava] KAFKA-1782: Junit3 Misusage",1,1,1,59,534,0,1,98,99,33,3,1,102,99,34,4,3,1,2,1,0,1
tests/kafkatest/sanity_checks/__init__.py,tests/kafkatest/sanity_checks/__init__.py,"KAFKA-2408: ConsoleConsumerService direct log output to file

console consumer writes to System.out, while (some) log4j loggers operate in other threads.

This occasionally led to funky interleaved output which disrupted parsing of consumed messages by ConsoleConsumerService, leading to spurious test failures.

This fix directs log output to a separate file.

Author: Geoff Anderson <geoff@confluent.io>

Reviewers: Ewen Cheslack-Postava

Closes #123 from granders/KAFKA-2408 and squashes the following commits:

247b0e0 [Geoff Anderson] Updated line counting to use wc -l
66d6f4f [Geoff Anderson] lower -> uperrcase constants
e67f554 [Geoff Anderson] Changed incorrect license header
af67e01 [Geoff Anderson] Merged in upstream trunk
8f89044 [Geoff Anderson] Added another lifecycle check. Wait for log file to exist before exmaning contents.
521a84b [Geoff Anderson] Updated console consumer to directo log output directly to file rather than stdout",0,14,0,0,0,0,0,14,14,14,1,1,14,14,14,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/TopicAlreadyMarkedForDeletionException.scala,core/src/main/scala/kafka/common/TopicAlreadyMarkedForDeletionException.scala,Adding a file missed while committing KAFKA-2345,0,21,0,3,18,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/KafkaConfigConfigDefTest.scala,core/src/test/scala/unit/kafka/server/KafkaConfigConfigDefTest.scala,kafka-2271; transient unit test failure in KafkaConfigConfigDefTest.testFromPropsToProps; patched by Jason Gustafson; reviewed by Jun Rao,153,4,4,324,3979,2,8,403,403,37,11,2,432,403,39,29,18,3,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/internals/DelayedTask.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/DelayedTask.java,KAFKA-2123: add callback in commit api and use a delayed queue for async requests; reviewed by Ewen Cheslack-Postava and Guozhang Wang,0,24,0,4,25,0,0,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/consumer/internals/DelayedTaskQueue.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/DelayedTaskQueue.java,KAFKA-2123: add callback in commit api and use a delayed queue for async requests; reviewed by Ewen Cheslack-Postava and Guozhang Wang,12,96,0,48,303,7,7,96,96,96,1,1,96,96,96,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/clients/consumer/internals/SendFailedException.java,clients/src/main/java/org/apache/kafka/clients/consumer/internals/SendFailedException.java,KAFKA-2123: add callback in commit api and use a delayed queue for async requests; reviewed by Ewen Cheslack-Postava and Guozhang Wang,0,27,0,6,52,0,0,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
clients/src/test/java/org/apache/kafka/clients/consumer/internals/DelayedTaskQueueTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/internals/DelayedTaskQueueTest.java,KAFKA-2123: add callback in commit api and use a delayed queue for async requests; reviewed by Ewen Cheslack-Postava and Guozhang Wang,3,89,0,60,504,3,3,89,89,89,1,1,89,89,89,0,0,0,0,0,0,0
kafka-patch-review.py,kafka-patch-review.py,"KAFKA-1053 Kafka patch review tool that integrates JIRA and reviewboard; reviewed by Joel Koshy, Swapnil Ghike and Guozhang Wang##KAFKA-1053 Kafka patch review tool that integrates JIRA and reviewboard; reviewed by Joel Koshy, Swapnil Ghike and Guozhang Wang##KAFKA-1086 Improve GetOffsetShell to find metadata automatically; reviewed by Jun Rao##KAFKA-1094 Configure reviewboard url in kafka-patch-review tool; reviewed by Neha Narkhede##KAFKA-1097 Race condition while reassigning low throughput partition leads to incorrect ISR information in zookeeper; reviewed by Jun Rao, Guozhang Wang##KAFKA-1142 Patch review tool should take diff with origin from last divergent point; reviewed by Neha Narkhede##KAFKA-1442 RBTools post-review is deprecated; reviewed by Neha Narkhede##KAFKA-1550; Patch review tool should use git format-patch for patch generation; reviewed by Guozhang Wang and Joel Koshy##kafka-1567; Metric memory leaking after closing the clients; patched by Jiangjie Qin; reviewed by Guozhang Wang and Jun Rao##KAFKA-1560 Make arguments to jira-python API more explicit in kafka-patch-review's get_jira(); reviewed by Neha Narkhede##KAFKA-1854 Allow JIRA username and password to be prompted in the absence of a jira.ini file, during patch submission; reviewed by Neha Narkhede##KAFKA-2153 kafka-patch-review tool uploads a patch even if it is empty; reviewed by Neha Narkhede, Gwen Shapira##kafka-2248; Use Apache Rat to enforce copyright headers; patched by Ewen Cheslack-Postava; reviewed by Gwen Shapira, Joel Joshy and Jun Rao",33,353,37,163,1167,21,4,316,103,24,13,1,353,103,27,37,11,3,2,1,0,1
system_test/system_test_runner.py,system_test/system_test_runner.py,kafka-2005; Generate html report for system tests; patched by Ashish Singh; reviewed by Jun Rao,36,131,4,229,1478,7,7,331,164,55,6,2.5,438,164,73,107,101,18,2,1,0,1
core/src/main/scala/kafka/network/Handler.scala,core/src/main/scala/kafka/network/Handler.scala,"kafka-1928; Move kafka.network over to using the network classes in org.apache.kafka.common.network; patched by Gwen Shapira; reviewed by Joel Koshy, Jay Kreps, Jiangjie Qin, Guozhang Wang and Jun Rao",0,4,2,6,47,0,0,35,32,12,3,1,43,32,14,8,6,3,2,1,0,1
core/src/main/scala/kafka/coordinator/ConsumerGroupMetadata.scala,core/src/main/scala/kafka/coordinator/ConsumerGroupMetadata.scala,KAFKA-2208; add consumer side error handling upon coordinator failure; reviewed by Onur Karaman,8,10,8,51,438,1,7,133,131,44,3,4,148,131,49,15,8,5,2,1,0,1
core/src/main/scala/kafka/coordinator/PartitionAssignor.scala,core/src/main/scala/kafka/coordinator/PartitionAssignor.scala,KAFKA-2196; Remove identical topic constraint in round-robin assignor; reviewed by Guozhang Wang,9,4,8,63,655,1,4,125,129,62,2,3.5,133,129,66,8,8,4,2,1,0,1
core/src/main/scala/kafka/coordinator/DelayedJoinGroup.scala,core/src/main/scala/kafka/coordinator/DelayedJoinGroup.scala,KAFKA-1334; Add the heartbeat logic to consumer coordinator; reviewed by Guozhang Wang,3,10,19,13,106,3,3,39,44,10,4,1.5,60,44,15,21,19,5,2,1,0,1
dev-utils/test-patch.py,dev-utils/test-patch.py,"KAFKA-2153 kafka-patch-review tool uploads a patch even if it is empty; reviewed by Neha Narkhede, Gwen Shapira",91,16,7,368,2720,1,34,466,457,233,2,5.5,473,457,236,7,7,4,1,0,1,1
core/src/main/scala/kafka/common/ConsumerRebalanceFailedException.scala,core/src/main/scala/kafka/common/ConsumerRebalanceFailedException.scala,"KAFKA-2140 follow up, checking in newly renamed file ConsumerRebalanceFailedException",1,26,0,4,27,1,1,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java,KAFKA-2140 Improve code readability; reviewed by Neha Narkhede,32,1,1,194,1608,1,13,268,270,17,16,1.0,364,270,23,96,32,6,2,1,0,1
core/src/main/scala/kafka/api/StopReplicaResponse.scala,core/src/main/scala/kafka/api/StopReplicaResponse.scala,KAFKA-2140 Improve code readability; reviewed by Neha Narkhede,7,4,4,50,295,1,4,75,66,6,13,3,117,66,9,42,9,3,2,1,0,1
core/src/main/scala/kafka/javaapi/Implicits.scala,core/src/main/scala/kafka/javaapi/Implicits.scala,KAFKA-2140 Improve code readability; reviewed by Neha Narkhede,6,0,5,24,235,0,6,54,128,3,16,1.0,182,128,11,128,87,8,2,1,0,1
core/src/main/scala/kafka/log/CleanerConfig.scala,core/src/main/scala/kafka/log/CleanerConfig.scala,KAFKA-2140 Improve code readability; reviewed by Neha Narkhede,0,10,10,11,80,0,0,41,41,10,4,1.0,54,41,14,13,10,3,1,0,1,1
core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala,core/src/main/scala/kafka/producer/KafkaLog4jAppender.scala,kafka-2109; Support retries in KafkaLog4jAppender; patched by Dave Beech; reviewed by Jun Rao,19,5,0,65,613,2,10,102,91,4,26,2.0,291,91,11,189,38,7,2,1,0,1
core/src/main/scala/kafka/network/BoundedByteBufferReceive.scala,core/src/main/scala/kafka/network/BoundedByteBufferReceive.scala,kafka-1926; Replace kafka.utils.Utils with o.a.k.common.utils.Utils; patched by Tong Li; reviewed by Jun Rao,13,2,2,51,286,1,3,90,87,8,11,1,117,87,11,27,6,2,2,1,0,1
core/src/test/scala/unit/kafka/log4j/KafkaLog4jAppenderTest.scala,core/src/test/scala/unit/kafka/log4j/KafkaLog4jAppenderTest.scala,kafka-1926; Replace kafka.utils.Utils with o.a.k.common.utils.Utils; patched by Tong Li; reviewed by Jun Rao,11,2,2,100,669,1,5,208,189,9,22,4.0,484,189,22,276,55,13,2,1,0,1
core/src/main/scala/kafka/common/BrokerEndPointNotAvailableException.scala,core/src/main/scala/kafka/common/BrokerEndPointNotAvailableException.scala,kafka-1809; Refactor brokers to allow listening on multiple ports and IPs; patched by Gwen Shapira; reviewed by Joel Koshy and Jun Rao,1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
system_test/utils/kafka_system_test_utils.py,system_test/utils/kafka_system_test_utils.py,kafka-1809; Refactor brokers to allow listening on multiple ports and IPs; patched by Gwen Shapira; reviewed by Joel Koshy and Jun Rao,318,1,0,1764,13963,1,48,2512,681,68,37,3,3428,701,93,916,174,25,2,1,0,1
core/src/main/scala/kafka/network/BoundedByteBufferSend.scala,core/src/main/scala/kafka/network/BoundedByteBufferSend.scala,kafka-2044; Support requests and responses from o.a.k.common in KafkaApis; patched by Gwen Shapira; reviewed by Jun Rao,8,8,0,40,296,2,4,71,58,10,7,2,96,58,14,25,13,4,2,1,0,1
core/src/main/scala/kafka/common/NoOffsetsCommittedException.scala,core/src/main/scala/kafka/common/NoOffsetsCommittedException.scala,KAFKA-1910; Refactor new consumer and fixed a bunch of corner cases / unit tests; reviewed by Onur Karaman and Jay Kreps,1,27,0,4,27,1,1,27,27,27,1,1,27,27,27,0,0,0,2,1,0,1
core/src/main/scala/kafka/coordinator/ConsumerRegistry.scala,core/src/main/scala/kafka/coordinator/ConsumerRegistry.scala,KAFKA-1910; Refactor new consumer and fixed a bunch of corner cases / unit tests; reviewed by Onur Karaman and Jay Kreps,0,4,4,12,85,0,0,52,52,26,2,1.0,56,52,28,4,4,2,2,1,0,1
core/src/main/scala/kafka/coordinator/GroupRegistry.scala,core/src/main/scala/kafka/coordinator/GroupRegistry.scala,KAFKA-1910; Refactor new consumer and fixed a bunch of corner cases / unit tests; reviewed by Onur Karaman and Jay Kreps,0,6,1,18,149,0,0,79,74,40,2,1.5,80,74,40,1,1,0,2,1,0,1
clients/src/main/java/org/apache/kafka/clients/consumer/CommitType.java,clients/src/main/java/org/apache/kafka/clients/consumer/CommitType.java,KAFKA-1915: Add checkstyle for java code.,0,12,0,4,19,0,0,17,12,8,2,1.0,17,12,8,0,0,0,1,0,0,0
core/src/main/scala/kafka/utils/Crc32.java,core/src/main/scala/kafka/utils/Crc32.java,KAFKA-1915: Add checkstyle for java code.,13,20,20,573,4770,2,5,637,637,318,2,3.0,657,637,328,20,20,10,1,0,1,1
core/src/main/scala/kafka/common/GenerateBrokerIdException.scala,core/src/main/scala/kafka/common/GenerateBrokerIdException.scala,KAFKA-1070 Auto assign broker id; reviewed by Neha Narkhede,3,27,0,6,63,3,3,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/InconsistentBrokerIdException.scala,core/src/main/scala/kafka/common/InconsistentBrokerIdException.scala,KAFKA-1070 Auto assign broker id; reviewed by Neha Narkhede,3,27,0,6,63,3,3,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
core/src/main/scala/kafka/utils/DoublyLinkedList.scala,core/src/main/scala/kafka/utils/DoublyLinkedList.scala,KAFKA-1650; (Follow-up patch) to support no data loss in mirror maker; reviewed by Joel Koshy,10,126,0,71,331,5,5,126,126,126,1,1,126,126,126,0,0,0,0,0,0,0
clients/src/main/java/org/apache/kafka/common/errors/DeserializationException.java,clients/src/main/java/org/apache/kafka/common/errors/DeserializationException.java,kafka-1797; add the serializer/deserializer api to the new java client; patched by Jun Rao; reviewed by Neha Narkhede,5,47,0,21,103,5,5,47,47,47,1,1,47,47,47,0,0,0,0,0,0,0
core/src/main/scala/kafka/utils/ByteBoundedBlockingQueue.scala,core/src/main/scala/kafka/utils/ByteBoundedBlockingQueue.scala,KAFKA-1650; avoid data loss when mirror maker shutdown uncleanly; reviewed by Guozhang Wang,37,11,0,113,802,1,11,230,219,115,2,1.0,230,219,115,0,0,0,1,0,0,0
core/src/main/scala/kafka/common/UnknownTopicOrPartitionException.scala,core/src/main/scala/kafka/common/UnknownTopicOrPartitionException.scala,KAFKA-1770; clarify descriptive comments of UnknownTopicOrPartitionException; reviewed by Guozhang Wang,1,3,1,4,27,0,1,26,23,6,4,1.0,35,23,9,9,6,2,1,0,1,1
core/src/main/scala/kafka/common/ClientIdAndBroker.scala,core/src/main/scala/kafka/common/ClientIdAndBroker.scala,kafka-1481; Stop using dashes AND underscores as separators in MBean names; patched by Vladimir Tretyakov; reviewed by Joel Koshy and Jun Rao,0,11,3,9,66,0,0,34,26,17,2,1.5,37,26,18,3,3,2,1,0,1,1
core/src/main/scala/kafka/common/ClientIdAndTopic.scala,core/src/main/scala/kafka/common/ClientIdAndTopic.scala,kafka-1481; Stop using dashes AND underscores as separators in MBean names; patched by Vladimir Tretyakov; reviewed by Joel Koshy and Jun Rao,0,11,3,9,60,0,0,35,27,18,2,2.5,38,27,19,3,3,2,1,0,1,1
system_test/mirror_maker_testsuite/mirror_maker_test.py,system_test/mirror_maker_testsuite/mirror_maker_test.py,KAFKA-1746 System tests don't handle errors well; reviewed by Neha Narkhede,18,1,0,166,1305,1,3,324,325,32,10,2.0,401,325,40,77,45,8,2,1,0,1
system_test/offset_management_testsuite/offset_management_test.py,system_test/offset_management_testsuite/offset_management_test.py,KAFKA-1746 System tests don't handle errors well; reviewed by Neha Narkhede,19,1,0,146,1228,1,3,299,298,150,2,1.0,299,298,150,0,0,0,2,1,0,1
system_test/replication_testsuite/replica_basic_test.py,system_test/replication_testsuite/replica_basic_test.py,KAFKA-1746 System tests don't handle errors well; reviewed by Neha Narkhede,37,2,0,230,1711,1,3,461,256,26,18,3.5,850,256,47,389,113,22,2,1,0,1
system_test/utils/testcase_env.py,system_test/utils/testcase_env.py,KAFKA-1747 TestcaseEnv improperly shares state between instances; reviewed by Neha Narkhede,7,58,59,63,394,1,3,173,65,19,9,2,244,65,27,71,59,8,1,0,1,1
system_test/utils/metrics.py,system_test/utils/metrics.py,KAFKA-1725 Configuration file bugs in system tests add noise to output and break a few tests; reviewed by Neha Narkhede,39,2,0,225,1788,1,12,298,254,60,5,1,313,254,63,15,9,3,2,1,0,1
clients/src/main/java/org/apache/kafka/common/message/KafkaLZ4BlockInputStream.java,clients/src/main/java/org/apache/kafka/common/message/KafkaLZ4BlockInputStream.java,kafka-1493; Use a well-documented LZ4 compression format and remove redundant LZ4HC option; patched by James Oliver; reviewed by Jun Rao,31,233,0,158,1019,11,11,233,233,233,1,1,233,233,233,0,0,0,2,1,0,1
clients/src/main/java/org/apache/kafka/common/message/KafkaLZ4BlockOutputStream.java,clients/src/main/java/org/apache/kafka/common/message/KafkaLZ4BlockOutputStream.java,kafka-1493; Use a well-documented LZ4 compression format and remove redundant LZ4HC option; patched by James Oliver; reviewed by Jun Rao,51,387,0,260,1664,30,30,387,387,387,1,1,387,387,387,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/NotEnoughReplicasAfterAppendException.scala,core/src/main/scala/kafka/common/NotEnoughReplicasAfterAppendException.scala,kafka-1555; provide strong consistency with reasonable availability; patched by Gwen Shapira; reviewed by Joel Koshy and Jun Rao,1,27,0,4,27,1,1,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/NotEnoughReplicasException.scala,core/src/main/scala/kafka/common/NotEnoughReplicasException.scala,kafka-1555; provide strong consistency with reasonable availability; patched by Gwen Shapira; reviewed by Joel Koshy and Jun Rao,1,25,0,4,27,1,1,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/MessageSetSizeTooLargeException.scala,core/src/main/scala/kafka/common/MessageSetSizeTooLargeException.scala,kafka-1670; Corrupt log files for segment.bytes values close to Int.MaxInt; patched by Sriharsha Chintalapani; reviewed by Jay Kreps and Jun Rao,1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
contrib/hadoop-consumer/src/main/java/kafka/etl/impl/DataGenerator.java,contrib/hadoop-consumer/src/main/java/kafka/etl/impl/DataGenerator.java,kafka-1123; Broker IPv6 addresses parsed incorrectly; patched by Krzysztof Szafrański; reviewed by Jun Rao,8,4,3,86,817,2,4,126,141,13,10,2.5,184,141,18,58,16,6,2,1,0,1
system_test/utils/system_test_utils.py,system_test/utils/system_test_utils.py,KAFKA-1611 Improve system test configuration; reviewed by Neha Narkhede and Guozhang Wang,122,16,12,384,2682,1,34,638,326,80,8,2.0,672,326,84,34,12,4,2,1,0,1
core/src/main/scala/kafka/server/DelayedRequestKey.scala,core/src/main/scala/kafka/server/DelayedRequestKey.scala,kafka-1430; Purgatory redesign; patched by Guozhang Wang; reviewed by Jun Rao,1,38,0,12,72,1,1,38,38,38,1,1,38,38,38,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/FetchRequestPurgatory.scala,core/src/main/scala/kafka/server/FetchRequestPurgatory.scala,kafka-1430; Purgatory redesign; patched by Guozhang Wang; reviewed by Jun Rao,5,69,0,31,252,4,4,69,69,69,1,1,69,69,69,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/ProducerRequestPurgatory.scala,core/src/main/scala/kafka/server/ProducerRequestPurgatory.scala,kafka-1430; Purgatory redesign; patched by Guozhang Wang; reviewed by Jun Rao,6,69,0,32,294,4,4,69,69,69,1,1,69,69,69,0,0,0,0,0,0,0
core/src/main/scala/kafka/api/HeartbeatRequestAndHeader.scala,core/src/main/scala/kafka/api/HeartbeatRequestAndHeader.scala,kafka-1462 (followup patch); Add new request and response formats for the new consumer and coordinator communication; patched by Jun Rao; reviewed by Jay Kreps,2,13,7,28,246,2,2,45,39,22,2,4.0,52,39,26,7,7,4,1,0,1,1
core/src/main/scala/kafka/api/HeartbeatResponseAndHeader.scala,core/src/main/scala/kafka/api/HeartbeatResponseAndHeader.scala,kafka-1462 (followup patch); Add new request and response formats for the new consumer and coordinator communication; patched by Jun Rao; reviewed by Jay Kreps,1,5,5,13,96,1,1,28,28,14,2,2.5,33,28,16,5,5,2,1,0,1,1
core/src/main/scala/kafka/api/JoinGroupRequestAndHeader.scala,core/src/main/scala/kafka/api/JoinGroupRequestAndHeader.scala,kafka-1462 (followup patch); Add new request and response formats for the new consumer and coordinator communication; patched by Jun Rao; reviewed by Jay Kreps,2,11,6,28,242,2,2,45,40,22,2,3.5,51,40,26,6,6,3,1,0,1,1
core/src/main/scala/kafka/api/JoinGroupResponseAndHeader.scala,core/src/main/scala/kafka/api/JoinGroupResponseAndHeader.scala,kafka-1462 (followup patch); Add new request and response formats for the new consumer and coordinator communication; patched by Jun Rao; reviewed by Jay Kreps,1,5,5,13,96,1,1,28,28,14,2,2.5,33,28,16,5,5,2,1,0,1,1
core/src/main/scala/kafka/api/LeaderAndIsrResponse.scala,core/src/main/scala/kafka/api/LeaderAndIsrResponse.scala,kafka-1462; Add new request and response formats for the new consumer and coordinator communication; patched by Jun Rao; reviewed by Guozhang Wang and Jay Kreps,7,2,2,50,298,0,4,75,66,6,13,2,108,66,8,33,7,3,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/Props.java,contrib/hadoop-consumer/src/main/java/kafka/etl/Props.java,kafka-1406; Fix scaladoc/javadoc warnings; patched by Alan Lee; reviewed by Jun Rao,57,34,39,216,1546,0,32,458,460,114,4,1.5,504,460,126,46,39,12,2,1,0,1
clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerExampleTest.java,clients/src/test/java/org/apache/kafka/clients/consumer/ConsumerExampleTest.java,KAFKA-1328 follow up: Updated javadoc,8,2,3,32,266,0,5,297,298,148,2,1.0,300,298,150,3,3,2,1,0,1,1
clients/src/main/java/org/apache/kafka/clients/consumer/OffsetMetadata.java,clients/src/main/java/org/apache/kafka/clients/consumer/OffsetMetadata.java,KAFKA-1328 New consumer APIs; reviewed by Jun Rao and Guozhang Wang,6,59,0,26,172,4,4,59,59,59,1,1,59,59,59,0,0,0,0,0,0,0
system_test/migration_tool_testsuite/migration_tool_test.py,system_test/migration_tool_testsuite/migration_tool_test.py,"kafka-1318; waiting for producer to stop is not reliable in system tests; patched by Jun Rao; reviewed by Guozhang Wang, Timothy Chen and Neha Narkhede",17,0,5,150,1169,1,3,308,322,44,7,1,360,322,51,52,38,7,1,0,1,1
core/src/main/scala/kafka/tools/newproducer/MirrorMaker.scala,core/src/main/scala/kafka/tools/newproducer/MirrorMaker.scala,"kafka-1307; potential socket leak in new producer and clean up; reviewed by Jay Kreps, Guozhang Wang and Neha Narkhede",22,2,2,146,1063,1,8,187,184,31,6,2.0,200,184,33,13,6,2,2,1,0,1
core/src/main/scala/kafka/common/NoReplicaOnlineException.scala,core/src/main/scala/kafka/common/NoReplicaOnlineException.scala,KAFKA-1028 per topic configuration of preference for consistency over availability; reviewed by Neha Narkhede and Jay Kreps,2,2,1,5,49,0,2,29,28,14,2,1.0,30,28,15,1,1,0,1,0,1,1
core/src/main/scala/kafka/common/ConsumerCoordinatorNotAvailableException.scala,core/src/main/scala/kafka/common/ConsumerCoordinatorNotAvailableException.scala,"KAFKA-1012; Consumer offset management in Kafka; patched by Tejas Patil and Joel Koshy; feedback and reviews from Neha Narkhede, Jun Rao, Guozhang Wang, Sriram Subramanian, Joe Stein, Chris Riccomini",1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/NotCoordinatorForConsumerException.scala,core/src/main/scala/kafka/common/NotCoordinatorForConsumerException.scala,"KAFKA-1012; Consumer offset management in Kafka; patched by Tejas Patil and Joel Koshy; feedback and reviews from Neha Narkhede, Jun Rao, Guozhang Wang, Sriram Subramanian, Joe Stein, Chris Riccomini",1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/OffsetsLoadInProgressException.scala,core/src/main/scala/kafka/common/OffsetsLoadInProgressException.scala,"KAFKA-1012; Consumer offset management in Kafka; patched by Tejas Patil and Joel Koshy; feedback and reviews from Neha Narkhede, Jun Rao, Guozhang Wang, Sriram Subramanian, Joe Stein, Chris Riccomini",1,26,0,4,27,1,1,26,26,26,1,1,26,26,26,0,0,0,0,0,0,0
project/Build.scala,project/Build.scala,KAFKA-1236 Fix various breakages in the perf tests. Make the producer test use either the old or the new producer.,0,1,1,122,1267,0,0,153,143,8,18,1.0,223,143,12,70,50,4,2,1,0,1
clients/src/main/java/org/apache/kafka/common/errors/RetryableException.java,clients/src/main/java/org/apache/kafka/common/errors/RetryableException.java,trivial fix to add missing license header using .gradlew licenseFormatMain and ./gradlew licenseFormatTest; patched by Jun Rao,4,16,0,15,77,0,4,47,31,16,3,1,48,31,16,1,1,0,2,1,0,1
project/build/KafkaProject.scala,project/build/KafkaProject.scala,KAFKA-1164 kafka should depend on snappy 1.0.5 (instead of 1.0.4.1); reviewed by Neha Narkhede,0,1,1,187,1325,0,0,261,190,9,28,2.0,453,190,16,192,46,7,2,1,0,1
clients/src/main/java/kafka/clients/producer/DefaultPartitioner.java,clients/src/main/java/kafka/clients/producer/DefaultPartitioner.java,KAFKA-1227 New producer!,3,35,0,16,147,1,1,35,35,35,1,1,35,35,35,0,0,0,0,0,0,0
clients/src/main/java/kafka/clients/producer/Partitioner.java,clients/src/main/java/kafka/clients/producer/Partitioner.java,KAFKA-1227 New producer!,0,30,0,5,48,0,0,30,30,30,1,1,30,30,30,0,0,0,0,0,0,0
clients/src/main/java/kafka/clients/producer/RecordSend.java,clients/src/main/java/kafka/clients/producer/RecordSend.java,KAFKA-1227 New producer!,10,88,0,42,265,7,7,88,88,88,1,1,88,88,88,0,0,0,0,0,0,0
clients/src/main/java/kafka/common/ByteSerialization.java,clients/src/main/java/kafka/common/ByteSerialization.java,KAFKA-1227 New producer!,2,18,0,11,51,2,2,18,18,18,1,1,18,18,18,0,0,0,0,0,0,0
clients/src/main/java/kafka/common/Deserializer.java,clients/src/main/java/kafka/common/Deserializer.java,KAFKA-1227 New producer!,0,18,0,4,20,0,0,18,18,18,1,1,18,18,18,0,0,0,0,0,0,0
clients/src/main/java/kafka/common/Serializer.java,clients/src/main/java/kafka/common/Serializer.java,KAFKA-1227 New producer!,0,21,0,4,20,0,0,21,21,21,1,1,21,21,21,0,0,0,0,0,0,0
clients/src/main/java/kafka/common/StringSerialization.java,clients/src/main/java/kafka/common/StringSerialization.java,KAFKA-1227 New producer!,10,58,0,42,218,5,5,58,58,58,1,1,58,58,58,0,0,0,0,0,0,0
clients/src/main/java/kafka/common/errors/MessageTooLargeException.java,clients/src/main/java/kafka/common/errors/MessageTooLargeException.java,KAFKA-1227 New producer!,4,23,0,16,76,4,4,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/LogCleaningAbortedException.scala,core/src/main/scala/kafka/common/LogCleaningAbortedException.scala,"kafka-1074; Reassign partitions should delete the old replicas from disk; patched by Jun Rao; reviewed by Jay Kreps, Neha Narkhede and Guozhang Wang",0,4,3,3,14,0,0,24,23,12,2,2.5,27,23,14,3,3,2,0,0,0,0
core/src/main/scala/kafka/common/ThreadShutdownException.scala,core/src/main/scala/kafka/common/ThreadShutdownException.scala,"kafka-1074; Reassign partitions should delete the old replicas from disk; patched by Jun Rao; reviewed by Jay Kreps, Neha Narkhede and Guozhang Wang",0,24,0,3,12,0,0,24,24,24,1,1,24,24,24,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/NotAssignedReplicaException.scala,core/src/main/scala/kafka/common/NotAssignedReplicaException.scala,"KAFKA-1097 Race condition while reassigning low throughput partition leads to incorrect ISR information in zookeeper; reviewed by Jun Rao, Guozhang Wang",2,23,0,5,49,2,2,23,23,23,1,1,23,23,23,0,0,0,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/impl/SimpleKafkaETLMapper.java,contrib/hadoop-consumer/src/main/java/kafka/etl/impl/SimpleKafkaETLMapper.java,kafka-946; Kafka Hadoop Consumer fails when verifying message checksum; patched by Sam Meder; reviewed by Jun Rao,5,4,3,45,343,1,4,91,87,23,4,1.0,100,87,25,9,6,2,2,1,0,1
core/src/main/scala/kafka/utils/Annotations_2.8.scala,core/src/main/scala/kafka/utils/Annotations_2.8.scala,KAFKA-1046 New files,0,36,0,4,16,0,0,72,36,18,4,1.0,78,36,20,6,6,2,0,0,0,0
core/src/main/scala/kafka/admin/AddPartitionsCommand.scala,core/src/main/scala/kafka/admin/AddPartitionsCommand.scala,KAFKA-1046 Added support for Scala 2.10 builds while maintaining compatibility with 2.8.x; reviewed by Neha and Jun,16,1,1,95,781,1,3,127,127,42,3,1,129,127,43,2,1,1,1,0,1,1
core/src/main/scala/kafka/admin/DeleteTopicCommand.scala,core/src/main/scala/kafka/admin/DeleteTopicCommand.scala,KAFKA-1046 Added support for Scala 2.10 builds while maintaining compatibility with 2.8.x; reviewed by Neha and Jun,6,1,1,43,264,1,1,66,66,22,3,1,68,66,23,2,1,1,1,0,1,1
core/src/main/scala/kafka/admin/CreateTopicCommand.scala,core/src/main/scala/kafka/admin/CreateTopicCommand.scala,KAFKA-1046 Added support for Scala 2.10 builds while maintaining compatibility with 2.8.x; reviewed by Neha and Jun,14,1,1,90,696,1,3,117,110,11,11,4,169,110,15,52,15,5,1,0,1,1
core/src/main/scala/kafka/admin/ListTopicCommand.scala,core/src/main/scala/kafka/admin/ListTopicCommand.scala,KAFKA-1046 Added support for Scala 2.10 builds while maintaining compatibility with 2.8.x; reviewed by Neha and Jun,23,1,1,80,612,1,2,107,89,12,9,2,130,89,14,23,10,3,2,1,0,1
core/src/main/scala/kafka/server/KafkaZooKeeper.scala,core/src/main/scala/kafka/server/KafkaZooKeeper.scala,KAFKA-992 Double Check on Broker Registration to Avoid False NodeExist Exception; reviewed by Neha Narkhede and Swapnil Ghike,8,1,1,46,272,1,5,0,0,0,1,1,1,1,1,1,1,1,2,1,0,1
contrib/hadoop-producer/src/main/java/kafka/bridge/hadoop/KafkaOutputFormat.java,contrib/hadoop-producer/src/main/java/kafka/bridge/hadoop/KafkaOutputFormat.java,"KAFKA-991; Rename config queue size to queue bytes in hadoop producer; patched by Swapnil Ghike, reviewed by Joel Koshy.",15,3,4,93,849,1,6,201,115,15,13,2,350,115,27,149,60,11,2,1,0,1
contrib/hadoop-producer/src/main/java/kafka/bridge/hadoop/KafkaRecordWriter.java,contrib/hadoop-producer/src/main/java/kafka/bridge/hadoop/KafkaRecordWriter.java,"KAFKA-991; Rename config queue size to queue bytes in hadoop producer; patched by Swapnil Ghike, reviewed by Joel Koshy.",10,11,9,58,414,4,4,86,75,10,9,3,149,75,17,63,15,7,2,1,0,1
core/src/main/scala/kafka/admin/CheckReassignmentStatus.scala,core/src/main/scala/kafka/admin/CheckReassignmentStatus.scala,KAFKA-941 Add Apache 2.0 license to missing code source files,13,16,1,81,612,0,3,207,97,34,6,1.0,228,97,38,21,17,4,2,1,0,1
core/src/main/scala/kafka/common/BrokerNotExistException.scala,core/src/main/scala/kafka/common/BrokerNotExistException.scala,KAFKA-941 Add Apache 2.0 license to missing code source files,0,16,0,0,0,0,0,38,22,10,4,1.0,60,22,15,22,22,6,2,1,0,1
core/src/main/scala/kafka/consumer/ConsumerTopicStat.scala,core/src/main/scala/kafka/consumer/ConsumerTopicStat.scala,KAFKA-941 Add Apache 2.0 license to missing code source files,0,16,0,0,0,0,0,26,52,4,7,3,101,52,14,75,40,11,1,0,1,1
core/src/main/scala/kafka/producer/async/AsyncProducerStats.scala,core/src/main/scala/kafka/producer/async/AsyncProducerStats.scala,KAFKA-941 Add Apache 2.0 license to missing code source files,0,16,0,0,0,0,0,16,33,2,8,1.0,83,33,10,67,25,8,2,1,0,1
core/src/main/scala/kafka/utils/Topic.scala,core/src/main/scala/kafka/utils/Topic.scala,KAFKA-941 Add Apache 2.0 license to missing code source files,0,16,0,0,0,0,0,16,44,4,4,1.0,68,44,17,52,41,13,1,0,1,1
core/src/main/scala/kafka/admin/ShutdownBroker.scala,core/src/main/scala/kafka/admin/ShutdownBroker.scala,KAFKA-935: Fix shutdown tool to work with new controlled shutdown API; reviewed by Neha Narkhede,13,3,3,94,659,1,2,128,121,26,5,7,180,121,36,52,17,10,2,1,0,1
core/src/main/scala/kafka/common/InvalidOffsetException.scala,core/src/main/scala/kafka/common/InvalidOffsetException.scala,"KAFKA-905 Logs can have same offsets causing recovery failure; reviewed by Jun, Neha and Jay",1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/HighwaterMarkCheckpoint.scala,core/src/main/scala/kafka/server/HighwaterMarkCheckpoint.scala,"kafka-903; Attempt to swap the new high watermark file with the old one failed on Windows; patched by Jun Rao; reviewed by Neha Narkhede, Jay Kreps and Sriram Subramania",11,7,3,82,501,1,2,121,125,17,7,2,157,125,22,36,14,5,1,0,1,1
core/src/test/scala/unit/kafka/integration/LazyInitProducerTest.scala,core/src/test/scala/unit/kafka/integration/LazyInitProducerTest.scala,KAFKA-901 Kafka server can become unavailable if clients send several metadata requests; reviewed by Jun Rao,20,7,2,122,1111,4,6,173,184,8,21,3,356,184,17,183,47,9,2,1,0,1
core/src/test/scala/unit/kafka/log/LogOffsetTest.scala,core/src/test/scala/unit/kafka/log/LogOffsetTest.scala,kafka-871; Rename ZkConfig properties; patched by Swapnil Ghike; reviewed by Jun Rao,14,1,1,164,1379,1,9,0,0,0,3,3,8,4,3,8,4,3,2,1,0,1
core/src/main/scala/kafka/common/LeaderElectionNotNeededException.scala,core/src/main/scala/kafka/common/LeaderElectionNotNeededException.scala,Missing file from KAFKA-828,2,27,0,5,49,2,2,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
system_test/utils/replication_utils.py,system_test/utils/replication_utils.py,kafka-791; Fix validation bugs in System Test; patched by John Fung; reviewed by Jun Rao,1,3,0,30,184,1,1,70,60,23,3,1,70,60,23,0,0,0,2,1,0,1
core/src/main/scala/kafka/admin/AdminOperationException.scala,core/src/main/scala/kafka/admin/AdminOperationException.scala,KAFKA-554 Dynamic per-topic configuration. This patch adds a mechanism for storing per-topic configurations in zookeeper and dynamically making config changes across the cluster. Reviewed by Neha and Jun.,2,6,18,5,55,3,2,23,33,8,3,3,56,33,19,33,18,11,0,0,0,0
core/src/main/scala/kafka/common/LeaderNotAvailableException.scala,core/src/main/scala/kafka/common/LeaderNotAvailableException.scala,KAFKA-774 Periodic refresh of topic metadata on the producer does not check for error code in the response; reviewed by Swapnil Ghike,2,4,3,5,49,2,2,26,25,9,3,1,30,25,10,4,3,1,2,1,0,1
core/src/main/scala/kafka/network/Transmission.scala,core/src/main/scala/kafka/network/Transmission.scala,Processor thread blocks due to infinite loop during fetch response send; patched by Sriram Subramanian; reviewed by Jun Rao; kafka-756,13,17,5,70,351,1,5,122,110,12,10,2.5,160,110,16,38,9,4,2,1,0,1
contrib/hadoop-producer/src/main/java/kafka/bridge/examples/TextPublisher.java,contrib/hadoop-producer/src/main/java/kafka/bridge/examples/TextPublisher.java,KAFKA-713 Update Hadoop producer for Kafka 0.8 changes; reviewed by Neha Narkhede,4,6,8,38,290,3,2,65,68,13,5,3,85,68,17,20,8,4,2,1,0,1
contrib/hadoop-producer/src/main/java/kafka/bridge/pig/AvroKafkaStorage.java,contrib/hadoop-producer/src/main/java/kafka/bridge/pig/AvroKafkaStorage.java,KAFKA-713 Update Hadoop producer for Kafka 0.8 changes; reviewed by Neha Narkhede,12,3,5,85,524,1,10,114,117,23,5,3,135,117,27,21,6,4,2,1,0,1
core/src/test/scala/other/kafka/TestLogPerformance.scala,core/src/test/scala/other/kafka/TestLogPerformance.scala,KAFKA-631 Implement a log cleaner for Kafka. Reviewed by Neha.,4,2,1,37,349,1,1,60,53,4,14,1.0,82,53,6,22,6,2,2,1,0,1
core/src/test/scala/other/kafka/TestZKConsumerOffsets.scala,core/src/test/scala/other/kafka/TestZKConsumerOffsets.scala,Use uniform convention for naming properties keys; kafka-648; patched by Sriram Subramanian; reviewed by Jun Rao,9,1,1,47,302,1,4,73,72,8,9,1,91,72,10,18,6,2,2,1,0,1
core/src/main/scala/kafka/common/OffsetMetadataTooLargeException.scala,core/src/main/scala/kafka/common/OffsetMetadataTooLargeException.scala,KAFKA-657 Add APIs for the consumer to commit and fetch offsets on the broker.,1,27,0,4,27,1,1,27,27,27,1,1,27,27,27,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/ControllerMovedException.scala,core/src/main/scala/kafka/common/ControllerMovedException.scala,"KAFKA-532 Multiple controllers can co-exist during soft failures; patched by Neha Narkhede; reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1411010 13f79535-47bb-0310-9956-ffa450edef68",2,23,0,5,49,2,2,23,23,23,1,1,23,23,23,0,0,0,0,0,0,0
core/src/main/scala/kafka/tools/ProducerShell.scala,core/src/main/scala/kafka/tools/ProducerShell.scala,"KAFKA-544 Store the key given to the producer in the message. Expose this key in the consumer. Patch reviewed by Jun.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1410055 13f79535-47bb-0310-9956-ffa450edef68",5,1,1,43,297,1,1,68,78,10,7,1,97,78,14,29,15,4,2,1,0,1
system_test/migration_tool_testsuite/__init__.py,system_test/migration_tool_testsuite/__init__.py,"Add more test cases to System Test ; patched by John Fung; reviewed by Jun Rao; KAFKA-571

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1401875 13f79535-47bb-0310-9956-ffa450edef68",0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0
core/src/main/scala/kafka/javaapi/message/MessageSet.scala,core/src/main/scala/kafka/javaapi/message/MessageSet.scala,"Change MessageSet.sizeInBytes to Int; patched by Swapnil Ghike; reviewed by Jun Rao; kafka-556

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1401760 13f79535-47bb-0310-9956-ffa450edef68",3,1,1,14,86,0,1,56,53,7,8,1.0,68,53,8,12,6,2,2,1,0,1
core/src/main/scala/kafka/server/MessageSetSend.scala,core/src/main/scala/kafka/server/MessageSetSend.scala,"Change MessageSet.sizeInBytes to Int; patched by Swapnil Ghike; reviewed by Jun Rao; kafka-556

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1401760 13f79535-47bb-0310-9956-ffa450edef68",8,5,5,39,274,1,3,71,68,12,6,1.0,85,68,14,14,6,2,2,1,0,1
core/src/main/scala/kafka/common/ReplicaNotAvailableException.scala,core/src/main/scala/kafka/common/ReplicaNotAvailableException.scala,"KAFKA-432 allow consumer to read from followers; patched by Yang Ye; reviewed by Neha and Jun

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1397422 13f79535-47bb-0310-9956-ffa450edef68",2,3,2,5,49,2,2,26,25,13,2,1.0,28,25,14,2,2,1,1,0,1,1
core/src/main/scala/kafka/common/AdminCommandFailedException.scala,core/src/main/scala/kafka/common/AdminCommandFailedException.scala,"KAFKA-42 part 2: new files

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396814 13f79535-47bb-0310-9956-ffa450edef68",2,23,0,5,49,2,2,46,23,23,2,1.0,46,23,23,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/PartitionOfflineException.scala,core/src/main/scala/kafka/common/PartitionOfflineException.scala,"KAFKA-42 Cluster expansion feature; patched by Neha; reviewed by Jun

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396808 13f79535-47bb-0310-9956-ffa450edef68",2,4,2,5,49,2,2,28,26,7,4,2.0,36,26,9,8,4,2,2,1,0,1
core/src/main/scala/kafka/common/StateChangeFailedException.scala,core/src/main/scala/kafka/common/StateChangeFailedException.scala,"KAFKA-42 Cluster expansion feature; patched by Neha; reviewed by Jun

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396808 13f79535-47bb-0310-9956-ffa450edef68",2,3,3,5,49,3,2,23,23,6,4,1.0,32,23,8,9,3,2,1,0,1,1
system_test/mirror_maker_testsuite/__init__.py,system_test/mirror_maker_testsuite/__init__.py,"KAFKA-502 Adding 30 more system tests, reviewed by Jun and Neha; patched by John Fung

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396687 13f79535-47bb-0310-9956-ffa450edef68",0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0
system_test/system_test_env.py,system_test/system_test_env.py,"KAFKA-502 Adding 30 more system tests, reviewed by Jun and Neha; patched by John Fung

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396687 13f79535-47bb-0310-9956-ffa450edef68",13,73,9,79,481,2,3,138,64,46,3,2,147,73,49,9,9,3,1,0,1,1
system_test/utils/setup_utils.py,system_test/utils/setup_utils.py,"KAFKA-502 Adding 30 more system tests, reviewed by Jun and Neha; patched by John Fung

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1396687 13f79535-47bb-0310-9956-ffa450edef68",2,7,3,16,102,1,2,47,27,16,3,1,50,27,17,3,3,1,1,0,1,1
core/src/main/scala/kafka/common/MessageSizeTooLargeException.scala,core/src/main/scala/kafka/common/MessageSizeTooLargeException.scala,"KAFKA-506 Move to logical offsets. Reviewed by Jun and Neha.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1395729 13f79535-47bb-0310-9956-ffa450edef68",1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,0,0,0,0
core/src/main/scala/kafka/log/SegmentList.scala,core/src/main/scala/kafka/log/SegmentList.scala,"KAFKA-506 Move to logical offsets. Reviewed by Jun and Neha.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1395729 13f79535-47bb-0310-9956-ffa450edef68",10,3,3,51,508,2,4,100,85,17,6,1.5,140,85,23,40,28,7,2,1,0,1
core/src/main/scala/kafka/message/FileMessageSet.scala,core/src/main/scala/kafka/message/FileMessageSet.scala,"IndexOutOfBoundsException thrown by kafka.consumer.ConsumerFetcherThread; patched by Jun Rao; reviewed by Jay Kreps and Neha Narkhede; kafka-528

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1391854 13f79535-47bb-0310-9956-ffa450edef68",31,3,4,145,1027,2,15,243,285,14,17,1,343,285,20,100,36,6,2,1,0,1
core/src/test/scala/unit/kafka/log/SegmentListTest.scala,core/src/test/scala/unit/kafka/log/SegmentListTest.scala,"log.truncateTo needs to handle targetOffset smaller than the lowest offset in the log ; patched by Swapnil Ghike; reviewed by Jun Rao; KAFKA-463

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1386641 13f79535-47bb-0310-9956-ffa450edef68",9,40,33,91,633,4,3,113,57,23,5,1,153,57,31,40,33,8,2,1,0,1
core/src/main/scala/kafka/javaapi/ProducerRequest.scala,core/src/main/scala/kafka/javaapi/ProducerRequest.scala,"Improve Kafka internal metrics; patched by Jun Rao; reviewed by Joel Koshy and Neha Narkhede; KAFKA-203

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1384202 13f79535-47bb-0310-9956-ffa450edef68",4,1,1,18,165,0,4,44,51,6,7,1,74,51,11,30,18,4,2,1,0,1
core/src/main/scala/kafka/server/KafkaController.scala,core/src/main/scala/kafka/server/KafkaController.scala,"Improve Kafka internal metrics; patched by Jun Rao; reviewed by Joel Koshy and Neha Narkhede; KAFKA-203

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1384202 13f79535-47bb-0310-9956-ffa450edef68",77,42,21,487,3591,5,33,615,288,56,11,14,1479,329,134,864,323,79,2,1,0,1
core/src/main/scala/kafka/common/BrokerNotAvailableException.scala,core/src/main/scala/kafka/common/BrokerNotAvailableException.scala,"KAFKA-498: Controller has race conditions and synchronization bugs; patched by Neha Narkhede; reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1382988 13f79535-47bb-0310-9956-ffa450edef68",1,22,0,4,27,1,1,22,22,22,1,1,22,22,22,0,0,0,2,1,0,1
core/src/main/scala/kafka/utils/TopicNameValidator.scala,core/src/main/scala/kafka/utils/TopicNameValidator.scala,"Handle topic names with / on Kafka server; patched by Swapnil Ghike; reviewed by Jay Kreps and Jun Rao; kafka-495

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1381853 13f79535-47bb-0310-9956-ffa450edef68",5,41,0,19,149,1,1,41,41,41,1,1,41,41,41,0,0,0,0,0,0,0
system_test/utils/pyh.py,system_test/utils/pyh.py,"KAFKA-489 Add metrics collection and graphs to the system test framework; patched by Neha Narkhede; reviewed by Jun Rao and John Fung

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1378666 13f79535-47bb-0310-9956-ffa450edef68",49,161,0,115,898,15,16,161,161,161,1,1,161,161,161,0,0,0,0,0,0,0
core/src/main/scala/kafka/server/KafkaRequestHandlers.scala,core/src/main/scala/kafka/server/KafkaRequestHandlers.scala,"Message size not checked at the server (patch v3); patched by Swapnil Ghike; reviewed by Jun Rao; KAFKA-469

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1378590 13f79535-47bb-0310-9956-ffa450edef68",32,8,4,146,1098,1,13,194,134,16,12,2.5,238,134,20,44,10,4,2,1,0,1
system_test/__init__.py,system_test/__init__.py,"KAFKA-440 Regression/system test framework; patched by John Fung; reviewed by Neha Narkhede

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1376147 13f79535-47bb-0310-9956-ffa450edef68",0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0
system_test/replication_testsuite/__init__.py,system_test/replication_testsuite/__init__.py,"KAFKA-440 Regression/system test framework; patched by John Fung; reviewed by Neha Narkhede

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1376147 13f79535-47bb-0310-9956-ffa450edef68",0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0
system_test/utils/__init__.py,system_test/utils/__init__.py,"KAFKA-440 Regression/system test framework; patched by John Fung; reviewed by Neha Narkhede

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1376147 13f79535-47bb-0310-9956-ffa450edef68",0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/integration/LogCorruptionTest.scala,core/src/test/scala/unit/kafka/integration/LogCorruptionTest.scala,"KAFKA-385 Fix race condition between checkSatisfied and expire in RequestPurgatory; fixed constant expiration of follower fetch requests as checkSatisfied was not getting called; add metrics to the RequestPurgatory; add a KafkaTimer convenience class; patched by Joel Koshy; reviewed by Jun Rao and Jay Kreps.

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1374069 13f79535-47bb-0310-9956-ffa450edef68",12,1,1,68,513,1,1,100,94,10,10,1.5,132,94,13,32,16,3,2,1,0,1
core/src/test/scala/unit/kafka/controller/ControllerBasicTest.scala,core/src/test/scala/unit/kafka/controller/ControllerBasicTest.scala,"recommit: revisit the become leader and become follower state change operations using V3 design; patched by Yang Ye; reviewed by Neha Narkhede and Jun Rao; kafka-343

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1368092 13f79535-47bb-0310-9956-ffa450edef68",10,18,14,73,601,6,6,104,100,21,5,11,157,100,31,53,20,11,2,1,0,1
core/src/test/scala/unit/kafka/integration/BackwardsCompatibilityTest.scala,core/src/test/scala/unit/kafka/integration/BackwardsCompatibilityTest.scala,"recommit: revisit the become leader and become follower state change operations using V3 design; patched by Yang Ye; reviewed by Neha Narkhede and Jun Rao; kafka-343

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1368092 13f79535-47bb-0310-9956-ffa450edef68",4,1,1,47,344,1,3,72,76,7,11,1,107,76,10,35,11,3,2,1,0,1
core/src/main/scala/kafka/server/StateChangeCommand.scala,core/src/main/scala/kafka/server/StateChangeCommand.scala,"revert commit to KAFKA-343 due to unit test failures

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1367811 13f79535-47bb-0310-9956-ffa450edef68",11,92,0,61,417,2,2,184,92,61,3,1,187,92,62,3,3,1,1,0,1,1
core/src/main/scala/kafka/server/StateChangeCommandHandler.scala,core/src/main/scala/kafka/server/StateChangeCommandHandler.scala,"revert commit to KAFKA-343 due to unit test failures

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1367811 13f79535-47bb-0310-9956-ffa450edef68",12,79,0,46,296,5,5,158,79,79,2,1.0,158,79,79,0,0,0,0,0,0,0
core/src/main/scala/kafka/utils/ZkQueue.scala,core/src/main/scala/kafka/utils/ZkQueue.scala,"revert commit to KAFKA-343 due to unit test failures

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1367811 13f79535-47bb-0310-9956-ffa450edef68",10,127,0,65,511,6,6,254,127,127,2,1.0,254,127,127,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/controller/ControllerToBrokerRequestTest.scala,core/src/test/scala/unit/kafka/controller/ControllerToBrokerRequestTest.scala,"revert commit to KAFKA-343 due to unit test failures

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1367811 13f79535-47bb-0310-9956-ffa450edef68",6,173,0,109,801,6,6,346,173,173,2,1.0,346,173,173,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/server/StateChangeTest.scala,core/src/test/scala/unit/kafka/server/StateChangeTest.scala,"revert commit to KAFKA-343 due to unit test failures

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1367811 13f79535-47bb-0310-9956-ffa450edef68",13,124,0,76,492,6,6,248,124,124,2,1.0,248,124,124,0,0,0,0,0,0,0
core/src/main/scala/kafka/log/LogStats.scala,core/src/main/scala/kafka/log/LogStats.scala,"KAFKA-405 Improve high watermark maintenance to store high watermarks for all partitions in a single .highwatermark file; patched by Neha Narkhede; reviewed by Jay Kreps and Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1365841 13f79535-47bb-0310-9956-ffa450edef68",4,1,1,18,121,0,4,44,43,9,5,1,53,43,11,9,6,2,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLRecordReader.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLRecordReader.java,"KAFKA-350 Enable message replication in the presence of failures; patched by Neha Narkhede; reviewed by Jun Rao and Jay Kreps

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1365199 13f79535-47bb-0310-9956-ffa450edef68",26,4,4,129,865,1,7,180,164,60,3,1,184,164,61,4,4,1,2,1,0,1
core/src/main/scala/kafka/consumer/storage/OracleOffsetStorage.scala,core/src/main/scala/kafka/consumer/storage/OracleOffsetStorage.scala,"KAFKA-350 Enable message replication in the presence of failures; patched by Neha Narkhede; reviewed by Jun Rao and Jay Kreps

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1365199 13f79535-47bb-0310-9956-ffa450edef68",21,4,3,109,664,3,10,155,157,31,5,1,171,157,34,16,6,3,2,1,0,1
core/src/main/scala/kafka/common/RequestTimedOutException.scala,core/src/main/scala/kafka/common/RequestTimedOutException.scala,"KAFKA-353 Tie producer-side ack with high watermark and progress of replicas; patched by Joel Koshy; reviewed by Jun Rao, Jay Kreps

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1355578 13f79535-47bb-0310-9956-ffa450edef68",1,29,0,4,27,1,1,29,29,29,1,1,29,29,29,0,0,0,0,0,0,0
core/src/main/scala/kafka/consumer/FetcherRunnable.scala,core/src/main/scala/kafka/consumer/FetcherRunnable.scala,"Consumer doesn't receive all data; patched by Jun Rao; reviewed by John Fung; KAFKA-372

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1354094 13f79535-47bb-0310-9956-ffa450edef68",23,2,1,108,734,1,4,150,143,14,11,2,235,143,21,85,25,8,2,1,0,1
core/src/main/scala/kafka/common/NotLeaderForPartitionException.scala,core/src/main/scala/kafka/common/NotLeaderForPartitionException.scala,"KAFKA-46: Message replication feature without failures; patched by Neha Narkhede; reviewed by Jun Rao, Jay Kreps, Prashanth Menon

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344964 13f79535-47bb-0310-9956-ffa450edef68",1,4,4,4,27,1,1,25,25,12,2,2.5,29,25,14,4,4,2,2,1,0,1
core/src/main/scala/kafka/common/UnknownTopicException.scala,core/src/main/scala/kafka/common/UnknownTopicException.scala,"KAFKA-46: Message replication feature without failures; patched by Neha Narkhede; reviewed by Jun Rao, Jay Kreps, Prashanth Menon

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344964 13f79535-47bb-0310-9956-ffa450edef68",1,25,0,4,27,1,1,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/test/scala/unit/kafka/consumer/TopicCountTest.scala,core/src/test/scala/unit/kafka/consumer/TopicCountTest.scala,"KAFKA-46: Message replication feature without failures; patched by Neha Narkhede; reviewed by Jun Rao, Jay Kreps, Prashanth Menon

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344964 13f79535-47bb-0310-9956-ffa450edef68",1,2,2,12,89,1,1,0,0,0,2,1.5,4,2,2,4,2,2,2,1,0,1
clients/cpp/src/encoder.hpp,clients/cpp/src/encoder.hpp,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",1,1,1,23,173,0,1,62,49,16,4,1.0,67,49,17,5,3,1,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLInputFormat.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLInputFormat.java,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",3,3,2,51,339,0,3,79,61,20,4,2.5,83,61,21,4,2,1,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLJob.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLJob.java,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",21,1,1,114,899,0,4,172,156,43,4,1.5,174,156,44,2,1,0,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLKey.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLKey.java,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",16,1,1,69,360,0,11,104,88,26,4,1.5,106,88,26,2,1,0,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLUtils.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLUtils.java,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",27,1,1,158,1128,0,12,205,204,51,4,1.5,213,204,53,8,6,2,2,1,0,1
core/src/main/scala/kafka/consumer/Fetcher.scala,core/src/main/scala/kafka/consumer/Fetcher.scala,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",10,6,6,53,399,2,2,102,79,13,8,2.0,135,79,17,33,11,4,2,1,0,1
core/src/main/scala/kafka/message/CompressionUtils.scala,core/src/main/scala/kafka/message/CompressionUtils.scala,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",32,1,1,114,849,1,12,160,143,20,8,1.0,287,143,36,127,108,16,2,1,0,1
examples/src/main/java/kafka/examples/ExampleUtils.java,examples/src/main/java/kafka/examples/ExampleUtils.java,"KAFKA-348 merge trunk to branch 1239902:1310937 patch by Joe Stein reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1344526 13f79535-47bb-0310-9956-ffa450edef68",1,1,1,13,72,0,1,32,31,8,4,1.5,40,31,10,8,6,2,2,1,0,1
core/src/main/scala/kafka/common/NoEpochForPartitionException.scala,core/src/main/scala/kafka/common/NoEpochForPartitionException.scala,"KAFKA-301 Implement broker startup procedure; patched by Neha Narkhede; reviewed by Jun Rao and Jay Kreps

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1329509 13f79535-47bb-0310-9956-ffa450edef68",1,25,0,4,27,1,1,25,25,25,1,1,25,25,25,0,0,0,0,0,0,0
core/src/main/scala/kafka/common/QueueFullException.scala,core/src/main/scala/kafka/common/QueueFullException.scala,"KAFKA-301 Implement broker startup procedure; patched by Neha Narkhede; reviewed by Jun Rao and Jay Kreps

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1329509 13f79535-47bb-0310-9956-ffa450edef68",1,2,2,4,27,0,1,23,22,8,3,1,31,22,10,8,6,3,0,0,0,0
core/src/test/scala/unit/kafka/zk/ZKLoadBalanceTest.scala,core/src/test/scala/unit/kafka/zk/ZKLoadBalanceTest.scala,"KAFKA-320 testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException; patched by nehanarkhede; reviewed by junrao and prashanth menon

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1310937 13f79535-47bb-0310-9956-ffa450edef68",6,5,5,81,632,2,4,126,126,25,5,1,140,126,28,14,6,3,2,1,0,1
core/src/main/scala/kafka/javaapi/producer/async/CallbackHandler.java,core/src/main/scala/kafka/javaapi/producer/async/CallbackHandler.java,"Separate out Kafka mirroring into a stand-alone app; patched by Joel Koshy; reviewed by Jun Rao and Neha Narkhede; KAFKA-249

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1310645 13f79535-47bb-0310-9956-ffa450edef68",0,1,1,12,140,0,0,77,76,26,3,1,84,76,28,7,6,2,2,1,0,1
core/src/main/scala/kafka/javaapi/producer/async/EventHandler.java,core/src/main/scala/kafka/javaapi/producer/async/EventHandler.java,"Separate out Kafka mirroring into a stand-alone app; patched by Joel Koshy; reviewed by Jun Rao and Neha Narkhede; KAFKA-249

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1310645 13f79535-47bb-0310-9956-ffa450edef68",0,3,3,11,93,0,0,48,47,16,3,1,57,47,19,9,6,3,2,1,0,1
clients/go/src/consumer.go,clients/go/src/consumer.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",28,10,11,130,782,6,9,199,184,50,4,11.0,231,184,58,32,11,8,2,1,0,1
clients/go/src/kafka.go,clients/go/src/kafka.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",11,10,10,63,392,2,3,95,97,24,4,6.0,121,97,30,26,10,6,2,1,0,1
clients/go/src/message.go,clients/go/src/message.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",22,2,2,130,947,0,12,182,107,46,4,2.0,205,107,51,23,19,6,2,1,0,1
clients/go/src/payload_codec.go,clients/go/src/payload_codec.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",11,3,1,64,366,1,7,116,116,39,3,3,120,116,40,4,3,1,1,0,1,1
clients/go/src/publisher.go,clients/go/src/publisher.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",5,6,2,26,159,2,3,55,59,14,4,3.0,71,59,18,16,8,4,2,1,0,1
clients/go/src/request.go,clients/go/src/request.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",6,1,1,52,367,0,5,101,108,25,4,2.0,116,108,29,15,13,4,2,1,0,1
clients/go/src/timing.go,clients/go/src/timing.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",4,3,3,22,119,3,3,49,50,12,4,2.0,56,50,14,7,3,2,2,1,0,1
clients/go/tools/consumer/consumer.go,clients/go/tools/consumer/consumer.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",14,8,15,79,432,4,4,111,113,28,4,5.5,137,113,34,26,15,6,2,1,0,1
clients/go/tools/offsets/offsets.go,clients/go/tools/offsets/offsets.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",4,4,2,33,211,1,2,62,62,21,3,5,68,62,23,6,4,2,2,1,0,1
clients/go/tools/publisher/publisher.go,clients/go/tools/publisher/publisher.go,"reverting previous commit for KAFKA-296 because patch didn't apply cleanly

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1300801 13f79535-47bb-0310-9956-ffa450edef68",8,2,2,58,345,1,2,89,75,22,4,3.0,95,75,24,6,2,2,2,1,0,1
core/src/main/scala/kafka/javaapi/producer/SyncProducer.scala,core/src/main/scala/kafka/javaapi/producer/SyncProducer.scala,"Add acknowledgement to the produce request; patched by Prashanth Menon; reviewed by Jun Rao; KAFKA-49

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1300435 13f79535-47bb-0310-9956-ffa450edef68",4,7,10,20,180,2,4,43,47,11,4,2.0,72,47,18,29,13,7,2,1,0,1
core/src/main/scala/kafka/network/SocketServerStats.scala,core/src/main/scala/kafka/network/SocketServerStats.scala,"Add acknowledgement to the produce request; patched by Prashanth Menon; reviewed by Jun Rao; KAFKA-49

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1300435 13f79535-47bb-0310-9956-ffa450edef68",9,1,1,52,375,1,4,88,79,13,7,1,100,79,14,12,6,2,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/producer/ProducerTest.scala,core/src/test/scala/unit/kafka/javaapi/producer/ProducerTest.scala,"use propertyExists to test if both broker.list and zk.connect are present; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-290

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1294959 13f79535-47bb-0310-9956-ffa450edef68",31,4,0,499,4182,4,21,632,630,79,8,3.5,659,630,82,27,6,3,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLRequest.java,contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLRequest.java,"new consumer request format; patched by Prashanth Menon; reviewed by Jun Rao and Jay Kreps; KAFKA-240

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1243407 13f79535-47bb-0310-9956-ffa450edef68",23,6,5,88,551,5,16,129,112,43,3,1,134,112,45,5,5,2,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/integration/PrimitiveApiTest.scala,core/src/test/scala/unit/kafka/javaapi/integration/PrimitiveApiTest.scala,"new consumer request format; patched by Prashanth Menon; reviewed by Jun Rao and Jay Kreps; KAFKA-240

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1243407 13f79535-47bb-0310-9956-ffa450edef68",55,167,167,315,2551,10,9,415,416,69,6,2.5,602,416,100,187,167,31,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/producer/SyncProducerTest.scala,core/src/test/scala/unit/kafka/javaapi/producer/SyncProducerTest.scala,"KAFKA-262 Bug in the consumer rebalancing logic causes one consumer to release partitions that it does not own; patched by Neha Narkhede; reviewed by Jun Rao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1242552 13f79535-47bb-0310-9956-ffa450edef68",10,2,2,74,496,0,4,84,173,17,5,2,192,173,38,108,76,22,2,1,0,1
core/src/main/scala/kafka/common/FailedToSendMessageException.scala,core/src/main/scala/kafka/common/FailedToSendMessageException.scala,"refactor the async producer; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-253

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1241754 13f79535-47bb-0310-9956-ffa450edef68",0,6,6,3,24,1,0,23,22,8,3,1,35,22,12,12,6,4,0,0,0,0
core/src/main/scala/kafka/javaapi/producer/ProducerData.scala,core/src/main/scala/kafka/javaapi/producer/ProducerData.scala,"refactor the async producer; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-253

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1241754 13f79535-47bb-0310-9956-ffa450edef68",3,5,3,12,180,2,3,36,33,12,3,1,45,33,15,9,6,3,2,1,0,1
core/src/main/scala/kafka/producer/ConfigBrokerPartitionInfo.scala,core/src/main/scala/kafka/producer/ConfigBrokerPartitionInfo.scala,"refactor the async producer; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-253

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1241754 13f79535-47bb-0310-9956-ffa450edef68",6,8,9,43,338,2,4,95,95,16,6,2.0,116,95,19,21,9,4,2,1,0,1
core/src/main/scala/kafka/producer/async/AsyncProducerStatsMBean.scala,core/src/main/scala/kafka/producer/async/AsyncProducerStatsMBean.scala,"refactor the async producer; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-253

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1241754 13f79535-47bb-0310-9956-ffa450edef68",0,0,1,7,22,0,0,26,22,6,4,1.0,34,22,8,8,6,2,2,1,0,1
core/src/main/scala/kafka/api/MultiFetchRequest.scala,core/src/main/scala/kafka/api/MultiFetchRequest.scala,"KAFKA 256 Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions; patched by nehanarkhede; reviewed by joelkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1239766 13f79535-47bb-0310-9956-ffa450edef68",7,0,2,35,215,0,3,56,59,14,4,1.0,66,59,16,10,6,2,2,1,0,1
core/src/main/scala/kafka/server/MultiMessageSetSend.scala,core/src/main/scala/kafka/server/MultiMessageSetSend.scala,"KAFKA 256 Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions; patched by nehanarkhede; reviewed by joelkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1239766 13f79535-47bb-0310-9956-ffa450edef68",0,0,3,12,108,0,0,33,38,8,4,1.5,45,38,11,12,6,3,2,1,0,1
core/src/main/scala/kafka/utils/Range.scala,core/src/main/scala/kafka/utils/Range.scala,"KAFKA 256 Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions; patched by nehanarkhede; reviewed by joelkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1239766 13f79535-47bb-0310-9956-ffa450edef68",7,1,2,14,86,0,2,41,42,10,4,1.5,51,42,13,10,6,2,2,1,0,1
core/src/main/scala/kafka/producer/ZKBrokerPartitionInfo.scala,core/src/main/scala/kafka/producer/ZKBrokerPartitionInfo.scala,"KAFKA-238 getTopicMetadata API; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1235492 13f79535-47bb-0310-9956-ffa450edef68",30,3,7,240,1901,1,11,376,359,63,6,2.5,501,359,84,125,57,21,2,1,0,1
core/src/main/scala/kafka/producer/async/AsyncProducer.scala,core/src/main/scala/kafka/producer/async/AsyncProducer.scala,"KAFKA-238 getTopicMetadata API; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1235492 13f79535-47bb-0310-9956-ffa450edef68",14,6,9,107,740,1,4,141,130,18,8,3.5,200,130,25,59,20,7,2,1,0,1
core/src/main/scala/kafka/common/ConsumerReblanceFailedException.scala,core/src/main/scala/kafka/common/ConsumerReblanceFailedException.scala,"Bug in mirroring code causes mirroring to halt; patched by Jun Rao; reviewed by Neha Narkhede; KAFKA-225

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1213990 13f79535-47bb-0310-9956-ffa450edef68",1,26,0,4,27,1,1,26,26,26,1,1,26,26,26,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/message/CompressionUtilsTest.scala,core/src/test/scala/unit/kafka/message/CompressionUtilsTest.scala,"Add Snappy Compression as a Codec; patched by Joe Stein; reviewed by Neha Narkhede and Jun Rao; KAFKA-187

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1202045 13f79535-47bb-0310-9956-ffa450edef68",3,17,0,34,344,1,3,75,41,19,4,1.5,83,41,21,8,8,2,2,1,0,1
clients/go/kafka_test.go,clients/go/kafka_test.go,"KAFKA 158 Support for compression in go clients; patched by jeffregydamick; reviewed by nehanarkhede

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1189773 13f79535-47bb-0310-9956-ffa450edef68",45,170,21,194,1718,9,10,277,149,138,2,10.0,298,170,149,21,21,10,2,1,0,1
clients/go/src/converts.go,clients/go/src/converts.go,"KAFKA 158 Support for compression in go clients; patched by jeffregydamick; reviewed by nehanarkhede

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1189773 13f79535-47bb-0310-9956-ffa450edef68",4,0,2,24,144,0,4,51,53,26,2,1.5,53,53,26,2,2,1,2,1,0,1
clients/python/kafka.py,clients/python/kafka.py,"KAFKA-162 Upgrade the python client to use the new message format.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1186535 13f79535-47bb-0310-9956-ffa450edef68",7,3,2,41,360,1,5,73,68,24,3,1,86,68,29,13,11,4,2,1,0,1
clients/php/src/lib/Kafka/Encoder.php,clients/php/src/lib/Kafka/Encoder.php,"Php Client support for compression attribute; patched by AaronR; KAFKA-159

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185774 13f79535-47bb-0310-9956-ffa450edef68",2,8,6,20,146,2,2,73,71,36,2,3.5,79,71,40,6,6,3,2,1,0,1
clients/php/src/lib/Kafka/Message.php,clients/php/src/lib/Kafka/Message.php,"Php Client support for compression attribute; patched by AaronR; KAFKA-159

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185774 13f79535-47bb-0310-9956-ffa450edef68",8,11,12,35,198,2,8,126,127,63,2,3.5,138,127,69,12,12,6,2,1,0,1
clients/php/src/lib/Kafka/Producer.php,clients/php/src/lib/Kafka/Producer.php,"Php Client support for compression attribute; patched by AaronR; KAFKA-159

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185774 13f79535-47bb-0310-9956-ffa450edef68",9,7,1,37,217,2,6,122,116,61,2,2.0,123,116,62,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/IAsyncProducerConfigShared.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/IAsyncProducerConfigShared.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,1,15,9,30,0,0,26,39,9,3,2,48,39,16,22,15,7,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/ISyncProducerConfigShared.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/ISyncProducerConfigShared.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,0,2,11,46,0,0,30,31,10,3,1,39,31,13,9,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/Consumer.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/Consumer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",17,99,152,141,686,9,5,235,287,78,3,2,394,287,131,159,152,53,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/ConsumerIterator.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/ConsumerIterator.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",21,29,16,139,607,4,9,211,197,70,3,2,234,197,78,23,16,8,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/Fetcher.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/Fetcher.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",13,2,2,108,514,2,5,170,169,57,3,2,179,169,60,9,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/FetcherRunnable.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/FetcherRunnable.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",16,7,6,147,877,3,4,191,189,64,3,2,204,189,68,13,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/IConsumer.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/IConsumer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,0,10,13,62,0,0,74,83,25,3,1,91,83,30,17,10,6,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/ZookeeperConsumerConnector.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/ZookeeperConsumerConnector.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",27,7,8,219,1342,3,11,322,322,107,3,2,337,322,112,15,8,5,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/ConsumerTimeoutException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/ConsumerTimeoutException.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,3,0,11,36,0,0,28,14,9,3,1,32,18,11,4,4,1,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/KafkaConnection.cs,clients/csharp/src/Kafka/Kafka.Client/KafkaConnection.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",15,32,105,120,672,14,11,224,204,56,4,1.5,540,296,135,316,204,79,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Messages/BufferedMessageSet.cs,clients/csharp/src/Kafka/Kafka.Client/Messages/BufferedMessageSet.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",44,262,35,296,1445,20,21,407,227,136,3,2,449,262,150,42,35,14,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Messages/Message.cs,clients/csharp/src/Kafka/Kafka.Client/Messages/Message.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",22,136,60,202,1214,14,11,329,252,110,3,2,396,252,132,67,60,22,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Messages/MessageSet.cs,clients/csharp/src/Kafka/Kafka.Client/Messages/MessageSet.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",3,1,1,26,142,1,2,91,90,30,3,1,99,90,33,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Async/AsyncProducer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Async/AsyncProducer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",14,63,27,105,641,11,9,207,170,69,3,2,241,170,80,34,27,11,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Async/AsyncProducerPool.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Async/AsyncProducerPool.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",10,31,14,114,660,13,8,224,206,75,3,2,245,206,82,21,14,7,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Async/IAsyncProducer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Async/IAsyncProducer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,2,1,15,100,0,0,80,78,27,3,2,88,78,29,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/IProducerPool.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/IProducerPool.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,2,1,13,66,0,0,58,56,19,3,2,66,56,22,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/ConfigBrokerPartitionInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/ConfigBrokerPartitionInfo.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",8,8,10,56,287,4,6,119,120,40,3,2,136,120,45,17,10,6,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/DefaultPartitioner.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/DefaultPartitioner.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",2,1,1,18,92,1,1,50,49,17,3,1,58,49,19,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/ZKBrokerPartitionInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/ZKBrokerPartitionInfo.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",26,13,8,212,1366,7,14,342,336,114,3,2,357,336,119,15,8,5,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Producer.StrMsg.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Producer.StrMsg.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",4,4,4,28,170,8,4,96,95,32,3,2,107,95,36,11,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Producer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Producer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",24,32,25,212,1203,14,11,337,329,112,3,2,369,329,123,32,25,11,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerPool.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerPool.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",12,36,10,88,510,11,7,206,179,69,3,2,223,179,74,17,10,6,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/ISyncProducer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/ISyncProducer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,2,1,14,75,0,0,60,58,20,3,2,68,58,23,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/SyncProducer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/SyncProducer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",14,58,22,85,476,8,7,155,118,52,3,2,184,118,61,29,22,10,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/SyncProducerPool.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Sync/SyncProducerPool.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",11,32,16,112,688,13,8,226,209,75,3,2,249,209,83,23,16,8,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/FetchRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/FetchRequest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",8,2,2,85,460,2,8,156,155,52,3,2,165,155,55,9,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/MultiFetchRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/MultiFetchRequest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",4,3,3,60,309,3,4,109,108,36,3,2,119,108,40,10,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/MultiProducerRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/MultiProducerRequest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,10,21,81,448,6,5,134,144,45,3,2,162,144,54,28,21,9,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/OffsetRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/OffsetRequest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,2,2,67,383,2,5,136,135,45,3,2,145,135,48,9,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/ProducerRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/ProducerRequest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",6,17,32,92,558,5,6,142,156,47,3,2,181,156,60,39,32,13,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/KafkaBinaryReader.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/KafkaBinaryReader.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",10,18,1,68,300,2,8,148,130,49,3,2,156,130,52,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/Guard.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/Guard.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",13,39,15,80,421,6,7,120,95,40,3,2,142,95,47,22,15,7,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/ReflectionHelper.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/ReflectionHelper.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",2,2,4,36,193,2,2,56,57,19,3,2,67,57,22,11,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperAwareKafkaClientBase.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperAwareKafkaClientBase.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",2,1,1,13,58,2,1,44,43,15,3,1,52,43,17,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperClient.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperClient.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",0,1,1,53,405,0,0,469,468,156,3,1,477,468,159,8,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/BrokerTopicsListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/BrokerTopicsListener.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",17,3,3,193,1549,1,5,257,256,86,3,1,267,256,89,10,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/ZKRebalancerListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/ZKRebalancerListener.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",33,6,6,294,1994,4,12,368,367,123,3,2,381,367,127,13,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/ZKSessionExpireListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/ZKSessionExpireListener.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",3,2,2,45,259,2,3,86,85,29,3,2,95,85,32,9,7,3,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",60,33,32,421,2159,20,30,893,891,298,3,2,932,891,311,39,32,13,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperConnection.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperConnection.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",21,8,8,145,790,8,13,327,326,109,3,2,342,326,114,15,8,5,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperStringSerializer.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperStringSerializer.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",4,3,3,26,138,2,3,72,71,24,3,2,82,71,27,10,7,3,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ConsumerRebalancingTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ConsumerRebalancingTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,10,42,204,2114,6,5,236,267,79,3,2,285,267,95,49,42,16,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ConsumerTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ConsumerTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",10,63,66,236,1591,7,6,307,309,102,3,2,380,309,127,73,66,24,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/IntegrationFixtureBase.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/IntegrationFixtureBase.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,105,4,118,472,2,2,147,101,49,3,2,158,105,53,11,7,4,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/KafkaIntegrationTest.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/KafkaIntegrationTest.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",34,141,162,361,2536,19,17,488,327,122,4,1.5,838,508,210,350,181,88,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/MockAlwaysZeroPartitioner.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/MockAlwaysZeroPartitioner.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",1,2,0,13,52,0,1,34,19,11,3,1,38,19,13,4,4,1,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ProducerTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ProducerTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",21,115,134,178,1198,4,3,226,244,75,3,2,367,244,122,141,134,47,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestHelper.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestHelper.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",3,12,15,29,198,4,3,48,50,16,3,2,70,50,23,22,15,7,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestMultipleBrokersHelper.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestMultipleBrokersHelper.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,23,26,52,344,5,3,78,80,26,3,2,111,80,37,33,26,11,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZKBrokerPartitionInfoTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZKBrokerPartitionInfoTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",14,106,98,409,2783,25,12,464,455,155,3,2,569,455,190,105,98,35,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperAwareProducerTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperAwareProducerTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",20,36,51,174,1227,5,3,224,238,75,3,2,282,238,94,58,51,19,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperClientTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperClientTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",21,82,37,351,2392,15,21,411,365,137,3,2,455,365,152,44,37,15,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperConnectionTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/ZooKeeperConnectionTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,17,20,91,602,6,5,118,120,39,3,2,145,120,48,27,20,9,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/MessageSetTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/MessageSetTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",4,11,9,74,650,2,2,108,105,36,3,2,124,105,41,16,9,5,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/MessageTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/MessageTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",4,21,14,85,821,4,4,138,68,34,4,1.5,227,130,57,89,68,22,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Producers/PartitioningTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Producers/PartitioningTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",5,6,3,51,325,1,5,73,69,24,3,2,83,69,28,10,7,3,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/MultiProducerRequestTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/MultiProducerRequestTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",1,2,2,36,358,1,1,69,86,17,4,1.5,164,86,41,95,86,24,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/ProducerRequestTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/ProducerRequestTests.cs,"Add compression to C# client; patched by Eric Hauser; KAFKA-153

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1185772 13f79535-47bb-0310-9956-ffa450edef68",1,4,4,36,410,1,1,77,86,19,4,1.5,174,86,44,97,86,24,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/AbstractRequest.cs,clients/csharp/src/Kafka/Kafka.Client/AbstractRequest.cs,"KAFKA-143 Fixing source code files to have the Apache license header ;patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1183191 13f79535-47bb-0310-9956-ffa450edef68",0,17,1,15,67,0,0,51,35,26,2,1.0,52,35,26,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Properties/AssemblyInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Properties/AssemblyInfo.cs,"KAFKA-143 Fixing source code files to have the Apache license header ;patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1183191 13f79535-47bb-0310-9956-ffa450edef68",0,17,1,16,111,0,0,34,36,11,3,1,71,36,24,37,36,12,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Properties/AssemblyInfo.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Properties/AssemblyInfo.cs,"KAFKA-143 Fixing source code files to have the Apache license header ;patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1183191 13f79535-47bb-0310-9956-ffa450edef68",0,17,1,15,109,0,0,51,36,17,3,1,53,36,18,2,1,1,2,1,0,1
project/plugins/Plugins.scala,project/plugins/Plugins.scala,"KAFKA-143 Fixing source code files to have the Apache license header ;patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1183191 13f79535-47bb-0310-9956-ffa450edef68",0,17,0,5,32,0,0,23,17,12,2,1.0,23,17,12,0,0,0,2,1,0,1
clients/cpp/src/encoder_helper.hpp,clients/cpp/src/encoder_helper.hpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",2,16,0,34,225,0,2,79,63,40,2,1.0,79,63,40,0,0,0,2,1,0,1
clients/cpp/src/example.cpp,clients/cpp/src/example.cpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",4,16,0,27,211,0,1,54,38,27,2,1.0,54,38,27,0,0,0,2,1,0,1
clients/cpp/src/producer.cpp,clients/cpp/src/producer.cpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",13,17,0,75,411,0,9,117,100,58,2,1.0,117,100,58,0,0,0,2,1,0,1
clients/cpp/src/producer.hpp,clients/cpp/src/producer.hpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",5,17,0,56,437,0,3,116,99,58,2,1.0,116,99,58,0,0,0,2,1,0,1
clients/cpp/src/tests/encoder_helper_tests.cpp,clients/cpp/src/tests/encoder_helper_tests.cpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",6,17,0,45,400,0,5,88,71,44,2,1.0,88,71,44,0,0,0,2,1,0,1
clients/cpp/src/tests/encoder_tests.cpp,clients/cpp/src/tests/encoder_tests.cpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",2,17,0,32,445,0,2,69,52,34,2,1.0,69,52,34,0,0,0,2,1,0,1
clients/cpp/src/tests/producer_tests.cpp,clients/cpp/src/tests/producer_tests.cpp,"KAFKA-141 Follow up patch to fix the cpp files; patched by Lorenzo, nehanarkhede; reviewed by junrao, jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182158 13f79535-47bb-0310-9956-ffa450edef68",2,17,0,38,432,0,1,76,59,38,2,1.0,76,59,38,0,0,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumer.cs,clients/csharp/src/Kafka/Kafka.Client/Consumer.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",18,18,1,133,1137,0,7,249,232,124,2,1.0,250,232,125,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/KafkaException.cs,clients/csharp/src/Kafka/Kafka.Client/KafkaException.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",6,18,1,44,162,0,2,98,81,49,2,1.0,99,81,50,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Message.cs,clients/csharp/src/Kafka/Kafka.Client/Message.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",9,18,1,56,395,0,8,157,140,78,2,1.0,158,140,79,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producer.cs,clients/csharp/src/Kafka/Kafka.Client/Producer.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",11,18,1,66,330,0,7,152,135,76,2,1.0,153,135,76,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Request/FetchRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Request/FetchRequest.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",6,17,1,53,348,0,6,129,113,64,2,1.0,130,113,65,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Request/MultiFetchRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Request/MultiFetchRequest.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",5,18,1,37,245,0,3,79,62,40,2,1.0,80,62,40,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Request/MultiProducerRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Request/MultiProducerRequest.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",6,17,1,40,251,0,4,87,71,44,2,1.0,88,71,44,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Request/OffsetRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Request/OffsetRequest.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",4,18,1,48,317,0,4,107,90,54,2,1.0,108,90,54,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Request/ProducerRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Request/ProducerRequest.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",7,18,1,56,389,0,5,115,98,58,2,1.0,116,98,58,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/RequestType.cs,clients/csharp/src/Kafka/Kafka.Client/RequestType.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",0,18,1,12,31,0,0,53,36,26,2,1.0,54,36,27,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Util/BitWorks.cs,clients/csharp/src/Kafka/Kafka.Client/Util/BitWorks.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",7,18,1,39,194,0,4,86,69,43,2,1.0,87,69,44,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Util/Crc32.cs,clients/csharp/src/Kafka/Kafka.Client/Util/Crc32.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",18,18,1,95,562,0,11,132,115,66,2,1.0,133,115,66,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.Watcher.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/ZooKeeperClient.Watcher.cs,"KAFKA-143 Check and make sure that all source code distributed by the project is covered by one or more approved licenses; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1182023 13f79535-47bb-0310-9956-ffa450edef68",69,8,6,446,2299,0,25,682,680,341,2,1.5,688,680,344,6,6,3,1,0,1,1
clients/php/src/examples/autoloader.php,clients/php/src/examples/autoloader.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",5,17,0,37,111,0,1,40,23,10,4,1.0,57,23,14,17,17,4,2,1,0,1
clients/php/src/examples/consume.php,clients/php/src/examples/consume.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,17,0,45,140,0,0,53,36,13,4,1.0,70,36,18,17,17,4,2,1,0,1
clients/php/src/examples/produce.php,clients/php/src/examples/produce.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,17,0,40,123,0,0,46,29,12,4,1.0,63,29,16,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php,clients/php/src/tests/Kafka/BoundedByteBuffer/ReceiveTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",9,17,0,98,765,0,9,133,116,33,4,1.0,150,116,38,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php,clients/php/src/tests/Kafka/BoundedByteBuffer/SendTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",7,17,0,67,397,0,7,100,83,25,4,1.0,117,83,29,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/EncoderTest.php,clients/php/src/tests/Kafka/EncoderTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",3,17,0,53,209,0,3,61,44,15,4,1.0,78,44,20,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/FetchRequestTest.php,clients/php/src/tests/Kafka/FetchRequestTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",6,17,0,71,508,0,6,88,71,22,4,1.0,105,71,26,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/MessageTest.php,clients/php/src/tests/Kafka/MessageTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",8,17,0,50,215,0,8,62,45,16,4,1.0,79,45,20,17,17,4,2,1,0,1
clients/php/src/tests/Kafka/ProducerTest.php,clients/php/src/tests/Kafka/ProducerTest.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",6,17,0,56,198,0,5,76,59,19,4,1.0,93,59,23,17,17,4,2,1,0,1
clients/php/src/tests/bootstrap.php,clients/php/src/tests/bootstrap.php,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",5,17,0,45,144,0,1,53,36,13,4,1.0,70,36,18,17,17,4,2,1,0,1
clients/ruby/lib/kafka.rb,clients/ruby/lib/kafka.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,14,0,11,119,0,0,27,14,7,4,1.0,41,14,10,14,14,4,2,1,0,1
clients/ruby/lib/kafka/batch.rb,clients/ruby/lib/kafka/batch.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",2,14,0,11,30,0,2,27,14,7,4,1.0,41,14,10,14,14,4,2,1,0,1
clients/ruby/lib/kafka/consumer.rb,clients/ruby/lib/kafka/consumer.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",30,14,0,109,734,0,14,149,135,37,4,1.0,163,135,41,14,14,4,2,1,0,1
clients/ruby/lib/kafka/error_codes.rb,clients/ruby/lib/kafka/error_codes.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",2,14,0,19,59,0,1,35,21,9,4,1.0,49,21,12,14,14,4,2,1,0,1
clients/ruby/lib/kafka/io.rb,clients/ruby/lib/kafka/io.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",9,14,0,34,155,0,5,53,39,13,4,1.0,67,39,17,14,14,4,2,1,0,1
clients/ruby/lib/kafka/message.rb,clients/ruby/lib/kafka/message.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",5,14,0,24,151,0,4,49,35,12,4,1.0,63,35,16,14,14,4,2,1,0,1
clients/ruby/lib/kafka/producer.rb,clients/ruby/lib/kafka/producer.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",9,14,0,38,288,0,5,63,49,16,4,1.0,77,49,19,14,14,4,2,1,0,1
clients/ruby/lib/kafka/request_type.rb,clients/ruby/lib/kafka/request_type.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,14,0,9,21,0,0,23,14,6,4,1.0,37,14,9,14,14,4,2,1,0,1
clients/ruby/spec/batch_spec.rb,clients/ruby/spec/batch_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",2,14,0,17,95,0,2,35,21,9,4,1.0,49,21,12,14,14,4,2,1,0,1
clients/ruby/spec/consumer_spec.rb,clients/ruby/spec/consumer_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",16,14,0,101,792,0,15,134,120,34,4,1.0,148,120,37,14,14,4,2,1,0,1
clients/ruby/spec/io_spec.rb,clients/ruby/spec/io_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",9,14,0,65,359,0,9,91,77,23,4,1.0,105,77,26,14,14,4,2,1,0,1
clients/ruby/spec/kafka_spec.rb,clients/ruby/spec/kafka_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,14,0,5,19,0,0,21,14,5,4,1.0,35,14,9,14,14,4,2,1,0,1
clients/ruby/spec/message_spec.rb,clients/ruby/spec/message_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",7,14,0,46,275,0,7,69,55,17,4,1.0,83,55,21,14,14,4,2,1,0,1
clients/ruby/spec/producer_spec.rb,clients/ruby/spec/producer_spec.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",10,14,0,82,585,0,10,109,95,27,4,1.0,123,95,31,14,14,4,2,1,0,1
clients/ruby/spec/spec_helper.rb,clients/ruby/spec/spec_helper.rb,"KAFKA-141 Check and make sure that the files that have been donated have been updated to reflect the new ASF copyright;patched by nehanarkhede; reviewd by jjkoshy

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1180185 13f79535-47bb-0310-9956-ffa450edef68",0,14,0,3,6,0,0,18,14,4,4,1.0,32,14,8,14,14,4,2,1,0,1
core/src/main/scala/kafka/common/InvalidMessageSizeException.scala,core/src/main/scala/kafka/common/InvalidMessageSizeException.scala,"KAFKA-141; Added the ASF header to the kafka source and test files where it was missing, deleted the CONTRIBUTORS file to match the convention of other Apache projects; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1179788 13f79535-47bb-0310-9956-ffa450edef68",1,17,0,4,27,0,1,26,17,13,2,1.0,26,17,13,0,0,0,2,1,0,1
core/src/main/scala/kafka/common/NoBrokersForPartitionException.scala,core/src/main/scala/kafka/common/NoBrokersForPartitionException.scala,"KAFKA-141; Added the ASF header to the kafka source and test files where it was missing, deleted the CONTRIBUTORS file to match the convention of other Apache projects; patched by nehanarkhede; reviewed by junrao

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1179788 13f79535-47bb-0310-9956-ffa450edef68",1,17,0,4,27,0,1,26,17,13,2,1.0,26,17,13,0,0,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/AsyncProducerConfig.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/AsyncProducerConfig.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,40,290,0,2,74,73,37,2,1.5,81,73,40,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/BrokerPartitionInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/BrokerPartitionInfo.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,49,150,0,1,72,71,36,2,1.5,79,71,40,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/BrokerPartitionInfoCollection.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/BrokerPartitionInfoCollection.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,31,109,0,3,52,51,26,2,1.5,59,51,30,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/Consumer.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/Consumer.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,104,285,0,0,136,135,68,2,1.5,143,135,72,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/ConsumerConfig.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/ConsumerConfig.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,41,275,0,2,72,71,36,2,1.5,79,71,40,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/KafkaClientConfiguration.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/KafkaClientConfiguration.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",5,8,7,64,301,0,3,93,92,46,2,1.5,100,92,50,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/KafkaServer.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/KafkaServer.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,16,15,32,87,0,0,52,51,26,2,1.0,67,51,34,15,15,8,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/ProducerConfig.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/ProducerConfig.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,55,390,0,2,94,93,47,2,1.5,101,93,50,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/SyncProducerConfig.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/SyncProducerConfig.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,35,207,0,2,66,65,33,2,1.5,73,65,36,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/ZKConfig.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/ZKConfig.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,22,108,0,2,43,42,22,2,1.5,50,42,25,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cfg/ZooKeeperServers.cs,clients/csharp/src/Kafka/Kafka.Client/Cfg/ZooKeeperServers.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,26,120,0,0,45,44,22,2,1.5,52,44,26,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cluster/Broker.cs,clients/csharp/src/Kafka/Kafka.Client/Cluster/Broker.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,18,95,0,1,68,67,34,2,1.5,75,67,38,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cluster/Cluster.cs,clients/csharp/src/Kafka/Kafka.Client/Cluster/Cluster.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",10,8,7,75,406,0,5,130,129,65,2,1.5,137,129,68,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Cluster/Partition.cs,clients/csharp/src/Kafka/Kafka.Client/Cluster/Partition.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",11,8,7,58,302,0,6,147,146,74,2,1.5,154,146,77,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/ConsumerIteratorState.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/ConsumerIteratorState.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,9,7,11,21,0,0,28,26,14,2,1.5,35,26,18,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/FetchedDataChunk.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/FetchedDataChunk.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",6,8,7,36,161,0,3,58,57,29,2,1.5,65,57,32,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/IConsumerConnector.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/IConsumerConnector.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,11,51,0,0,45,44,22,2,1.5,52,44,26,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/KafkaMessageStream.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/KafkaMessageStream.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,28,135,0,3,53,52,26,2,1.5,60,52,30,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/PartitionTopicInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/PartitionTopicInfo.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",13,8,7,115,538,0,8,198,197,99,2,1.5,205,197,102,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Consumers/TopicCount.cs,clients/csharp/src/Kafka/Kafka.Client/Consumers/TopicCount.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",10,8,7,76,431,0,5,111,110,56,2,1.5,118,110,59,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/KafkaException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/KafkaException.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",7,8,7,45,157,0,3,100,99,50,2,1.5,107,99,54,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/MessageSizeTooLargeException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/MessageSizeTooLargeException.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,18,1,8,19,0,0,25,17,12,2,1.0,26,18,13,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZKRebalancerException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZKRebalancerException.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,18,4,15,38,0,2,33,19,16,2,1.0,37,19,18,4,4,2,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZooKeeperException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZooKeeperException.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",4,18,1,25,84,0,4,45,28,22,2,1.0,46,28,23,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZooKeeperTimeoutException.cs,clients/csharp/src/Kafka/Kafka.Client/Exceptions/ZooKeeperTimeoutException.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,18,1,16,45,0,2,34,17,17,2,1.0,35,18,18,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/KafkaClientBase.cs,clients/csharp/src/Kafka/Kafka.Client/KafkaClientBase.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,14,48,0,1,44,43,22,2,1.5,51,43,26,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Messages/BoundedBuffer.cs,clients/csharp/src/Kafka/Kafka.Client/Messages/BoundedBuffer.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,12,46,0,1,38,37,19,2,1.5,45,37,22,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Async/ICallbackHandler.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Async/ICallbackHandler.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,9,33,0,0,35,34,18,2,1.5,42,34,21,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Async/MessageSent.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Async/MessageSent.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,6,37,0,0,31,30,16,2,1.5,38,30,19,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/IProducer.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/IProducer.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,13,66,0,0,46,45,23,2,1.5,53,45,26,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/IBrokerPartitionInfo.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/IBrokerPartitionInfo.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,13,62,0,0,49,48,24,2,1.5,56,48,28,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/IPartitioner.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/Partitioning/IPartitioner.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,9,33,0,0,36,35,18,2,1.5,43,35,22,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerData.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerData.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,25,138,0,3,95,94,48,2,1.5,102,94,51,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerPoolData.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerPoolData.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,18,99,0,1,65,64,32,2,1.5,72,64,36,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerTypes.cs,clients/csharp/src/Kafka/Kafka.Client/Producers/ProducerTypes.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,10,25,0,0,29,28,14,2,1.5,36,28,18,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/RequestContext.cs,clients/csharp/src/Kafka/Kafka.Client/RequestContext.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,16,72,0,1,54,36,18,3,1,97,53,32,43,36,14,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/AbstractRequest.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/AbstractRequest.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,29,150,0,1,61,60,30,2,1.5,68,60,34,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Requests/RequestTypes.cs,clients/csharp/src/Kafka/Kafka.Client/Requests/RequestTypes.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,12,35,0,0,53,52,26,2,1.5,60,52,30,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/DefaultEncoder.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/DefaultEncoder.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,12,38,0,1,41,40,20,2,1.5,48,40,24,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/IEncoder.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/IEncoder.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,9,31,0,0,41,40,20,2,1.5,48,40,24,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/IWritable.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/IWritable.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,10,33,0,0,43,42,22,2,1.5,50,42,25,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/KafkaBinaryWriter.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/KafkaBinaryWriter.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",8,8,7,53,240,0,7,127,126,64,2,1.5,134,126,67,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Serialization/StringEncoder.cs,clients/csharp/src/Kafka/Kafka.Client/Serialization/StringEncoder.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,14,61,0,1,43,42,22,2,1.5,50,42,25,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/BitWorks.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/BitWorks.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",7,8,7,36,177,0,4,83,82,42,2,1.5,90,82,45,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/Crc32Hasher.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/Crc32Hasher.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",16,17,1,89,497,0,9,138,122,69,2,1.0,139,122,70,1,1,0,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/ErrorMapping.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/ErrorMapping.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,9,7,13,63,0,0,29,27,14,2,1.5,36,27,18,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/Extensions.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/Extensions.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",4,8,7,29,162,0,2,49,48,24,2,1.5,56,48,28,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/KafkaScheduler.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/KafkaScheduler.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",7,8,7,55,222,0,3,86,85,43,2,1.5,93,85,46,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/ZKGroupDirs.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/ZKGroupDirs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,9,7,19,84,0,1,39,37,20,2,1.5,46,37,23,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/ZKGroupTopicDirs.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/ZKGroupTopicDirs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,14,76,0,1,32,31,16,2,1.5,39,31,20,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/Utils/ZkUtils.cs,clients/csharp/src/Kafka/Kafka.Client/Utils/ZkUtils.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",16,8,7,118,564,0,6,147,146,74,2,1.5,154,146,77,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ChildChangedEventItem.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ChildChangedEventItem.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",4,8,7,52,203,0,3,113,112,56,2,1.5,120,112,60,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/DataChangedEventItem.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/DataChangedEventItem.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",6,8,7,83,344,0,4,161,160,80,2,1.5,168,160,84,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperChildChangedEventArgs.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperChildChangedEventArgs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,22,84,0,1,60,59,30,2,1.5,67,59,34,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperDataChangedEventArgs.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperDataChangedEventArgs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",3,8,7,36,129,0,2,88,87,44,2,1.5,95,87,48,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperEventArgs.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperEventArgs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,18,66,0,2,56,55,28,2,1.5,63,55,32,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperEventTypes.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperEventTypes.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,12,36,0,0,35,34,18,2,1.5,42,34,21,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperSessionCreatedEventArgs.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperSessionCreatedEventArgs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,19,55,0,1,46,45,23,2,1.5,53,45,26,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperStateChangedEventArgs.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Events/ZooKeeperStateChangedEventArgs.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,21,66,0,1,55,54,28,2,1.5,62,54,31,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperConnection.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperConnection.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,24,165,0,0,164,163,82,2,1.5,171,163,86,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperSerializer.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/IZooKeeperSerializer.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,9,32,0,0,48,47,24,2,1.5,55,47,28,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperChildListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperChildListener.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,10,37,0,0,43,42,22,2,1.5,50,42,25,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperDataListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperDataListener.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,10,39,0,0,49,48,24,2,1.5,56,48,28,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperStateListener.cs,clients/csharp/src/Kafka/Kafka.Client/ZooKeeperIntegration/Listeners/IZooKeeperStateListener.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,8,7,10,39,0,0,42,41,21,2,1.5,49,41,24,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/Properties/AssemblyInfo.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/Properties/AssemblyInfo.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",0,18,1,15,109,0,0,52,36,17,3,1,89,36,30,37,36,12,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestsSetup.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.IntegrationTests/TestsSetup.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,18,66,0,1,35,34,18,2,1.5,42,34,21,7,7,4,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/FetchRequestTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/FetchRequestTests.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,34,430,0,1,76,86,25,3,1,169,86,56,93,86,31,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/MultiFetchRequestTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/MultiFetchRequestTests.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",2,8,7,41,323,0,2,78,86,26,3,1,171,86,57,93,86,31,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/OffsetRequestTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Request/OffsetRequestTests.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",1,8,7,33,398,0,1,73,83,24,3,1,163,83,54,90,83,30,2,1,0,1
clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Util/BitWorksTests.cs,clients/csharp/src/Kafka/Tests/Kafka.Client.Tests/Util/BitWorksTests.cs,"Fix ASF license headers for C# client; patched by Eric Hauser; #KAFKA-137

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1174493 13f79535-47bb-0310-9956-ffa450edef68",8,8,7,67,395,0,7,121,104,40,3,1,232,120,77,111,104,37,2,1,0,1
clients/python/setup.py,clients/python/setup.py,"KAFKA-93 | add license to missed files and remove LinkedIn copyright line per ASF guideline. Thanks to Joel Koshy for pointing it out

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156299 13f79535-47bb-0310-9956-ffa450edef68",0,18,0,12,44,0,0,33,18,16,2,1.0,33,18,16,0,0,0,2,1,0,1
core/src/test/scala/unit/kafka/producer/ProducerMethodsTest.scala,core/src/test/scala/unit/kafka/producer/ProducerMethodsTest.scala,"KAFKA-93 | add license to missed files and remove LinkedIn copyright line per ASF guideline. Thanks to Joel Koshy for pointing it out

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156299 13f79535-47bb-0310-9956-ffa450edef68",3,16,13,32,254,1,1,57,54,28,2,3.0,70,54,35,13,13,6,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/UndefinedPropertyException.java,contrib/hadoop-consumer/src/main/java/kafka/etl/UndefinedPropertyException.java,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,7,33,0,1,28,27,14,2,1.0,34,27,17,6,6,3,2,1,0,1
contrib/hadoop-consumer/src/main/java/kafka/etl/impl/SimpleKafkaETLJob.java,contrib/hadoop-consumer/src/main/java/kafka/etl/impl/SimpleKafkaETLJob.java,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",7,7,6,60,474,0,4,104,103,52,2,1.0,110,103,55,6,6,3,2,1,0,1
core/src/main/scala/kafka/api/MultiFetchResponse.scala,core/src/main/scala/kafka/api/MultiFetchResponse.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",3,7,6,29,194,0,2,52,51,26,2,1.0,58,51,29,6,6,3,2,1,0,1
core/src/main/scala/kafka/api/MultiProducerRequest.scala,core/src/main/scala/kafka/api/MultiProducerRequest.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",7,7,6,35,215,0,3,57,56,28,2,1.0,63,56,32,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/InvalidConfigException.scala,core/src/main/scala/kafka/common/InvalidConfigException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,4,27,0,1,25,24,12,2,1.0,31,24,16,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/OffsetOutOfRangeException.scala,core/src/main/scala/kafka/common/OffsetOutOfRangeException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,4,27,0,1,26,25,13,2,1.0,32,25,16,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/UnavailableProducerException.scala,core/src/main/scala/kafka/common/UnavailableProducerException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,4,27,0,1,24,23,12,2,1.0,30,23,15,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/UnknownCodecException.scala,core/src/main/scala/kafka/common/UnknownCodecException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,4,27,0,1,26,25,13,2,1.0,32,25,16,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/UnknownException.scala,core/src/main/scala/kafka/common/UnknownException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",0,7,6,2,8,0,0,23,22,12,2,1.0,29,22,14,6,6,3,2,1,0,1
core/src/main/scala/kafka/common/UnknownMagicByteException.scala,core/src/main/scala/kafka/common/UnknownMagicByteException.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",1,7,6,4,27,0,1,26,25,13,2,1.0,32,25,16,6,6,3,2,1,0,1
core/src/main/scala/kafka/javaapi/MultiFetchResponse.scala,core/src/main/scala/kafka/javaapi/MultiFetchResponse.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",3,7,6,22,144,0,2,45,44,22,2,1.0,51,44,26,6,6,3,2,1,0,1
core/src/main/scala/kafka/network/ConnectionConfig.scala,core/src/main/scala/kafka/network/ConnectionConfig.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",0,7,6,9,38,0,0,27,26,14,2,1.0,33,26,16,6,6,3,2,1,0,1
core/src/main/scala/kafka/producer/async/CallbackHandler.scala,core/src/main/scala/kafka/producer/async/CallbackHandler.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",2,7,6,11,148,0,2,75,74,38,2,1.0,81,74,40,6,6,3,2,1,0,1
core/src/test/scala/unit/kafka/javaapi/integration/ProducerConsumerTestHarness.scala,core/src/test/scala/unit/kafka/javaapi/integration/ProducerConsumerTestHarness.scala,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",2,7,6,31,173,0,2,53,52,26,2,1.0,59,52,30,6,6,3,2,1,0,1
perf/src/main/java/kafka/perf/KafkaSimulatorMXBean.java,perf/src/main/java/kafka/perf/KafkaSimulatorMXBean.java,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",0,7,6,10,46,0,0,28,27,14,2,1.0,34,27,17,6,6,3,2,1,0,1
perf/src/main/java/kafka/perf/consumer/SimplePerfConsumer.java,perf/src/main/java/kafka/perf/consumer/SimplePerfConsumer.java,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",9,7,6,80,486,0,5,109,108,54,2,1.0,115,108,58,6,6,3,2,1,0,1
perf/src/main/java/kafka/perf/producer/Producer.java,perf/src/main/java/kafka/perf/producer/Producer.java,"KAFKA-93 | Change and add ASF source header to follow standard ASF source header (http://www.apache.org/legal/src-headers.html).

git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1156232 13f79535-47bb-0310-9956-ffa450edef68",8,7,6,76,566,0,6,102,101,51,2,1.0,108,101,54,6,6,3,2,1,0,1
clients/php/src/lib/Kafka/BoundedByteBuffer/Receive.php,clients/php/src/lib/Kafka/BoundedByteBuffer/Receive.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",17,154,0,60,351,5,5,154,154,154,1,1,154,154,154,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/BoundedByteBuffer/Send.php,clients/php/src/lib/Kafka/BoundedByteBuffer/Send.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",10,118,0,42,254,4,4,118,118,118,1,1,118,118,118,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/FetchRequest.php,clients/php/src/lib/Kafka/FetchRequest.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",7,126,0,36,234,7,7,126,126,126,1,1,126,126,126,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/MessageSet.php,clients/php/src/lib/Kafka/MessageSet.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",9,122,0,40,236,8,8,122,122,122,1,1,122,122,122,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/Request.php,clients/php/src/lib/Kafka/Request.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",0,30,0,4,8,0,0,30,30,30,1,1,30,30,30,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/RequestKeys.php,clients/php/src/lib/Kafka/RequestKeys.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",0,30,0,8,29,0,0,30,30,30,1,1,30,30,30,0,0,0,2,1,0,1
clients/php/src/lib/Kafka/SimpleConsumer.php,clients/php/src/lib/Kafka/SimpleConsumer.php,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",11,142,0,51,326,7,8,142,142,142,1,1,142,142,142,0,0,0,2,1,0,1
perf/report-html/js/exporting.js,perf/report-html/js/exporting.js,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",54,21,0,13,2535,22,33,21,21,21,1,1,21,21,21,0,0,0,2,1,0,1
perf/report-html/js/highcharts.js,perf/report-html/js/highcharts.js,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",1313,135,0,128,31083,244,354,135,135,135,1,1,135,135,135,0,0,0,2,1,0,1
perf/report-html/js/kafka.js,perf/report-html/js/kafka.js,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",12,91,0,80,409,4,5,91,91,91,1,1,91,91,91,0,0,0,2,1,0,1
perf/src/main/java/kafka/perf/KafkaPerfSimulator.java,perf/src/main/java/kafka/perf/KafkaPerfSimulator.java,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",75,385,0,302,2481,22,22,385,385,385,1,1,385,385,385,0,0,0,2,1,0,1
perf/src/main/java/kafka/perf/PerfTimer.java,perf/src/main/java/kafka/perf/PerfTimer.java,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",19,149,0,130,892,7,7,149,149,149,1,1,149,149,149,0,0,0,2,1,0,1
perf/src/main/java/kafka/perf/jmx/BrokerJmxClient.java,perf/src/main/java/kafka/perf/jmx/BrokerJmxClient.java,"Initial checkin of Kafka to Apache SVN. This corresponds to https://github.com/kafka-dev/kafka/commit/709afe4ec75489bc00a44335de8821fa726bb97e except that git specific files have been removed and code has been put into trunk/branches/site/etc. This is just a copy of master, branches and history are not being converted since we can't find a good tool for it.



git-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/trunk@1152970 13f79535-47bb-0310-9956-ffa450edef68",4,49,0,42,267,4,4,49,49,49,1,1,49,49,49,0,0,0,2,1,0,1
